From 2fd1e9c0a5d2d778d2a9a851ab2fe4dbf27a2a6a Mon Sep 17 00:00:00 2001
From: mikec <Cheng.Mike@inventec.com>
Date: Tue, 13 Dec 2022 17:31:14 +0800
Subject: [PATCH] Kernel-sync-Aspeed-tag-00.05.03-soc-i3c-drivers

Sympton/Reason:
    Sync codebase with AspeedTech-BMC/linux with tag 00.05.03

Root Cause:
    N/A

Solution/Change:
    i3c drivers
	- i3cdev
	- master
	- i3c-ibi-mqueue
	- i3c-hub
	- i3c-slave-mqueue
	- i3c-slave-eeprom
	- i3c-mux-imx3102
	- master/ast2600-i3c-global
	- master/ast2600-i3c-master
	- master/mipi-i3c-hci
	- i3c-mctp
	- i3c-target-mctp

    soc drivers
	- speed-bmc-dev
	- aspeed-espi
	- aspeed-host-bmc-dev
	- aspeed-lpc
	- aspeed-mctp
	- aspeed-otp
	- aspeed-ssp
	- aspeed-udma
	- aspeed-usb-ahp
	- aspeed-usb-phy
	- ast_video
	- rvas

Entry Test:
    N/A
---
 drivers/i3c/Kconfig                        |   80 +
 drivers/i3c/Makefile                       |    7 +
 drivers/i3c/device.c                       |  241 ++
 drivers/i3c/i3c-ast-bridge-ic.c            |  279 ++
 drivers/i3c/i3c-hub.c                      |  699 ++++
 drivers/i3c/i3c-ibi-mqueue.c               |  246 ++
 drivers/i3c/i3c-mux-imx3102.c              |  227 ++
 drivers/i3c/i3c-slave-eeprom.c             |  175 +
 drivers/i3c/i3c-slave-mqueue.c             |  206 ++
 drivers/i3c/i3cdev.c                       |  509 +++
 drivers/i3c/internals.h                    |   15 +
 drivers/i3c/master.c                       |  881 ++++-
 drivers/i3c/master/Kconfig                 |   28 +
 drivers/i3c/master/Makefile                |    1 +
 drivers/i3c/master/ast2600-i3c-global.c    |  141 +
 drivers/i3c/master/ast2600-i3c-master.c    | 2884 +++++++++++++++++
 drivers/i3c/master/mipi-i3c-hci/core.c     |    9 +-
 drivers/i3c/master/mipi-i3c-hci/dma.c      |    2 +-
 drivers/i3c/master/mipi-i3c-hci/hci.h      |    2 +-
 drivers/i3c/mctp/Kconfig                   |   14 +
 drivers/i3c/mctp/Makefile                  |    3 +
 drivers/i3c/mctp/i3c-mctp.c                |  628 ++++
 drivers/i3c/mctp/i3c-target-mctp.c         |  389 +++
 drivers/soc/aspeed/Kconfig                 |   75 +
 drivers/soc/aspeed/Makefile                |   20 +-
 drivers/soc/aspeed/aspeed-bmc-dev.c        |  503 +++
 drivers/soc/aspeed/aspeed-espi-ctrl.c      |  237 ++
 drivers/soc/aspeed/aspeed-espi-ctrl.h      |  308 ++
 drivers/soc/aspeed/aspeed-espi-flash.c     |  355 ++
 drivers/soc/aspeed/aspeed-espi-flash.h     |   45 +
 drivers/soc/aspeed/aspeed-espi-ioc.h       |  195 ++
 drivers/soc/aspeed/aspeed-espi-mmbi.c      |  343 ++
 drivers/soc/aspeed/aspeed-espi-oob.c       |  558 ++++
 drivers/soc/aspeed/aspeed-espi-oob.h       |   70 +
 drivers/soc/aspeed/aspeed-espi-perif.c     |  520 +++
 drivers/soc/aspeed/aspeed-espi-perif.h     |   45 +
 drivers/soc/aspeed/aspeed-espi-vw.c        |  137 +
 drivers/soc/aspeed/aspeed-espi-vw.h        |   21 +
 drivers/soc/aspeed/aspeed-host-bmc-dev.c   |  519 +++
 drivers/soc/aspeed/aspeed-lpc-mbox.c       |  418 +++
 drivers/soc/aspeed/aspeed-lpc-pcc.c        |  636 ++++
 drivers/soc/aspeed/aspeed-lpc-snoop.c      |   21 +-
 drivers/soc/aspeed/aspeed-mctp.c           | 2298 +++++++++++++
 drivers/soc/aspeed/aspeed-otp.c            |  639 ++++
 drivers/soc/aspeed/aspeed-ssp.c            |  208 ++
 drivers/soc/aspeed/aspeed-udma.c           |  464 +++
 drivers/soc/aspeed/aspeed-usb-ahp.c        |   47 +
 drivers/soc/aspeed/aspeed-usb-phy.c        |   70 +
 drivers/soc/aspeed/ast_video.c             | 3411 ++++++++++++++++++++
 drivers/soc/aspeed/rvas/Kconfig            |    9 +
 drivers/soc/aspeed/rvas/Makefile           |    3 +
 drivers/soc/aspeed/rvas/hardware_engines.c | 2225 +++++++++++++
 drivers/soc/aspeed/rvas/hardware_engines.h |  500 +++
 drivers/soc/aspeed/rvas/video.h            |   43 +
 drivers/soc/aspeed/rvas/video_debug.h      |   32 +
 drivers/soc/aspeed/rvas/video_engine.c     | 1205 +++++++
 drivers/soc/aspeed/rvas/video_engine.h     |  293 ++
 drivers/soc/aspeed/rvas/video_ioctl.h      |  275 ++
 drivers/soc/aspeed/rvas/video_main.c       | 1618 ++++++++++
 fs/ext4/file.c                             |    5 +-
 fs/iomap/buffered-io.c                     |    2 +-
 fs/iomap/direct-io.c                       |   29 +-
 include/linux/aspeed-mctp.h                |  155 +
 include/linux/aspeed_pcie_io.h             |    9 +
 include/linux/bpf.h                        |  101 +-
 include/linux/bpf_verifier.h               |   18 +
 include/linux/i3c/ccc.h                    |   16 +
 include/linux/i3c/device.h                 |   79 +
 include/linux/i3c/master.h                 |   92 +-
 include/linux/i3c/mctp/i3c-mctp.h          |   50 +
 include/linux/i3c/target.h                 |   23 +
 include/linux/iomap.h                      |   11 +-
 include/linux/jtag.h                       |   49 +
 include/linux/kernel.h                     |    2 +-
 include/linux/mm.h                         |    3 +-
 include/linux/mmc/host.h                   |    2 +
 include/linux/mtd/mtd.h                    |    6 +-
 include/linux/netdev_features.h            |    4 +-
 include/linux/pagemap.h                    |   58 +-
 include/linux/pci_ids.h                    |    3 +
 include/linux/soc/aspeed/aspeed-udma.h     |   30 +
 include/linux/stmmac.h                     |    1 +
 include/linux/sunrpc/clnt.h                |    1 +
 include/linux/uio.h                        |    4 +-
 include/uapi/linux/aspeed-mctp.h           |  136 +
 include/uapi/linux/aspeed-otp.h            |   39 +
 include/uapi/linux/aspeed-video.h          |   23 +
 include/uapi/linux/i3c/i3cdev.h            |   37 +
 include/uapi/linux/if_alg.h                |    3 +
 include/uapi/linux/jtag.h                  |  370 +++
 include/uapi/linux/rfkill.h                |    2 +-
 include/uapi/linux/v4l2-controls.h         |    6 +
 include/uapi/linux/videodev2.h             |    2 +
 include/uapi/linux/virtio_ids.h            |   14 +-
 kernel/bpf/btf.c                           |   16 +-
 kernel/bpf/helpers.c                       |   12 +-
 kernel/bpf/map_iter.c                      |    4 +-
 kernel/bpf/verifier.c                      |  488 ++-
 lib/hexdump.c                              |   41 +-
 lib/iov_iter.c                             |   98 +-
 mm/filemap.c                               |    4 +-
 mm/gup.c                                   |  122 +-
 net/core/bpf_sk_storage.c                  |    2 +-
 net/core/sock_map.c                        |    2 +-
 104 files changed, 27712 insertions(+), 494 deletions(-)
 create mode 100644 drivers/i3c/i3c-ast-bridge-ic.c
 create mode 100644 drivers/i3c/i3c-hub.c
 create mode 100644 drivers/i3c/i3c-ibi-mqueue.c
 create mode 100644 drivers/i3c/i3c-mux-imx3102.c
 create mode 100644 drivers/i3c/i3c-slave-eeprom.c
 create mode 100644 drivers/i3c/i3c-slave-mqueue.c
 create mode 100644 drivers/i3c/i3cdev.c
 create mode 100644 drivers/i3c/master/ast2600-i3c-global.c
 create mode 100644 drivers/i3c/master/ast2600-i3c-master.c
 create mode 100644 drivers/i3c/mctp/Kconfig
 create mode 100644 drivers/i3c/mctp/Makefile
 create mode 100644 drivers/i3c/mctp/i3c-mctp.c
 create mode 100644 drivers/i3c/mctp/i3c-target-mctp.c
 create mode 100644 drivers/soc/aspeed/aspeed-bmc-dev.c
 create mode 100644 drivers/soc/aspeed/aspeed-espi-ctrl.c
 create mode 100644 drivers/soc/aspeed/aspeed-espi-ctrl.h
 create mode 100644 drivers/soc/aspeed/aspeed-espi-flash.c
 create mode 100644 drivers/soc/aspeed/aspeed-espi-flash.h
 create mode 100644 drivers/soc/aspeed/aspeed-espi-ioc.h
 create mode 100644 drivers/soc/aspeed/aspeed-espi-mmbi.c
 create mode 100644 drivers/soc/aspeed/aspeed-espi-oob.c
 create mode 100644 drivers/soc/aspeed/aspeed-espi-oob.h
 create mode 100644 drivers/soc/aspeed/aspeed-espi-perif.c
 create mode 100644 drivers/soc/aspeed/aspeed-espi-perif.h
 create mode 100644 drivers/soc/aspeed/aspeed-espi-vw.c
 create mode 100644 drivers/soc/aspeed/aspeed-espi-vw.h
 create mode 100644 drivers/soc/aspeed/aspeed-host-bmc-dev.c
 create mode 100644 drivers/soc/aspeed/aspeed-lpc-mbox.c
 create mode 100644 drivers/soc/aspeed/aspeed-lpc-pcc.c
 create mode 100644 drivers/soc/aspeed/aspeed-mctp.c
 create mode 100644 drivers/soc/aspeed/aspeed-otp.c
 create mode 100644 drivers/soc/aspeed/aspeed-ssp.c
 create mode 100644 drivers/soc/aspeed/aspeed-udma.c
 create mode 100644 drivers/soc/aspeed/aspeed-usb-ahp.c
 create mode 100644 drivers/soc/aspeed/aspeed-usb-phy.c
 create mode 100644 drivers/soc/aspeed/ast_video.c
 create mode 100644 drivers/soc/aspeed/rvas/Kconfig
 create mode 100644 drivers/soc/aspeed/rvas/Makefile
 create mode 100644 drivers/soc/aspeed/rvas/hardware_engines.c
 create mode 100644 drivers/soc/aspeed/rvas/hardware_engines.h
 create mode 100644 drivers/soc/aspeed/rvas/video.h
 create mode 100644 drivers/soc/aspeed/rvas/video_debug.h
 create mode 100644 drivers/soc/aspeed/rvas/video_engine.c
 create mode 100644 drivers/soc/aspeed/rvas/video_engine.h
 create mode 100644 drivers/soc/aspeed/rvas/video_ioctl.h
 create mode 100644 drivers/soc/aspeed/rvas/video_main.c
 create mode 100644 include/linux/aspeed-mctp.h
 create mode 100644 include/linux/aspeed_pcie_io.h
 create mode 100644 include/linux/i3c/mctp/i3c-mctp.h
 create mode 100644 include/linux/i3c/target.h
 create mode 100644 include/linux/jtag.h
 create mode 100644 include/linux/soc/aspeed/aspeed-udma.h
 create mode 100644 include/uapi/linux/aspeed-mctp.h
 create mode 100644 include/uapi/linux/aspeed-otp.h
 create mode 100644 include/uapi/linux/aspeed-video.h
 create mode 100644 include/uapi/linux/i3c/i3cdev.h
 create mode 100644 include/uapi/linux/jtag.h

diff --git a/drivers/i3c/Kconfig b/drivers/i3c/Kconfig
index 30a441506f61..c2b527b2430d 100644
--- a/drivers/i3c/Kconfig
+++ b/drivers/i3c/Kconfig
@@ -20,5 +20,85 @@ menuconfig I3C
 	  will be called i3c.
 
 if I3C
+
+source "drivers/i3c/mctp/Kconfig"
+
+config I3CDEV
+	tristate "I3C device interface"
+	depends on I3C
+	help
+	  Say Y here to use i3c-* device files, usually found in the /dev
+	  directory on your system.  They make it possible to have user-space
+	  programs use the I3C devices.
+
+	  This support is also available as a module.  If so, the module
+	  will be called i3cdev.
+
+	  Note that this application programming interface is EXPERIMENTAL
+	  and hence SUBJECT TO CHANGE WITHOUT NOTICE while it stabilizes.
+
+config I3C_HUB
+	tristate "I3C HUB support"
+	depends on I3C
+	select REGMAP_I3C
+	help
+	  This enables support for I3C HUB. Say Y here to use I3C HUB driver to
+	  configure I3C HUB device.
+
+	  I3C HUB drivers will be loaded automatically when I3C device with BCR
+	  equals to 0xC2 (HUB device) is detected on the bus.
+
+
+if I3CDEV
+config I3CDEV_FORCE_CREATE
+	bool "force create I3C device interface"
+	default y
+	help
+	  Say 'y' to force create I3C devices under /dev/bus/i3c/ regardless of
+	  driver binding.  This option is to help development so it shall be
+	  turned off in production.
+
+config I3CDEV_XFER_HDR_DDR
+	bool "transfer with HDR-DDR mode"
+	default y
+	help
+	  Say 'y' to use the HDR-DDR mode to transfer data if the device support it.
+	  This option is to help development so it shall be turned off in production.
+endif # I3CDEV
+
+config I3C_AST_BRIDGE_IC
+	bool "Aspeed Bridge IC driver"
+	default y
+	help
+	  Say y to enable Aspeed bridge IC device driver.
+
+config I3C_MUX_IMX3102
+	bool "IMX/IML3102 I3C multiplexer driver"
+	default y
+	select REGMAP_I3C
+	help
+	  Say y to enable Renesas IMX3102 I3C 2:1 multiplexer.
+
+choice
+	prompt "I3C secondary master / slave mode driver selection"
+	default I3C_SLAVE_MQUEUE
+
+config I3C_SLAVE_MQUEUE
+	bool "I3C mqueue (message queue) secondary master and slave driver"
+	help
+	  Some protocols over I3C are designed for bi-directional transferring
+	  messages by using I3C Master Write protocol. This driver is used to
+	  receive and queue messages from the remote I3C main master device.
+
+	  Userspace can get the messages by reading sysfs file that this driver
+	  exposes.
+
+config I3C_SLAVE_EEPROM
+	bool "I3C EEPROM secondary master and slave driver"
+	help
+	  This driver makes the slave mode I3C controller simulate the EEPROM.
+endchoice
+
 source "drivers/i3c/master/Kconfig"
+
 endif # I3C
diff --git a/drivers/i3c/Makefile b/drivers/i3c/Makefile
index 11982efbc6d9..3b6cb6063b69 100644
--- a/drivers/i3c/Makefile
+++ b/drivers/i3c/Makefile
@@ -1,4 +1,11 @@
 # SPDX-License-Identifier: GPL-2.0
 i3c-y				:= device.o master.o
 obj-$(CONFIG_I3C)		+= i3c.o
+obj-$(CONFIG_I3CDEV)		+= i3cdev.o
 obj-$(CONFIG_I3C)		+= master/
+obj-$(CONFIG_I3C_AST_BRIDGE_IC) += i3c-ast-bridge-ic.o
+obj-$(CONFIG_I3C_SLAVE_MQUEUE) 	+= i3c-slave-mqueue.o
+obj-$(CONFIG_I3C_SLAVE_EEPROM) 	+= i3c-slave-eeprom.o
+obj-$(CONFIG_I3C_MUX_IMX3102) 	+= i3c-mux-imx3102.o
+obj-$(CONFIG_I3C)		+= mctp/
+obj-$(CONFIG_I3C_HUB)		+= i3c-hub.o
diff --git a/drivers/i3c/device.c b/drivers/i3c/device.c
index e92d3e9a52bd..26f2d745fc38 100644
--- a/drivers/i3c/device.c
+++ b/drivers/i3c/device.c
@@ -50,6 +50,73 @@ int i3c_device_do_priv_xfers(struct i3c_device *dev,
 }
 EXPORT_SYMBOL_GPL(i3c_device_do_priv_xfers);
 
+/**
+ * i3c_device_send_hdr_cmds() - send HDR commands to a specific device
+ *
+ * @dev: device to which these commands should be sent
+ * @cmds: array of commands
+ * @ncmds: number of commands
+ *
+ * Send one or several HDR commands to @dev.
+ *
+ * This function can sleep and thus cannot be called in atomic context.
+ *
+ * Return: 0 in case of success, a negative error core otherwise.
+ */
+int i3c_device_send_hdr_cmds(struct i3c_device *dev, struct i3c_hdr_cmd *cmds,
+			     int ncmds)
+{
+	struct i3c_master_controller *master;
+	enum i3c_hdr_mode mode;
+	int ret, i;
+
+	if (ncmds < 1)
+		return 0;
+
+	mode = cmds[0].mode;
+	for (i = 1; i < ncmds; i++) {
+		if (mode != cmds[i].mode)
+			return -EINVAL;
+	}
+
+	master = i3c_dev_get_master(dev->desc);
+	if (!master)
+		return -EINVAL;
+
+	i3c_bus_normaluse_lock(&master->bus);
+	for (i = 0; i < ncmds; i++)
+		cmds[i].addr = dev->desc->info.dyn_addr;
+
+	ret = i3c_master_send_hdr_cmds_locked(master, cmds, ncmds);
+	i3c_bus_normaluse_unlock(&master->bus);
+
+	return ret;
+}
+EXPORT_SYMBOL_GPL(i3c_device_send_hdr_cmds);
+
+/**
+ * i3c_device_generate_ibi() - request In-Band Interrupt
+ *
+ * @dev: target device
+ * @data: IBI payload
+ * @len: payload length in bytes
+ *
+ * Request In-Band Interrupt with or without data payload.
+ *
+ * Return: 0 in case of success, a negative error code otherwise.
+ */
+int i3c_device_generate_ibi(struct i3c_device *dev, const u8 *data, int len)
+{
+	int ret;
+
+	i3c_bus_normaluse_lock(dev->bus);
+	ret = i3c_dev_generate_ibi_locked(dev->desc, data, len);
+	i3c_bus_normaluse_unlock(dev->bus);
+
+	return ret;
+}
+EXPORT_SYMBOL_GPL(i3c_device_generate_ibi);
+
 /**
  * i3c_device_get_info() - get I3C device information
  *
@@ -176,6 +243,33 @@ void i3c_device_free_ibi(struct i3c_device *dev)
 }
 EXPORT_SYMBOL_GPL(i3c_device_free_ibi);
 
+/**
+ * i3c_device_send_ccc_cmd() - send ccc to the target device
+ * @dev: device on which you want to release IBI resources
+ * @ccc_id: CCC ID you want to send.  Only support SETAASA, RSTDAA for now.
+ *
+ * This function provides a interface to send CCC from high layer driver.
+ * This is needed for the bus topologic with I3C MUX or switch devices.
+ * The I3C MUX may not enable the local/slave port by default.  The master
+ * controller needs to attach the I3C MUX device, and program the mode
+ * registers to enable the local/slave port.  Then the devices hehind
+ * the MUX may need for CCC for initialization (e.g. SETAASA to bring them
+ * from I2C mode to I3C mode)
+ */
+int i3c_device_send_ccc_cmd(struct i3c_device *dev, u8 ccc_id)
+{
+	int ret;
+
+	if (dev->desc) {
+		i3c_bus_normaluse_lock(dev->bus);
+		ret = i3c_dev_send_ccc_cmd_locked(dev->desc, ccc_id);
+		i3c_bus_normaluse_unlock(dev->bus);
+	}
+
+	return ret;
+}
+EXPORT_SYMBOL_GPL(i3c_device_send_ccc_cmd);
+
 /**
  * i3cdev_to_dev() - Returns the device embedded in @i3cdev
  * @i3cdev: I3C device
@@ -283,3 +377,150 @@ void i3c_driver_unregister(struct i3c_driver *drv)
 	driver_unregister(&drv->driver);
 }
 EXPORT_SYMBOL_GPL(i3c_driver_unregister);
+
+/**
+ * i3c_device_getstatus_ccc() - receive device status
+ *
+ * @dev: I3C device to get the status for
+ * @info: I3C device info to fill the status in
+ *
+ * Receive I3C device status from I3C master device via corresponding CCC
+ * command
+ *
+ * Return: 0 in case of success, a negative error code otherwise.
+ */
+int i3c_device_getstatus_ccc(struct i3c_device *dev, struct i3c_device_info *info)
+{
+	int ret = -EINVAL;
+
+	i3c_bus_normaluse_lock(dev->bus);
+	if (dev->desc)
+		ret = i3c_dev_getstatus_locked(dev->desc, &dev->desc->info);
+	i3c_bus_normaluse_unlock(dev->bus);
+	i3c_device_get_info(dev, info);
+
+	return ret;
+}
+EXPORT_SYMBOL_GPL(i3c_device_getstatus_ccc);
+
+/**
+ * i3c_device_control_pec() - enable or disable PEC support in HW
+ *
+ * @dev: I3C device to get the status for
+ * @pec: flag telling whether PEC support shall be enabled or disabled
+ *
+ * Try to enable or disable HW support for PEC (Packet Error Check).
+ * In case no HW support for PEC, software implementation could be used.
+ *
+ * Return: 0 in case of success, -EOPNOTSUPP in case PEC is not supported by HW,
+ *         other negative error codes when PEC enabling failed.
+ */
+int i3c_device_control_pec(struct i3c_device *dev, bool pec)
+{
+	return i3c_dev_control_pec(dev->desc, pec);
+}
+EXPORT_SYMBOL_GPL(i3c_device_control_pec);
+
+/**
+ * i3c_device_setmrl_ccc() - set maximum read length
+ *
+ * @dev: I3C device to set the length for
+ * @info: I3C device info to fill the length in
+ * @read_len: maximum read length value to be set
+ * @ibi_len: maximum ibi payload length to be set
+ *
+ * Set I3C device maximum read length from I3C master device via corresponding CCC command
+ *
+ * Return: 0 in case of success, a negative error code otherwise.
+ */
+int i3c_device_setmrl_ccc(struct i3c_device *dev, struct i3c_device_info *info, u16 read_len,
+			  u8 ibi_len)
+{
+	struct i3c_master_controller *master = i3c_dev_get_master(dev->desc);
+	int ret = -EINVAL;
+
+	i3c_bus_normaluse_lock(dev->bus);
+	if (master)
+		ret = i3c_master_setmrl_locked(master, &dev->desc->info, read_len, ibi_len);
+	i3c_bus_normaluse_unlock(dev->bus);
+	i3c_device_get_info(dev, info);
+
+	return ret;
+}
+EXPORT_SYMBOL_GPL(i3c_device_setmrl_ccc);
+
+/**
+ * i3c_device_setmwl_ccc() - set maximum write length
+ *
+ * @dev: I3C device to set the length for
+ * @info: I3C device info to fill the length in
+ * @write_len: maximum write length value to be set
+ *
+ * Set I3C device maximum write length from I3C master device via corresponding CCC command
+ *
+ * Return: 0 in case of success, a negative error code otherwise.
+ */
+int i3c_device_setmwl_ccc(struct i3c_device *dev, struct i3c_device_info *info, u16 write_len)
+{
+	struct i3c_master_controller *master = i3c_dev_get_master(dev->desc);
+	int ret = -EINVAL;
+
+	i3c_bus_normaluse_lock(dev->bus);
+	if (master)
+		ret = i3c_master_setmwl_locked(master, &dev->desc->info, write_len);
+	i3c_bus_normaluse_unlock(dev->bus);
+	i3c_device_get_info(dev, info);
+
+	return ret;
+}
+EXPORT_SYMBOL_GPL(i3c_device_setmwl_ccc);
+
+/**
+ * i3c_device_getmrl_ccc() - get maximum read length
+ *
+ * @dev: I3C device to get the length for
+ * @info: I3C device info to fill the length in
+ *
+ * Receive I3C device maximum read length from I3C master device via corresponding CCC command
+ *
+ * Return: 0 in case of success, a negative error code otherwise.
+ */
+int i3c_device_getmrl_ccc(struct i3c_device *dev, struct i3c_device_info *info)
+{
+	struct i3c_master_controller *master = i3c_dev_get_master(dev->desc);
+	int ret = -EINVAL;
+
+	i3c_bus_normaluse_lock(dev->bus);
+	if (master)
+		ret = i3c_master_getmrl_locked(master, &dev->desc->info);
+	i3c_bus_normaluse_unlock(dev->bus);
+	i3c_device_get_info(dev, info);
+
+	return ret;
+}
+EXPORT_SYMBOL_GPL(i3c_device_getmrl_ccc);
+
+/**
+ * i3c_device_getmwl_ccc() - get maximum write length
+ *
+ * @dev: I3C device to get the length for
+ * @info: I3C device info to fill the length in
+ *
+ * Receive I3C device maximum write length from I3C master device via corresponding CCC command
+ *
+ * Return: 0 in case of success, a negative error code otherwise.
+ */
+int i3c_device_getmwl_ccc(struct i3c_device *dev, struct i3c_device_info *info)
+{
+	struct i3c_master_controller *master = i3c_dev_get_master(dev->desc);
+	int ret = -EINVAL;
+
+	i3c_bus_normaluse_lock(dev->bus);
+	if (master)
+		ret = i3c_master_getmwl_locked(master, &dev->desc->info);
+	i3c_bus_normaluse_unlock(dev->bus);
+	i3c_device_get_info(dev, info);
+
+	return ret;
+}
+EXPORT_SYMBOL_GPL(i3c_device_getmwl_ccc);
diff --git a/drivers/i3c/i3c-ast-bridge-ic.c b/drivers/i3c/i3c-ast-bridge-ic.c
new file mode 100644
index 000000000000..cd2a8225ef3f
--- /dev/null
+++ b/drivers/i3c/i3c-ast-bridge-ic.c
@@ -0,0 +1,279 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (c) 2021 Aspeed Technology Inc.
+ *
+ * Aspeed Bridge IC driver
+ *
+ */
+
+#include <linux/i3c/device.h>
+#include <linux/i3c/master.h>
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/of.h>
+#include <linux/sysfs.h>
+#include "internals.h"
+#include <linux/delay.h>
+#define MQ_MSGBUF_SIZE		256
+#define MQ_QUEUE_SIZE		4
+#define MQ_QUEUE_NEXT(x)	(((x) + 1) & (MQ_QUEUE_SIZE - 1))
+
+#define IBI_QUEUE_STATUS_PEC_ERR	BIT(30)
+#define IBI_STATUS_LAST_FRAG	BIT(24)
+#define PID_MANUF_ID_ASPEED	0x03f6
+#define POLLIING_INTERVAL_MS	2000
+
+struct mq_msg {
+	int len;
+	u8 *buf;
+};
+
+struct astbic {
+	struct bin_attribute bin;
+	struct kernfs_node *kn;
+
+	struct i3c_device *i3cdev;
+
+	spinlock_t lock;
+	int in;
+	int out;
+	struct mutex mq_lock;
+	struct mq_msg *curr;
+	int truncated;
+	struct mq_msg queue[MQ_QUEUE_SIZE];
+};
+
+static u8 mdb_table[] = {
+	0xbf, /* Aspeed BIC */
+	0,
+};
+
+static void i3c_ibi_mqueue_callback(struct i3c_device *dev,
+				    const struct i3c_ibi_payload *payload)
+{
+	struct astbic *mq = dev_get_drvdata(&dev->dev);
+	struct mq_msg *msg;
+	u8 *buf = (u8 *)payload->data;
+	struct i3c_device_info info;
+	u32 status;
+	const u8 *mdb;
+
+	mutex_lock(&mq->mq_lock);
+	i3c_device_get_info(dev, &info);
+	msg = mq->curr;
+
+	/* first DW is IBI status */
+	status = *(u32 *)buf;
+
+	/* then the raw data */
+	buf += sizeof(status);
+	memcpy(&msg->buf[msg->len], buf, payload->len - sizeof(status));
+	msg->len += payload->len - sizeof(status);
+	if (status & IBI_QUEUE_STATUS_PEC_ERR) {
+		for (mdb = mdb_table; *mdb != 0; mdb++)
+			if (buf[0] == *mdb)
+				break;
+		if (!(*mdb)) {
+			dev_err(&dev->dev, "ibi crc/pec error: mdb = %x", buf[0]);
+			mutex_unlock(&mq->mq_lock);
+			return;
+		}
+	}
+	/* if last fragment, notify and update pointers */
+	if (status & IBI_STATUS_LAST_FRAG) {
+		/* check pending-read-notification */
+		if (IS_MDB_PENDING_READ_NOTIFY(msg->buf[0])) {
+			struct i3c_priv_xfer xfers[1] = {
+				{
+					.rnw = true,
+					.len = info.max_read_len,
+					.data.in = msg->buf,
+				},
+			};
+
+			i3c_device_do_priv_xfers(dev, xfers, 1);
+
+			msg->len = xfers[0].len;
+		}
+
+		mq->in = MQ_QUEUE_NEXT(mq->in);
+		mq->curr = &mq->queue[mq->in];
+		mq->curr->len = 0;
+
+		if (mq->out == mq->in)
+			mq->out = MQ_QUEUE_NEXT(mq->out);
+		kernfs_notify(mq->kn);
+	}
+	mutex_unlock(&mq->mq_lock);
+}
+
+static ssize_t i3c_astbic_bin_read(struct file *filp, struct kobject *kobj,
+				   struct bin_attribute *attr, char *buf,
+				   loff_t pos, size_t count)
+{
+	struct astbic *mq;
+	struct mq_msg *msg;
+	unsigned long flags;
+	bool more = false;
+	ssize_t ret = 0;
+
+	mq = dev_get_drvdata(container_of(kobj, struct device, kobj));
+
+	spin_lock_irqsave(&mq->lock, flags);
+	if (mq->out != mq->in) {
+		msg = &mq->queue[mq->out];
+
+		if (msg->len <= count) {
+			ret = msg->len;
+			memcpy(buf, msg->buf, ret);
+		} else {
+			ret = -EOVERFLOW; /* Drop this HUGE one. */
+		}
+
+		mq->out = MQ_QUEUE_NEXT(mq->out);
+		if (mq->out != mq->in)
+			more = true;
+	}
+	spin_unlock_irqrestore(&mq->lock, flags);
+
+	if (more)
+		kernfs_notify(mq->kn);
+
+	return ret;
+}
+
+static ssize_t i3c_astbic_bin_write(struct file *filp, struct kobject *kobj,
+				    struct bin_attribute *attr, char *buf,
+				    loff_t pos, size_t count)
+{
+	struct astbic *astbic;
+	// struct i3c_device *i3c;
+	struct i3c_priv_xfer xfers = {
+		.rnw = false,
+		.len = count,
+	};
+	int ret = -EACCES;
+
+	astbic = dev_get_drvdata(container_of(kobj, struct device, kobj));
+	if (!astbic) {
+		count = -1;
+		goto out;
+	}
+
+	xfers.data.out = buf;
+	ret = i3c_device_do_priv_xfers(astbic->i3cdev, &xfers, 1);
+out:
+	return (!ret) ? count : ret;
+}
+
+static void i3c_ast_bridgeic_remove(struct i3c_device *i3cdev)
+{
+	struct device *dev = &i3cdev->dev;
+	struct astbic *astbic;
+
+	astbic = dev_get_drvdata(dev);
+
+	i3c_device_disable_ibi(i3cdev);
+	i3c_device_free_ibi(i3cdev);
+
+	kernfs_put(astbic->kn);
+	sysfs_remove_bin_file(&dev->kobj, &astbic->bin);
+	devm_kfree(dev, astbic);
+}
+
+static int i3c_ast_bridgeic_probe(struct i3c_device *i3cdev)
+{
+	struct device *dev = &i3cdev->dev;
+	struct astbic *astbic;
+	struct i3c_ibi_setup ibireq = {};
+	int ret, i;
+	struct i3c_device_info info;
+	void *buf;
+
+	if (dev->type == &i3c_masterdev_type)
+		return -ENOTSUPP;
+
+	astbic = devm_kzalloc(dev, sizeof(*astbic), GFP_KERNEL);
+	if (!astbic)
+		return -ENOMEM;
+
+	BUILD_BUG_ON(!is_power_of_2(MQ_QUEUE_SIZE));
+
+	buf = devm_kmalloc_array(dev, MQ_QUEUE_SIZE, MQ_MSGBUF_SIZE,
+				 GFP_KERNEL);
+	if (!buf)
+		return -ENOMEM;
+
+	for (i = 0; i < MQ_QUEUE_SIZE; i++) {
+		astbic->queue[i].buf = (u8 *)buf + i * MQ_MSGBUF_SIZE;
+		astbic->queue[i].len = 0;
+	}
+	spin_lock_init(&astbic->lock);
+	mutex_init(&astbic->mq_lock);
+	astbic->curr = &astbic->queue[0];
+
+	astbic->i3cdev = i3cdev;
+
+	sysfs_bin_attr_init(&astbic->bin);
+	astbic->bin.attr.name = "mqueue";
+	astbic->bin.attr.mode = 0600;
+	astbic->bin.read = i3c_astbic_bin_read;
+	astbic->bin.write = i3c_astbic_bin_write;
+	astbic->bin.size = MQ_MSGBUF_SIZE * MQ_QUEUE_SIZE;
+	ret = sysfs_create_bin_file(&dev->kobj, &astbic->bin);
+
+	astbic->kn = kernfs_find_and_get(dev->kobj.sd, astbic->bin.attr.name);
+	if (!astbic->kn) {
+		sysfs_remove_bin_file(&dev->kobj, &astbic->bin);
+		return -EFAULT;
+	}
+
+	i3c_device_get_info(i3cdev, &info);
+
+	ret = i3c_device_setmrl_ccc(i3cdev, &info, MQ_MSGBUF_SIZE,
+					    min(MQ_MSGBUF_SIZE, __UINT8_MAX__));
+	if (ret) {
+		ret = i3c_device_getmrl_ccc(i3cdev, &info);
+		if (ret)
+			return ret;
+	}
+
+	dev_set_drvdata(dev, astbic);
+
+	ibireq.handler = i3c_ibi_mqueue_callback;
+	ibireq.max_payload_len = MQ_MSGBUF_SIZE;
+	ibireq.num_slots = MQ_QUEUE_SIZE;
+
+	ret = i3c_device_request_ibi(astbic->i3cdev, &ibireq);
+	ret |= i3c_device_enable_ibi(astbic->i3cdev);
+	if (ret) {
+		kernfs_put(astbic->kn);
+		sysfs_remove_bin_file(&dev->kobj, &astbic->bin);
+		return ret;
+	}
+	return 0;
+}
+
+static const struct i3c_device_id i3c_ast_bridgeic_ids[] = {
+	I3C_DEVICE(0x3f6, 0x7341, (void *)0),
+	I3C_DEVICE(0x3f6, 0x8000, (void *)0),
+	I3C_DEVICE(0x3f6, 0x8001, (void *)0),
+	I3C_DEVICE(0x3f6, 0x0503, (void *)0),
+	I3C_DEVICE(0x3f6, 0xA001, (void *)0),
+	{ /* sentinel */ },
+};
+MODULE_DEVICE_TABLE(i3c, i3c_ast_bridgeic_ids);
+
+static struct i3c_driver astbic_driver = {
+	.driver = {
+		.name = "i3c-ast-bridgeic",
+	},
+	.probe = i3c_ast_bridgeic_probe,
+	.remove = i3c_ast_bridgeic_remove,
+	.id_table = i3c_ast_bridgeic_ids,
+};
+module_i3c_driver(astbic_driver);
+
+MODULE_AUTHOR("Andy Chung <Andy_Chung@wiwynn.com>");
+MODULE_DESCRIPTION("I3C Aspeed bridge IC driver");
+MODULE_LICENSE("GPL v2");
diff --git a/drivers/i3c/i3c-hub.c b/drivers/i3c/i3c-hub.c
new file mode 100644
index 000000000000..59576b244531
--- /dev/null
+++ b/drivers/i3c/i3c-hub.c
@@ -0,0 +1,699 @@
+// SPDX-License-Identifier: GPL-2.0
+/* Copyright (C) 2021 Intel Corporation.*/
+
+#include <linux/bitfield.h>
+#include <linux/debugfs.h>
+#include <linux/module.h>
+#include <linux/property.h>
+#include <linux/regmap.h>
+
+#include <linux/i3c/device.h>
+#include <linux/i3c/master.h>
+
+#define I3C_HUB_TP_MAX_COUNT				0x08
+
+/* I3C HUB REGISTERS */
+
+/*
+ * In this driver Controller - Target convention is used. All the abbreviations are
+ * based on this convention. For instance: CP - Controller Port, TP - Target Port.
+ */
+
+/* Device Information Registers */
+#define I3C_HUB_DEV_INFO_0				0x00
+#define I3C_HUB_DEV_INFO_1				0x01
+#define I3C_HUB_PID_5					0x02
+#define I3C_HUB_PID_4					0x03
+#define I3C_HUB_PID_3					0x04
+#define I3C_HUB_PID_2					0x05
+#define I3C_HUB_PID_1					0x06
+#define I3C_HUB_PID_0					0x07
+#define I3C_HUB_BCR					0x08
+#define I3C_HUB_DCR					0x09
+#define I3C_HUB_DEV_CAPAB				0x0A
+#define I3C_HUB_DEV_REV					0x0B
+
+/* Device Configuration Registers */
+#define I3C_HUB_PROTECTION_CODE				0x10
+#define  REGISTERS_LOCK_CODE				0x00
+#define  REGISTERS_UNLOCK_CODE				0x69
+#define  CP1_REGISTERS_UNLOCK_CODE			0x6A
+
+#define I3C_HUB_CP_CONF					0x11
+#define I3C_HUB_TP_ENABLE				0x12
+#define  TPn_ENABLE(n)					BIT(n)
+
+#define I3C_HUB_DEV_CONF				0x13
+#define I3C_HUB_IO_STRENGTH				0x14
+#define I3C_HUB_NET_OPER_MODE_CONF			0x15
+#define I3C_HUB_LDO_CONF				0x16
+#define  CP0_LDO_VOLTAGE_MASK				GENMASK(1, 0)
+#define  CP0_LDO_VOLTAGE(x)				(((x) << 0) & CP0_LDO_VOLTAGE_MASK)
+#define  CP1_LDO_VOLTAGE_MASK				GENMASK(3, 2)
+#define  CP1_LDO_VOLTAGE(x)				(((x) << 2) & CP1_LDO_VOLTAGE_MASK)
+#define  TP0145_LDO_VOLTAGE_MASK			GENMASK(5, 4)
+#define  TP0145_LDO_VOLTAGE(x)				(((x) << 4) & TP0145_LDO_VOLTAGE_MASK)
+#define  TP2367_LDO_VOLTAGE_MASK			GENMASK(7, 6)
+#define  TP2367_LDO_VOLTAGE(x)				(((x) << 6) & TP2367_LDO_VOLTAGE_MASK)
+#define  LDO_VOLTAGE_1_0V				0x00
+#define  LDO_VOLTAGE_1_1V				0x01
+#define  LDO_VOLTAGE_1_2V				0x02
+#define  LDO_VOLTAGE_1_8V				0x03
+
+#define I3C_HUB_TP_IO_MODE_CONF				0x17
+#define I3C_HUB_TP_SMBUS_AGNT_EN			0x18
+#define  TPn_SMBUS_MODE_EN(n)				BIT(n)
+
+#define I3C_HUB_LDO_AND_PULLUP_CONF			0x19
+#define  CP0_LDO_EN					BIT(0)
+#define  CP1_LDO_EN					BIT(1)
+/*
+ * I3C HUB does not provide a way to control LDO or pull-up for individual ports. It is possible
+ * for group of ports TP0/TP1/TP4/TP5 and TP2/TP3/TP6/TP7.
+ */
+#define  TP0145_LDO_EN					BIT(2)
+#define  TP2367_LDO_EN					BIT(3)
+#define  TP0145_PULLUP_CONF_MASK			GENMASK(7, 6)
+#define  TP0145_PULLUP_CONF(x)				(((x) << 6) & TP0145_PULLUP_CONF_MASK)
+#define  TP2367_PULLUP_CONF_MASK			GENMASK(5, 4)
+#define  TP2367_PULLUP_CONF(x)				(((x) << 4) & TP2367_PULLUP_CONF_MASK)
+#define  PULLUP_250R					0x00
+#define  PULLUP_500R					0x01
+#define  PULLUP_1K					0x02
+#define  PULLUP_2K					0x03
+
+#define I3C_HUB_CP_IBI_CONF				0x1A
+#define I3C_HUB_TP_IBI_CONF				0x1B
+#define I3C_HUB_IBI_MDB_CUSTOM				0x1C
+#define I3C_HUB_JEDEC_CONTEXT_ID			0x1D
+#define I3C_HUB_TP_GPIO_MODE_EN				0x1E
+#define  TPn_GPIO_MODE_EN(n)				BIT(n)
+
+/* Device Status and IBI Registers */
+#define I3C_HUB_DEV_AND_IBI_STS				0x20
+#define I3C_HUB_TP_SMBUS_AGNT_IBI_STS			0x21
+
+/* Controller Port Control/Status Registers */
+#define I3C_HUB_CP_MUX_SET				0x38
+#define I3C_HUB_CP_MUX_STS				0x39
+
+/* Target Ports Control Registers */
+#define I3C_HUB_TP_SMBUS_AGNT_TRANS_START		0x50
+#define I3C_HUB_TP_NET_CON_CONF				0x51
+#define  TPn_NET_CON(n)					BIT(n)
+
+#define I3C_HUB_TP_PULLUP_EN				0x53
+#define  TPn_PULLUP_EN(n)				BIT(n)
+
+#define I3C_HUB_TP_SCL_OUT_EN				0x54
+#define I3C_HUB_TP_SDA_OUT_EN				0x55
+#define I3C_HUB_TP_SCL_OUT_LEVEL			0x56
+#define I3C_HUB_TP_SDA_OUT_LEVEL			0x57
+#define I3C_HUB_TP_IN_DETECT_MODE_CONF			0x58
+#define I3C_HUB_TP_SCL_IN_DETECT_IBI_EN			0x59
+#define I3C_HUB_TP_SDA_IN_DETECT_IBI_EN			0x5A
+
+/* Target Ports Status Registers */
+#define I3C_HUB_TP_SCL_IN_LEVEL_STS			0x60
+#define I3C_HUB_TP_SDA_IN_LEVEL_STS			0x61
+#define I3C_HUB_TP_SCL_IN_DETECT_FLG			0x62
+#define I3C_HUB_TP_SDA_IN_DETECT_FLG			0x63
+
+/* SMBus Agent Configuration and Status Registers */
+#define I3C_HUB_TP0_SMBUS_AGNT_STS			0x64
+#define I3C_HUB_TP1_SMBUS_AGNT_STS			0x65
+#define I3C_HUB_TP2_SMBUS_AGNT_STS			0x66
+#define I3C_HUB_TP3_SMBUS_AGNT_STS			0x67
+#define I3C_HUB_TP4_SMBUS_AGNT_STS			0x68
+#define I3C_HUB_TP5_SMBUS_AGNT_STS			0x69
+#define I3C_HUB_TP6_SMBUS_AGNT_STS			0x6A
+#define I3C_HUB_TP7_SMBUS_AGNT_STS			0x6B
+#define I3C_HUB_ONCHIP_TD_AND_SMBUS_AGNT_CONF		0x6C
+
+/* Special Function Registers */
+#define I3C_HUB_LDO_AND_CPSEL_STS			0x79
+#define I3C_HUB_BUS_RESET_SCL_TIMEOUT			0x7A
+#define I3C_HUB_ONCHIP_TD_PROTO_ERR_FLG			0x7B
+#define I3C_HUB_DEV_CMD					0x7C
+#define I3C_HUB_ONCHIP_TD_STS				0x7D
+#define I3C_HUB_ONCHIP_TD_ADDR_CONF			0x7E
+#define I3C_HUB_PAGE_PTR				0x7F
+
+/* LDO DT settings */
+#define I3C_HUB_DT_LDO_DISABLED				0x00
+#define I3C_HUB_DT_LDO_1_0V				0x01
+#define I3C_HUB_DT_LDO_1_1V				0x02
+#define I3C_HUB_DT_LDO_1_2V				0x03
+#define I3C_HUB_DT_LDO_1_8V				0x04
+#define I3C_HUB_DT_LDO_NOT_DEFINED			0xFF
+
+/* Pull-up DT settings */
+#define I3C_HUB_DT_PULLUP_DISABLED			0x00
+#define I3C_HUB_DT_PULLUP_250R				0x01
+#define I3C_HUB_DT_PULLUP_500R				0x02
+#define I3C_HUB_DT_PULLUP_1K				0x03
+#define I3C_HUB_DT_PULLUP_2K				0x04
+#define I3C_HUB_DT_PULLUP_NOT_DEFINED			0xFF
+
+/* TP DT setting */
+#define I3C_HUB_DT_TP_MODE_DISABLED			0x00
+#define I3C_HUB_DT_TP_MODE_I3C				0x01
+#define I3C_HUB_DT_TP_MODE_I3C_PERF			0x02
+#define I3C_HUB_DT_TP_MODE_SMBUS			0x03
+#define I3C_HUB_DT_TP_MODE_GPIO				0x04
+#define I3C_HUB_DT_TP_MODE_NOT_DEFINED			0xFF
+
+/* TP pull-up status */
+#define I3C_HUB_DT_TP_PULLUP_DISABLED			0x00
+#define I3C_HUB_DT_TP_PULLUP_ENABLED			0x01
+#define I3C_HUB_DT_TP_PULLUP_NOT_DEFINED		0xFF
+
+struct tp_setting {
+	u8 mode;
+	u8 pullup_en;
+};
+
+struct dt_settings {
+	u8 cp0_ldo;
+	u8 cp1_ldo;
+	u8 tp0145_ldo;
+	u8 tp2367_ldo;
+	u8 tp0145_pullup;
+	u8 tp2367_pullup;
+	struct tp_setting tp[I3C_HUB_TP_MAX_COUNT];
+};
+
+struct i3c_hub {
+	struct i3c_device *i3cdev;
+	struct regmap *regmap;
+	struct dt_settings settings;
+
+	/* Offset for reading HUB's register. */
+	u8 reg_addr;
+	struct dentry *debug_dir;
+};
+
+struct hub_setting {
+	const char * const name;
+	const u8 value;
+};
+
+static const struct hub_setting ldo_settings[] = {
+	{"disabled",	I3C_HUB_DT_LDO_DISABLED},
+	{"1.0V",	I3C_HUB_DT_LDO_1_0V},
+	{"1.1V",	I3C_HUB_DT_LDO_1_1V},
+	{"1.2V",	I3C_HUB_DT_LDO_1_2V},
+	{"1.8V",	I3C_HUB_DT_LDO_1_8V},
+};
+
+static const struct hub_setting pullup_settings[] = {
+	{"disabled",	I3C_HUB_DT_PULLUP_DISABLED},
+	{"250R",	I3C_HUB_DT_PULLUP_250R},
+	{"500R",	I3C_HUB_DT_PULLUP_500R},
+	{"1k",		I3C_HUB_DT_PULLUP_1K},
+	{"2k",		I3C_HUB_DT_PULLUP_2K},
+};
+
+static const struct hub_setting tp_mode_settings[] = {
+	{"disabled",	I3C_HUB_DT_TP_MODE_DISABLED},
+	{"i3c",		I3C_HUB_DT_TP_MODE_I3C},
+	{"i3c-perf",	I3C_HUB_DT_TP_MODE_I3C_PERF},
+	{"smbus",	I3C_HUB_DT_TP_MODE_SMBUS},
+	{"gpio",	I3C_HUB_DT_TP_MODE_GPIO},
+};
+
+static const struct hub_setting tp_pullup_settings[] = {
+	{"disabled",	I3C_HUB_DT_TP_PULLUP_DISABLED},
+	{"enabled",	I3C_HUB_DT_TP_PULLUP_ENABLED},
+};
+
+static u8 i3c_hub_ldo_dt_to_reg(u8 dt_value)
+{
+	switch (dt_value) {
+	case I3C_HUB_DT_LDO_1_1V:
+		return LDO_VOLTAGE_1_1V;
+	case I3C_HUB_DT_LDO_1_2V:
+		return LDO_VOLTAGE_1_2V;
+	case I3C_HUB_DT_LDO_1_8V:
+		return LDO_VOLTAGE_1_8V;
+	default:
+		return LDO_VOLTAGE_1_0V;
+	}
+}
+
+static u8 i3c_hub_pullup_dt_to_reg(u8 dt_value)
+{
+	switch (dt_value) {
+	case I3C_HUB_DT_PULLUP_250R:
+		return PULLUP_250R;
+	case I3C_HUB_DT_PULLUP_500R:
+		return PULLUP_500R;
+	case I3C_HUB_DT_PULLUP_1K:
+		return PULLUP_1K;
+	default:
+		return PULLUP_2K;
+	}
+}
+
+static int i3c_hub_of_get_setting(const struct device_node *node, const char *setting_name,
+				  const struct hub_setting settings[], const u8 settings_count,
+				  u8 *setting_value)
+{
+	const char *sval;
+	int ret;
+	int i;
+
+	ret = of_property_read_string(node, setting_name, &sval);
+	if (ret)
+		return ret;
+
+	for (i = 0; i < settings_count; ++i) {
+		const struct hub_setting * const setting = &settings[i];
+
+		if (!strcmp(setting->name, sval)) {
+			*setting_value = setting->value;
+			return 0;
+		}
+	}
+
+	return -EINVAL;
+}
+
+static void i3c_hub_tp_of_get_setting(struct device *dev, const struct device_node *node,
+				      struct tp_setting tp_setting[])
+{
+	struct device_node *tp_node;
+	int id;
+
+	for_each_available_child_of_node(node, tp_node) {
+		int ret;
+
+		if (!tp_node->name || of_node_cmp(tp_node->name, "target-port"))
+			continue;
+
+		if (!tp_node->full_name ||
+		    (sscanf(tp_node->full_name, "target-port@%i", &id) != 1)) {
+			dev_warn(dev, "Invalid target port node found in DT - %s\n",
+				 tp_node->full_name);
+			continue;
+		}
+
+		if (id >= I3C_HUB_TP_MAX_COUNT) {
+			dev_warn(dev, "Invalid target port index found in DT - %i\n", id);
+			continue;
+		}
+		ret = i3c_hub_of_get_setting(tp_node, "mode", tp_mode_settings,
+					     ARRAY_SIZE(tp_mode_settings), &tp_setting[id].mode);
+		if (ret)
+			dev_warn(dev, "Invalid or not specified setting for target port[%i].mode\n",
+				 id);
+
+		ret = i3c_hub_of_get_setting(tp_node, "pullup", tp_pullup_settings,
+					     ARRAY_SIZE(tp_pullup_settings),
+					     &tp_setting[id].pullup_en);
+		if (ret)
+			dev_warn(dev,
+				 "Invalid or not specified setting for target port[%i].pullup\n",
+				 id);
+	}
+}
+
+static void i3c_hub_of_get_configuration(struct device *dev, const struct device_node *node)
+{
+	struct i3c_hub *priv = dev_get_drvdata(dev);
+	int ret;
+
+	ret = i3c_hub_of_get_setting(node, "cp0-ldo", ldo_settings, ARRAY_SIZE(ldo_settings),
+				     &priv->settings.cp0_ldo);
+	if (ret)
+		dev_warn(dev, "Invalid or not specified setting for cp0-ldo\n");
+
+	ret = i3c_hub_of_get_setting(node, "cp1-ldo", ldo_settings, ARRAY_SIZE(ldo_settings),
+				     &priv->settings.cp1_ldo);
+	if (ret)
+		dev_warn(dev, "Invalid or not specified setting for cp1-ldo\n");
+
+	ret = i3c_hub_of_get_setting(node, "tp0145-ldo", ldo_settings, ARRAY_SIZE(ldo_settings),
+				     &priv->settings.tp0145_ldo);
+	if (ret)
+		dev_warn(dev, "Invalid or not specified setting for tp0145-ldo\n");
+
+	ret = i3c_hub_of_get_setting(node, "tp2367-ldo", ldo_settings, ARRAY_SIZE(ldo_settings),
+				     &priv->settings.tp2367_ldo);
+	if (ret)
+		dev_warn(dev, "Invalid or not specified setting for tp2367-ldo\n");
+
+	ret = i3c_hub_of_get_setting(node, "tp0145-pullup", pullup_settings,
+				     ARRAY_SIZE(pullup_settings), &priv->settings.tp0145_pullup);
+	if (ret)
+		dev_warn(dev, "Invalid or not specified setting for tp0145-pullup\n");
+
+	ret = i3c_hub_of_get_setting(node, "tp2367-pullup", pullup_settings,
+				     ARRAY_SIZE(pullup_settings), &priv->settings.tp2367_pullup);
+	if (ret)
+		dev_warn(dev, "Invalid or not specified setting for tp2367-pullup\n");
+
+	i3c_hub_tp_of_get_setting(dev, node, priv->settings.tp);
+}
+
+static void i3c_hub_of_default_configuration(struct device *dev)
+{
+	struct i3c_hub *priv = dev_get_drvdata(dev);
+	int id;
+
+	priv->settings.cp0_ldo = I3C_HUB_DT_LDO_NOT_DEFINED;
+	priv->settings.cp1_ldo = I3C_HUB_DT_LDO_NOT_DEFINED;
+	priv->settings.tp0145_ldo = I3C_HUB_DT_LDO_NOT_DEFINED;
+	priv->settings.tp2367_ldo = I3C_HUB_DT_LDO_NOT_DEFINED;
+	priv->settings.tp0145_pullup = I3C_HUB_DT_PULLUP_NOT_DEFINED;
+	priv->settings.tp2367_pullup = I3C_HUB_DT_PULLUP_NOT_DEFINED;
+
+	for (id = 0; id < I3C_HUB_TP_MAX_COUNT; ++id) {
+		priv->settings.tp[id].mode = I3C_HUB_DT_TP_MODE_NOT_DEFINED;
+		priv->settings.tp[id].pullup_en = I3C_HUB_DT_TP_PULLUP_NOT_DEFINED;
+	}
+}
+
+static int i3c_hub_hw_configure_pullup(struct device *dev)
+{
+	struct i3c_hub *priv = dev_get_drvdata(dev);
+	u8 mask = 0, value = 0;
+
+	if (priv->settings.tp0145_pullup != I3C_HUB_DT_PULLUP_NOT_DEFINED) {
+		mask |= TP0145_PULLUP_CONF_MASK;
+		value |= TP0145_PULLUP_CONF(i3c_hub_pullup_dt_to_reg(priv->settings.tp0145_pullup));
+	}
+
+	if (priv->settings.tp2367_pullup != I3C_HUB_DT_PULLUP_NOT_DEFINED) {
+		mask |= TP2367_PULLUP_CONF_MASK;
+		value |= TP2367_PULLUP_CONF(i3c_hub_pullup_dt_to_reg(priv->settings.tp2367_pullup));
+	}
+
+	return regmap_update_bits(priv->regmap, I3C_HUB_LDO_AND_PULLUP_CONF, mask, value);
+}
+
+static int i3c_hub_hw_configure_ldo(struct device *dev)
+{
+	struct i3c_hub *priv = dev_get_drvdata(dev);
+	u8 mask_all = 0, val_all = 0;
+	u8 ldo_dis = 0, ldo_en = 0;
+	u32 reg_val;
+	u8 val;
+	int ret;
+
+	/* Get LDOs configuration to figure out what is going to be changed */
+	ret = regmap_read(priv->regmap, I3C_HUB_LDO_CONF, &reg_val);
+	if (ret)
+		return ret;
+
+	if (priv->settings.cp0_ldo != I3C_HUB_DT_LDO_NOT_DEFINED) {
+		val = CP0_LDO_VOLTAGE(i3c_hub_ldo_dt_to_reg(priv->settings.cp0_ldo));
+		if ((reg_val & CP0_LDO_VOLTAGE_MASK) != val)
+			ldo_dis |= CP0_LDO_EN;
+		if (priv->settings.cp0_ldo != I3C_HUB_DT_LDO_DISABLED)
+			ldo_en |= CP0_LDO_EN;
+		mask_all |= CP0_LDO_VOLTAGE_MASK;
+		val_all |= val;
+	}
+	if (priv->settings.cp1_ldo != I3C_HUB_DT_LDO_NOT_DEFINED) {
+		val = CP1_LDO_VOLTAGE(i3c_hub_ldo_dt_to_reg(priv->settings.cp1_ldo));
+		if ((reg_val & CP1_LDO_VOLTAGE_MASK) != val)
+			ldo_dis |= CP1_LDO_EN;
+		if (priv->settings.cp1_ldo != I3C_HUB_DT_LDO_DISABLED)
+			ldo_en |= CP1_LDO_EN;
+		mask_all |= CP1_LDO_VOLTAGE_MASK;
+		val_all |= val;
+	}
+	if (priv->settings.tp0145_ldo != I3C_HUB_DT_LDO_NOT_DEFINED) {
+		val = TP0145_LDO_VOLTAGE(i3c_hub_ldo_dt_to_reg(priv->settings.tp0145_ldo));
+		if ((reg_val & TP0145_LDO_VOLTAGE_MASK) != val)
+			ldo_dis |= TP0145_LDO_EN;
+		if (priv->settings.tp0145_ldo != I3C_HUB_DT_LDO_DISABLED)
+			ldo_en |= TP0145_LDO_EN;
+		mask_all |= TP0145_LDO_VOLTAGE_MASK;
+		val_all |= val;
+	}
+	if (priv->settings.tp2367_ldo != I3C_HUB_DT_LDO_NOT_DEFINED) {
+		val = TP2367_LDO_VOLTAGE(i3c_hub_ldo_dt_to_reg(priv->settings.tp2367_ldo));
+		if ((reg_val & TP2367_LDO_VOLTAGE_MASK) != val)
+			ldo_dis |= TP2367_LDO_EN;
+		if (priv->settings.tp2367_ldo != I3C_HUB_DT_LDO_DISABLED)
+			ldo_en |= TP2367_LDO_EN;
+		mask_all |= TP2367_LDO_VOLTAGE_MASK;
+		val_all |= val;
+	}
+
+	/* Disable all LDOs if LDO configuration is going to be changed. */
+	ret = regmap_update_bits(priv->regmap, I3C_HUB_LDO_AND_PULLUP_CONF, ldo_dis, 0);
+	if (ret)
+		return ret;
+
+	/* Set LDOs configuration */
+	ret = regmap_update_bits(priv->regmap, I3C_HUB_LDO_CONF, mask_all, val_all);
+	if (ret)
+		return ret;
+
+	/* Re-enable LDOs if needed */
+	return regmap_update_bits(priv->regmap, I3C_HUB_LDO_AND_PULLUP_CONF, ldo_en, ldo_en);
+}
+
+static int i3c_hub_hw_configure_tp(struct device *dev)
+{
+	struct i3c_hub *priv = dev_get_drvdata(dev);
+	u8 pullup_mask = 0, pullup_val = 0;
+	u8 smbus_mask = 0, smbus_val = 0;
+	u8 gpio_mask = 0, gpio_val = 0;
+	u8 i3c_mask = 0, i3c_val = 0;
+	int ret;
+	int i;
+
+	/* TBD: Read type of HUB from register I3C_HUB_DEV_INFO_0 to learn target ports count. */
+	for (i = 0; i < I3C_HUB_TP_MAX_COUNT; ++i) {
+		if (priv->settings.tp[i].mode != I3C_HUB_DT_TP_MODE_NOT_DEFINED) {
+			i3c_mask |= TPn_NET_CON(i);
+			smbus_mask |= TPn_SMBUS_MODE_EN(i);
+			gpio_mask |= TPn_GPIO_MODE_EN(i);
+
+			if (priv->settings.tp[i].mode == I3C_HUB_DT_TP_MODE_I3C)
+				i3c_val |= TPn_NET_CON(i);
+			else if (priv->settings.tp[i].mode == I3C_HUB_DT_TP_MODE_SMBUS)
+				smbus_val |= TPn_SMBUS_MODE_EN(i);
+			else if (priv->settings.tp[i].mode == I3C_HUB_DT_TP_MODE_GPIO)
+				gpio_val |= TPn_GPIO_MODE_EN(i);
+		}
+		if (priv->settings.tp[i].pullup_en != I3C_HUB_DT_TP_PULLUP_NOT_DEFINED) {
+			pullup_mask |= TPn_PULLUP_EN(i);
+			if (priv->settings.tp[i].pullup_en == I3C_HUB_DT_TP_PULLUP_ENABLED)
+				pullup_val |= TPn_PULLUP_EN(i);
+		}
+	}
+
+	ret = regmap_update_bits(priv->regmap, I3C_HUB_TP_NET_CON_CONF, i3c_mask, i3c_val);
+	if (ret)
+		return ret;
+
+	ret = regmap_update_bits(priv->regmap, I3C_HUB_TP_SMBUS_AGNT_EN, smbus_mask, smbus_val);
+	if (ret)
+		return ret;
+
+	ret = regmap_update_bits(priv->regmap, I3C_HUB_TP_GPIO_MODE_EN, gpio_mask, gpio_val);
+	if (ret)
+		return ret;
+
+	/* Enable TP here in case TP was configured */
+	ret = regmap_update_bits(priv->regmap, I3C_HUB_TP_ENABLE, i3c_mask | smbus_mask | gpio_mask,
+				 i3c_val | smbus_val | gpio_val);
+	if (ret)
+		return ret;
+
+	return regmap_update_bits(priv->regmap, I3C_HUB_TP_PULLUP_EN, pullup_mask, pullup_val);
+}
+
+static int i3c_hub_configure_hw(struct device *dev)
+{
+	int ret;
+
+	ret = i3c_hub_hw_configure_pullup(dev);
+	if (ret)
+		return ret;
+
+	ret = i3c_hub_hw_configure_ldo(dev);
+	if (ret)
+		return ret;
+
+	return i3c_hub_hw_configure_tp(dev);
+}
+
+static const struct i3c_device_id i3c_hub_ids[] = {
+	I3C_CLASS(I3C_DCR_HUB, NULL),
+	{ },
+};
+
+static int fops_access_reg_get(void *ctx, u64 *val)
+{
+	struct i3c_hub *priv = ctx;
+	u32 reg_val;
+	int ret;
+
+	ret = regmap_read(priv->regmap, priv->reg_addr, &reg_val);
+	if (ret)
+		return ret;
+
+	*val = reg_val & 0xFF;
+	return 0;
+}
+
+static int fops_access_reg_set(void *ctx, u64 val)
+{
+	struct i3c_hub *priv = ctx;
+
+	return regmap_write(priv->regmap, priv->reg_addr, val & 0xFF);
+}
+DEFINE_DEBUGFS_ATTRIBUTE(fops_access_reg, fops_access_reg_get, fops_access_reg_set, "0x%llX\n");
+
+static int i3c_hub_debugfs_init(struct i3c_hub *priv, const char *hub_id)
+{
+	struct dentry  *entry, *dt_conf_dir, *reg_dir;
+	int i;
+
+	entry = debugfs_create_dir(hub_id, NULL);
+	if (IS_ERR(entry))
+		return PTR_ERR(entry);
+
+	priv->debug_dir = entry;
+
+	entry = debugfs_create_dir("dt-conf", priv->debug_dir);
+	if (IS_ERR(entry))
+		goto err_remove;
+
+	dt_conf_dir = entry;
+
+	debugfs_create_u8("cp0-ldo", 0400, dt_conf_dir, &priv->settings.cp0_ldo);
+	debugfs_create_u8("cp1-ldo", 0400, dt_conf_dir, &priv->settings.cp1_ldo);
+	debugfs_create_u8("tp0145-ldo", 0400, dt_conf_dir, &priv->settings.tp0145_ldo);
+	debugfs_create_u8("tp2367-ldo", 0400, dt_conf_dir, &priv->settings.tp2367_ldo);
+	debugfs_create_u8("tp0145-pullup", 0400, dt_conf_dir, &priv->settings.tp0145_pullup);
+	debugfs_create_u8("tp2367-pullup", 0400, dt_conf_dir, &priv->settings.tp2367_pullup);
+
+	for (i = 0; i < I3C_HUB_TP_MAX_COUNT; ++i) {
+		char file_name[32];
+
+		sprintf(file_name, "tp%i.mode", i);
+		debugfs_create_u8(file_name, 0400, dt_conf_dir, &priv->settings.tp[i].mode);
+		sprintf(file_name, "tp%i.pullup_en", i);
+		debugfs_create_u8(file_name, 0400, dt_conf_dir, &priv->settings.tp[i].pullup_en);
+	}
+
+	entry = debugfs_create_dir("reg", priv->debug_dir);
+	if (IS_ERR(entry))
+		goto err_remove;
+
+	reg_dir = entry;
+
+	entry = debugfs_create_file_unsafe("access", 0600, reg_dir, priv, &fops_access_reg);
+	if (IS_ERR(entry))
+		goto err_remove;
+
+	debugfs_create_u8("offset", 0600, reg_dir, &priv->reg_addr);
+
+	return 0;
+
+err_remove:
+	debugfs_remove_recursive(priv->debug_dir);
+	return PTR_ERR(entry);
+}
+
+static int i3c_hub_probe(struct i3c_device *i3cdev)
+{
+	struct regmap_config i3c_hub_regmap_config = {
+		.reg_bits = 8,
+		.val_bits = 8,
+	};
+	struct device *dev = &i3cdev->dev;
+	struct device_node *node;
+	struct regmap *regmap;
+	struct i3c_hub *priv;
+	char hub_id[32];
+	int ret;
+
+	priv = devm_kzalloc(dev, sizeof(*priv), GFP_KERNEL);
+	if (!priv)
+		return -ENOMEM;
+
+	priv->i3cdev = i3cdev;
+	i3cdev_set_drvdata(i3cdev, priv);
+
+	sprintf(hub_id, "i3c-hub-%d-%llx", i3cdev->bus->id, i3cdev->desc->info.pid);
+	ret = i3c_hub_debugfs_init(priv, hub_id);
+	if (ret)
+		return dev_err_probe(dev, ret, "Failed to initialized DebugFS.\n");
+
+	i3c_hub_of_default_configuration(dev);
+
+	/* TBD: Support for multiple HUBs. */
+	/* Just get first hub node from DT */
+	node = of_get_child_by_name(dev->parent->of_node, "hub");
+	if (!node) {
+		dev_warn(dev, "Failed to find DT entry for the driver. Running with defaults.\n");
+	} else {
+		i3c_hub_of_get_configuration(dev, node);
+		of_node_put(node);
+	}
+
+	regmap = devm_regmap_init_i3c(i3cdev, &i3c_hub_regmap_config);
+	if (IS_ERR(regmap)) {
+		ret = PTR_ERR(regmap);
+		dev_err(dev, "Failed to register I3C HUB regmap\n");
+		goto error;
+	}
+
+	priv->regmap = regmap;
+
+	/* Unlock access to protected registers */
+	ret = regmap_write(priv->regmap, I3C_HUB_PROTECTION_CODE, REGISTERS_UNLOCK_CODE);
+	if (ret) {
+		dev_err(dev, "Failed to unlock HUB's protected registers\n");
+		goto error;
+	}
+
+	ret = i3c_hub_configure_hw(dev);
+	if (ret) {
+		dev_err(dev, "Failed to configure the HUB\n");
+		goto error;
+	}
+
+	/* Lock access to protected registers */
+	ret = regmap_write(priv->regmap, I3C_HUB_PROTECTION_CODE, REGISTERS_LOCK_CODE);
+	if (ret) {
+		dev_err(dev, "Failed to lock HUB's protected registers\n");
+		goto error;
+	}
+
+	/* TBD: Apply special/security lock here using DEV_CMD register */
+
+	return 0;
+
+error:
+	debugfs_remove_recursive(priv->debug_dir);
+	return ret;
+}
+
+static void i3c_hub_remove(struct i3c_device *i3cdev)
+{
+	struct i3c_hub *priv = i3cdev_get_drvdata(i3cdev);
+
+	debugfs_remove_recursive(priv->debug_dir);
+}
+
+static struct i3c_driver i3c_hub = {
+	.driver.name = "i3c-hub",
+	.id_table = i3c_hub_ids,
+	.probe = i3c_hub_probe,
+	.remove = i3c_hub_remove,
+};
+
+module_i3c_driver(i3c_hub);
+
+MODULE_AUTHOR("Zbigniew Lukwinski <zbigniew.lukwinski@linux.intel.com>");
+MODULE_DESCRIPTION("I3C HUB driver");
+MODULE_LICENSE("GPL");
diff --git a/drivers/i3c/i3c-ibi-mqueue.c b/drivers/i3c/i3c-ibi-mqueue.c
new file mode 100644
index 000000000000..8f1ddb9fa8f8
--- /dev/null
+++ b/drivers/i3c/i3c-ibi-mqueue.c
@@ -0,0 +1,246 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (c) 2021 Aspeed Technology Inc.
+ */
+
+#include <linux/i3c/device.h>
+#include <linux/i3c/master.h>
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/of.h>
+#include <linux/slab.h>
+#include <linux/spinlock.h>
+#include <linux/sysfs.h>
+#include <linux/delay.h>
+#include "internals.h"
+
+#define MQ_MSGBUF_SIZE		256
+#define MQ_QUEUE_SIZE		4
+#define MQ_QUEUE_NEXT(x)	(((x) + 1) & (MQ_QUEUE_SIZE - 1))
+
+#define IBI_QUEUE_STATUS_PEC_ERR	BIT(30)
+#define IBI_STATUS_LAST_FRAG	BIT(24)
+#define PID_MANUF_ID_ASPEED	0x03f6
+
+struct mq_msg {
+	int len;
+	u8 *buf;
+};
+
+struct mq_queue {
+	struct bin_attribute bin;
+	struct kernfs_node *kn;
+
+	spinlock_t lock;
+	int in;
+	int out;
+
+	struct mq_msg *curr;
+	int truncated;
+	struct mq_msg queue[MQ_QUEUE_SIZE];
+};
+
+static u8 mdb_table[] = {
+	0xbf, /* Aspeed BIC */
+	0,
+};
+
+static void i3c_ibi_mqueue_callback(struct i3c_device *dev,
+				    const struct i3c_ibi_payload *payload)
+{
+	struct mq_queue *mq = dev_get_drvdata(&dev->dev);
+	struct mq_msg *msg = mq->curr;
+	u8 *buf = (u8 *)payload->data;
+	struct i3c_device_info info;
+	u32 status;
+	const u8 *mdb;
+
+	i3c_device_get_info(dev, &info);
+	/* first DW is IBI status */
+	status = *(u32 *)buf;
+
+	/* then the raw data */
+	buf += sizeof(status);
+	memcpy(&msg->buf[msg->len], buf, payload->len - sizeof(status));
+	msg->len += payload->len - sizeof(status);
+	if (status & IBI_QUEUE_STATUS_PEC_ERR) {
+		for (mdb = mdb_table; *mdb != 0; mdb++)
+			if (buf[0] == *mdb)
+				break;
+		if (!(*mdb)) {
+			dev_err(&dev->dev, "ibi crc/pec error: mdb = %x", buf[0]);
+			return;
+		}
+	}
+	/* if last fragment, notify and update pointers */
+	if (status & IBI_STATUS_LAST_FRAG) {
+		/* check pending-read-notification */
+		if (IS_MDB_PENDING_READ_NOTIFY(msg->buf[0])) {
+			struct i3c_priv_xfer xfers[1] = {
+				{
+					.rnw = true,
+					.len = info.max_read_len,
+					.data.in = msg->buf,
+				},
+			};
+
+			i3c_device_do_priv_xfers(dev, xfers, 1);
+
+			msg->len = xfers[0].len;
+		}
+
+		spin_lock(&mq->lock);
+		mq->in = MQ_QUEUE_NEXT(mq->in);
+		mq->curr = &mq->queue[mq->in];
+		mq->curr->len = 0;
+
+		if (mq->out == mq->in)
+			mq->out = MQ_QUEUE_NEXT(mq->out);
+		spin_unlock(&mq->lock);
+		kernfs_notify(mq->kn);
+	}
+}
+
+static ssize_t i3c_ibi_mqueue_bin_read(struct file *filp, struct kobject *kobj,
+				       struct bin_attribute *attr, char *buf,
+				       loff_t pos, size_t count)
+{
+	struct mq_queue *mq;
+	struct mq_msg *msg;
+	unsigned long flags;
+	bool more = false;
+	ssize_t ret = 0;
+
+	mq = dev_get_drvdata(container_of(kobj, struct device, kobj));
+
+	spin_lock_irqsave(&mq->lock, flags);
+	if (mq->out != mq->in) {
+		msg = &mq->queue[mq->out];
+
+		if (msg->len <= count) {
+			ret = msg->len;
+			memcpy(buf, msg->buf, ret);
+		} else {
+			ret = -EOVERFLOW; /* Drop this HUGE one. */
+		}
+
+		mq->out = MQ_QUEUE_NEXT(mq->out);
+		if (mq->out != mq->in)
+			more = true;
+	}
+	spin_unlock_irqrestore(&mq->lock, flags);
+
+	if (more)
+		kernfs_notify(mq->kn);
+
+	return ret;
+}
+
+static int i3c_ibi_mqueue_probe(struct i3c_device *i3cdev)
+{
+	struct device *dev = &i3cdev->dev;
+	struct mq_queue *mq;
+	struct i3c_ibi_setup ibireq = {};
+	int ret, i;
+	struct i3c_device_info info;
+	void *buf;
+
+	if (dev->type == &i3c_masterdev_type)
+		return -ENOTSUPP;
+
+	mq = devm_kzalloc(dev, sizeof(*mq), GFP_KERNEL);
+	if (!mq)
+		return -ENOMEM;
+
+	BUILD_BUG_ON(!is_power_of_2(MQ_QUEUE_SIZE));
+
+	buf = devm_kmalloc_array(dev, MQ_QUEUE_SIZE, MQ_MSGBUF_SIZE,
+				 GFP_KERNEL);
+	if (!buf)
+		return -ENOMEM;
+
+	for (i = 0; i < MQ_QUEUE_SIZE; i++) {
+		mq->queue[i].buf = buf + i * MQ_MSGBUF_SIZE;
+		mq->queue[i].len = 0;
+	}
+
+	i3c_device_get_info(i3cdev, &info);
+
+	ret = i3c_device_setmrl_ccc(i3cdev, &info, MQ_MSGBUF_SIZE,
+					    min(MQ_MSGBUF_SIZE, __UINT8_MAX__));
+	if (ret) {
+		ret = i3c_device_getmrl_ccc(i3cdev, &info);
+		if (ret)
+			return ret;
+	}
+
+	dev_set_drvdata(dev, mq);
+
+	spin_lock_init(&mq->lock);
+	mq->curr = &mq->queue[0];
+
+	sysfs_bin_attr_init(&mq->bin);
+	mq->bin.attr.name = "ibi-mqueue";
+	mq->bin.attr.mode = 0400;
+	mq->bin.read = i3c_ibi_mqueue_bin_read;
+	mq->bin.size = MQ_MSGBUF_SIZE * MQ_QUEUE_SIZE;
+
+	ret = sysfs_create_bin_file(&dev->kobj, &mq->bin);
+	if (ret)
+		return ret;
+
+	mq->kn = kernfs_find_and_get(dev->kobj.sd, mq->bin.attr.name);
+	if (!mq->kn) {
+		sysfs_remove_bin_file(&dev->kobj, &mq->bin);
+		return -EFAULT;
+	}
+
+	ibireq.handler = i3c_ibi_mqueue_callback;
+	ibireq.max_payload_len = MQ_MSGBUF_SIZE;
+	ibireq.num_slots = MQ_QUEUE_SIZE;
+
+	ret = i3c_device_request_ibi(i3cdev, &ibireq);
+	ret |= i3c_device_enable_ibi(i3cdev);
+
+	if (ret) {
+		kernfs_put(mq->kn);
+		sysfs_remove_bin_file(&dev->kobj, &mq->bin);
+		return ret;
+	}
+
+	return 0;
+}
+
+static void i3c_ibi_mqueue_remove(struct i3c_device *i3cdev)
+{
+	struct mq_queue *mq = dev_get_drvdata(&i3cdev->dev);
+
+	i3c_device_disable_ibi(i3cdev);
+	i3c_device_free_ibi(i3cdev);
+
+	kernfs_put(mq->kn);
+	sysfs_remove_bin_file(&i3cdev->dev.kobj, &mq->bin);
+}
+
+static const struct i3c_device_id i3c_ibi_mqueue_ids[] = {
+	I3C_DEVICE(0x3f6, 0x8000, (void *)0),
+	I3C_DEVICE(0x3f6, 0x8001, (void *)0),
+	I3C_DEVICE(0x3f6, 0x0503, (void *)0),
+	I3C_DEVICE(0x3f6, 0xA001, (void *)0),
+	{ /* sentinel */ },
+};
+MODULE_DEVICE_TABLE(i3c, i3c_ibi_mqueue_ids);
+
+static struct i3c_driver ibi_mqueue_driver = {
+	.driver = {
+		.name = "i3c-ibi-mqueue",
+	},
+	.probe = i3c_ibi_mqueue_probe,
+	.remove = i3c_ibi_mqueue_remove,
+	.id_table = i3c_ibi_mqueue_ids,
+};
+module_i3c_driver(ibi_mqueue_driver);
+
+MODULE_AUTHOR("Dylan Hung <dylan_hung@aspeedtech.com>");
+MODULE_DESCRIPTION("I3C IBI mqueue driver");
+MODULE_LICENSE("GPL v2");
diff --git a/drivers/i3c/i3c-mux-imx3102.c b/drivers/i3c/i3c-mux-imx3102.c
new file mode 100644
index 000000000000..b6972893cbd6
--- /dev/null
+++ b/drivers/i3c/i3c-mux-imx3102.c
@@ -0,0 +1,227 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (c) 2021 Aspeed Technology Inc.
+ *
+ * IMX3102: 2-to-1 multiplexier
+ *
+ * +------------------   +
+ * | SoC                 |
+ * |                     |
+ * | I3C controller #0 - | --+
+ * |                     |    \                  dev   dev
+ * |                     |     +---------+       |     |
+ * |                     |     | IMX3102 | ---+--+--+--+--- i3c bus
+ * |                     |     +---------+    |     |
+ * |                     |    /               dev   dev
+ * | I3C controller #1 - | --+
+ * |                     |
+ * +---------------------+
+ */
+
+#include <linux/i3c/device.h>
+#include <linux/i3c/master.h>
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/of.h>
+#include <linux/slab.h>
+#include <linux/spinlock.h>
+#include <linux/sysfs.h>
+#include <linux/delay.h>
+#include <linux/regmap.h>
+#include "internals.h"
+
+#define IMX3102_DEVICE_TYPE_HI		0x0
+#define IMX3102_DEVICE_TYPE_LO		0x1
+
+#define IMX3102_PORT_CONF		0x40
+#define   IMX3102_PORT_CONF_M1_EN	BIT(7)
+#define   IMX3102_PORT_CONF_S_EN	BIT(6)
+#define IMX3102_PORT_SEL		0x41
+#define   IMX3102_PORT_SEL_M1		BIT(7)
+#define   IMX3102_PORT_SEL_S_EN		BIT(6)
+
+struct imx3102 {
+	struct regmap *regmap;
+
+	struct bin_attribute ownership;
+	struct bin_attribute reinit;
+	struct kernfs_node *kn;
+
+	struct i3c_device *i3cdev;
+};
+
+static ssize_t i3c_mux_imx3102_query(struct file *filp, struct kobject *kobj,
+				     struct bin_attribute *attr, char *buf,
+				     loff_t pos, size_t count)
+{
+	struct imx3102 *imx3102;
+	struct device *dev;
+	int ret;
+	u8 data[2];
+
+	imx3102 = dev_get_drvdata(container_of(kobj, struct device, kobj));
+	if (!imx3102)
+		return -1;
+
+	dev = &imx3102->i3cdev->dev;
+
+	ret = regmap_raw_read(imx3102->regmap, IMX3102_DEVICE_TYPE_HI, data, 2);
+	if (ret)
+		sprintf(buf, "N\n");
+	else
+		sprintf(buf, "Y\n");
+
+	return 2;
+}
+
+/* write whatever value to imx3102-mux to release the ownership */
+static ssize_t i3c_mux_imx3102_release_chan(struct file *filp,
+					    struct kobject *kobj,
+					    struct bin_attribute *attr,
+					    char *buf, loff_t pos, size_t count)
+{
+	struct imx3102 *imx3102;
+	struct device *dev;
+	struct regmap *regmap;
+	int ret;
+	u8 select;
+
+	imx3102 = dev_get_drvdata(container_of(kobj, struct device, kobj));
+	if (!imx3102) {
+		count = -1;
+		goto out;
+	}
+
+	dev = &imx3102->i3cdev->dev;
+	regmap = imx3102->regmap;
+	ret = regmap_raw_read(regmap, IMX3102_PORT_SEL, &select, 1);
+	if (ret)
+		goto out;
+
+	/* invert the bit to change the ownership */
+	select ^= IMX3102_PORT_SEL_M1;
+	regmap_raw_write(regmap, IMX3102_PORT_SEL, &select, 1);
+
+out:
+	return count;
+}
+
+static ssize_t i3c_mux_imx3102_bus_reinit(struct file *filp,
+					  struct kobject *kobj,
+					  struct bin_attribute *attr, char *buf,
+					  loff_t pos, size_t count)
+{
+	struct imx3102 *imx3102;
+	int ret;
+
+	imx3102 = dev_get_drvdata(container_of(kobj, struct device, kobj));
+	if (!imx3102) {
+		count = -1;
+		return count;
+	}
+
+	ret = i3c_device_send_ccc_cmd(imx3102->i3cdev, I3C_CCC_SETHID);
+	ret = i3c_device_send_ccc_cmd(imx3102->i3cdev, I3C_CCC_SETAASA);
+
+	return count;
+}
+
+static int i3c_mux_imx3102_probe(struct i3c_device *i3cdev)
+{
+	struct device *dev = &i3cdev->dev;
+	struct imx3102 *imx3102;
+	struct regmap *regmap;
+	struct regmap_config imx3102_i3c_regmap_config = {
+		.reg_bits = 8,
+		.pad_bits = 8,
+		.val_bits = 8,
+	};
+	int ret;
+	u8 data[2];
+
+	if (dev->type == &i3c_masterdev_type)
+		return -ENOTSUPP;
+
+	imx3102 = devm_kzalloc(dev, sizeof(*imx3102), GFP_KERNEL);
+	if (!imx3102)
+		return -ENOMEM;
+
+	imx3102->i3cdev = i3cdev;
+
+	/* register regmap */
+	regmap = devm_regmap_init_i3c(i3cdev, &imx3102_i3c_regmap_config);
+	if (IS_ERR(regmap)) {
+		dev_err(dev, "Failed to register i3c regmap %d\n",
+			(int)PTR_ERR(regmap));
+		return PTR_ERR(regmap);
+	}
+	imx3102->regmap = regmap;
+
+	sysfs_bin_attr_init(&imx3102->ownership);
+	imx3102->ownership.attr.name = "imx3102.ownership";
+	imx3102->ownership.attr.mode = 0600;
+	imx3102->ownership.read = i3c_mux_imx3102_query;
+	imx3102->ownership.write = i3c_mux_imx3102_release_chan;
+	imx3102->ownership.size = 2;
+	ret = sysfs_create_bin_file(&dev->kobj, &imx3102->ownership);
+
+	sysfs_bin_attr_init(&imx3102->reinit);
+	imx3102->reinit.attr.name = "imx3102.reinit";
+	imx3102->reinit.attr.mode = 0200;
+	imx3102->reinit.write = i3c_mux_imx3102_bus_reinit;
+	imx3102->reinit.size = 2;
+	ret = sysfs_create_bin_file(&dev->kobj, &imx3102->reinit);
+
+	imx3102->kn = kernfs_find_and_get(dev->kobj.sd, imx3102->ownership.attr.name);
+	dev_set_drvdata(dev, imx3102);
+
+	ret = regmap_raw_read(regmap, IMX3102_DEVICE_TYPE_HI, data, 2);
+	if (ret) {
+		dev_info(dev, "No ownership\n");
+		return 0;
+	}
+	dev_dbg(dev, "device ID %02x %02x\n", data[0], data[1]);
+
+	/* enable the slave port */
+	regmap_raw_read(regmap, IMX3102_PORT_CONF, &data[0], 2);
+	data[0] |= IMX3102_PORT_CONF_S_EN | IMX3102_PORT_CONF_M1_EN;
+	data[1] |= IMX3102_PORT_SEL_S_EN;
+	regmap_raw_write(regmap, IMX3102_PORT_CONF, data, 2);
+
+	/* send SETAASA to bring the devices behind the mux to I3C mode */
+	i3c_device_send_ccc_cmd(i3cdev, I3C_CCC_SETAASA);
+
+	return 0;
+}
+
+static void i3c_mux_imx3102_remove(struct i3c_device *i3cdev)
+{
+	struct device *dev = &i3cdev->dev;
+	struct imx3102 *imx3102;
+
+	imx3102 = dev_get_drvdata(dev);
+
+	kernfs_put(imx3102->kn);
+	sysfs_remove_bin_file(&dev->kobj, &imx3102->ownership);
+	devm_kfree(dev, imx3102);
+}
+
+static const struct i3c_device_id i3c_mux_imx3102_ids[] = {
+	I3C_DEVICE(0x266, 0x3102, (void *)0),
+	{ /* sentinel */ },
+};
+MODULE_DEVICE_TABLE(i3c, i3c_mux_imx3102_ids);
+
+static struct i3c_driver imx3102_driver = {
+	.driver = {
+		.name = "i3c-mux-imx3102",
+	},
+	.probe = i3c_mux_imx3102_probe,
+	.remove = i3c_mux_imx3102_remove,
+	.id_table = i3c_mux_imx3102_ids,
+};
+module_i3c_driver(imx3102_driver);
+
+MODULE_AUTHOR("Dylan Hung <dylan_hung@aspeedtech.com>");
+MODULE_DESCRIPTION("I3C IMX3102 multiplexer driver");
+MODULE_LICENSE("GPL v2");
diff --git a/drivers/i3c/i3c-slave-eeprom.c b/drivers/i3c/i3c-slave-eeprom.c
new file mode 100644
index 000000000000..abc1d693adb4
--- /dev/null
+++ b/drivers/i3c/i3c-slave-eeprom.c
@@ -0,0 +1,175 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (c) 2022 Aspeed Technology Inc.
+ */
+
+#include <linux/i3c/master.h>
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/of.h>
+#include <linux/slab.h>
+#include <linux/spinlock.h>
+#include <linux/sysfs.h>
+
+struct eeprom_data {
+	struct bin_attribute bin;
+	struct work_struct prep_rdata;
+	spinlock_t buffer_lock;
+	u16 buffer_idx;
+	u16 address_mask;
+	u8 num_address_bytes;
+
+	struct i3c_master_controller *i3c_controller;
+	u8 buffer[];
+};
+
+static void i3c_slave_eeprom_prep_rdata(struct work_struct *work)
+{
+	struct eeprom_data *eeprom = container_of(work, struct eeprom_data, prep_rdata);
+	struct i3c_slave_payload read_data, notify;
+	u32 mdb = IBI_MDB_ID(0b101, 0x1f);
+
+	notify.len = 1;
+	notify.data = &mdb;
+
+	read_data.len = eeprom->address_mask - eeprom->buffer_idx + 1;
+	read_data.data = &eeprom->buffer[eeprom->buffer_idx];
+	i3c_master_put_read_data(eeprom->i3c_controller, &read_data, &notify);
+}
+
+static void i3c_slave_eeprom_callback(struct i3c_master_controller *master,
+				      const struct i3c_slave_payload *payload)
+{
+	struct eeprom_data *eeprom = dev_get_drvdata(&master->dev);
+	int wr_len;
+	u8 *buf = (u8 *)payload->data;
+
+	if (!payload->len)
+		return;
+
+	if (eeprom->num_address_bytes == 2)
+		eeprom->buffer_idx = ((u16)buf[0] << 8) | buf[1];
+	else
+		eeprom->buffer_idx = (u16)buf[0];
+
+	wr_len = payload->len - eeprom->num_address_bytes;
+
+	pr_debug("len = %d, index=%d, wr_len=%d\n", payload->len,
+		 eeprom->buffer_idx, wr_len);
+
+	if (wr_len > 0) {
+		if (eeprom->buffer_idx + wr_len > eeprom->address_mask) {
+			u16 len = eeprom->address_mask - eeprom->buffer_idx + 1;
+
+			memcpy(&eeprom->buffer[eeprom->buffer_idx],
+			       &buf[eeprom->num_address_bytes], len);
+			memcpy(&eeprom->buffer[0],
+			       &buf[eeprom->num_address_bytes + len],
+			       wr_len - len);
+		} else {
+			memcpy(&eeprom->buffer[eeprom->buffer_idx],
+			       &buf[eeprom->num_address_bytes], wr_len);
+		}
+
+		eeprom->buffer_idx += wr_len;
+		eeprom->buffer_idx &= eeprom->address_mask;
+	}
+
+	/* prepare the read data outside of interrupt context */
+	schedule_work(&eeprom->prep_rdata);
+}
+
+static ssize_t i3c_slave_eeprom_bin_read(struct file *filp,
+					 struct kobject *kobj,
+					 struct bin_attribute *attr, char *buf,
+					 loff_t off, size_t count)
+{
+	struct eeprom_data *eeprom;
+	unsigned long flags;
+
+	eeprom = dev_get_drvdata(container_of(kobj, struct device, kobj));
+
+	spin_lock_irqsave(&eeprom->buffer_lock, flags);
+	memcpy(buf, &eeprom->buffer[off], count);
+	spin_unlock_irqrestore(&eeprom->buffer_lock, flags);
+
+	return count;
+}
+
+static ssize_t i3c_slave_eeprom_bin_write(struct file *filp,
+					  struct kobject *kobj,
+					  struct bin_attribute *attr, char *buf,
+					  loff_t off, size_t count)
+{
+	struct eeprom_data *eeprom;
+	unsigned long flags;
+
+	eeprom = dev_get_drvdata(container_of(kobj, struct device, kobj));
+
+	spin_lock_irqsave(&eeprom->buffer_lock, flags);
+	memcpy(&eeprom->buffer[off], buf, count);
+	spin_unlock_irqrestore(&eeprom->buffer_lock, flags);
+
+	return count;
+}
+
+int i3c_slave_eeprom_probe(struct i3c_master_controller *master)
+{
+	struct eeprom_data *eeprom;
+	int ret;
+	struct i3c_slave_setup req = {};
+	struct device *dev = &master->dev;
+
+	/* fixed parameters for testing: size 64 bytes, address size is 1 byte */
+	unsigned int size = 64;
+	unsigned int flag_addr16 = 0;
+
+	eeprom = devm_kzalloc(dev, sizeof(struct eeprom_data) + size, GFP_KERNEL);
+	if (!eeprom)
+		return -ENOMEM;
+
+	eeprom->num_address_bytes = flag_addr16 ? 2 : 1;
+	eeprom->address_mask = size - 1;
+	spin_lock_init(&eeprom->buffer_lock);
+	dev_set_drvdata(dev, eeprom);
+
+	memset(eeprom->buffer, 0xff, size);
+
+	sysfs_bin_attr_init(&eeprom->bin);
+	eeprom->bin.attr.name = "slave-eeprom";
+	eeprom->bin.attr.mode = 0600;
+	eeprom->bin.read = i3c_slave_eeprom_bin_read;
+	eeprom->bin.write = i3c_slave_eeprom_bin_write;
+	eeprom->bin.size = size;
+
+	eeprom->i3c_controller = master;
+
+	ret = sysfs_create_bin_file(&dev->kobj, &eeprom->bin);
+	if (ret)
+		return ret;
+
+	INIT_WORK(&eeprom->prep_rdata, i3c_slave_eeprom_prep_rdata);
+
+	req.handler = i3c_slave_eeprom_callback;
+	req.max_payload_len = size;
+	req.num_slots = 1;
+
+	ret = i3c_master_register_slave(master, &req);
+	if (ret) {
+		sysfs_remove_bin_file(&dev->kobj, &eeprom->bin);
+		return ret;
+	}
+
+	return 0;
+}
+
+int i3c_slave_eeprom_remove(struct i3c_master_controller *master)
+{
+	struct device *dev = &master->dev;
+	struct eeprom_data *eeprom = dev_get_drvdata(dev);
+
+	i3c_master_unregister_slave(master);
+	sysfs_remove_bin_file(&dev->kobj, &eeprom->bin);
+
+	return 0;
+}
diff --git a/drivers/i3c/i3c-slave-mqueue.c b/drivers/i3c/i3c-slave-mqueue.c
new file mode 100644
index 000000000000..ff05ee4e7de8
--- /dev/null
+++ b/drivers/i3c/i3c-slave-mqueue.c
@@ -0,0 +1,206 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (c) 2021 Aspeed Technology Inc.
+ */
+
+#include <linux/i3c/master.h>
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/of.h>
+#include <linux/slab.h>
+#include <linux/spinlock.h>
+#include <linux/sysfs.h>
+
+#define MQ_MSGBUF_SIZE		256
+#define MQ_QUEUE_SIZE		4
+#define MQ_QUEUE_NEXT(x)	(((x) + 1) & (MQ_QUEUE_SIZE - 1))
+
+#define IBI_STATUS_LAST_FRAG	BIT(24)
+#define MQ_MDB			IBI_MDB_ID(0b101, 0x1f)
+
+struct mq_msg {
+	int len;
+	u8 *buf;
+};
+
+struct mq_queue {
+	struct bin_attribute bin;
+	struct kernfs_node *kn;
+
+	spinlock_t lock;
+	int in;
+	int out;
+
+	struct mq_msg *curr;
+	int truncated;
+	struct mq_msg queue[MQ_QUEUE_SIZE];
+
+	struct i3c_master_controller *i3c_controller;
+	u8 mdb;
+};
+
+static void i3c_slave_mqueue_callback(struct i3c_master_controller *master,
+				      const struct i3c_slave_payload *payload)
+{
+	struct mq_queue *mq = dev_get_drvdata(&master->dev);
+	struct mq_msg *msg = mq->curr;
+
+	memcpy(msg->buf, (u8 *)payload->data, payload->len);
+	msg->len = payload->len;
+
+	spin_lock(&mq->lock);
+	mq->in = MQ_QUEUE_NEXT(mq->in);
+	mq->curr = &mq->queue[mq->in];
+	mq->curr->len = 0;
+
+	if (mq->out == mq->in)
+		mq->out = MQ_QUEUE_NEXT(mq->out);
+	spin_unlock(&mq->lock);
+	kernfs_notify(mq->kn);
+}
+
+static ssize_t i3c_slave_mqueue_bin_read(struct file *filp, struct kobject *kobj,
+				       struct bin_attribute *attr, char *buf,
+				       loff_t pos, size_t count)
+{
+	struct mq_queue *mq;
+	struct mq_msg *msg;
+	unsigned long flags;
+	bool more = false;
+	ssize_t ret = 0;
+
+	mq = dev_get_drvdata(container_of(kobj, struct device, kobj));
+
+	spin_lock_irqsave(&mq->lock, flags);
+	if (mq->out != mq->in) {
+		msg = &mq->queue[mq->out];
+
+		if (msg->len <= count) {
+			ret = msg->len;
+			memcpy(buf, msg->buf, ret);
+		} else {
+			ret = -EOVERFLOW; /* Drop this HUGE one. */
+		}
+
+		mq->out = MQ_QUEUE_NEXT(mq->out);
+		if (mq->out != mq->in)
+			more = true;
+	}
+	spin_unlock_irqrestore(&mq->lock, flags);
+
+	if (more)
+		kernfs_notify(mq->kn);
+
+	return ret;
+}
+
+static ssize_t i3c_slave_mqueue_bin_write(struct file *filp,
+					  struct kobject *kobj,
+					  struct bin_attribute *attr, char *buf,
+					  loff_t pos, size_t count)
+{
+	struct mq_queue *mq;
+	struct i3c_slave_payload payload, ibi;
+	u8 *data;
+
+	mq = dev_get_drvdata(container_of(kobj, struct device, kobj));
+
+	if (IS_MDB_PENDING_READ_NOTIFY(mq->mdb)) {
+		ibi.data = &mq->mdb;
+		ibi.len = 1;
+		payload.data = buf;
+		payload.len = count;
+		i3c_master_put_read_data(mq->i3c_controller, &payload, &ibi);
+	} else {
+		data = kmalloc(count + 1, GFP_KERNEL);
+		if (!data)
+			return -ENOMEM;
+
+		data[0] = mq->mdb;
+		memcpy(&data[1], buf, count);
+
+		payload.data = data;
+		payload.len = count + 1;
+		i3c_master_send_sir(mq->i3c_controller, &payload);
+		kfree(data);
+	}
+
+	return count;
+}
+
+int i3c_slave_mqueue_probe(struct i3c_master_controller *master)
+{
+	struct mq_queue *mq;
+	int ret, i;
+	void *buf;
+	struct i3c_slave_setup req = {};
+	struct device *dev = &master->dev;
+
+	mq = devm_kzalloc(dev, sizeof(*mq), GFP_KERNEL);
+	if (!mq)
+		return -ENOMEM;
+
+	BUILD_BUG_ON(!is_power_of_2(MQ_QUEUE_SIZE));
+
+	buf = devm_kmalloc_array(dev, MQ_QUEUE_SIZE, MQ_MSGBUF_SIZE,
+				 GFP_KERNEL);
+	if (!buf)
+		return -ENOMEM;
+
+	for (i = 0; i < MQ_QUEUE_SIZE; i++) {
+		mq->queue[i].buf = buf + i * MQ_MSGBUF_SIZE;
+		mq->queue[i].len = 0;
+	}
+
+	dev_set_drvdata(dev, mq);
+
+	spin_lock_init(&mq->lock);
+	mq->curr = &mq->queue[0];
+
+	sysfs_bin_attr_init(&mq->bin);
+	mq->bin.attr.name = "slave-mqueue";
+	mq->bin.attr.mode = 0600;
+	mq->bin.read = i3c_slave_mqueue_bin_read;
+	mq->bin.write = i3c_slave_mqueue_bin_write;
+	mq->bin.size = MQ_MSGBUF_SIZE * MQ_QUEUE_SIZE;
+
+	mq->i3c_controller = master;
+	mq->mdb = MQ_MDB;
+
+	ret = sysfs_create_bin_file(&dev->kobj, &mq->bin);
+	if (ret)
+		return ret;
+
+	mq->kn = kernfs_find_and_get(dev->kobj.sd, mq->bin.attr.name);
+	if (!mq->kn) {
+		sysfs_remove_bin_file(&dev->kobj, &mq->bin);
+		return -EFAULT;
+	}
+
+	req.handler = i3c_slave_mqueue_callback;
+	req.max_payload_len = MQ_MSGBUF_SIZE;
+	req.num_slots = MQ_QUEUE_SIZE;
+
+	ret = i3c_master_register_slave(master, &req);
+
+	if (ret) {
+		kernfs_put(mq->kn);
+		sysfs_remove_bin_file(&dev->kobj, &mq->bin);
+		return ret;
+	}
+
+	return 0;
+}
+
+int i3c_slave_mqueue_remove(struct i3c_master_controller *master)
+{
+	struct device *dev = &master->dev;
+	struct mq_queue *mq = dev_get_drvdata(dev);
+
+	i3c_master_unregister_slave(master);
+
+	kernfs_put(mq->kn);
+	sysfs_remove_bin_file(&dev->kobj, &mq->bin);
+
+	return 0;
+}
diff --git a/drivers/i3c/i3cdev.c b/drivers/i3c/i3cdev.c
new file mode 100644
index 000000000000..ef08b6056165
--- /dev/null
+++ b/drivers/i3c/i3cdev.c
@@ -0,0 +1,509 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Copyright (c) 2020 Synopsys, Inc. and/or its affiliates.
+ *
+ * Author: Vitor Soares <soares@synopsys.com>
+ */
+
+#include <linux/cdev.h>
+#include <linux/compat.h>
+#include <linux/device.h>
+#include <linux/fs.h>
+#include <linux/init.h>
+#include <linux/jiffies.h>
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/notifier.h>
+#include <linux/slab.h>
+#include <linux/uaccess.h>
+
+#include <linux/i3c/i3cdev.h>
+
+#include "internals.h"
+
+struct i3cdev_data {
+	struct i3c_device *i3c;
+	struct device *dev;
+	struct mutex xfer_lock; /* prevent detach while transferring */
+	struct cdev cdev;
+	int id;
+};
+
+static DEFINE_IDA(i3cdev_ida);
+static dev_t i3cdev_number;
+#define I3C_MINORS (MINORMASK + 1)
+
+static struct i3cdev_data *get_free_i3cdev(struct i3c_device *i3c)
+{
+	struct i3cdev_data *i3cdev;
+	int id;
+
+	id = ida_simple_get(&i3cdev_ida, 0, I3C_MINORS, GFP_KERNEL);
+	if (id < 0) {
+		pr_err("i3cdev: no minor number available!\n");
+		return ERR_PTR(id);
+	}
+
+	i3cdev = kzalloc(sizeof(*i3cdev), GFP_KERNEL);
+	if (!i3cdev) {
+		ida_simple_remove(&i3cdev_ida, id);
+		return ERR_PTR(-ENOMEM);
+	}
+
+	i3cdev->i3c = i3c;
+	i3cdev->id = id;
+	i3cdev_set_drvdata(i3c, i3cdev);
+
+	return i3cdev;
+}
+
+static void put_i3cdev(struct i3cdev_data *i3cdev)
+{
+	i3cdev_set_drvdata(i3cdev->i3c, NULL);
+	kfree(i3cdev);
+}
+
+static ssize_t
+i3cdev_read(struct file *file, char __user *buf, size_t count, loff_t *f_pos)
+{
+	struct i3cdev_data *i3cdev = file->private_data;
+	struct i3c_device *i3c = i3cdev->i3c;
+	struct i3c_priv_xfer xfers = {
+		.rnw = true,
+		.len = count,
+	};
+	int ret = -EACCES;
+	char *tmp;
+
+	mutex_lock(&i3cdev->xfer_lock);
+	if (!IS_ENABLED(CONFIG_I3CDEV_FORCE_CREATE) && i3c->dev.driver)
+		goto err_out;
+
+	tmp = kzalloc(count, GFP_KERNEL);
+	if (!tmp)
+		return -ENOMEM;
+
+	xfers.data.in = tmp;
+
+	dev_dbg(&i3c->dev, "Reading %zu bytes.\n", count);
+
+	ret = i3c_device_do_priv_xfers(i3c, &xfers, 1);
+	if (!ret)
+		ret = copy_to_user(buf, tmp, xfers.len) ? -EFAULT : xfers.len;
+
+	kfree(tmp);
+
+err_out:
+	mutex_unlock(&i3cdev->xfer_lock);
+	return ret;
+}
+
+static ssize_t
+i3cdev_write(struct file *file, const char __user *buf, size_t count,
+	     loff_t *f_pos)
+{
+	struct i3cdev_data *i3cdev = file->private_data;
+	struct i3c_device *i3c = i3cdev->i3c;
+	struct i3c_priv_xfer xfers = {
+		.rnw = false,
+		.len = count,
+	};
+	int ret = -EACCES;
+	char *tmp;
+
+	mutex_lock(&i3cdev->xfer_lock);
+	if (!IS_ENABLED(CONFIG_I3CDEV_FORCE_CREATE) && i3c->dev.driver)
+		goto err_out;
+
+	tmp = memdup_user(buf, count);
+	if (IS_ERR(tmp))
+		return PTR_ERR(tmp);
+
+	xfers.data.out = tmp;
+
+	dev_dbg(&i3c->dev, "Writing %zu bytes.\n", count);
+
+	ret = i3c_device_do_priv_xfers(i3c, &xfers, 1);
+	kfree(tmp);
+
+err_out:
+	mutex_unlock(&i3cdev->xfer_lock);
+	return (!ret) ? count : ret;
+}
+
+static int
+i3cdev_do_priv_xfer(struct i3c_device *dev, struct i3c_ioc_priv_xfer *xfers,
+		    unsigned int nxfers)
+{
+	struct i3c_priv_xfer *k_xfers;
+	u8 **data_ptrs;
+	int i, ret = 0;
+
+	/* Since we have nxfers we may allocate k_xfer + *data_ptrs together */
+	k_xfers = kcalloc(nxfers, sizeof(*k_xfers) + sizeof(*data_ptrs),
+			  GFP_KERNEL);
+	if (!k_xfers)
+		return -ENOMEM;
+
+	/* set data_ptrs to be after nxfers * i3c_priv_xfer */
+	data_ptrs = (void *)k_xfers + (nxfers * sizeof(*k_xfers));
+
+	for (i = 0; i < nxfers; i++) {
+		data_ptrs[i] = memdup_user((const u8 __user *)
+					   (uintptr_t)xfers[i].data,
+					   xfers[i].len);
+		if (IS_ERR(data_ptrs[i])) {
+			ret = PTR_ERR(data_ptrs[i]);
+			break;
+		}
+
+		k_xfers[i].len = xfers[i].len;
+		if (xfers[i].rnw) {
+			k_xfers[i].rnw = true;
+			k_xfers[i].data.in = data_ptrs[i];
+		} else {
+			k_xfers[i].rnw = false;
+			k_xfers[i].data.out = data_ptrs[i];
+		}
+	}
+
+	if (ret < 0) {
+		i--;
+		goto err_free_mem;
+	}
+
+	ret = i3c_device_do_priv_xfers(dev, k_xfers, nxfers);
+	if (ret)
+		goto err_free_mem;
+
+	for (i = 0; i < nxfers; i++) {
+		if (xfers[i].rnw) {
+			if (copy_to_user(u64_to_user_ptr(xfers[i].data),
+					 data_ptrs[i], xfers[i].len))
+				ret = -EFAULT;
+		}
+	}
+
+err_free_mem:
+	for (; i >= 0; i--)
+		kfree(data_ptrs[i]);
+	kfree(k_xfers);
+	return ret;
+}
+
+static int
+i3cdev_send_hdr_xfer(struct i3c_device *dev, struct i3c_ioc_priv_xfer *xfers,
+		    unsigned int nxfers)
+{
+	struct i3c_hdr_cmd *k_xfers;
+	u8 **data_ptrs;
+	u16 xfer_len;
+	int i, ret = 0;
+
+	/* Since we have nxfers we may allocate k_xfer + *data_ptrs together */
+	k_xfers = kcalloc(nxfers, sizeof(*k_xfers) + sizeof(*data_ptrs),
+			  GFP_KERNEL);
+	if (!k_xfers)
+		return -ENOMEM;
+
+	/* set data_ptrs to be after nxfers * i3c_priv_xfer */
+	data_ptrs = (void *)k_xfers + (nxfers * sizeof(*k_xfers));
+
+	for (i = 0; i < nxfers; i++) {
+		xfer_len = roundup(xfers[i].len, 2);
+		data_ptrs[i] = kzalloc(xfer_len, GFP_KERNEL);
+		if (!data_ptrs[i])
+			return -ENOMEM;
+		if (copy_from_user(data_ptrs[i],
+				   (const u8 __user *)(uintptr_t)xfers[i].data,
+				   xfers[i].len)) {
+			kfree(data_ptrs[i]);
+			return -EFAULT;
+		}
+		if (IS_ERR(data_ptrs[i])) {
+			ret = PTR_ERR(data_ptrs[i]);
+			break;
+		}
+		k_xfers[i].mode = I3C_HDR_DDR;
+		k_xfers[i].ndatawords = DIV_ROUND_UP(xfers[i].len, 2);
+		if (xfers[i].rnw) {
+			k_xfers[i].code = 0x80;
+			k_xfers[i].data.in = data_ptrs[i];
+		} else {
+			k_xfers[i].code = 0;
+			k_xfers[i].data.out = data_ptrs[i];
+		}
+	}
+
+	if (ret < 0) {
+		i--;
+		goto err_free_mem;
+	}
+
+	ret = i3c_device_send_hdr_cmds(dev, k_xfers, nxfers);
+	if (ret)
+		goto err_free_mem;
+
+	for (i = 0; i < nxfers; i++) {
+		if (xfers[i].rnw) {
+			if (copy_to_user(u64_to_user_ptr(xfers[i].data),
+					 data_ptrs[i], xfers[i].len))
+				ret = -EFAULT;
+		}
+	}
+
+err_free_mem:
+	for (; i >= 0; i--)
+		kfree(data_ptrs[i]);
+	kfree(k_xfers);
+	return ret;
+}
+
+static struct i3c_ioc_priv_xfer *
+i3cdev_get_ioc_priv_xfer(unsigned int cmd, struct i3c_ioc_priv_xfer *u_xfers,
+			 unsigned int *nxfers)
+{
+	u32 tmp = _IOC_SIZE(cmd);
+
+	if ((tmp % sizeof(struct i3c_ioc_priv_xfer)) != 0)
+		return ERR_PTR(-EINVAL);
+
+	*nxfers = tmp / sizeof(struct i3c_ioc_priv_xfer);
+	if (*nxfers == 0)
+		return ERR_PTR(-EINVAL);
+
+	return memdup_user(u_xfers, tmp);
+}
+
+static int
+i3cdev_ioc_priv_xfer(struct i3c_device *i3c, unsigned int cmd,
+		     struct i3c_ioc_priv_xfer *u_xfers)
+{
+	struct i3c_ioc_priv_xfer *k_xfers;
+	unsigned int nxfers;
+	int ret;
+
+	k_xfers = i3cdev_get_ioc_priv_xfer(cmd, u_xfers, &nxfers);
+	if (IS_ERR(k_xfers))
+		return PTR_ERR(k_xfers);
+
+	if (i3c->desc->info.hdr_cap & BIT(I3C_HDR_DDR) &&
+	    IS_ENABLED(CONFIG_I3CDEV_XFER_HDR_DDR))
+		ret = i3cdev_send_hdr_xfer(i3c, k_xfers, nxfers);
+	else
+		ret = i3cdev_do_priv_xfer(i3c, k_xfers, nxfers);
+
+	kfree(k_xfers);
+
+	return ret;
+}
+
+static long
+i3cdev_ioctl(struct file *file, unsigned int cmd, unsigned long arg)
+{
+	struct i3cdev_data *i3cdev = file->private_data;
+	struct i3c_device *i3c = i3cdev->i3c;
+	int ret = -EACCES;
+
+	dev_dbg(&i3c->dev, "ioctl, cmd=0x%02x, arg=0x%02lx\n", cmd, arg);
+
+	if (_IOC_TYPE(cmd) != I3C_DEV_IOC_MAGIC)
+		return -ENOTTY;
+
+	/* Use the xfer_lock to prevent device detach during ioctl call */
+	mutex_lock(&i3cdev->xfer_lock);
+	if (!IS_ENABLED(CONFIG_I3CDEV_FORCE_CREATE) && i3c->dev.driver)
+		goto err_no_dev;
+
+	/* Check command number and direction */
+	if (_IOC_NR(cmd) == _IOC_NR(I3C_IOC_PRIV_XFER(0)) &&
+	    _IOC_DIR(cmd) == (_IOC_READ | _IOC_WRITE))
+		ret = i3cdev_ioc_priv_xfer(i3c, cmd,
+					(struct i3c_ioc_priv_xfer __user *)arg);
+
+err_no_dev:
+	mutex_unlock(&i3cdev->xfer_lock);
+	return ret;
+}
+
+static int i3cdev_open(struct inode *inode, struct file *file)
+{
+	struct i3cdev_data *i3cdev = container_of(inode->i_cdev,
+						  struct i3cdev_data,
+						  cdev);
+	file->private_data = i3cdev;
+
+	return 0;
+}
+
+static int i3cdev_release(struct inode *inode, struct file *file)
+{
+	file->private_data = NULL;
+
+	return 0;
+}
+
+static const struct file_operations i3cdev_fops = {
+	.owner		= THIS_MODULE,
+	.read		= i3cdev_read,
+	.write		= i3cdev_write,
+	.unlocked_ioctl	= i3cdev_ioctl,
+	.compat_ioctl	= compat_ptr_ioctl,
+	.open		= i3cdev_open,
+	.release	= i3cdev_release,
+};
+
+/* ------------------------------------------------------------------------- */
+
+static struct class *i3cdev_class;
+
+static int i3cdev_attach(struct device *dev, void *dummy)
+{
+	struct i3cdev_data *i3cdev;
+	struct i3c_device *i3c;
+	int res;
+
+	if (dev->type == &i3c_masterdev_type)
+		return 0;
+
+	if (!IS_ENABLED(CONFIG_I3CDEV_FORCE_CREATE) && dev->driver)
+		return 0;
+
+	i3c = dev_to_i3cdev(dev);
+
+	/* Get a device */
+	i3cdev = get_free_i3cdev(i3c);
+	if (IS_ERR(i3cdev))
+		return PTR_ERR(i3cdev);
+
+	mutex_init(&i3cdev->xfer_lock);
+	cdev_init(&i3cdev->cdev, &i3cdev_fops);
+	i3cdev->cdev.owner = THIS_MODULE;
+	res = cdev_add(&i3cdev->cdev,
+		       MKDEV(MAJOR(i3cdev_number), i3cdev->id), 1);
+	if (res)
+		goto error_cdev;
+
+	/* register this i3c device with the driver core */
+	i3cdev->dev = device_create(i3cdev_class, &i3c->dev,
+				    MKDEV(MAJOR(i3cdev_number), i3cdev->id),
+				    NULL, "bus!i3c!%s", dev_name(&i3c->dev));
+	if (IS_ERR(i3cdev->dev)) {
+		res = PTR_ERR(i3cdev->dev);
+		goto error;
+	}
+	pr_debug("i3cdev: I3C device [%s] registered as minor %d\n",
+		 dev_name(&i3c->dev), i3cdev->id);
+	return 0;
+
+error:
+	cdev_del(&i3cdev->cdev);
+error_cdev:
+	put_i3cdev(i3cdev);
+	return res;
+}
+
+static int i3cdev_detach(struct device *dev, void *dummy)
+{
+	struct i3cdev_data *i3cdev;
+	struct i3c_device *i3c;
+
+	if (dev->type == &i3c_masterdev_type)
+		return 0;
+
+	i3c = dev_to_i3cdev(dev);
+
+	i3cdev = i3cdev_get_drvdata(i3c);
+	if (!i3cdev)
+		return 0;
+
+	/* Prevent transfers while cdev removal */
+	mutex_lock(&i3cdev->xfer_lock);
+	cdev_del(&i3cdev->cdev);
+	device_destroy(i3cdev_class, MKDEV(MAJOR(i3cdev_number), i3cdev->id));
+	mutex_unlock(&i3cdev->xfer_lock);
+
+	ida_simple_remove(&i3cdev_ida, i3cdev->id);
+	put_i3cdev(i3cdev);
+
+	pr_debug("i3cdev: device [%s] unregistered\n", dev_name(&i3c->dev));
+
+	return 0;
+}
+
+static int i3cdev_notifier_call(struct notifier_block *nb,
+				unsigned long action,
+				void *data)
+{
+	struct device *dev = data;
+
+	switch (action) {
+	case BUS_NOTIFY_ADD_DEVICE:
+	case BUS_NOTIFY_UNBOUND_DRIVER:
+		return i3cdev_attach(dev, NULL);
+	case BUS_NOTIFY_BIND_DRIVER:
+		if (IS_ENABLED(CONFIG_I3CDEV_FORCE_CREATE))
+			break;
+
+		fallthrough;
+	case BUS_NOTIFY_DEL_DEVICE:
+	case BUS_NOTIFY_REMOVED_DEVICE:
+		return i3cdev_detach(dev, NULL);
+	}
+
+	return 0;
+}
+
+static struct notifier_block i3cdev_notifier = {
+	.notifier_call = i3cdev_notifier_call,
+};
+
+static int __init i3cdev_init(void)
+{
+	int res;
+
+	/* Dynamically request unused major number */
+	res = alloc_chrdev_region(&i3cdev_number, 0, I3C_MINORS, "i3c");
+	if (res)
+		goto out;
+
+	/* Create a classe to populate sysfs entries*/
+	i3cdev_class = class_create(THIS_MODULE, "i3cdev");
+	if (IS_ERR(i3cdev_class)) {
+		res = PTR_ERR(i3cdev_class);
+		goto out_unreg_chrdev;
+	}
+
+	/* Keep track of busses which have devices to add or remove later */
+	res = bus_register_notifier(&i3c_bus_type, &i3cdev_notifier);
+	if (res)
+		goto out_unreg_class;
+
+	/* Bind to already existing device without driver right away */
+	i3c_for_each_dev(NULL, i3cdev_attach);
+
+	return 0;
+
+out_unreg_class:
+	class_destroy(i3cdev_class);
+out_unreg_chrdev:
+	unregister_chrdev_region(i3cdev_number, I3C_MINORS);
+out:
+	pr_err("%s: Driver Initialisation failed\n", __FILE__);
+	return res;
+}
+
+static void __exit i3cdev_exit(void)
+{
+	bus_unregister_notifier(&i3c_bus_type, &i3cdev_notifier);
+	i3c_for_each_dev(NULL, i3cdev_detach);
+	class_destroy(i3cdev_class);
+	unregister_chrdev_region(i3cdev_number, I3C_MINORS);
+}
+
+MODULE_AUTHOR("Vitor Soares <soares@synopsys.com>");
+MODULE_DESCRIPTION("I3C /dev entries driver");
+MODULE_LICENSE("GPL");
+
+module_init(i3cdev_init);
+module_exit(i3cdev_exit);
diff --git a/drivers/i3c/internals.h b/drivers/i3c/internals.h
index 86b7b44cfca2..a9c6979b31ca 100644
--- a/drivers/i3c/internals.h
+++ b/drivers/i3c/internals.h
@@ -9,8 +9,10 @@
 #define I3C_INTERNALS_H
 
 #include <linux/i3c/master.h>
+#include <linux/i3c/target.h>
 
 extern struct bus_type i3c_bus_type;
+extern const struct device_type i3c_masterdev_type;
 
 void i3c_bus_normaluse_lock(struct i3c_bus *bus);
 void i3c_bus_normaluse_unlock(struct i3c_bus *bus);
@@ -18,9 +20,22 @@ void i3c_bus_normaluse_unlock(struct i3c_bus *bus);
 int i3c_dev_do_priv_xfers_locked(struct i3c_dev_desc *dev,
 				 struct i3c_priv_xfer *xfers,
 				 int nxfers);
+int i3c_master_send_hdr_cmds_locked(struct i3c_master_controller *master,
+				    struct i3c_hdr_cmd *cmds, int ncmds);
 int i3c_dev_disable_ibi_locked(struct i3c_dev_desc *dev);
 int i3c_dev_enable_ibi_locked(struct i3c_dev_desc *dev);
 int i3c_dev_request_ibi_locked(struct i3c_dev_desc *dev,
 			       const struct i3c_ibi_setup *req);
 void i3c_dev_free_ibi_locked(struct i3c_dev_desc *dev);
+int i3c_dev_send_ccc_cmd_locked(struct i3c_dev_desc *dev, u8 ccc_id);
+int i3c_dev_getstatus_locked(struct i3c_dev_desc *dev, struct i3c_device_info *info);
+int i3c_master_getmrl_locked(struct i3c_master_controller *master, struct i3c_device_info *info);
+int i3c_master_getmwl_locked(struct i3c_master_controller *master, struct i3c_device_info *info);
+int i3c_master_setmrl_locked(struct i3c_master_controller *master,
+			     struct i3c_device_info *info, u16 read_len, u8 ibi_len);
+int i3c_master_setmwl_locked(struct i3c_master_controller *master,
+			     struct i3c_device_info *info, u16 write_len);
+int i3c_for_each_dev(void *data, int (*fn)(struct device *, void *));
+int i3c_dev_generate_ibi_locked(struct i3c_dev_desc *dev, const u8 *data, int len);
+int i3c_dev_control_pec(struct i3c_dev_desc *dev, bool pec);
 #endif /* I3C_INTERNAL_H */
diff --git a/drivers/i3c/master.c b/drivers/i3c/master.c
index dfe18dcd008d..f45569816493 100644
--- a/drivers/i3c/master.c
+++ b/drivers/i3c/master.c
@@ -262,6 +262,24 @@ static ssize_t modalias_show(struct device *dev,
 }
 static DEVICE_ATTR_RO(modalias);
 
+static ssize_t bus_reset_store(struct device *dev, struct device_attribute *da,
+			       const char *buf, size_t count)
+{
+	struct i3c_master_controller *master;
+	ssize_t ret = count;
+
+	master = dev_to_i3cmaster(dev);
+	dev_dbg(&master->dev, "Reset bus to return to i2c_mode...\n");
+	i3c_bus_maintenance_lock(&master->bus);
+	if (master->ops->bus_reset)
+		master->ops->bus_reset(master);
+
+	i3c_bus_maintenance_unlock(&master->bus);
+
+	return ret;
+}
+static DEVICE_ATTR_WO(bus_reset);
+
 static struct attribute *i3c_device_attrs[] = {
 	&dev_attr_bcr.attr,
 	&dev_attr_dcr.attr,
@@ -298,19 +316,24 @@ static const struct device_type i3c_device_type = {
 	.uevent = i3c_device_uevent,
 };
 
+const struct device_type i3c_target_device_type = {
+};
+
 static int i3c_device_match(struct device *dev, struct device_driver *drv)
 {
 	struct i3c_device *i3cdev;
 	struct i3c_driver *i3cdrv;
 
-	if (dev->type != &i3c_device_type)
+	if (dev->type != &i3c_device_type && dev->type != &i3c_target_device_type)
 		return 0;
 
 	i3cdev = dev_to_i3cdev(dev);
 	i3cdrv = drv_to_i3cdrv(drv);
-	if (i3c_device_match_id(i3cdev, i3cdrv->id_table))
-		return 1;
 
+	if ((dev->type == &i3c_device_type && !i3cdrv->target) ||
+	    (dev->type == &i3c_target_device_type && i3cdrv->target))
+		if (i3c_device_match_id(i3cdev, i3cdrv->id_table))
+			return 1;
 	return 0;
 }
 
@@ -330,7 +353,8 @@ static void i3c_device_remove(struct device *dev)
 	if (driver->remove)
 		driver->remove(i3cdev);
 
-	i3c_device_free_ibi(i3cdev);
+	if (!driver->target)
+		i3c_device_free_ibi(i3cdev);
 }
 
 struct bus_type i3c_bus_type = {
@@ -339,6 +363,7 @@ struct bus_type i3c_bus_type = {
 	.probe = i3c_device_probe,
 	.remove = i3c_device_remove,
 };
+EXPORT_SYMBOL_GPL(i3c_bus_type);
 
 static enum i3c_addr_slot_status
 i3c_bus_get_addr_slot_status(struct i3c_bus *bus, u16 addr)
@@ -524,6 +549,7 @@ static struct attribute *i3c_masterdev_attrs[] = {
 	&dev_attr_pid.attr,
 	&dev_attr_dynamic_address.attr,
 	&dev_attr_hdrcap.attr,
+	&dev_attr_bus_reset.attr,
 	NULL,
 };
 ATTRIBUTE_GROUPS(i3c_masterdev);
@@ -542,9 +568,10 @@ static void i3c_masterdev_release(struct device *dev)
 	of_node_put(dev->of_node);
 }
 
-static const struct device_type i3c_masterdev_type = {
+const struct device_type i3c_masterdev_type = {
 	.groups	= i3c_masterdev_groups,
 };
+EXPORT_SYMBOL_GPL(i3c_masterdev_type);
 
 static int i3c_bus_set_mode(struct i3c_bus *i3cbus, enum i3c_bus_mode mode,
 			    unsigned long max_i2c_scl_rate)
@@ -609,7 +636,7 @@ static void i3c_master_free_i2c_dev(struct i2c_dev_desc *dev)
 
 static struct i2c_dev_desc *
 i3c_master_alloc_i2c_dev(struct i3c_master_controller *master,
-			 const struct i2c_dev_boardinfo *boardinfo)
+			 u16 addr, u8 lvr)
 {
 	struct i2c_dev_desc *dev;
 
@@ -618,9 +645,8 @@ i3c_master_alloc_i2c_dev(struct i3c_master_controller *master,
 		return ERR_PTR(-ENOMEM);
 
 	dev->common.master = master;
-	dev->boardinfo = boardinfo;
-	dev->addr = boardinfo->base.addr;
-	dev->lvr = boardinfo->lvr;
+	dev->addr = addr;
+	dev->lvr = lvr;
 
 	return dev;
 }
@@ -645,13 +671,15 @@ static void i3c_ccc_cmd_dest_cleanup(struct i3c_ccc_cmd_dest *dest)
 
 static void i3c_ccc_cmd_init(struct i3c_ccc_cmd *cmd, bool rnw, u8 id,
 			     struct i3c_ccc_cmd_dest *dests,
-			     unsigned int ndests)
+			     unsigned int ndests, bool dbp, u8 db)
 {
 	cmd->rnw = rnw ? 1 : 0;
 	cmd->id = id;
 	cmd->dests = dests;
 	cmd->ndests = ndests;
 	cmd->err = I3C_ERROR_UNKNOWN;
+	cmd->dbp = dbp;
+	cmd->db = db;
 }
 
 static int i3c_master_send_ccc_cmd_locked(struct i3c_master_controller *master,
@@ -694,7 +722,7 @@ i3c_master_find_i2c_dev_by_addr(const struct i3c_master_controller *master,
 	struct i2c_dev_desc *dev;
 
 	i3c_bus_for_each_i2cdev(&master->bus, dev) {
-		if (dev->boardinfo->base.addr == addr)
+		if (dev->addr == addr)
 			return dev;
 	}
 
@@ -750,7 +778,7 @@ i3c_master_alloc_i3c_dev(struct i3c_master_controller *master,
 	return dev;
 }
 
-static int i3c_master_rstdaa_locked(struct i3c_master_controller *master,
+int i3c_master_rstdaa_locked(struct i3c_master_controller *master,
 				    u8 addr)
 {
 	enum i3c_addr_slot_status addrstat;
@@ -768,12 +796,13 @@ static int i3c_master_rstdaa_locked(struct i3c_master_controller *master,
 	i3c_ccc_cmd_dest_init(&dest, addr, 0);
 	i3c_ccc_cmd_init(&cmd, false,
 			 I3C_CCC_RSTDAA(addr == I3C_BROADCAST_ADDR),
-			 &dest, 1);
+			 &dest, 1, false, 0);
 	ret = i3c_master_send_ccc_cmd_locked(master, &cmd);
 	i3c_ccc_cmd_dest_cleanup(&dest);
 
 	return ret;
 }
+EXPORT_SYMBOL_GPL(i3c_master_rstdaa_locked);
 
 /**
  * i3c_master_entdaa_locked() - start a DAA (Dynamic Address Assignment)
@@ -798,7 +827,7 @@ int i3c_master_entdaa_locked(struct i3c_master_controller *master)
 	int ret;
 
 	i3c_ccc_cmd_dest_init(&dest, I3C_BROADCAST_ADDR, 0);
-	i3c_ccc_cmd_init(&cmd, false, I3C_CCC_ENTDAA, &dest, 1);
+	i3c_ccc_cmd_init(&cmd, false, I3C_CCC_ENTDAA, &dest, 1, false, 0);
 	ret = i3c_master_send_ccc_cmd_locked(master, &cmd);
 	i3c_ccc_cmd_dest_cleanup(&dest);
 
@@ -823,7 +852,7 @@ static int i3c_master_enec_disec_locked(struct i3c_master_controller *master,
 			 enable ?
 			 I3C_CCC_ENEC(addr == I3C_BROADCAST_ADDR) :
 			 I3C_CCC_DISEC(addr == I3C_BROADCAST_ADDR),
-			 &dest, 1);
+			 &dest, 1, false, 0);
 	ret = i3c_master_send_ccc_cmd_locked(master, &cmd);
 	i3c_ccc_cmd_dest_cleanup(&dest);
 
@@ -956,7 +985,7 @@ int i3c_master_defslvs_locked(struct i3c_master_controller *master)
 		desc++;
 	}
 
-	i3c_ccc_cmd_init(&cmd, false, I3C_CCC_DEFSLVS, &dest, 1);
+	i3c_ccc_cmd_init(&cmd, false, I3C_CCC_DEFSLVS, &dest, 1, false, 0);
 	ret = i3c_master_send_ccc_cmd_locked(master, &cmd);
 	i3c_ccc_cmd_dest_cleanup(&dest);
 
@@ -982,7 +1011,22 @@ static int i3c_master_setda_locked(struct i3c_master_controller *master,
 	setda->addr = newaddr << 1;
 	i3c_ccc_cmd_init(&cmd, false,
 			 setdasa ? I3C_CCC_SETDASA : I3C_CCC_SETNEWDA,
-			 &dest, 1);
+			 &dest, 1, false, 0);
+	ret = i3c_master_send_ccc_cmd_locked(master, &cmd);
+	i3c_ccc_cmd_dest_cleanup(&dest);
+
+	return ret;
+}
+
+static int i3c_master_setaasa_locked(struct i3c_master_controller *master)
+{
+	struct i3c_ccc_cmd_dest dest;
+	struct i3c_ccc_cmd cmd;
+	int ret;
+
+	i3c_ccc_cmd_dest_init(&dest, I3C_BROADCAST_ADDR, 0);
+	i3c_ccc_cmd_init(&cmd, false, I3C_CCC_SETAASA, &dest, 1, false, 0);
+
 	ret = i3c_master_send_ccc_cmd_locked(master, &cmd);
 	i3c_ccc_cmd_dest_cleanup(&dest);
 
@@ -1001,8 +1045,27 @@ static int i3c_master_setnewda_locked(struct i3c_master_controller *master,
 	return i3c_master_setda_locked(master, oldaddr, newaddr, false);
 }
 
-static int i3c_master_getmrl_locked(struct i3c_master_controller *master,
-				    struct i3c_device_info *info)
+static int i3c_master_sethid_locked(struct i3c_master_controller *master)
+{
+	struct i3c_ccc_cmd_dest dest;
+	struct i3c_ccc_cmd cmd;
+	struct i3c_ccc_sethid *sethid;
+	int ret;
+
+	sethid = i3c_ccc_cmd_dest_init(&dest, I3C_BROADCAST_ADDR, 1);
+	if (!sethid)
+		return -ENOMEM;
+
+	sethid->hid = 0;
+	i3c_ccc_cmd_init(&cmd, false, I3C_CCC_SETHID, &dest, 1, false, 0);
+
+	ret = i3c_master_send_ccc_cmd_locked(master, &cmd);
+	i3c_ccc_cmd_dest_cleanup(&dest);
+
+	return ret;
+}
+
+int i3c_master_getmrl_locked(struct i3c_master_controller *master, struct i3c_device_info *info)
 {
 	struct i3c_ccc_cmd_dest dest;
 	struct i3c_ccc_mrl *mrl;
@@ -1020,7 +1083,7 @@ static int i3c_master_getmrl_locked(struct i3c_master_controller *master,
 	if (!(info->bcr & I3C_BCR_IBI_PAYLOAD))
 		dest.payload.len -= 1;
 
-	i3c_ccc_cmd_init(&cmd, true, I3C_CCC_GETMRL, &dest, 1);
+	i3c_ccc_cmd_init(&cmd, true, I3C_CCC_GETMRL, &dest, 1, false, 0);
 	ret = i3c_master_send_ccc_cmd_locked(master, &cmd);
 	if (ret)
 		goto out;
@@ -1043,8 +1106,7 @@ static int i3c_master_getmrl_locked(struct i3c_master_controller *master,
 	return ret;
 }
 
-static int i3c_master_getmwl_locked(struct i3c_master_controller *master,
-				    struct i3c_device_info *info)
+int i3c_master_getmwl_locked(struct i3c_master_controller *master, struct i3c_device_info *info)
 {
 	struct i3c_ccc_cmd_dest dest;
 	struct i3c_ccc_mwl *mwl;
@@ -1055,7 +1117,7 @@ static int i3c_master_getmwl_locked(struct i3c_master_controller *master,
 	if (!mwl)
 		return -ENOMEM;
 
-	i3c_ccc_cmd_init(&cmd, true, I3C_CCC_GETMWL, &dest, 1);
+	i3c_ccc_cmd_init(&cmd, true, I3C_CCC_GETMWL, &dest, 1, false, 0);
 	ret = i3c_master_send_ccc_cmd_locked(master, &cmd);
 	if (ret)
 		goto out;
@@ -1073,6 +1135,61 @@ static int i3c_master_getmwl_locked(struct i3c_master_controller *master,
 	return ret;
 }
 
+int i3c_master_setmrl_locked(struct i3c_master_controller *master,
+			     struct i3c_device_info *info, u16 read_len, u8 ibi_len)
+{
+	struct i3c_ccc_cmd_dest dest;
+	struct i3c_ccc_cmd cmd;
+	struct i3c_ccc_mrl *mrl;
+	int ret;
+
+	mrl = i3c_ccc_cmd_dest_init(&dest, info->dyn_addr, sizeof(*mrl));
+	if (!mrl)
+		return -ENOMEM;
+
+	/*
+	 * When the device does not have IBI payload SETMRL only sends 2
+	 * bytes of data.
+	 */
+	if (!(info->bcr & I3C_BCR_IBI_PAYLOAD))
+		dest.payload.len -= 1;
+
+	mrl->read_len = cpu_to_be16(read_len);
+	mrl->ibi_len = ibi_len;
+	info->max_read_len = read_len;
+	info->max_ibi_len = mrl->ibi_len;
+	i3c_ccc_cmd_init(&cmd, false, I3C_CCC_SETMRL(false), &dest, 1, false,
+			 0);
+
+	ret = i3c_master_send_ccc_cmd_locked(master, &cmd);
+	i3c_ccc_cmd_dest_cleanup(&dest);
+
+	return ret;
+}
+
+int i3c_master_setmwl_locked(struct i3c_master_controller *master,
+			     struct i3c_device_info *info, u16 write_len)
+{
+	struct i3c_ccc_cmd_dest dest;
+	struct i3c_ccc_cmd cmd;
+	struct i3c_ccc_mwl *mwl;
+	int ret;
+
+	mwl = i3c_ccc_cmd_dest_init(&dest, info->dyn_addr, sizeof(*mwl));
+	if (!mwl)
+		return -ENOMEM;
+
+	mwl->len = cpu_to_be16(write_len);
+	info->max_write_len = write_len;
+	i3c_ccc_cmd_init(&cmd, false, I3C_CCC_SETMWL(false), &dest, 1, false,
+			 0);
+
+	ret = i3c_master_send_ccc_cmd_locked(master, &cmd);
+	i3c_ccc_cmd_dest_cleanup(&dest);
+
+	return ret;
+}
+
 static int i3c_master_getmxds_locked(struct i3c_master_controller *master,
 				     struct i3c_device_info *info)
 {
@@ -1086,7 +1203,7 @@ static int i3c_master_getmxds_locked(struct i3c_master_controller *master,
 	if (!getmaxds)
 		return -ENOMEM;
 
-	i3c_ccc_cmd_init(&cmd, true, I3C_CCC_GETMXDS, &dest, 1);
+	i3c_ccc_cmd_init(&cmd, true, I3C_CCC_GETMXDS, &dest, 1, false, 0);
 	ret = i3c_master_send_ccc_cmd_locked(master, &cmd);
 	if (ret)
 		goto out;
@@ -1122,7 +1239,7 @@ static int i3c_master_gethdrcap_locked(struct i3c_master_controller *master,
 	if (!gethdrcap)
 		return -ENOMEM;
 
-	i3c_ccc_cmd_init(&cmd, true, I3C_CCC_GETHDRCAP, &dest, 1);
+	i3c_ccc_cmd_init(&cmd, true, I3C_CCC_GETHDRCAP, &dest, 1, false, 0);
 	ret = i3c_master_send_ccc_cmd_locked(master, &cmd);
 	if (ret)
 		goto out;
@@ -1152,7 +1269,7 @@ static int i3c_master_getpid_locked(struct i3c_master_controller *master,
 	if (!getpid)
 		return -ENOMEM;
 
-	i3c_ccc_cmd_init(&cmd, true, I3C_CCC_GETPID, &dest, 1);
+	i3c_ccc_cmd_init(&cmd, true, I3C_CCC_GETPID, &dest, 1, false, 0);
 	ret = i3c_master_send_ccc_cmd_locked(master, &cmd);
 	if (ret)
 		goto out;
@@ -1182,7 +1299,7 @@ static int i3c_master_getbcr_locked(struct i3c_master_controller *master,
 	if (!getbcr)
 		return -ENOMEM;
 
-	i3c_ccc_cmd_init(&cmd, true, I3C_CCC_GETBCR, &dest, 1);
+	i3c_ccc_cmd_init(&cmd, true, I3C_CCC_GETBCR, &dest, 1, false, 0);
 	ret = i3c_master_send_ccc_cmd_locked(master, &cmd);
 	if (ret)
 		goto out;
@@ -1207,7 +1324,7 @@ static int i3c_master_getdcr_locked(struct i3c_master_controller *master,
 	if (!getdcr)
 		return -ENOMEM;
 
-	i3c_ccc_cmd_init(&cmd, true, I3C_CCC_GETDCR, &dest, 1);
+	i3c_ccc_cmd_init(&cmd, true, I3C_CCC_GETDCR, &dest, 1, false, 0);
 	ret = i3c_master_send_ccc_cmd_locked(master, &cmd);
 	if (ret)
 		goto out;
@@ -1220,6 +1337,32 @@ static int i3c_master_getdcr_locked(struct i3c_master_controller *master,
 	return ret;
 }
 
+int i3c_dev_getstatus_locked(struct i3c_dev_desc *dev,
+			     struct i3c_device_info *info)
+{
+	struct i3c_master_controller *master = i3c_dev_get_master(dev);
+	struct i3c_ccc_getstatus *getsts;
+	struct i3c_ccc_cmd_dest dest;
+	struct i3c_ccc_cmd cmd;
+	int ret;
+
+	getsts = i3c_ccc_cmd_dest_init(&dest, info->dyn_addr, sizeof(*getsts));
+	if (!getsts)
+		return -ENOMEM;
+
+	i3c_ccc_cmd_init(&cmd, true, I3C_CCC_GETSTATUS, &dest, 1, false, 0);
+	ret = i3c_master_send_ccc_cmd_locked(master, &cmd);
+	if (ret)
+		goto out;
+
+	info->status = getsts->status;
+
+out:
+	i3c_ccc_cmd_dest_cleanup(&dest);
+
+	return ret;
+}
+
 static int i3c_master_retrieve_dev_info(struct i3c_dev_desc *dev)
 {
 	struct i3c_master_controller *master = i3c_dev_get_master(dev);
@@ -1235,6 +1378,13 @@ static int i3c_master_retrieve_dev_info(struct i3c_dev_desc *dev)
 	    slot_status == I3C_ADDR_SLOT_I2C_DEV)
 		return -EINVAL;
 
+	if (master->jdec_spd && dev->boardinfo) {
+		dev->info.pid = dev->boardinfo->pid;
+		dev->info.dcr = dev->boardinfo->dcr;
+		dev->info.bcr = dev->boardinfo->bcr;
+		return 0;
+	}
+
 	ret = i3c_master_getpid_locked(master, &dev->info);
 	if (ret)
 		return ret;
@@ -1380,6 +1530,9 @@ static int i3c_master_reattach_i3c_dev(struct i3c_dev_desc *dev,
 		i3c_bus_set_addr_slot_status(&master->bus,
 					     dev->info.dyn_addr,
 					     I3C_ADDR_SLOT_I3C_DEV);
+		if (old_dyn_addr)
+			i3c_bus_set_addr_slot_status(&master->bus, old_dyn_addr,
+						     I3C_ADDR_SLOT_FREE);
 	}
 
 	if (master->ops->reattach_i3c_dev) {
@@ -1450,10 +1603,17 @@ static int i3c_master_early_i3c_dev_add(struct i3c_master_controller *master,
 	if (ret)
 		goto err_free_dev;
 
-	ret = i3c_master_setdasa_locked(master, i3cdev->info.static_addr,
-					i3cdev->boardinfo->init_dyn_addr);
-	if (ret)
-		goto err_detach_dev;
+	/*
+	 * JESD403-1 devices only support SETAASA (will be called in do_daa)
+	 * Here we use SETDASA for non-JESD403-1 devices
+	 */
+	if (!I3C_DCR_IS_JESD403_COMPLIANT(i3cdev->boardinfo->dcr)) {
+		ret = i3c_master_setdasa_locked(
+			master, i3cdev->info.static_addr,
+			i3cdev->boardinfo->init_dyn_addr);
+		if (ret)
+			goto err_detach_dev;
+	}
 
 	i3cdev->info.dyn_addr = i3cdev->boardinfo->init_dyn_addr;
 	ret = i3c_master_reattach_i3c_dev(i3cdev, 0);
@@ -1532,7 +1692,11 @@ int i3c_master_do_daa(struct i3c_master_controller *master)
 	int ret;
 
 	i3c_bus_maintenance_lock(&master->bus);
-	ret = master->ops->do_daa(master);
+	if (master->jdec_spd) {
+		i3c_master_sethid_locked(master);
+		i3c_master_setaasa_locked(master);
+	} else
+		ret = master->ops->do_daa(master);
 	i3c_bus_maintenance_unlock(&master->bus);
 
 	if (ret)
@@ -1546,6 +1710,29 @@ int i3c_master_do_daa(struct i3c_master_controller *master)
 }
 EXPORT_SYMBOL_GPL(i3c_master_do_daa);
 
+/**
+ * i3c_master_enable_hj() - enable hot-join
+ * @master: master broadcast the enec ccc to enable hot-join.
+ *
+ * This function must be called after the master init done to satisfy
+ * the description "Hot-Join does not allow Targets to join the I3C
+ * Bus before the I3C Bus has been configured." in i3c specification.
+ *
+ * Return: a 0 in case of success, an negative error code otherwise.
+ */
+int i3c_master_enable_hj(struct i3c_master_controller *master)
+{
+	if (!master->init_done)
+		return -ENOPROTOOPT;
+
+	i3c_bus_maintenance_lock(&master->bus);
+	i3c_master_enec_locked(master, I3C_BROADCAST_ADDR, I3C_CCC_EVENT_HJ);
+	i3c_bus_maintenance_unlock(&master->bus);
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(i3c_master_enable_hj);
+
 /**
  * i3c_master_set_info() - set master device information
  * @master: master used to send frames on the bus
@@ -1579,10 +1766,6 @@ int i3c_master_set_info(struct i3c_master_controller *master,
 	if (!i3c_bus_dev_addr_is_avail(&master->bus, info->dyn_addr))
 		return -EINVAL;
 
-	if (I3C_BCR_DEVICE_ROLE(info->bcr) == I3C_BCR_I3C_MASTER &&
-	    master->secondary)
-		return -EINVAL;
-
 	if (master->this)
 		return -EINVAL;
 
@@ -1591,7 +1774,10 @@ int i3c_master_set_info(struct i3c_master_controller *master,
 		return PTR_ERR(i3cdev);
 
 	master->this = i3cdev;
-	master->bus.cur_master = master->this;
+	if (master->secondary)
+		master->bus.cur_master = NULL;
+	else
+		master->bus.cur_master = master->this;
 
 	ret = i3c_master_attach_i3c_dev(master, i3cdev);
 	if (ret)
@@ -1689,7 +1875,9 @@ static int i3c_master_bus_init(struct i3c_master_controller *master)
 					     i2cboardinfo->base.addr,
 					     I3C_ADDR_SLOT_I2C_DEV);
 
-		i2cdev = i3c_master_alloc_i2c_dev(master, i2cboardinfo);
+		i2cdev = i3c_master_alloc_i2c_dev(master,
+						  i2cboardinfo->base.addr,
+						  i2cboardinfo->lvr);
 		if (IS_ERR(i2cdev)) {
 			ret = PTR_ERR(i2cdev);
 			goto err_detach_devs;
@@ -1721,6 +1909,9 @@ static int i3c_master_bus_init(struct i3c_master_controller *master)
 		goto err_bus_cleanup;
 	}
 
+	if (master->secondary)
+		return 0;
+
 	/*
 	 * Reset all dynamic address that may have been assigned before
 	 * (assigned by the bootloader for example).
@@ -1759,9 +1950,15 @@ static int i3c_master_bus_init(struct i3c_master_controller *master)
 			goto err_rstdaa;
 		}
 
-		i3c_bus_set_addr_slot_status(&master->bus,
-					     i3cboardinfo->init_dyn_addr,
-					     I3C_ADDR_SLOT_I3C_DEV);
+		/*
+		 * If the static address equals to the assigned dynamic address,
+		 * don't reserve the address slot here, it will be set after the
+		 * DA has been assigned.
+		 */
+		if (i3cboardinfo->static_addr != i3cboardinfo->init_dyn_addr)
+			i3c_bus_set_addr_slot_status(
+				&master->bus, i3cboardinfo->init_dyn_addr,
+				I3C_ADDR_SLOT_I3C_DEV);
 
 		/*
 		 * Only try to create/attach devices that have a static
@@ -1775,9 +1972,18 @@ static int i3c_master_bus_init(struct i3c_master_controller *master)
 			i3c_master_early_i3c_dev_add(master, i3cboardinfo);
 	}
 
+	/*
+	 * Not support mix mode on JEDEC bus context. Here We only handle
+	 * the I2C part so simply return with success code.
+	 */
+	if (master->jdec_spd && master->bus.mode != I3C_BUS_MODE_PURE)
+		return 0;
+
 	ret = i3c_master_do_daa(master);
 	if (ret)
-		goto err_rstdaa;
+		dev_dbg(&master->dev,
+			"Failed to do DAA: %d. However, devices with static address can still be accessed\n",
+			ret);
 
 	return 0;
 
@@ -1900,6 +2106,16 @@ int i3c_master_add_i3c_dev_locked(struct i3c_master_controller *master,
 			i3c_dev_free_ibi_locked(olddev);
 		}
 		mutex_unlock(&olddev->ibi_lock);
+		if (olddev->info.max_ibi_len != newdev->info.max_ibi_len ||
+		    olddev->info.max_read_len != newdev->info.max_read_len)
+			i3c_master_setmrl_locked(master, &newdev->info,
+					      olddev->info.max_read_len,
+					      olddev->info.max_ibi_len);
+		if (olddev->info.max_write_len != newdev->info.max_write_len)
+			i3c_master_setmwl_locked(master, &newdev->info,
+					      olddev->info.max_write_len);
+		if (olddev->info.pec != newdev->info.pec)
+			i3c_device_control_pec(newdev->dev, olddev->info.pec);
 
 		old_dyn_addr = olddev->info.dyn_addr;
 
@@ -1907,10 +2123,6 @@ int i3c_master_add_i3c_dev_locked(struct i3c_master_controller *master,
 		i3c_master_free_i3c_dev(olddev);
 	}
 
-	ret = i3c_master_reattach_i3c_dev(newdev, old_dyn_addr);
-	if (ret)
-		goto err_detach_dev;
-
 	/*
 	 * Depending on our previous state, the expected dynamic address might
 	 * differ:
@@ -2031,6 +2243,8 @@ of_i3c_master_add_i3c_boardinfo(struct i3c_master_controller *master,
 	struct device *dev = &master->dev;
 	enum i3c_addr_slot_status addrstatus;
 	u32 init_dyn_addr = 0;
+	u32 bcr = 0;
+	u32 dcr = 0;
 
 	boardinfo = devm_kzalloc(dev, sizeof(*boardinfo), GFP_KERNEL);
 	if (!boardinfo)
@@ -2064,6 +2278,16 @@ of_i3c_master_add_i3c_boardinfo(struct i3c_master_controller *master,
 	    I3C_PID_RND_LOWER_32BITS(boardinfo->pid))
 		return -EINVAL;
 
+	if (!of_property_read_u32(node, "dcr", &dcr)) {
+		if (dcr > I3C_DCR_MAX)
+			return -EINVAL;
+
+		boardinfo->dcr = dcr;
+	}
+
+	if (!of_property_read_u32(node, "bcr", &bcr))
+		boardinfo->bcr = bcr;
+
 	boardinfo->init_dyn_addr = init_dyn_addr;
 	boardinfo->of_node = of_node_get(node);
 	list_add_tail(&boardinfo->node, &master->boardinfo.i3c);
@@ -2101,17 +2325,30 @@ static int of_populate_i3c_bus(struct i3c_master_controller *master)
 	struct device *dev = &master->dev;
 	struct device_node *i3cbus_np = dev->of_node;
 	struct device_node *node;
-	int ret;
+	int ret, i;
 	u32 val;
 
 	if (!i3cbus_np)
 		return 0;
 
+	if (of_get_property(i3cbus_np, "jdec-spd", NULL)) {
+		master->jdec_spd = 1;
+	}
+
+	/* For SPD bus, undo unnecessary address reservations. */
+	if (master->jdec_spd) {
+		for (i = 0; i < 7; i++)
+			i3c_bus_set_addr_slot_status(&master->bus, I3C_BROADCAST_ADDR ^ BIT(i),
+						     I3C_ADDR_SLOT_FREE);
+	}
+
 	for_each_available_child_of_node(i3cbus_np, node) {
-		ret = of_i3c_master_add_dev(master, node);
-		if (ret) {
-			of_node_put(node);
-			return ret;
+		if (node->name && of_node_cmp(node->name, "hub")) {
+			ret = of_i3c_master_add_dev(master, node);
+			if (ret) {
+				of_node_put(node);
+				return ret;
+			}
 		}
 	}
 
@@ -2166,15 +2403,127 @@ static u32 i3c_master_i2c_funcs(struct i2c_adapter *adapter)
 	return I2C_FUNC_SMBUS_EMUL | I2C_FUNC_I2C;
 }
 
+static u8 i3c_master_i2c_get_lvr(struct i2c_client *client)
+{
+	/* Fall back to no spike filters and FM bus mode. */
+	u8 lvr = I3C_LVR_I2C_INDEX(2) | I3C_LVR_I2C_FM_MODE;
+
+	if (client->dev.of_node) {
+		u32 reg[3];
+
+		if (!of_property_read_u32_array(client->dev.of_node, "reg",
+						reg, ARRAY_SIZE(reg)))
+			lvr = reg[2];
+	}
+
+	return lvr;
+}
+
+static int i3c_master_i2c_attach(struct i2c_adapter *adap, struct i2c_client *client)
+{
+	struct i3c_master_controller *master = i2c_adapter_to_i3c_master(adap);
+	enum i3c_addr_slot_status status;
+	struct i2c_dev_desc *i2cdev;
+	int ret;
+
+	/* Already added by board info? */
+	if (i3c_master_find_i2c_dev_by_addr(master, client->addr))
+		return 0;
+
+	status = i3c_bus_get_addr_slot_status(&master->bus, client->addr);
+	if (status != I3C_ADDR_SLOT_FREE)
+		return -EBUSY;
+
+	i3c_bus_set_addr_slot_status(&master->bus, client->addr,
+				     I3C_ADDR_SLOT_I2C_DEV);
+
+	i2cdev = i3c_master_alloc_i2c_dev(master, client->addr,
+					  i3c_master_i2c_get_lvr(client));
+	if (IS_ERR(i2cdev)) {
+		ret = PTR_ERR(i2cdev);
+		goto out_clear_status;
+	}
+
+	ret = i3c_master_attach_i2c_dev(master, i2cdev);
+	if (ret)
+		goto out_free_dev;
+
+	return 0;
+
+out_free_dev:
+	i3c_master_free_i2c_dev(i2cdev);
+out_clear_status:
+	i3c_bus_set_addr_slot_status(&master->bus, client->addr,
+				     I3C_ADDR_SLOT_FREE);
+
+	return ret;
+}
+
+static int i3c_master_i2c_detach(struct i2c_adapter *adap, struct i2c_client *client)
+{
+	struct i3c_master_controller *master = i2c_adapter_to_i3c_master(adap);
+	struct i2c_dev_desc *dev;
+
+	dev = i3c_master_find_i2c_dev_by_addr(master, client->addr);
+	if (!dev)
+		return -ENODEV;
+
+	i3c_master_detach_i2c_dev(dev);
+	i3c_bus_set_addr_slot_status(&master->bus, dev->addr,
+				     I3C_ADDR_SLOT_FREE);
+	i3c_master_free_i2c_dev(dev);
+
+	return 0;
+}
+
 static const struct i2c_algorithm i3c_master_i2c_algo = {
 	.master_xfer = i3c_master_i2c_adapter_xfer,
 	.functionality = i3c_master_i2c_funcs,
 };
 
+static int i3c_i2c_notifier_call(struct notifier_block *nb, unsigned long action,
+				 void *data)
+{
+	struct i2c_adapter *adap;
+	struct i2c_client *client;
+	struct device *dev = data;
+	struct i3c_master_controller *master;
+	int ret;
+
+	if (dev->type != &i2c_client_type)
+		return 0;
+
+	client = to_i2c_client(dev);
+	adap = client->adapter;
+
+	if (adap->algo != &i3c_master_i2c_algo)
+		return 0;
+
+	master = i2c_adapter_to_i3c_master(adap);
+
+	i3c_bus_maintenance_lock(&master->bus);
+	switch (action) {
+	case BUS_NOTIFY_ADD_DEVICE:
+		ret = i3c_master_i2c_attach(adap, client);
+		break;
+	case BUS_NOTIFY_DEL_DEVICE:
+		ret = i3c_master_i2c_detach(adap, client);
+		break;
+	}
+	i3c_bus_maintenance_unlock(&master->bus);
+
+	return ret;
+}
+
+static struct notifier_block i2cdev_notifier = {
+	.notifier_call = i3c_i2c_notifier_call,
+};
+
 static int i3c_master_i2c_adapter_init(struct i3c_master_controller *master)
 {
 	struct i2c_adapter *adap = i3c_master_to_i2c_adapter(master);
 	struct i2c_dev_desc *i2cdev;
+	struct i2c_dev_boardinfo *i2cboardinfo;
 	int ret;
 
 	adap->dev.parent = master->dev.parent;
@@ -2194,8 +2543,13 @@ static int i3c_master_i2c_adapter_init(struct i3c_master_controller *master)
 	 * We silently ignore failures here. The bus should keep working
 	 * correctly even if one or more i2c devices are not registered.
 	 */
-	i3c_bus_for_each_i2cdev(&master->bus, i2cdev)
-		i2cdev->dev = i2c_new_client_device(adap, &i2cdev->boardinfo->base);
+	list_for_each_entry(i2cboardinfo, &master->boardinfo.i2c, node) {
+		i2cdev = i3c_master_find_i2c_dev_by_addr(master,
+							 i2cboardinfo->base.addr);
+		if (WARN_ON(!i2cdev))
+			continue;
+		i2cdev->dev = i2c_new_client_device(adap, &i2cboardinfo->base);
+	}
 
 	return 0;
 }
@@ -2470,9 +2824,6 @@ int i3c_master_register(struct i3c_master_controller *master,
 	struct i2c_dev_boardinfo *i2cbi;
 	int ret;
 
-	/* We do not support secondary masters yet. */
-	if (secondary)
-		return -ENOTSUPP;
 
 	ret = i3c_master_check_ops(ops);
 	if (ret)
@@ -2555,6 +2906,15 @@ int i3c_master_register(struct i3c_master_controller *master,
 	master->init_done = true;
 	i3c_bus_normaluse_lock(&master->bus);
 	i3c_master_register_new_i3c_devs(master);
+#ifdef CONFIG_I3C_SLAVE_MQUEUE
+	if (master->secondary)
+		i3c_slave_mqueue_probe(master);
+#endif
+
+#ifdef CONFIG_I3C_SLAVE_EEPROM
+	if (master->secondary)
+		i3c_slave_eeprom_probe(master);
+#endif
 	i3c_bus_normaluse_unlock(&master->bus);
 
 	return 0;
@@ -2591,6 +2951,255 @@ int i3c_master_unregister(struct i3c_master_controller *master)
 }
 EXPORT_SYMBOL_GPL(i3c_master_unregister);
 
+static int i3c_target_bus_init(struct i3c_master_controller *master)
+{
+	return master->target_ops->bus_init(master);
+}
+
+static void i3c_target_bus_cleanup(struct i3c_master_controller *master)
+{
+	if (master->target_ops->bus_cleanup)
+		master->target_ops->bus_cleanup(master);
+}
+
+static void i3c_targetdev_release(struct device *dev)
+{
+	struct i3c_master_controller *master = container_of(dev, struct i3c_master_controller, dev);
+	struct i3c_bus *bus = &master->bus;
+
+	mutex_lock(&i3c_core_lock);
+	idr_remove(&i3c_bus_idr, bus->id);
+	mutex_unlock(&i3c_core_lock);
+
+	of_node_put(dev->of_node);
+}
+
+static void i3c_target_device_release(struct device *dev)
+{
+	struct i3c_device *i3cdev = dev_to_i3cdev(dev);
+	struct i3c_dev_desc *desc = i3cdev->desc;
+
+	kfree(i3cdev);
+	kfree(desc);
+}
+
+static void
+i3c_target_register_new_i3c_dev(struct i3c_master_controller *master, struct i3c_device_info info)
+{
+	struct i3c_dev_desc *desc;
+	int ret;
+
+	desc = kzalloc(sizeof(*desc), GFP_KERNEL);
+	if (!desc)
+		return;
+
+	desc->dev = kzalloc(sizeof(*desc->dev), GFP_KERNEL);
+	if (!desc->dev) {
+		kfree(desc);
+		return;
+	}
+
+	desc->dev->bus = &master->bus;
+	desc->dev->desc = desc;
+	desc->dev->dev.parent = &master->dev;
+	desc->dev->dev.type = &i3c_target_device_type;
+	desc->dev->dev.bus = &i3c_bus_type;
+	desc->dev->dev.release = i3c_target_device_release;
+	desc->info = info;
+	desc->common.master = master;
+	dev_set_name(&desc->dev->dev, "%d-target", master->bus.id);
+
+	ret = device_register(&desc->dev->dev);
+	if (ret)
+		dev_err(&master->dev, "Failed to add I3C target device (err = %d)\n", ret);
+
+	master->this = desc;
+}
+
+static void i3c_target_unregister_i3c_dev(struct i3c_master_controller *master)
+{
+	struct i3c_dev_desc *i3cdev = master->this;
+
+	if (device_is_registered(&i3cdev->dev->dev))
+		device_unregister(&i3cdev->dev->dev);
+	else
+		put_device(&i3cdev->dev->dev);
+}
+
+static void i3c_target_read_device_info(struct device_node *np, struct i3c_device_info *info)
+{
+	u64 pid;
+	u32 dcr;
+	int ret;
+
+	ret = of_property_read_u64(np, "pid", &pid);
+	if (ret)
+		info->pid = 0;
+	else
+		info->pid = pid;
+
+	ret = of_property_read_u32(np, "dcr", &dcr);
+	if (ret)
+		info->pid = 0;
+	else
+		info->dcr = dcr;
+}
+
+static int i3c_target_check_ops(const struct i3c_target_ops *ops)
+{
+	if (!ops || !ops->bus_init)
+		return -EINVAL;
+
+	return 0;
+}
+
+int i3c_target_register(struct i3c_master_controller *master, struct device *parent,
+			const struct i3c_target_ops *ops)
+{
+	struct i3c_bus *i3cbus = i3c_master_get_bus(master);
+	struct i3c_device_info info;
+	int ret;
+
+	ret = i3c_target_check_ops(ops);
+	if (ret)
+		return ret;
+
+	master->dev.parent = parent;
+	master->dev.of_node = of_node_get(parent->of_node);
+	master->dev.bus = &i3c_bus_type;
+	master->dev.release = i3c_targetdev_release;
+	master->target_ops = ops;
+	i3cbus->mode = I3C_BUS_MODE_PURE;
+
+	init_rwsem(&i3cbus->lock);
+	mutex_lock(&i3c_core_lock);
+	ret = idr_alloc(&i3c_bus_idr, i3cbus, 0, 0, GFP_KERNEL);
+	mutex_unlock(&i3c_core_lock);
+	if (ret < 0)
+		return ret;
+	i3cbus->id = ret;
+
+	device_initialize(&master->dev);
+	dev_set_name(&master->dev, "i3c-%d", i3cbus->id);
+
+	ret = device_add(&master->dev);
+	if (ret)
+		goto err_put_device;
+
+	i3c_target_read_device_info(master->dev.of_node, &info);
+
+	i3c_target_register_new_i3c_dev(master, info);
+
+	ret = i3c_target_bus_init(master);
+	if (ret)
+		goto err_cleanup_bus;
+
+	return 0;
+
+err_cleanup_bus:
+	i3c_target_bus_cleanup(master);
+
+err_put_device:
+	put_device(&master->dev);
+
+	return ret;
+}
+EXPORT_SYMBOL_GPL(i3c_target_register);
+
+int i3c_target_unregister(struct i3c_master_controller *master)
+{
+	i3c_target_unregister_i3c_dev(master);
+	i3c_target_bus_cleanup(master);
+	device_unregister(&master->dev);
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(i3c_target_unregister);
+
+int i3c_target_read_register(struct i3c_device *dev, const struct i3c_target_read_setup *setup)
+{
+	dev->desc->target_info.read_handler = setup->handler;
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(i3c_target_read_register);
+
+int i3c_register(struct i3c_master_controller *master,
+		 struct device *parent,
+		 const struct i3c_master_controller_ops *master_ops,
+		 const struct i3c_target_ops *target_ops,
+		 bool secondary)
+{
+	const char *role;
+	int ret;
+
+	ret = of_property_read_string(parent->of_node, "initial-role", &role);
+	if (ret || !strcmp("primary", role)) {
+		return i3c_master_register(master, parent, master_ops, secondary);
+	} else if (!strcmp("target", role)) {
+		master->target = true;
+		return i3c_target_register(master, parent, target_ops);
+	} else {
+		return -EOPNOTSUPP;
+	}
+}
+EXPORT_SYMBOL_GPL(i3c_register);
+
+int i3c_unregister(struct i3c_master_controller *master)
+{
+	if (master->target)
+		i3c_target_unregister(master);
+	else
+		i3c_master_unregister(master);
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(i3c_unregister);
+
+int i3c_master_send_hdr_cmds_locked(struct i3c_master_controller *master,
+				    struct i3c_hdr_cmd *cmds, int ncmds)
+{
+	int i;
+
+	if (!cmds || !master || ncmds <= 0)
+		return -EINVAL;
+
+	if (!master->ops->send_hdr_cmds)
+		return -ENOTSUPP;
+
+	for (i = 0; i < ncmds; i++) {
+		if (!(master->this->info.hdr_cap & BIT(cmds[i].mode)))
+			return -ENOTSUPP;
+	}
+
+	return master->ops->send_hdr_cmds(master, cmds, ncmds);
+}
+
+/**
+ * i3c_master_send_hdr_cmds() - send HDR commands on the I3C bus
+ * @master: master used to send frames on the bus
+ * @cmds: array of HDR commands
+ * @ncmds: number of commands to send
+ *
+ * Send one or several HDR commands.
+ *
+ * This function can sleep and thus cannot be called in atomic context.
+ *
+ * Return: 0 in case of success, a negative error code otherwise.
+ */
+int i3c_master_send_hdr_cmds(struct i3c_master_controller *master,
+			     struct i3c_hdr_cmd *cmds, int ncmds)
+{
+	int ret;
+
+	i3c_bus_normaluse_lock(&master->bus);
+	ret = i3c_master_send_hdr_cmds_locked(master, cmds, ncmds);
+	i3c_bus_normaluse_unlock(&master->bus);
+
+	return ret;
+}
+EXPORT_SYMBOL_GPL(i3c_master_send_hdr_cmds);
+
 int i3c_dev_do_priv_xfers_locked(struct i3c_dev_desc *dev,
 				 struct i3c_priv_xfer *xfers,
 				 int nxfers)
@@ -2604,10 +3213,38 @@ int i3c_dev_do_priv_xfers_locked(struct i3c_dev_desc *dev,
 	if (!master || !xfers)
 		return -EINVAL;
 
-	if (!master->ops->priv_xfers)
-		return -ENOTSUPP;
+	if (!master->target) {
+		if (!master->ops->priv_xfers)
+			return -EOPNOTSUPP;
+
+		return master->ops->priv_xfers(dev, xfers, nxfers);
+	}
 
-	return master->ops->priv_xfers(dev, xfers, nxfers);
+	if (!master->target_ops->priv_xfers)
+		return -EOPNOTSUPP;
+
+	return master->target_ops->priv_xfers(dev, xfers, nxfers);
+}
+
+int i3c_dev_generate_ibi_locked(struct i3c_dev_desc *dev, const u8 *data, int len)
+
+{
+	struct i3c_master_controller *master;
+
+	if (!dev)
+		return -ENOENT;
+
+	master = i3c_dev_get_master(dev);
+	if (!master)
+		return -EINVAL;
+
+	if (!master->target)
+		return -EINVAL;
+
+	if (!master->target_ops->generate_ibi)
+		return -EOPNOTSUPP;
+
+	return master->target_ops->generate_ibi(dev, data, len);
 }
 
 int i3c_dev_disable_ibi_locked(struct i3c_dev_desc *dev)
@@ -2695,14 +3332,136 @@ void i3c_dev_free_ibi_locked(struct i3c_dev_desc *dev)
 	dev->ibi = NULL;
 }
 
+int i3c_dev_send_ccc_cmd_locked(struct i3c_dev_desc *dev, u8 ccc_id)
+{
+	struct i3c_master_controller *master = i3c_dev_get_master(dev);
+	int ret;
+
+	switch (ccc_id) {
+	case I3C_CCC_SETAASA:
+		ret = i3c_master_setaasa_locked(master);
+		break;
+	case I3C_CCC_SETHID:
+		ret = i3c_master_sethid_locked(master);
+		break;
+	case I3C_CCC_RSTDAA(false):
+		ret = i3c_master_rstdaa_locked(master, dev->info.dyn_addr);
+		break;
+	case I3C_CCC_RSTDAA(true):
+		ret = i3c_master_rstdaa_locked(master, I3C_BROADCAST_ADDR);
+		break;
+	default:
+		dev_err(&master->dev, "Unpermitted ccc: %x\n", ccc_id);
+		return -ENOTSUPP;
+	}
+
+	return ret;
+}
+EXPORT_SYMBOL_GPL(i3c_dev_send_ccc_cmd_locked);
+
+int i3c_master_register_slave(struct i3c_master_controller *master,
+			      const struct i3c_slave_setup *req)
+{
+	if (!master->ops->register_slave)
+		return -ENOTSUPP;
+
+	return master->ops->register_slave(master, req);
+}
+
+int i3c_master_unregister_slave(struct i3c_master_controller *master)
+{
+	if (!master->ops->unregister_slave)
+		return -ENOTSUPP;
+
+	return master->ops->unregister_slave(master);
+}
+
+int i3c_master_send_sir(struct i3c_master_controller *master,
+			struct i3c_slave_payload *payload)
+{
+	int ret;
+
+	if (!master->ops->send_sir)
+		return -ENOTSUPP;
+
+	i3c_bus_normaluse_lock(&master->bus);
+	ret = master->ops->send_sir(master, payload);
+	i3c_bus_normaluse_unlock(&master->bus);
+
+	return ret;
+}
+
+int i3c_master_put_read_data(struct i3c_master_controller *master,
+			     struct i3c_slave_payload *data,
+			     struct i3c_slave_payload *ibi_notify)
+{
+	int ret;
+
+	if (!master->ops->put_read_data)
+		return -ENOTSUPP;
+
+	i3c_bus_normaluse_lock(&master->bus);
+	ret = master->ops->put_read_data(master, data, ibi_notify);
+	i3c_bus_normaluse_unlock(&master->bus);
+
+	return ret;
+}
+
+int i3c_for_each_dev(void *data, int (*fn)(struct device *, void *))
+{
+	int res;
+
+	mutex_lock(&i3c_core_lock);
+	res = bus_for_each_dev(&i3c_bus_type, NULL, data, fn);
+	mutex_unlock(&i3c_core_lock);
+
+	return res;
+}
+EXPORT_SYMBOL_GPL(i3c_for_each_dev);
+
+int i3c_dev_control_pec(struct i3c_dev_desc *dev, bool pec)
+{
+	struct i3c_master_controller *master = i3c_dev_get_master(dev);
+
+	if (!master->pec_supported)
+		return -EOPNOTSUPP;
+
+	dev->info.pec = pec;
+
+	/*
+	 * TODO: There are two cases which shall be covered
+	 * 1. Controller doesn't support PEC.
+	 *    In this case we could just fallback to SW implementation.
+	 * 2. Device doesn't support PEC.
+	 *    Then we really can't use PEC - and should error-out.
+	 */
+
+	return 0;
+}
+
 static int __init i3c_init(void)
 {
-	return bus_register(&i3c_bus_type);
+	int res = bus_register_notifier(&i2c_bus_type, &i2cdev_notifier);
+
+	if (res)
+		return res;
+
+	res = bus_register(&i3c_bus_type);
+	if (res)
+		goto out_unreg_notifier;
+
+	return 0;
+
+out_unreg_notifier:
+	bus_unregister_notifier(&i2c_bus_type, &i2cdev_notifier);
+
+	return res;
 }
 subsys_initcall(i3c_init);
 
 static void __exit i3c_exit(void)
 {
+	bus_unregister_notifier(&i2c_bus_type, &i2cdev_notifier);
 	idr_destroy(&i3c_bus_idr);
 	bus_unregister(&i3c_bus_type);
 }
diff --git a/drivers/i3c/master/Kconfig b/drivers/i3c/master/Kconfig
index 3b8f95916f46..4cb8ea236eb6 100644
--- a/drivers/i3c/master/Kconfig
+++ b/drivers/i3c/master/Kconfig
@@ -22,6 +22,34 @@ config DW_I3C_MASTER
 	  This driver can also be built as a module.  If so, the module
 	  will be called dw-i3c-master.
 
+config AST2600_I3C_MASTER
+	tristate "Aspeed AST2600 I3C master driver"
+	depends on I3C
+	depends on HAS_IOMEM
+	depends on MACH_ASPEED_G6
+	help
+	  Support for Aspeed AST2600 MIPI I3C Controller.
+
+	  This driver can also be built as a module.  If so, the module
+	  will be called ast2600-i3c-master.
+
+if AST2600_I3C_MASTER
+config AST2600_I3C_IBI_MAX_PAYLOAD
+	int "Max IBI payload size"
+	default 255
+
+config AST2600_I3C_MRL
+	int "Max read length"
+	default 256
+
+config AST2600_I3C_CCC_WORKAROUND
+	bool "Workaround for AST2600A1 errata item#30"
+	default n
+	help
+	  Say y to enable the workaround for AST2600A1 errata item#30.  No need
+	  to enable this option if you are using AST2600A2 or later versions.
+endif
+
 config SVC_I3C_MASTER
 	tristate "Silvaco I3C Dual-Role Master driver"
 	depends on I3C
diff --git a/drivers/i3c/master/Makefile b/drivers/i3c/master/Makefile
index b3fee0f690b2..77e9c5b4bdaf 100644
--- a/drivers/i3c/master/Makefile
+++ b/drivers/i3c/master/Makefile
@@ -1,5 +1,6 @@
 # SPDX-License-Identifier: GPL-2.0-only
 obj-$(CONFIG_CDNS_I3C_MASTER)		+= i3c-master-cdns.o
 obj-$(CONFIG_DW_I3C_MASTER)		+= dw-i3c-master.o
+obj-$(CONFIG_AST2600_I3C_MASTER)	+= ast2600-i3c-global.o ast2600-i3c-master.o
 obj-$(CONFIG_SVC_I3C_MASTER)		+= svc-i3c-master.o
 obj-$(CONFIG_MIPI_I3C_HCI)		+= mipi-i3c-hci/
diff --git a/drivers/i3c/master/ast2600-i3c-global.c b/drivers/i3c/master/ast2600-i3c-global.c
new file mode 100644
index 000000000000..3536849dbe5f
--- /dev/null
+++ b/drivers/i3c/master/ast2600-i3c-global.c
@@ -0,0 +1,141 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Copyright (C) 2021 ASPEED Technology Inc.
+ *
+ * Author: Dylan Hung <dylan_hung@aspeedtech.com>
+ * Based on a work from: Ryan Chen <ryan_chen@aspeedtech.com>
+ */
+#include <linux/clk.h>
+#include <linux/module.h>
+#include <linux/of_platform.h>
+#include <linux/platform_device.h>
+#include <linux/of_address.h>
+#include <linux/io.h>
+#include <linux/reset.h>
+#include <linux/delay.h>
+#include <linux/slab.h>
+
+#define I3CG_REG0(x)			((x * 0x10) + 0x10)
+#define I3CG_REG0_SDA_PULLUP_EN_MASK	GENMASK(29, 28)
+#define I3CG_REG0_SDA_PULLUP_EN_2K	(0x1 << 28)
+#define I3CG_REG0_SDA_PULLUP_EN_750	(0x2 << 28)
+#define I3CG_REG0_SDA_PULLUP_EN_545	(0x3 << 28)
+
+#define I3CG_REG1(x)			((x * 0x10) + 0x14)
+#define I3CG_REG1_I2C_MODE		BIT(0)
+#define I3CG_REG1_TEST_MODE		BIT(1)
+#define I3CG_REG1_ACT_MODE_MASK		GENMASK(3, 2)
+#define I3CG_REG1_ACT_MODE(x)		(((x) << 2) & I3CG_REG1_ACT_MODE_MASK)
+#define I3CG_REG1_PENDING_INT_MASK	GENMASK(7, 4)
+#define I3CG_REG1_PENDING_INT(x)	(((x) << 4) & I3CG_REG1_PENDING_INT_MASK)
+#define I3CG_REG1_SA_MASK		GENMASK(14, 8)
+#define I3CG_REG1_SA(x)			(((x) << 8) & I3CG_REG1_SA_MASK)
+#define I3CG_REG1_SA_EN			BIT(15)
+#define I3CG_REG1_INST_ID_MASK		GENMASK(19, 16)
+#define I3CG_REG1_INST_ID(x)		(((x) << 16) & I3CG_REG1_INST_ID_MASK)
+
+struct aspeed_i3c_global {
+	void __iomem *regs;
+	struct reset_control *rst;
+};
+
+static const struct of_device_id aspeed_i3c_of_match[] = {
+	{ .compatible = "aspeed,ast2600-i3c-global", },
+	{},
+};
+MODULE_DEVICE_TABLE(of, aspeed_i3c_of_match);
+
+static u32 pullup_resistor_ohm_to_reg(u32 ohm)
+{
+	switch (ohm) {
+	case 545:
+		return I3CG_REG0_SDA_PULLUP_EN_545;
+	case 750:
+		return I3CG_REG0_SDA_PULLUP_EN_750;
+	case 2000:
+	default:
+		return I3CG_REG0_SDA_PULLUP_EN_2K;
+	}
+}
+
+static int aspeed_i3c_global_probe(struct platform_device *pdev)
+{
+	struct aspeed_i3c_global *i3cg;
+	u32 reg0, reg1, num_i3cs;
+	u32 *pullup_resistors;
+	int i, ret;
+
+	i3cg = devm_kzalloc(&pdev->dev, sizeof(*i3cg), GFP_KERNEL);
+	if (!i3cg)
+		return -ENOMEM;
+
+	i3cg->regs = devm_platform_ioremap_resource(pdev, 0);
+	if (IS_ERR(i3cg->regs))
+		return -ENOMEM;
+
+	i3cg->rst = devm_reset_control_get(&pdev->dev, NULL);
+	if (IS_ERR(i3cg->rst)) {
+		dev_err(&pdev->dev,
+			"missing or invalid reset controller device tree entry");
+		return PTR_ERR(i3cg->rst);
+	}
+
+	reset_control_assert(i3cg->rst);
+	udelay(3);
+	reset_control_deassert(i3cg->rst);
+
+	ret = of_property_read_u32(pdev->dev.of_node, "num-i3cs", &num_i3cs);
+	if (ret < 0) {
+		dev_err(&pdev->dev, "unable to get number of i3c controllers");
+		return -ENOMEM;
+	}
+
+	pullup_resistors = kcalloc(num_i3cs, sizeof(u32), GFP_KERNEL);
+	if (!pullup_resistors)
+		return -ENOMEM;
+
+	ret = of_property_read_u32_array(pdev->dev.of_node, "pull-up-resistors",
+					 pullup_resistors, num_i3cs);
+	if (ret < 0) {
+		dev_warn(&pdev->dev,
+			 "use 2K Ohm SDA pull up resistor by default");
+	}
+
+	reg1 = I3CG_REG1_ACT_MODE(1) | I3CG_REG1_PENDING_INT(0xc) |
+	       I3CG_REG1_SA(0x74);
+
+	for (i = 0; i < num_i3cs; i++) {
+		reg0 = readl(i3cg->regs + I3CG_REG0(i));
+		reg0 &= ~I3CG_REG0_SDA_PULLUP_EN_MASK;
+		reg0 |= pullup_resistor_ohm_to_reg(pullup_resistors[i]);
+		writel(reg0, i3cg->regs + I3CG_REG0(i));
+
+		reg1 &= ~I3CG_REG1_INST_ID_MASK;
+		reg1 |= I3CG_REG1_INST_ID(i);
+		writel(reg1, i3cg->regs + I3CG_REG1(i));
+	}
+
+	kfree(pullup_resistors);
+
+	return 0;
+}
+
+static struct platform_driver aspeed_i3c_driver = {
+	.probe  = aspeed_i3c_global_probe,
+	.driver = {
+		.name = KBUILD_MODNAME,
+		.of_match_table = of_match_ptr(aspeed_i3c_of_match),
+	},
+};
+
+//static int __init aspeed_i3c_global_init(void)
+//{
+//	return platform_driver_register(&aspeed_i3c_driver);
+//}
+//postcore_initcall(aspeed_i3c_global_init);
+module_platform_driver(aspeed_i3c_driver);
+
+MODULE_AUTHOR("Ryan Chen <ryan_chen@aspeedtech.com>");
+MODULE_AUTHOR("Dylan Hung <dylan_hung@aspeedtech.com>");
+MODULE_DESCRIPTION("ASPEED I3C Global Driver");
+MODULE_LICENSE("GPL v2");
diff --git a/drivers/i3c/master/ast2600-i3c-master.c b/drivers/i3c/master/ast2600-i3c-master.c
new file mode 100644
index 000000000000..ef155f9d3592
--- /dev/null
+++ b/drivers/i3c/master/ast2600-i3c-master.c
@@ -0,0 +1,2884 @@
+// SPDX-License-Identifier: GPL-2.0-or-later
+/*
+ * Copyright (c) 2021 ASPEED Technology Inc.
+ *
+ * Derived from dw-i3c-master.c by Vitor Soares <vitor.soares@synopsys.com>
+ */
+
+#include <linux/bitops.h>
+#include <linux/bitfield.h>
+#include <linux/clk.h>
+#include <linux/completion.h>
+#include <linux/err.h>
+#include <linux/errno.h>
+#include <linux/i3c/master.h>
+#include <linux/interrupt.h>
+#include <linux/ioport.h>
+#include <linux/iopoll.h>
+#include <linux/list.h>
+#include <linux/module.h>
+#include <linux/of.h>
+#include <linux/platform_device.h>
+#include <linux/reset.h>
+#include <linux/slab.h>
+#include <linux/mfd/syscon.h>
+#include <linux/regmap.h>
+
+#define I3C_CHANNEL_MAX 5
+
+#define DEVICE_CTRL			0x0
+#define DEV_CTRL_ENABLE			BIT(31)
+#define DEV_CTRL_RESUME			BIT(30)
+#define DEV_CTRL_AUTO_HJ_DISABLE	BIT(27)
+#define DEV_CTRL_SLAVE_MDB		GENMASK(23, 16)
+#define DEV_CTRL_SLAVE_PEC_EN		BIT(10)
+#define DEV_CRTL_IBI_PAYLOAD_EN		BIT(9)
+#define DEV_CTRL_HOT_JOIN_NACK		BIT(8)
+#define DEV_CTRL_I2C_SLAVE_PRESENT	BIT(7)
+#define DEV_CTRL_IBA_INCLUDE		BIT(0)
+
+#define DEVICE_ADDR			0x4
+#define DEV_ADDR_DYNAMIC_ADDR_VALID	BIT(31)
+#define DEV_ADDR_DYNAMIC(x)		(((x) << 16) & GENMASK(22, 16))
+#define DEV_ADDR_STATIC_ADDR_VALID	BIT(15)
+#define DEV_ADDR_STATIC(x)		(((x) << 0) & GENMASK(6, 0))
+
+#define HW_CAPABILITY			0x8
+#define COMMAND_QUEUE_PORT		0xc
+#define COMMAND_PORT_PEC		BIT(31)
+#define COMMAND_PORT_TOC		BIT(30)
+#define COMMAND_PORT_READ_TRANSFER	BIT(28)
+#define COMMAND_PORT_SDAP		BIT(27)
+#define COMMAND_PORT_ROC		BIT(26)
+#define COMMAND_PORT_DBP(x)		((x) << 25)
+#define COMMAND_PORT_SPEED(x)		(((x) << 21) & GENMASK(23, 21))
+#define   SPEED_I3C_SDR0		0x0
+#define   SPEED_I3C_SDR1		0x1
+#define   SPEED_I3C_SDR2		0x2
+#define   SPEED_I3C_SDR3		0x3
+#define   SPEED_I3C_SDR4		0x4
+#define   SPEED_I3C_HDR_TS		0x5
+#define   SPEED_I3C_HDR_DDR		0x6
+#define   SPEED_I3C_I2C_FM		0x7
+#define   SPEED_I2C_FM			0x0
+#define   SPEED_I2C_FMP			0x1
+#define COMMAND_PORT_DEV_INDEX(x)	(((x) << 16) & GENMASK(20, 16))
+#define COMMAND_PORT_CP			BIT(15)
+#define COMMAND_PORT_CMD(x)		(((x) << 7) & GENMASK(14, 7))
+#define COMMAND_PORT_TID(x)		(((x) << 3) & GENMASK(6, 3))
+
+#define COMMAND_PORT_ARG_DBP(x)		(((x) << 8) & GENMASK(15, 8))
+#define COMMAND_PORT_ARG_DATA_LEN(x)	(((x) << 16) & GENMASK(31, 16))
+#define COMMAND_PORT_ARG_DATA_LEN_MAX	65536
+#define COMMAND_PORT_TRANSFER_ARG	0x01
+
+#define COMMAND_ATTR_SLAVE_DATA		0x0
+#define COMMAND_PORT_SLAVE_TID(x)      (((x) << 3) & GENMASK(5, 3))
+#define COMMAND_PORT_SLAVE_DATA_LEN	GENMASK(31, 16)
+
+#define COMMAND_PORT_SDA_DATA_BYTE_3(x)	(((x) << 24) & GENMASK(31, 24))
+#define COMMAND_PORT_SDA_DATA_BYTE_2(x)	(((x) << 16) & GENMASK(23, 16))
+#define COMMAND_PORT_SDA_DATA_BYTE_1(x)	(((x) << 8) & GENMASK(15, 8))
+#define COMMAND_PORT_SDA_BYTE_STRB_3	BIT(5)
+#define COMMAND_PORT_SDA_BYTE_STRB_2	BIT(4)
+#define COMMAND_PORT_SDA_BYTE_STRB_1	BIT(3)
+#define COMMAND_PORT_SHORT_DATA_ARG	0x02
+
+#define COMMAND_PORT_DEV_COUNT(x)	(((x) << 21) & GENMASK(25, 21))
+#define COMMAND_PORT_ADDR_ASSGN_CMD	0x03
+
+#define RESPONSE_QUEUE_PORT		0x10
+#define RESPONSE_PORT_ERR_STATUS(x)	(((x) & GENMASK(31, 28)) >> 28)
+#define RESPONSE_NO_ERROR		0
+#define RESPONSE_ERROR_CRC		1
+#define RESPONSE_ERROR_PARITY		2
+#define RESPONSE_ERROR_FRAME		3
+#define RESPONSE_ERROR_IBA_NACK		4
+#define RESPONSE_ERROR_ADDRESS_NACK	5
+#define RESPONSE_ERROR_OVER_UNDER_FLOW	6
+#define RESPONSE_ERROR_TRANSF_ABORT	8
+#define RESPONSE_ERROR_I2C_W_NACK_ERR	9
+#define RESPONSE_ERROR_EARLY_TERMINATE	10
+#define RESPONSE_ERROR_PEC_ERR		12
+#define RESPONSE_PORT_TID(x)		(((x) & GENMASK(27, 24)) >> 24)
+#define   TID_SLAVE_IBI_DONE		0b0001
+#define   TID_MASTER_READ_DATA		0b0010
+#define   TID_MASTER_WRITE_DATA		0b1000
+#define   TID_CCC_WRITE_DATA		0b1111
+#define RESPONSE_PORT_DATA_LEN(x)	((x) & GENMASK(15, 0))
+
+#define RX_TX_DATA_PORT			0x14
+#define IBI_QUEUE_STATUS		0x18
+#define IBI_QUEUE_STATUS_RSP_NACK	BIT(31)
+#define IBI_QUEUE_STATUS_PEC_ERR	BIT(30)
+#define IBI_QUEUE_STATUS_LAST_FRAG	BIT(24)
+#define IBI_QUEUE_STATUS_IBI_ID(x)	(((x) & GENMASK(15, 8)) >> 8)
+#define IBI_QUEUE_STATUS_DATA_LEN(x)	((x) & GENMASK(7, 0))
+
+#define IBI_QUEUE_IBI_ADDR(x)		(IBI_QUEUE_STATUS_IBI_ID(x) >> 1)
+#define IBI_QUEUE_IBI_RNW(x)		(IBI_QUEUE_STATUS_IBI_ID(x) & BIT(0))
+#define IBI_TYPE_MR(x)                                                         \
+	((IBI_QUEUE_IBI_ADDR(x) != I3C_HOT_JOIN_ADDR) && !IBI_QUEUE_IBI_RNW(x))
+#define IBI_TYPE_HJ(x)                                                         \
+	((IBI_QUEUE_IBI_ADDR(x) == I3C_HOT_JOIN_ADDR) && !IBI_QUEUE_IBI_RNW(x))
+#define IBI_TYPE_SIR(x)                                                        \
+	((IBI_QUEUE_IBI_ADDR(x) != I3C_HOT_JOIN_ADDR) && IBI_QUEUE_IBI_RNW(x))
+
+#define IBI_QUEUE_DATA			0x18
+#define QUEUE_THLD_CTRL			0x1c
+#define QUEUE_THLD_CTRL_IBI_STA_MASK	GENMASK(31, 24)
+#define QUEUE_THLD_CTRL_IBI_STA(x)	(((x) - 1) << 24)
+#define QUEUE_THLD_CTRL_IBI_DAT_MASK	GENMASK(23, 16)
+#define QUEUE_THLD_CTRL_IBI_DAT(x)	((x) << 16)
+#define QUEUE_THLD_CTRL_RESP_BUF_MASK	GENMASK(15, 8)
+#define QUEUE_THLD_CTRL_RESP_BUF(x)	(((x) - 1) << 8)
+
+#define DATA_BUFFER_THLD_CTRL		0x20
+#define DATA_BUFFER_THLD_CTRL_RX_BUF	GENMASK(11, 8)
+
+#define IBI_QUEUE_CTRL			0x24
+#define IBI_MR_REQ_REJECT		0x2C
+#define IBI_SIR_REQ_REJECT		0x30
+#define IBI_REQ_REJECT_ALL		GENMASK(31, 0)
+
+#define RESET_CTRL			0x34
+#define RESET_CTRL_BUS			BIT(31)
+#define RESET_CTRL_BUS_RESET_TYPE	GENMASK(30, 29)
+#define   BUS_RESET_TYPE_EXIT		0b00
+#define   BUS_RESET_TYPE_SCL_LOW	0b11
+#define RESET_CTRL_IBI_QUEUE		BIT(5)
+#define RESET_CTRL_RX_FIFO		BIT(4)
+#define RESET_CTRL_TX_FIFO		BIT(3)
+#define RESET_CTRL_RESP_QUEUE		BIT(2)
+#define RESET_CTRL_CMD_QUEUE		BIT(1)
+#define RESET_CTRL_SOFT			BIT(0)
+#define RESET_CTRL_ALL                  (RESET_CTRL_IBI_QUEUE	              |\
+					 RESET_CTRL_RX_FIFO	              |\
+					 RESET_CTRL_TX_FIFO	              |\
+					 RESET_CTRL_RESP_QUEUE	              |\
+					 RESET_CTRL_CMD_QUEUE	              |\
+					 RESET_CTRL_SOFT)
+#define RESET_CTRL_QUEUES		(RESET_CTRL_IBI_QUEUE	              |\
+					 RESET_CTRL_RX_FIFO	              |\
+					 RESET_CTRL_TX_FIFO	              |\
+					 RESET_CTRL_RESP_QUEUE	              |\
+					 RESET_CTRL_CMD_QUEUE)
+
+#define SLV_EVENT_CTRL			0x38
+#define SLV_EVENT_CTRL_MWL_UPD		BIT(7)
+#define SLV_EVENT_CTRL_MRL_UPD		BIT(6)
+#define SLV_EVENT_CTRL_SIR_EN		BIT(0)
+#define SLV_EVETN_CTRL_W1C_MASK		(SLV_EVENT_CTRL_MWL_UPD |\
+					 SLV_EVENT_CTRL_MRL_UPD)
+
+#define INTR_STATUS			0x3c
+#define INTR_STATUS_EN			0x40
+#define INTR_SIGNAL_EN			0x44
+#define INTR_FORCE			0x48
+#define INTR_BUSOWNER_UPDATE_STAT	BIT(13)
+#define INTR_IBI_UPDATED_STAT		BIT(12)
+#define INTR_READ_REQ_RECV_STAT		BIT(11)
+#define INTR_DEFSLV_STAT		BIT(10)
+#define INTR_TRANSFER_ERR_STAT		BIT(9)
+#define INTR_DYN_ADDR_ASSGN_STAT	BIT(8)
+#define INTR_CCC_UPDATED_STAT		BIT(6)
+#define INTR_TRANSFER_ABORT_STAT	BIT(5)
+#define INTR_RESP_READY_STAT		BIT(4)
+#define INTR_CMD_QUEUE_READY_STAT	BIT(3)
+#define INTR_IBI_THLD_STAT		BIT(2)
+#define INTR_RX_THLD_STAT		BIT(1)
+#define INTR_TX_THLD_STAT		BIT(0)
+#define INTR_ALL			(INTR_BUSOWNER_UPDATE_STAT |	\
+					INTR_IBI_UPDATED_STAT |		\
+					INTR_READ_REQ_RECV_STAT |	\
+					INTR_DEFSLV_STAT |		\
+					INTR_TRANSFER_ERR_STAT |	\
+					INTR_DYN_ADDR_ASSGN_STAT |	\
+					INTR_CCC_UPDATED_STAT |		\
+					INTR_TRANSFER_ABORT_STAT |	\
+					INTR_RESP_READY_STAT |		\
+					INTR_CMD_QUEUE_READY_STAT |	\
+					INTR_IBI_THLD_STAT |		\
+					INTR_TX_THLD_STAT |		\
+					INTR_RX_THLD_STAT)
+#define INTR_MASTER_MASK		(INTR_TRANSFER_ERR_STAT |	\
+					 INTR_RESP_READY_STAT)
+#define INTR_2ND_MASTER_MASK		(INTR_TRANSFER_ERR_STAT |	\
+					 INTR_RESP_READY_STAT	|	\
+					 INTR_IBI_UPDATED_STAT  |	\
+					 INTR_CCC_UPDATED_STAT)
+#define QUEUE_STATUS_LEVEL		0x4c
+#define QUEUE_STATUS_IBI_STATUS_CNT(x)	(((x) & GENMASK(28, 24)) >> 24)
+#define QUEUE_STATUS_IBI_BUF_BLR(x)	(((x) & GENMASK(23, 16)) >> 16)
+#define QUEUE_STATUS_LEVEL_RESP(x)	(((x) & GENMASK(15, 8)) >> 8)
+#define QUEUE_STATUS_LEVEL_CMD(x)	((x) & GENMASK(7, 0))
+
+#define DATA_BUFFER_STATUS_LEVEL	0x50
+#define DATA_BUFFER_STATUS_LEVEL_TX(x)	((x) & GENMASK(7, 0))
+
+#define PRESENT_STATE			0x54
+#define   CM_TFR_ST_STS			GENMASK(21, 16)
+#define     CM_TFR_ST_STS_HALT		0x13
+#define   CM_TFR_STS			GENMASK(13, 8)
+#define     CM_TFR_STS_MASTER_SERV_IBI	0xe
+#define     CM_TFR_STS_MASTER_HALT	0xf
+#define     CM_TFR_STS_SLAVE_HALT	0x6
+
+#define CCC_DEVICE_STATUS		0x58
+#define DEVICE_ADDR_TABLE_POINTER	0x5c
+#define DEVICE_ADDR_TABLE_DEPTH(x)	(((x) & GENMASK(31, 16)) >> 16)
+#define DEVICE_ADDR_TABLE_ADDR(x)	((x) & GENMASK(7, 0))
+
+#define DEV_CHAR_TABLE_POINTER		0x60
+#define VENDOR_SPECIFIC_REG_POINTER	0x6c
+#define SLV_MIPI_PID_VALUE		0x70
+#define PID_MANUF_ID_ASPEED		0x03f6
+
+#define SLV_PID_VALUE			0x74
+#define SLV_PID_PART_ID(x)		(((x) << 16) & GENMASK(31, 16))
+#define SLV_PID_INST_ID(x)		(((x) << 12) & GENMASK(15, 12))
+#define SLV_PID_DCR(x)			((x) & GENMASK(11, 0))
+
+#define PID_PART_ID_AST2600_SERIES	0x0500
+#define PID_PART_ID_AST1030_A0		0x8000
+
+#define SLV_CHAR_CTRL			0x78
+#define SLV_CHAR_GET_DCR(x)		(((x) & GENMASK(15, 8)) >> 8)
+#define SLV_CHAR_GET_BCR(x)		(((x) & GENMASK(7, 0)) >> 0)
+#define SLV_MAX_LEN			0x7c
+#define MAX_READ_TURNAROUND		0x80
+#define MAX_DATA_SPEED			0x84
+#define SLV_DEBUG_STATUS		0x88
+#define SLV_INTR_REQ			0x8c
+#define SLV_INTR_REQ_IBI_STS(x)		((x) & GENMASK(9, 8) >> 8)
+#define SLV_IBI_STS_OK			0x1
+
+#define DEVICE_CTRL_EXTENDED		0xb0
+#define DEVICE_CTRL_ROLE_MASK		GENMASK(1, 0)
+#define DEVICE_CTRL_ROLE_MASTER		0
+#define DEVICE_CTRL_ROLE_SLAVE		1
+#define SCL_I3C_OD_TIMING		0xb4
+#define SCL_I3C_PP_TIMING		0xb8
+#define SCL_I3C_TIMING_HCNT		GENMASK(23, 16)
+#define SCL_I3C_TIMING_LCNT		GENMASK(7, 0)
+#define SCL_I3C_TIMING_CNT_MIN		5
+
+#define SCL_I2C_FM_TIMING		0xbc
+#define SCL_I2C_FM_TIMING_HCNT		GENMASK(31, 16)
+#define SCL_I2C_FM_TIMING_LCNT		GENMASK(15, 0)
+
+#define SCL_I2C_FMP_TIMING		0xc0
+#define SCL_I2C_FMP_TIMING_HCNT		GENMASK(23, 16)
+#define SCL_I2C_FMP_TIMING_LCNT		GENMASK(15, 0)
+
+#define SCL_EXT_LCNT_TIMING		0xc8
+#define SCL_EXT_LCNT_4(x)		(((x) << 24) & GENMASK(31, 24))
+#define SCL_EXT_LCNT_3(x)		(((x) << 16) & GENMASK(23, 16))
+#define SCL_EXT_LCNT_2(x)		(((x) << 8) & GENMASK(15, 8))
+#define SCL_EXT_LCNT_1(x)		((x) & GENMASK(7, 0))
+
+#define SCL_EXT_TERMN_LCNT_TIMING	0xcc
+#define SDA_HOLD_SWITCH_DLY_TIMING	0xd0
+#define SDA_TX_HOLD			GENMASK(18, 16)
+#define   SDA_TX_HOLD_MIN		0b001
+#define   SDA_TX_HOLD_MAX		0b111
+#define SDA_PP_OD_SWITCH_DLY		GENMASK(10, 8)
+#define SDA_OD_PP_SWITCH_DLY		GENMASK(2, 0)
+#define BUS_FREE_TIMING			0xd4
+#define BUS_I3C_AVAILABLE_TIME(x)	(((x) << 16) & GENMASK(31, 16))
+#define BUS_I3C_MST_FREE(x)		((x) & GENMASK(15, 0))
+
+#define BUS_IDLE_TIMING			0xd8
+#define SCL_LOW_MST_EXT_TIMEOUT		0xdc
+#define I3C_VER_ID			0xe0
+#define I3C_VER_TYPE			0xe4
+#define EXTENDED_CAPABILITY		0xe8
+#define SLAVE_CONFIG			0xec
+
+#define DEV_ADDR_TABLE_LEGACY_I2C_DEV	BIT(31)
+#define DEV_ADDR_TABLE_DEV_NACK_RETRY	GENMASK(30, 29)
+#define DEV_ADDR_TABLE_IBI_ADDR_MASK	GENMASK(25, 24)
+#define   IBI_ADDR_MASK_OFF		0b00
+#define   IBI_ADDR_MASK_LAST_3BITS	0b01
+#define   IBI_ADDR_MASK_LAST_4BITS	0b10
+#define DEV_ADDR_TABLE_DA_PARITY	BIT(23)
+#define DEV_ADDR_TABLE_DYNAMIC_ADDR	GENMASK(22, 16)
+#define DEV_ADDR_TABLE_MR_REJECT	BIT(14)
+#define DEV_ADDR_TABLE_SIR_REJECT	BIT(13)
+#define DEV_ADDR_TABLE_IBI_WITH_DATA	BIT(12)
+#define DEV_ADDR_TABLE_IBI_PEC_EN	BIT(11)
+#define DEV_ADDR_TABLE_STATIC_ADDR	GENMASK(6, 0)
+
+#define DEV_ADDR_TABLE_LOC(start, idx)	((start) + ((idx) << 2))
+#define GET_DAT_FROM_POS(_master, _pos)                                        \
+	(readl(_master->regs + DEV_ADDR_TABLE_LOC(_master->datstartaddr, _pos)))
+
+#define MAX_DEVS			128
+#define MAX_IBI_FRAG_SIZE		124
+
+#define I3C_BUS_SDR1_SCL_RATE		8000000
+#define I3C_BUS_SDR2_SCL_RATE		6000000
+#define I3C_BUS_SDR3_SCL_RATE		4000000
+#define I3C_BUS_SDR4_SCL_RATE		2000000
+#define I3C_BUS_I2C_STD_TLOW_MIN_NS	4700
+#define I3C_BUS_I2C_STD_THIGH_MIN_NS	4000
+#define I3C_BUS_I2C_STD_TR_MAX_NS	1000
+#define I3C_BUS_I2C_STD_TF_MAX_NS	300
+#define I3C_BUS_I2C_FM_TLOW_MIN_NS	1300
+#define I3C_BUS_I2C_FM_THIGH_MIN_NS	600
+#define I3C_BUS_I2C_FM_TR_MAX_NS	300
+#define I3C_BUS_I2C_FM_TF_MAX_NS	300
+#define I3C_BUS_I2C_FMP_TLOW_MIN_NS	500
+#define I3C_BUS_I2C_FMP_THIGH_MIN_NS	260
+#define I3C_BUS_I2C_FMP_TR_MAX_NS	120
+#define I3C_BUS_I2C_FMP_TF_MAX_NS	120
+#define I3C_BUS_JESD403_PP_TLOW_MIN_NS	35
+#define I3C_BUS_JESD403_PP_THIGH_MIN_NS	35
+#define I3C_BUS_JESD403_PP_TR_MAX_NS	5
+#define I3C_BUS_JESD403_PP_TF_MAX_NS	5
+#define I3C_BUS_THIGH_MAX_NS		41
+
+#define I3C_BUS_EXT_TERMN_CNT		4
+#define JESD403_TIMED_RESET_NS_DEF	52428800
+
+#define XFER_TIMEOUT			(msecs_to_jiffies(1000))
+
+#define ast_setbits(x, set)		writel(readl(x) | (set), x)
+#define ast_clrbits(x, clr)		writel(readl(x) & ~(clr), x)
+#define ast_clrsetbits(x, clr, set)	writel((readl(x) & ~(clr)) | (set), x)
+
+#define MAX_GROUPS			(1 << 4)
+#define MAX_DEVS_IN_GROUP		(1 << 3)
+#define ALL_DEVS_IN_GROUP_ARE_FREE	((1 << MAX_DEVS_IN_GROUP) - 1)
+#define ADDR_GRP_SHIFT			3
+#define ADDR_GRP_MASK			GENMASK(6, ADDR_GRP_SHIFT)
+#define ADDR_GRP(x)			(((x) & ADDR_GRP_MASK) >> ADDR_GRP_SHIFT)
+#define ADDR_HID_MASK			GENMASK(ADDR_GRP_SHIFT - 1, 0)
+#define ADDR_HID(x)			((x) & ADDR_HID_MASK)
+
+struct aspeed_i3c_master_caps {
+	u8 cmdfifodepth;
+	u8 datafifodepth;
+};
+
+struct aspeed_i3c_cmd {
+	u32 cmd_lo;
+	u32 cmd_hi;
+	u16 tx_len;
+	const void *tx_buf;
+	u16 rx_len;
+	void *rx_buf;
+	u8 error;
+};
+
+struct aspeed_i3c_xfer {
+	struct list_head node;
+	struct completion comp;
+	int ret;
+	unsigned int ncmds;
+	struct aspeed_i3c_cmd cmds[];
+};
+
+struct aspeed_i3c_dev_group {
+	u32 dat[8];
+	u32 free_pos;
+	int hw_index;
+	struct {
+		u32 set;
+		u32 clr;
+	} mask;
+};
+
+struct aspeed_i3c_master {
+	struct device *dev;
+	struct i3c_master_controller base;
+	struct regmap *i3cg;
+	u16 maxdevs;
+	u16 datstartaddr;
+	u32 free_pos;
+	struct aspeed_i3c_dev_group dev_group[MAX_GROUPS];
+	struct {
+		struct list_head list;
+		struct aspeed_i3c_xfer *cur;
+		spinlock_t lock;
+	} xferqueue;
+	struct {
+		struct i3c_dev_desc *slots[MAX_DEVS];
+		u32 received_ibi_len[MAX_DEVS];
+		spinlock_t lock;
+	} ibi;
+	struct aspeed_i3c_master_caps caps;
+	void __iomem *regs;
+	struct reset_control *core_rst;
+	struct clk *core_clk;
+	char version[5];
+	char type[5];
+	u8 addrs[MAX_DEVS];
+	u32 channel;
+	bool secondary;
+	struct {
+		u32 *buf;
+		void (*callback)(struct i3c_master_controller *m,
+				 const struct i3c_slave_payload *payload);
+	} slave_data;
+	struct completion sir_complete;
+	struct completion data_read_complete;
+
+	struct {
+		unsigned long core_rate;
+		unsigned long core_period;
+		u32 i3c_od_scl_freq;
+		u32 i3c_od_scl_low;
+		u32 i3c_od_scl_high;
+		u32 i3c_pp_scl_freq;
+		u32 i3c_pp_scl_low;
+		u32 i3c_pp_scl_high;
+	} timing;
+	struct work_struct hj_work;
+};
+
+struct aspeed_i3c_i2c_dev_data {
+	struct i3c_generic_ibi_pool *ibi_pool;
+	u8 index;
+	s8 ibi;
+};
+
+static u8 even_parity(u8 p)
+{
+	p ^= p >> 4;
+	p &= 0xf;
+
+	return (0x9669 >> p) & 1;
+}
+
+#define I3CG_REG1(x)			((x * 0x10) + 0x14)
+#define SDA_OUT_SW_MODE_EN		BIT(31)
+#define SCL_OUT_SW_MODE_EN		BIT(30)
+#define SDA_IN_SW_MODE_EN		BIT(29)
+#define SCL_IN_SW_MODE_EN		BIT(28)
+#define SDA_IN_SW_MODE_VAL		BIT(27)
+#define SDA_OUT_SW_MODE_VAL		BIT(25)
+#define SDA_SW_MODE_OE			BIT(24)
+#define SCL_IN_SW_MODE_VAL		BIT(23)
+#define SCL_OUT_SW_MODE_VAL		BIT(21)
+#define SCL_SW_MODE_OE			BIT(20)
+
+static void aspeed_i3c_isolate_scl_sda(struct aspeed_i3c_master *master, bool iso)
+{
+	if (iso) {
+		regmap_write_bits(master->i3cg, I3CG_REG1(master->channel),
+				  SCL_IN_SW_MODE_VAL | SDA_IN_SW_MODE_VAL,
+				  SCL_IN_SW_MODE_VAL | SDA_IN_SW_MODE_VAL);
+		regmap_write_bits(master->i3cg, I3CG_REG1(master->channel),
+				  SCL_IN_SW_MODE_EN | SDA_IN_SW_MODE_EN,
+				  SCL_IN_SW_MODE_EN | SDA_IN_SW_MODE_EN);
+	} else {
+		regmap_write_bits(master->i3cg, I3CG_REG1(master->channel),
+				  SCL_IN_SW_MODE_EN | SDA_IN_SW_MODE_EN, 0);
+	}
+}
+
+static void aspeed_i3c_toggle_scl_in(struct aspeed_i3c_master *master, u32 times)
+{
+	for (; times; times--) {
+		regmap_write_bits(master->i3cg, I3CG_REG1(master->channel),
+				  SCL_IN_SW_MODE_VAL, 0);
+		regmap_write_bits(master->i3cg, I3CG_REG1(master->channel),
+				  SCL_IN_SW_MODE_VAL, SCL_IN_SW_MODE_VAL);
+	}
+}
+
+static void aspeed_i3c_gen_stop_to_internal(struct aspeed_i3c_master *master)
+{
+	regmap_write_bits(master->i3cg, I3CG_REG1(master->channel),
+			  SCL_IN_SW_MODE_VAL, SCL_IN_SW_MODE_VAL);
+	regmap_write_bits(master->i3cg, I3CG_REG1(master->channel),
+			  SDA_IN_SW_MODE_VAL, 0);
+	regmap_write_bits(master->i3cg, I3CG_REG1(master->channel),
+			  SDA_IN_SW_MODE_VAL, SDA_IN_SW_MODE_VAL);
+}
+
+static bool aspeed_i3c_fsm_is_idle(struct aspeed_i3c_master *master)
+{
+	/*
+	 * Clear the IBI queue to enable the hardware to generate SCL and
+	 * begin detecting the T-bit low to stop reading IBI data.
+	 */
+	readl(master->regs + IBI_QUEUE_DATA);
+	if (FIELD_GET(CM_TFR_STS, readl(master->regs + PRESENT_STATE)))
+		return false;
+	return true;
+}
+
+static void aspeed_i3c_gen_tbits_in(struct aspeed_i3c_master *master)
+{
+	bool is_idle;
+	int ret;
+
+	regmap_write_bits(master->i3cg, I3CG_REG1(master->channel),
+			  SDA_IN_SW_MODE_VAL, SDA_IN_SW_MODE_VAL);
+	regmap_write_bits(master->i3cg, I3CG_REG1(master->channel),
+			  SDA_IN_SW_MODE_EN, SDA_IN_SW_MODE_EN);
+
+	regmap_write_bits(master->i3cg, I3CG_REG1(master->channel),
+			  SDA_IN_SW_MODE_VAL, 0);
+	ret = readx_poll_timeout_atomic(aspeed_i3c_fsm_is_idle, master, is_idle,
+					is_idle, 0, 2000000);
+	regmap_write_bits(master->i3cg, I3CG_REG1(master->channel),
+			  SDA_IN_SW_MODE_EN, 0);
+	if (ret)
+		dev_err(master->dev,
+			"Failed to recovery the i3c fsm from %lx to idle: %d",
+			FIELD_GET(CM_TFR_STS,
+				  readl(master->regs + PRESENT_STATE)),
+			ret);
+}
+
+static bool aspeed_i3c_master_supports_ccc_cmd(struct i3c_master_controller *m,
+					   const struct i3c_ccc_cmd *cmd)
+{
+	if (cmd->ndests > 1)
+		return false;
+
+	switch (cmd->id) {
+	case I3C_CCC_ENEC(true):
+	case I3C_CCC_ENEC(false):
+	case I3C_CCC_DISEC(true):
+	case I3C_CCC_DISEC(false):
+	case I3C_CCC_ENTAS(0, true):
+	case I3C_CCC_ENTAS(0, false):
+	case I3C_CCC_RSTDAA(true):
+	case I3C_CCC_RSTDAA(false):
+	case I3C_CCC_ENTDAA:
+	case I3C_CCC_SETMWL(true):
+	case I3C_CCC_SETMWL(false):
+	case I3C_CCC_SETMRL(true):
+	case I3C_CCC_SETMRL(false):
+	case I3C_CCC_ENTHDR(0):
+	case I3C_CCC_SETDASA:
+	case I3C_CCC_SETNEWDA:
+	case I3C_CCC_GETMWL:
+	case I3C_CCC_GETMRL:
+	case I3C_CCC_GETPID:
+	case I3C_CCC_GETBCR:
+	case I3C_CCC_GETDCR:
+	case I3C_CCC_GETSTATUS:
+	case I3C_CCC_GETMXDS:
+	case I3C_CCC_GETHDRCAP:
+	case I3C_CCC_SETAASA:
+	case I3C_CCC_SETHID:
+		return true;
+	default:
+		return false;
+	}
+}
+
+static inline struct aspeed_i3c_master *
+to_aspeed_i3c_master(struct i3c_master_controller *master)
+{
+	return container_of(master, struct aspeed_i3c_master, base);
+}
+
+static void aspeed_i3c_master_disable(struct aspeed_i3c_master *master)
+{
+	writel(readl(master->regs + DEVICE_CTRL) & ~DEV_CTRL_ENABLE,
+	       master->regs + DEVICE_CTRL);
+}
+
+static void aspeed_i3c_master_enable(struct aspeed_i3c_master *master)
+{
+	writel(readl(master->regs + DEVICE_CTRL) | DEV_CTRL_ENABLE,
+	       master->regs + DEVICE_CTRL);
+}
+
+static void aspeed_i3c_master_resume(struct aspeed_i3c_master *master)
+{
+	writel(readl(master->regs + DEVICE_CTRL) | DEV_CTRL_RESUME,
+	       master->regs + DEVICE_CTRL);
+}
+
+static void aspeed_i3c_master_set_role(struct aspeed_i3c_master *master)
+{
+	u32 reg;
+	u32 role = DEVICE_CTRL_ROLE_MASTER;
+
+	if (master->secondary)
+		role = DEVICE_CTRL_ROLE_SLAVE;
+
+	reg = readl(master->regs + DEVICE_CTRL_EXTENDED);
+	reg = (reg & ~DEVICE_CTRL_ROLE_MASK) | role;
+	writel(reg, master->regs + DEVICE_CTRL_EXTENDED);
+}
+
+static int aspeed_i3c_master_get_free_pos(struct aspeed_i3c_master *master)
+{
+	if (!(master->free_pos & GENMASK(master->maxdevs - 1, 0)))
+		return -ENOSPC;
+
+	return ffs(master->free_pos) - 1;
+}
+
+static void aspeed_i3c_master_init_group_dat(struct aspeed_i3c_master *master)
+{
+	struct aspeed_i3c_dev_group *dev_grp;
+	int i, j;
+	u32 def_set, def_clr;
+
+	def_clr = DEV_ADDR_TABLE_IBI_ADDR_MASK;
+
+	/* For now don't support Hot-Join */
+	def_set = DEV_ADDR_TABLE_MR_REJECT | DEV_ADDR_TABLE_SIR_REJECT;
+
+	for (i = 0; i < MAX_GROUPS; i++) {
+		dev_grp = &master->dev_group[i];
+		dev_grp->hw_index = -1;
+		dev_grp->free_pos = ALL_DEVS_IN_GROUP_ARE_FREE;
+		dev_grp->mask.set = def_set;
+		dev_grp->mask.clr = def_clr;
+		for (j = 0; j < MAX_DEVS_IN_GROUP; j++)
+			dev_grp->dat[j] = 0;
+	}
+
+	for (i = 0; i < master->maxdevs; i++)
+		writel(def_set,
+		       master->regs +
+			       DEV_ADDR_TABLE_LOC(master->datstartaddr, i));
+}
+
+static int aspeed_i3c_master_set_group_dat(struct aspeed_i3c_master *master, u8 addr,
+				       u32 val)
+{
+	struct aspeed_i3c_dev_group *dev_grp = &master->dev_group[ADDR_GRP(addr)];
+	u8 idx = ADDR_HID(addr);
+
+	val &= ~DEV_ADDR_TABLE_DA_PARITY;
+	val |= FIELD_PREP(DEV_ADDR_TABLE_DA_PARITY, even_parity(addr));
+	dev_grp->dat[idx] = val;
+
+	if (val) {
+		dev_grp->free_pos &= ~BIT(idx);
+
+		/*
+		 * reserve the hw dat resource for the first member of the
+		 * group. all the members in the group share the same hw dat.
+		 */
+		if (dev_grp->hw_index == -1) {
+			dev_grp->hw_index = aspeed_i3c_master_get_free_pos(master);
+			if (dev_grp->hw_index < 0)
+				goto out;
+
+			master->free_pos &= ~BIT(dev_grp->hw_index);
+			val &= dev_grp->mask.clr;
+			val |= dev_grp->mask.set;
+			writel(val, master->regs + DEV_ADDR_TABLE_LOC(
+							   master->datstartaddr,
+							   dev_grp->hw_index));
+		}
+	} else {
+		dev_grp->free_pos |= BIT(idx);
+
+		/*
+		 * release the hw dat resource if all the members in the group
+		 * are free.
+		 */
+		if (dev_grp->free_pos == ALL_DEVS_IN_GROUP_ARE_FREE) {
+			writel(dev_grp->mask.set,
+			       master->regs +
+				       DEV_ADDR_TABLE_LOC(master->datstartaddr,
+							  dev_grp->hw_index));
+			master->free_pos |= BIT(dev_grp->hw_index);
+			dev_grp->hw_index = -1;
+		}
+	}
+out:
+	return dev_grp->hw_index;
+}
+
+static u32 aspeed_i3c_master_get_group_dat(struct aspeed_i3c_master *master, u8 addr)
+{
+	struct aspeed_i3c_dev_group *dev_grp = &master->dev_group[ADDR_GRP(addr)];
+
+	return dev_grp->dat[ADDR_HID(addr)];
+}
+
+static int aspeed_i3c_master_get_group_hw_index(struct aspeed_i3c_master *master,
+					    u8 addr)
+{
+	struct aspeed_i3c_dev_group *dev_grp = &master->dev_group[ADDR_GRP(addr)];
+
+	return dev_grp->hw_index;
+}
+
+static struct aspeed_i3c_dev_group *
+aspeed_i3c_master_get_group(struct aspeed_i3c_master *master, u8 addr)
+{
+	return &master->dev_group[ADDR_GRP(addr)];
+}
+
+static int aspeed_i3c_master_sync_hw_dat(struct aspeed_i3c_master *master, u8 addr)
+{
+	struct aspeed_i3c_dev_group *dev_grp = &master->dev_group[ADDR_GRP(addr)];
+	u32 dat = dev_grp->dat[ADDR_HID(addr)];
+	int hw_index = dev_grp->hw_index;
+
+	if (!dat || hw_index < 0)
+		return -1;
+
+	dat &= ~dev_grp->mask.clr;
+	dat |= dev_grp->mask.set;
+	writel(dat, master->regs +
+			    DEV_ADDR_TABLE_LOC(master->datstartaddr, hw_index));
+	return hw_index;
+}
+
+static void aspeed_i3c_master_wr_tx_fifo(struct aspeed_i3c_master *master,
+				     const u8 *bytes, int nbytes)
+{
+	/*
+	 * ensure all memory accesses are done before we move the data from
+	 * memory to the hardware FIFO
+	 */
+	wmb();
+
+	writesl(master->regs + RX_TX_DATA_PORT, bytes, nbytes / 4);
+	if (nbytes & 3) {
+		u32 tmp = 0;
+
+		memcpy(&tmp, bytes + (nbytes & ~3), nbytes & 3);
+		writesl(master->regs + RX_TX_DATA_PORT, &tmp, 1);
+		dev_dbg(master->dev, "TX data = %08x\n", tmp);
+	}
+}
+
+static void aspeed_i3c_master_read_fifo(struct aspeed_i3c_master *master, u32 fifo_reg,
+				    u8 *bytes, int nbytes)
+{
+	readsl(master->regs + fifo_reg, bytes, nbytes / 4);
+	if (nbytes & 3) {
+		u32 tmp;
+
+		readsl(master->regs + fifo_reg, &tmp, 1);
+		memcpy(bytes + (nbytes & ~3), &tmp, nbytes & 3);
+	}
+}
+
+static void aspeed_i3c_master_read_rx_fifo(struct aspeed_i3c_master *master,
+					      u8 *bytes, int nbytes)
+{
+	aspeed_i3c_master_read_fifo(master, RX_TX_DATA_PORT, bytes, nbytes);
+}
+
+static void aspeed_i3c_master_read_ibi_fifo(struct aspeed_i3c_master *master,
+					       u8 *bytes, int nbytes)
+{
+	aspeed_i3c_master_read_fifo(master, IBI_QUEUE_DATA, bytes, nbytes);
+}
+
+static struct aspeed_i3c_xfer *
+aspeed_i3c_master_alloc_xfer(struct aspeed_i3c_master *master, unsigned int ncmds)
+{
+	struct aspeed_i3c_xfer *xfer;
+
+	xfer = kzalloc(struct_size(xfer, cmds, ncmds), GFP_KERNEL);
+	if (!xfer)
+		return NULL;
+
+	INIT_LIST_HEAD(&xfer->node);
+	xfer->ncmds = ncmds;
+	xfer->ret = -ETIMEDOUT;
+
+	return xfer;
+}
+
+static void aspeed_i3c_master_free_xfer(struct aspeed_i3c_xfer *xfer)
+{
+	kfree(xfer);
+}
+
+static void aspeed_i3c_master_start_xfer_locked(struct aspeed_i3c_master *master)
+{
+	struct aspeed_i3c_xfer *xfer = master->xferqueue.cur;
+	unsigned int i;
+	u32 thld_ctrl;
+
+	if (!xfer)
+		return;
+
+	for (i = 0; i < xfer->ncmds; i++) {
+		struct aspeed_i3c_cmd *cmd = &xfer->cmds[i];
+
+		aspeed_i3c_master_wr_tx_fifo(master, cmd->tx_buf, cmd->tx_len);
+	}
+
+	thld_ctrl = readl(master->regs + QUEUE_THLD_CTRL);
+	thld_ctrl &= ~QUEUE_THLD_CTRL_RESP_BUF_MASK;
+	thld_ctrl |= QUEUE_THLD_CTRL_RESP_BUF(xfer->ncmds);
+	writel(thld_ctrl, master->regs + QUEUE_THLD_CTRL);
+
+	for (i = 0; i < xfer->ncmds; i++) {
+		struct aspeed_i3c_cmd *cmd = &xfer->cmds[i];
+
+		writel(cmd->cmd_hi, master->regs + COMMAND_QUEUE_PORT);
+		writel(cmd->cmd_lo, master->regs + COMMAND_QUEUE_PORT);
+	}
+}
+
+static void aspeed_i3c_master_enqueue_xfer(struct aspeed_i3c_master *master,
+				       struct aspeed_i3c_xfer *xfer)
+{
+	unsigned long flags;
+
+	init_completion(&xfer->comp);
+	spin_lock_irqsave(&master->xferqueue.lock, flags);
+	if (master->xferqueue.cur) {
+		list_add_tail(&xfer->node, &master->xferqueue.list);
+	} else {
+		master->xferqueue.cur = xfer;
+		aspeed_i3c_master_start_xfer_locked(master);
+	}
+	spin_unlock_irqrestore(&master->xferqueue.lock, flags);
+}
+
+static void aspeed_i3c_master_dequeue_xfer_locked(struct aspeed_i3c_master *master,
+					      struct aspeed_i3c_xfer *xfer)
+{
+	if (master->xferqueue.cur == xfer) {
+		u32 status;
+
+		master->xferqueue.cur = NULL;
+
+		writel(RESET_CTRL_RX_FIFO | RESET_CTRL_TX_FIFO |
+		       RESET_CTRL_RESP_QUEUE | RESET_CTRL_CMD_QUEUE,
+		       master->regs + RESET_CTRL);
+
+		readl_poll_timeout_atomic(master->regs + RESET_CTRL, status,
+					  !status, 10, 1000000);
+	} else {
+		list_del_init(&xfer->node);
+	}
+}
+
+static void aspeed_i3c_master_dequeue_xfer(struct aspeed_i3c_master *master,
+				       struct aspeed_i3c_xfer *xfer)
+{
+	unsigned long flags;
+
+	spin_lock_irqsave(&master->xferqueue.lock, flags);
+	aspeed_i3c_master_dequeue_xfer_locked(master, xfer);
+	spin_unlock_irqrestore(&master->xferqueue.lock, flags);
+}
+
+static void aspeed_i3c_master_sir_handler(struct aspeed_i3c_master *master,
+				      u32 ibi_status)
+{
+	struct aspeed_i3c_i2c_dev_data *data;
+	struct i3c_dev_desc *dev;
+	struct i3c_ibi_slot *slot;
+	u8 addr = IBI_QUEUE_IBI_ADDR(ibi_status);
+	u8 length = IBI_QUEUE_STATUS_DATA_LEN(ibi_status);
+	u8 *buf;
+	bool data_consumed = false;
+
+	dev = master->ibi.slots[addr];
+	if (!dev) {
+		pr_warn("no matching dev\n");
+		goto out;
+	}
+
+	spin_lock(&master->ibi.lock);
+	data = i3c_dev_get_master_data(dev);
+	slot = i3c_generic_ibi_get_free_slot(data->ibi_pool);
+	if (!slot) {
+		pr_err("no free ibi slot\n");
+		goto out_unlock;
+	}
+	master->ibi.received_ibi_len[addr] += length;
+	if (master->ibi.received_ibi_len[addr] >
+	    slot->dev->ibi->max_payload_len) {
+		pr_err("received ibi payload %d > device requested buffer %d",
+		       master->ibi.received_ibi_len[addr],
+		       slot->dev->ibi->max_payload_len);
+		goto out_unlock;
+	}
+	if (ibi_status & IBI_QUEUE_STATUS_LAST_FRAG)
+		master->ibi.received_ibi_len[addr] = 0;
+	buf = slot->data;
+	/* prepend ibi status */
+	memcpy(buf, &ibi_status, sizeof(ibi_status));
+	buf += sizeof(ibi_status);
+
+	aspeed_i3c_master_read_ibi_fifo(master, buf, length);
+	slot->len = length + sizeof(ibi_status);
+	i3c_master_queue_ibi(dev, slot);
+	data_consumed = true;
+out_unlock:
+	spin_unlock(&master->ibi.lock);
+
+out:
+	/* Consume data from the FIFO if it's not been done already. */
+	if (!data_consumed) {
+		int nwords = (length + 3) >> 2;
+		int i;
+
+		for (i = 0; i < nwords; i++)
+			readl(master->regs + IBI_QUEUE_DATA);
+		if (FIELD_GET(CM_TFR_STS,
+			      readl(master->regs + PRESENT_STATE)) ==
+		    CM_TFR_STS_MASTER_SERV_IBI)
+			aspeed_i3c_gen_tbits_in(master);
+		master->ibi.received_ibi_len[addr] = 0;
+	}
+}
+
+static void aspeed_i3c_master_demux_ibis(struct aspeed_i3c_master *master)
+{
+	u32 nibi, status;
+	int i;
+	u8 addr;
+
+	nibi = readl(master->regs + QUEUE_STATUS_LEVEL);
+	nibi = QUEUE_STATUS_IBI_STATUS_CNT(nibi);
+	if (!nibi)
+		return;
+
+	for (i = 0; i < nibi; i++) {
+		status = readl(master->regs + IBI_QUEUE_STATUS);
+		addr = IBI_QUEUE_IBI_ADDR(status);
+
+		/* FIXME: how to handle the unrecognized slave? */
+		if (status & IBI_QUEUE_STATUS_RSP_NACK)
+			pr_warn_once("ibi from unrecognized slave %02x\n",
+				     addr);
+
+		if (IBI_TYPE_SIR(status))
+			aspeed_i3c_master_sir_handler(master, status);
+
+		if (IBI_TYPE_HJ(status))
+			queue_work(master->base.wq, &master->hj_work);
+
+		if (IBI_TYPE_MR(status))
+			pr_info("get mr from %02x\n", addr);
+	}
+}
+
+static void aspeed_i3c_master_end_xfer_locked(struct aspeed_i3c_master *master, u32 isr)
+{
+	struct aspeed_i3c_xfer *xfer = master->xferqueue.cur;
+	int i, ret = 0;
+	u32 nresp;
+
+	if (!xfer)
+		return;
+
+	nresp = readl(master->regs + QUEUE_STATUS_LEVEL);
+	nresp = QUEUE_STATUS_LEVEL_RESP(nresp);
+
+	for (i = 0; i < nresp; i++) {
+		struct aspeed_i3c_cmd *cmd;
+		u32 resp;
+
+		resp = readl(master->regs + RESPONSE_QUEUE_PORT);
+
+		cmd = &xfer->cmds[RESPONSE_PORT_TID(resp)];
+		cmd->rx_len = RESPONSE_PORT_DATA_LEN(resp);
+		cmd->error = RESPONSE_PORT_ERR_STATUS(resp);
+		if (cmd->rx_len && !cmd->error)
+			aspeed_i3c_master_read_rx_fifo(master, cmd->rx_buf,
+						   cmd->rx_len);
+	}
+
+	for (i = 0; i < nresp; i++) {
+		switch (xfer->cmds[i].error) {
+		case RESPONSE_NO_ERROR:
+			break;
+		case RESPONSE_ERROR_PARITY:
+		case RESPONSE_ERROR_IBA_NACK:
+		case RESPONSE_ERROR_TRANSF_ABORT:
+		case RESPONSE_ERROR_CRC:
+		case RESPONSE_ERROR_FRAME:
+		case RESPONSE_ERROR_PEC_ERR:
+			ret = -EIO;
+			break;
+		case RESPONSE_ERROR_OVER_UNDER_FLOW:
+			ret = -ENOSPC;
+			break;
+		case RESPONSE_ERROR_I2C_W_NACK_ERR:
+		case RESPONSE_ERROR_ADDRESS_NACK:
+		default:
+			ret = -EINVAL;
+			break;
+		}
+	}
+
+	xfer->ret = ret;
+	complete(&xfer->comp);
+
+	if (ret < 0) {
+		aspeed_i3c_master_dequeue_xfer_locked(master, xfer);
+		aspeed_i3c_master_resume(master);
+	}
+
+	xfer = list_first_entry_or_null(&master->xferqueue.list,
+					struct aspeed_i3c_xfer,
+					node);
+	if (xfer)
+		list_del_init(&xfer->node);
+
+	master->xferqueue.cur = xfer;
+	aspeed_i3c_master_start_xfer_locked(master);
+}
+
+struct i3c_scl_timing_cfg {
+	unsigned long fscl;
+	u16 period_hi;
+	u16 period_lo;
+};
+
+static struct i3c_scl_timing_cfg jesd403_timing_cfg[5] = {
+	{ .fscl = I3C_BUS_TYP_I3C_SCL_RATE, .period_hi = 40, .period_lo = 40 },
+	{ .fscl = I3C_BUS_SDR1_SCL_RATE, .period_hi = 50, .period_lo = 75 },
+	{ .fscl = I3C_BUS_SDR2_SCL_RATE, .period_hi = 65, .period_lo = 100 },
+	{ .fscl = I3C_BUS_SDR3_SCL_RATE, .period_hi = 100, .period_lo = 150 },
+	{ .fscl = I3C_BUS_SDR4_SCL_RATE, .period_hi = 200, .period_lo = 300 }
+};
+
+struct i3c_scl_timing_cfg *ast2600_i3c_jesd403_scl_search(unsigned long fscl)
+{
+	int i;
+
+	for (i = 0; i < 5; i++) {
+		if (fscl == jesd403_timing_cfg[i].fscl)
+			return &jesd403_timing_cfg[i];
+	}
+
+	/* use typical 12.5M SCL if not found */
+	return &jesd403_timing_cfg[0];
+}
+
+static int calc_i2c_clk(struct aspeed_i3c_master *master, unsigned long fscl,
+			u16 *hcnt, u16 *lcnt)
+{
+	unsigned long core_rate, core_period;
+	u32 period_cnt, margin;
+	u32 hcnt_min, lcnt_min;
+
+	core_rate = master->timing.core_rate;
+	core_period = master->timing.core_period;
+
+	if (fscl <= 100000) {
+		lcnt_min = DIV_ROUND_UP(I3C_BUS_I2C_STD_TLOW_MIN_NS +
+						I3C_BUS_I2C_STD_TF_MAX_NS,
+					core_period);
+		hcnt_min = DIV_ROUND_UP(I3C_BUS_I2C_STD_THIGH_MIN_NS +
+						I3C_BUS_I2C_STD_TR_MAX_NS,
+					core_period);
+	} else if (fscl <= 400000) {
+		lcnt_min = DIV_ROUND_UP(I3C_BUS_I2C_FM_TLOW_MIN_NS +
+						I3C_BUS_I2C_FM_TF_MAX_NS,
+					core_period);
+		hcnt_min = DIV_ROUND_UP(I3C_BUS_I2C_FM_THIGH_MIN_NS +
+						I3C_BUS_I2C_FM_TR_MAX_NS,
+					core_period);
+	} else {
+		lcnt_min = DIV_ROUND_UP(I3C_BUS_I2C_FMP_TLOW_MIN_NS +
+						I3C_BUS_I2C_FMP_TF_MAX_NS,
+					core_period);
+		hcnt_min = DIV_ROUND_UP(I3C_BUS_I2C_FMP_THIGH_MIN_NS +
+						I3C_BUS_I2C_FMP_TR_MAX_NS,
+					core_period);
+	}
+
+	period_cnt = DIV_ROUND_UP(core_rate, fscl);
+	margin = (period_cnt - hcnt_min - lcnt_min) >> 1;
+	*lcnt = lcnt_min + margin;
+	*hcnt = max(period_cnt - *lcnt, hcnt_min);
+
+	return 0;
+}
+
+static int aspeed_i3c_clk_cfg(struct aspeed_i3c_master *master)
+{
+	unsigned long core_rate, core_period;
+	u32 scl_timing;
+	u16 hcnt, lcnt;
+
+	core_rate = master->timing.core_rate;
+	core_period = master->timing.core_period;
+
+	/* I3C PP mode */
+	if (master->timing.i3c_pp_scl_high && master->timing.i3c_pp_scl_low) {
+		hcnt = DIV_ROUND_CLOSEST(master->timing.i3c_pp_scl_high,
+					 core_period);
+		lcnt = DIV_ROUND_CLOSEST(master->timing.i3c_pp_scl_low,
+					 core_period);
+	} else if (master->base.jdec_spd) {
+		struct i3c_scl_timing_cfg *pp_timing;
+
+		pp_timing = ast2600_i3c_jesd403_scl_search(
+			master->base.bus.scl_rate.i3c);
+		hcnt = DIV_ROUND_UP(pp_timing->period_hi, core_period);
+		lcnt = DIV_ROUND_UP(pp_timing->period_lo, core_period);
+	} else {
+		hcnt = DIV_ROUND_UP(I3C_BUS_THIGH_MAX_NS, core_period) - 1;
+		if (hcnt < SCL_I3C_TIMING_CNT_MIN)
+			hcnt = SCL_I3C_TIMING_CNT_MIN;
+
+		lcnt = DIV_ROUND_UP(core_rate, I3C_BUS_TYP_I3C_SCL_RATE) - hcnt;
+		if (lcnt < SCL_I3C_TIMING_CNT_MIN)
+			lcnt = SCL_I3C_TIMING_CNT_MIN;
+	}
+	hcnt = min_t(u16, hcnt, FIELD_MAX(SCL_I3C_TIMING_HCNT));
+	lcnt = min_t(u16, lcnt, FIELD_MAX(SCL_I3C_TIMING_LCNT));
+	scl_timing = FIELD_PREP(SCL_I3C_TIMING_HCNT, hcnt) |
+		     FIELD_PREP(SCL_I3C_TIMING_LCNT, lcnt);
+	writel(scl_timing, master->regs + SCL_I3C_PP_TIMING);
+
+	/* I3C OD mode:
+	 * User defined
+	 *     check if hcnt/lcnt exceed the max value of the register
+	 *
+	 * JESD403 timing constrain for I2C/I3C OP mode
+	 *     tHIGH > 260, tLOW > 500 (same with MIPI 1.1 FMP constrain)
+	 *
+	 * MIPI 1.1 timing constrain for I3C OP mode
+	 *     tHIGH < 41, tLOW > 200
+	 */
+	if (master->timing.i3c_od_scl_high && master->timing.i3c_od_scl_low) {
+		hcnt = DIV_ROUND_CLOSEST(master->timing.i3c_od_scl_high,
+					 core_period);
+		lcnt = DIV_ROUND_CLOSEST(master->timing.i3c_od_scl_low,
+					 core_period);
+	} else if (master->base.jdec_spd) {
+		calc_i2c_clk(master, I3C_BUS_I2C_FM_PLUS_SCL_RATE, &hcnt, &lcnt);
+	} else {
+		lcnt = DIV_ROUND_UP(I3C_BUS_TLOW_OD_MIN_NS, core_period);
+		scl_timing = readl(master->regs + SCL_I3C_PP_TIMING);
+		hcnt = FIELD_GET(SCL_I3C_TIMING_HCNT, scl_timing);
+	}
+	hcnt = min_t(u16, hcnt, FIELD_MAX(SCL_I3C_TIMING_HCNT));
+	lcnt = min_t(u16, lcnt, FIELD_MAX(SCL_I3C_TIMING_LCNT));
+	scl_timing = FIELD_PREP(SCL_I3C_TIMING_HCNT, hcnt) |
+		     FIELD_PREP(SCL_I3C_TIMING_LCNT, lcnt);
+	writel(scl_timing, master->regs + SCL_I3C_OD_TIMING);
+
+	/* I2C FM mode */
+	calc_i2c_clk(master, master->base.bus.scl_rate.i2c, &hcnt, &lcnt);
+	scl_timing = FIELD_PREP(SCL_I2C_FM_TIMING_HCNT, hcnt) |
+		     FIELD_PREP(SCL_I2C_FM_TIMING_LCNT, lcnt);
+	writel(scl_timing, master->regs + SCL_I2C_FM_TIMING);
+
+	/*
+	 * I3C register 0xd4[15:0] BUS_FREE_TIMING used to control several parameters:
+	 * - tCAS & tCASr (tHD_STA in JESD403)
+	 * - tCBP & tCBPr (tSU_STO in JESD403)
+	 * - bus free time between a STOP condition and a START condition
+	 *
+	 * The constraints of these two parameters are different in different bus contexts
+	 * - MIPI I3C, mixed bus: 0xd4[15:0] = I2C SCL low period (handled in aspeed_i2c_clk_cfg)
+	 * - MIPI I3C, pure bus : 0xd4[15:0] = I3C SCL PP low period
+	 * - JESD403            : 0xd4[15:0] = I3C SCL OD low period
+	 */
+	if (!(readl(master->regs + DEVICE_CTRL) & DEV_CTRL_I2C_SLAVE_PRESENT)) {
+		if (master->base.jdec_spd)
+			scl_timing = readl(master->regs + SCL_I3C_OD_TIMING);
+		else
+			scl_timing = readl(master->regs + SCL_I3C_PP_TIMING);
+
+		lcnt = FIELD_GET(SCL_I3C_TIMING_LCNT, scl_timing);
+		scl_timing = BUS_I3C_AVAILABLE_TIME(0xffff);
+		scl_timing |= BUS_I3C_MST_FREE(lcnt);
+		writel(scl_timing, master->regs + BUS_FREE_TIMING);
+	}
+
+	/* Extend SDR: use PP mode hcnt */
+	scl_timing = readl(master->regs + SCL_I3C_PP_TIMING);
+	hcnt = scl_timing >> 16;
+	lcnt = DIV_ROUND_UP(core_rate, I3C_BUS_SDR1_SCL_RATE) - hcnt;
+	scl_timing = SCL_EXT_LCNT_1(lcnt);
+	lcnt = DIV_ROUND_UP(core_rate, I3C_BUS_SDR2_SCL_RATE) - hcnt;
+	scl_timing |= SCL_EXT_LCNT_2(lcnt);
+	lcnt = DIV_ROUND_UP(core_rate, I3C_BUS_SDR3_SCL_RATE) - hcnt;
+	scl_timing |= SCL_EXT_LCNT_3(lcnt);
+	lcnt = DIV_ROUND_UP(core_rate, I3C_BUS_SDR4_SCL_RATE) - hcnt;
+	scl_timing |= SCL_EXT_LCNT_4(lcnt);
+	writel(scl_timing, master->regs + SCL_EXT_LCNT_TIMING);
+
+	ast_clrsetbits(master->regs + SCL_EXT_TERMN_LCNT_TIMING, GENMASK(3, 0),
+		      I3C_BUS_EXT_TERMN_CNT);
+
+	return 0;
+}
+
+static int aspeed_i2c_clk_cfg(struct aspeed_i3c_master *master)
+{
+	unsigned long core_rate, core_period;
+	u16 hcnt, lcnt;
+	u32 scl_timing;
+
+	core_rate = master->timing.core_rate;
+	core_period = master->timing.core_period;
+
+	calc_i2c_clk(master, I3C_BUS_I2C_FM_PLUS_SCL_RATE, &hcnt, &lcnt);
+	hcnt = min_t(u16, hcnt, FIELD_MAX(SCL_I2C_FMP_TIMING_HCNT));
+	lcnt = min_t(u16, lcnt, FIELD_MAX(SCL_I2C_FMP_TIMING_LCNT));
+	scl_timing = FIELD_PREP(SCL_I2C_FMP_TIMING_HCNT, hcnt) |
+		     FIELD_PREP(SCL_I2C_FMP_TIMING_LCNT, lcnt);
+	writel(scl_timing, master->regs + SCL_I2C_FMP_TIMING);
+
+	calc_i2c_clk(master, master->base.bus.scl_rate.i2c, &hcnt, &lcnt);
+	scl_timing = FIELD_PREP(SCL_I2C_FM_TIMING_HCNT, hcnt) |
+		     FIELD_PREP(SCL_I2C_FM_TIMING_LCNT, lcnt);
+	writel(scl_timing, master->regs + SCL_I2C_FM_TIMING);
+
+	scl_timing = BUS_I3C_AVAILABLE_TIME(0xffff);
+	scl_timing |= BUS_I3C_MST_FREE(lcnt);
+	writel(scl_timing, master->regs + BUS_FREE_TIMING);
+	writel(readl(master->regs + DEVICE_CTRL) | DEV_CTRL_I2C_SLAVE_PRESENT,
+	       master->regs + DEVICE_CTRL);
+
+	return 0;
+}
+
+static int aspeed_i3c_master_set_info(struct aspeed_i3c_master *master,
+				       struct i3c_device_info *info)
+{
+#define ASPEED_SCU_REV_ID_REG 0x14
+#define ASPEED_HW_REV(x) (((x)&GENMASK(31, 16)) >> 16)
+
+	struct regmap *scu;
+	unsigned int reg;
+	u32 part_id, inst_id;
+
+	writel(PID_MANUF_ID_ASPEED << 1, master->regs + SLV_MIPI_PID_VALUE);
+
+	scu = syscon_regmap_lookup_by_phandle(master->dev->of_node, "aspeed,scu");
+	if (IS_ERR(scu)) {
+		dev_err(master->dev, "cannot to find SCU regmap\n");
+		return -ENODEV;
+	}
+	regmap_read(scu, ASPEED_SCU_REV_ID_REG, &reg);
+	part_id = ASPEED_HW_REV(reg);
+	inst_id = master->base.bus.id;
+
+	reg = SLV_PID_PART_ID(part_id) | SLV_PID_INST_ID(inst_id) |
+	      SLV_PID_DCR(0);
+	writel(reg, master->regs + SLV_PID_VALUE);
+
+	reg = readl(master->regs + SLV_CHAR_CTRL);
+	info->dcr = SLV_CHAR_GET_DCR(reg);
+	info->bcr = SLV_CHAR_GET_BCR(reg);
+	info->pid = (u64)readl(master->regs + SLV_MIPI_PID_VALUE) << 32;
+	info->pid |= readl(master->regs + SLV_PID_VALUE);
+	info->hdr_cap = I3C_CCC_HDR_MODE(I3C_HDR_DDR);
+
+	return 0;
+};
+
+static int aspeed_i3c_master_bus_init(struct i3c_master_controller *m)
+{
+	struct aspeed_i3c_master *master = to_aspeed_i3c_master(m);
+	struct i3c_bus *bus = i3c_master_get_bus(m);
+	struct i3c_device_info info = { };
+	u32 thld_ctrl, wait_enable_us;
+	int ret;
+
+	aspeed_i3c_master_set_role(master);
+
+	switch (bus->mode) {
+	case I3C_BUS_MODE_MIXED_FAST:
+	case I3C_BUS_MODE_MIXED_LIMITED:
+		ret = aspeed_i2c_clk_cfg(master);
+		if (ret)
+			return ret;
+		fallthrough;
+	case I3C_BUS_MODE_PURE:
+		ret = aspeed_i3c_clk_cfg(master);
+		if (ret)
+			return ret;
+		break;
+	default:
+		return -EINVAL;
+	}
+
+	thld_ctrl = readl(master->regs + QUEUE_THLD_CTRL);
+	thld_ctrl &= ~QUEUE_THLD_CTRL_RESP_BUF_MASK;
+	writel(thld_ctrl, master->regs + QUEUE_THLD_CTRL);
+
+	thld_ctrl = readl(master->regs + DATA_BUFFER_THLD_CTRL);
+	thld_ctrl &= ~DATA_BUFFER_THLD_CTRL_RX_BUF;
+	writel(thld_ctrl, master->regs + DATA_BUFFER_THLD_CTRL);
+
+	writel(INTR_ALL, master->regs + INTR_STATUS);
+	if (master->secondary) {
+		writel(INTR_2ND_MASTER_MASK, master->regs + INTR_STATUS_EN);
+		/*
+		 * No need for INTR_IBI_UPDATED_STAT signal, check this bit
+		 * when INTR_RESP_READY_STAT signal is up.  This can guarantee
+		 * the SIR payload is ACKed by the master.
+		 */
+		writel(INTR_2ND_MASTER_MASK & ~INTR_IBI_UPDATED_STAT,
+		       master->regs + INTR_SIGNAL_EN);
+	} else {
+		writel(INTR_MASTER_MASK, master->regs + INTR_STATUS_EN);
+		writel(INTR_MASTER_MASK, master->regs + INTR_SIGNAL_EN);
+	}
+
+	memset(&info, 0, sizeof(info));
+	ret = aspeed_i3c_master_set_info(master, &info);
+	if (ret < 0)
+		return ret;
+
+	ret = i3c_master_get_free_addr(m, 0);
+	if (ret < 0)
+		return ret;
+
+	if (master->secondary)
+		writel(DEV_ADDR_STATIC_ADDR_VALID | DEV_ADDR_STATIC(ret),
+		       master->regs + DEVICE_ADDR);
+	else
+		writel(DEV_ADDR_DYNAMIC_ADDR_VALID | DEV_ADDR_DYNAMIC(ret),
+		       master->regs + DEVICE_ADDR);
+
+	info.dyn_addr = ret;
+
+	ret = i3c_master_set_info(&master->base, &info);
+	if (ret)
+		return ret;
+
+	thld_ctrl = readl(master->regs + QUEUE_THLD_CTRL);
+	thld_ctrl &=
+		~(QUEUE_THLD_CTRL_IBI_STA_MASK | QUEUE_THLD_CTRL_IBI_DAT_MASK);
+	thld_ctrl |= QUEUE_THLD_CTRL_IBI_STA(1);
+	thld_ctrl |= QUEUE_THLD_CTRL_IBI_DAT(MAX_IBI_FRAG_SIZE >> 2);
+	writel(thld_ctrl, master->regs + QUEUE_THLD_CTRL);
+
+	writel(IBI_REQ_REJECT_ALL, master->regs + IBI_SIR_REQ_REJECT);
+	writel(IBI_REQ_REJECT_ALL, master->regs + IBI_MR_REQ_REJECT);
+
+	/* For now don't support Hot-Join */
+	ast_setbits(master->regs + DEVICE_CTRL,
+		   DEV_CTRL_AUTO_HJ_DISABLE |
+		   DEV_CTRL_HOT_JOIN_NACK |
+		   DEV_CRTL_IBI_PAYLOAD_EN);
+
+	if (master->secondary)
+		aspeed_i3c_isolate_scl_sda(master, true);
+	aspeed_i3c_master_enable(master);
+	if (master->secondary) {
+		wait_enable_us =
+			DIV_ROUND_UP(master->timing.core_period *
+					     FIELD_GET(GENMASK(31, 16),
+						       readl(master->regs +
+							     BUS_FREE_TIMING)),
+				     NSEC_PER_USEC);
+		udelay(wait_enable_us);
+		aspeed_i3c_toggle_scl_in(master, 8);
+		if (!(readl(master->regs + DEVICE_CTRL) & DEV_CTRL_ENABLE)) {
+			dev_warn(master->dev, "failed to enable controller");
+			aspeed_i3c_isolate_scl_sda(master, false);
+			return -EACCES;
+		}
+		aspeed_i3c_gen_stop_to_internal(master);
+		aspeed_i3c_isolate_scl_sda(master, false);
+	}
+
+	/* workaround for aspeed slave devices.  The aspeed slave devices need
+	 * for a dummy ccc and resume before accessing. Hide this workarond here
+	 * and later the i3c subsystem code will do the rstdaa again.
+	 */
+	if (!master->secondary)
+		i3c_master_rstdaa_locked(m, I3C_BROADCAST_ADDR);
+
+	return 0;
+}
+
+static void aspeed_i3c_master_bus_cleanup(struct i3c_master_controller *m)
+{
+	struct aspeed_i3c_master *master = to_aspeed_i3c_master(m);
+
+	aspeed_i3c_master_disable(master);
+}
+
+static void aspeed_i3c_master_bus_reset(struct i3c_master_controller *m)
+{
+	struct aspeed_i3c_master *master = to_aspeed_i3c_master(m);
+	u32 reset;
+	int i;
+
+	if (master->base.jdec_spd) {
+		reset = RESET_CTRL_BUS |
+			FIELD_PREP(RESET_CTRL_BUS_RESET_TYPE, BUS_RESET_TYPE_SCL_LOW);
+
+		writel(reset, master->regs + RESET_CTRL);
+	} else {
+		regmap_write_bits(master->i3cg, I3CG_REG1(master->channel),
+				  SDA_OUT_SW_MODE_VAL | SCL_OUT_SW_MODE_VAL,
+				  SDA_OUT_SW_MODE_VAL | SCL_OUT_SW_MODE_VAL);
+		regmap_write_bits(master->i3cg, I3CG_REG1(master->channel),
+				  SDA_SW_MODE_OE | SCL_SW_MODE_OE,
+				  SDA_SW_MODE_OE | SCL_SW_MODE_OE);
+		regmap_write_bits(master->i3cg, I3CG_REG1(master->channel),
+				  SDA_OUT_SW_MODE_EN | SCL_OUT_SW_MODE_EN,
+				  SDA_OUT_SW_MODE_EN | SCL_OUT_SW_MODE_EN);
+		regmap_write_bits(master->i3cg, I3CG_REG1(master->channel),
+				  SDA_IN_SW_MODE_VAL | SCL_IN_SW_MODE_VAL,
+				  SDA_IN_SW_MODE_VAL | SCL_IN_SW_MODE_VAL);
+		regmap_write_bits(master->i3cg, I3CG_REG1(master->channel),
+				  SDA_IN_SW_MODE_EN | SCL_IN_SW_MODE_EN,
+				  SDA_IN_SW_MODE_EN | SCL_IN_SW_MODE_EN);
+		regmap_write_bits(master->i3cg, I3CG_REG1(master->channel),
+				  SCL_OUT_SW_MODE_VAL, 0);
+		for (i = 0; i < 7; i++) {
+			regmap_write_bits(master->i3cg, I3CG_REG1(master->channel),
+					  SDA_OUT_SW_MODE_VAL, 0);
+			regmap_write_bits(master->i3cg, I3CG_REG1(master->channel),
+					  SDA_OUT_SW_MODE_VAL,
+					  SDA_OUT_SW_MODE_VAL);
+		}
+		regmap_write_bits(master->i3cg, I3CG_REG1(master->channel),
+				  SCL_OUT_SW_MODE_VAL, SCL_OUT_SW_MODE_VAL);
+		regmap_write_bits(master->i3cg, I3CG_REG1(master->channel),
+				  SDA_OUT_SW_MODE_VAL, 0);
+		regmap_write_bits(master->i3cg, I3CG_REG1(master->channel),
+				  SDA_OUT_SW_MODE_VAL, SDA_OUT_SW_MODE_VAL);
+		regmap_write_bits(master->i3cg, I3CG_REG1(master->channel),
+				  SDA_OUT_SW_MODE_EN | SCL_OUT_SW_MODE_EN, 0);
+		regmap_write_bits(master->i3cg, I3CG_REG1(master->channel),
+				  SDA_IN_SW_MODE_EN | SCL_IN_SW_MODE_EN, 0);
+	}
+}
+
+static int aspeed_i3c_ccc_set(struct aspeed_i3c_master *master,
+			  struct i3c_ccc_cmd *ccc)
+{
+	struct aspeed_i3c_xfer *xfer;
+	struct aspeed_i3c_cmd *cmd;
+	int ret, pos = 0;
+
+	if (ccc->id & I3C_CCC_DIRECT) {
+		pos = aspeed_i3c_master_sync_hw_dat(master, ccc->dests[0].addr);
+		if (pos < 0)
+			return pos;
+	}
+
+	xfer = aspeed_i3c_master_alloc_xfer(master, 1);
+	if (!xfer)
+		return -ENOMEM;
+
+	cmd = xfer->cmds;
+	cmd->tx_buf = ccc->dests[0].payload.data;
+	cmd->tx_len = ccc->dests[0].payload.len;
+
+	cmd->cmd_hi = COMMAND_PORT_ARG_DATA_LEN(ccc->dests[0].payload.len) |
+		      COMMAND_PORT_TRANSFER_ARG | COMMAND_PORT_ARG_DBP(ccc->db);
+
+	cmd->cmd_lo = COMMAND_PORT_CP |
+		      COMMAND_PORT_DEV_INDEX(pos) |
+		      COMMAND_PORT_CMD(ccc->id) |
+		      COMMAND_PORT_TOC |
+		      COMMAND_PORT_ROC |
+		      COMMAND_PORT_DBP(ccc->dbp);
+
+	if (ccc->id == I3C_CCC_SETHID || ccc->id == I3C_CCC_DEVCTRL)
+		cmd->cmd_lo |= COMMAND_PORT_SPEED(SPEED_I3C_I2C_FM);
+
+	dev_dbg(master->dev, "%s:cmd_hi=0x%08x cmd_lo=0x%08x tx_len=%d id=%x\n",
+		__func__, cmd->cmd_hi, cmd->cmd_lo, cmd->tx_len, ccc->id);
+
+	aspeed_i3c_master_enqueue_xfer(master, xfer);
+	if (!wait_for_completion_timeout(&xfer->comp, XFER_TIMEOUT))
+		aspeed_i3c_master_dequeue_xfer(master, xfer);
+
+	ret = xfer->ret;
+	if (ret)
+		dev_err(master->dev, "xfer error: %x\n", xfer->cmds[0].error);
+	if (xfer->cmds[0].error == RESPONSE_ERROR_IBA_NACK)
+		ccc->err = I3C_ERROR_M2;
+
+	aspeed_i3c_master_free_xfer(xfer);
+
+	return ret;
+}
+
+static int aspeed_i3c_ccc_get(struct aspeed_i3c_master *master, struct i3c_ccc_cmd *ccc)
+{
+	struct aspeed_i3c_xfer *xfer;
+	struct aspeed_i3c_cmd *cmd;
+	int ret, pos;
+
+	pos = aspeed_i3c_master_sync_hw_dat(master, ccc->dests[0].addr);
+	if (pos < 0)
+		return pos;
+
+	xfer = aspeed_i3c_master_alloc_xfer(master, 1);
+	if (!xfer)
+		return -ENOMEM;
+
+	cmd = xfer->cmds;
+	cmd->rx_buf = ccc->dests[0].payload.data;
+	cmd->rx_len = ccc->dests[0].payload.len;
+
+	cmd->cmd_hi = COMMAND_PORT_ARG_DATA_LEN(ccc->dests[0].payload.len) |
+		      COMMAND_PORT_TRANSFER_ARG | COMMAND_PORT_ARG_DBP(ccc->db);
+
+	cmd->cmd_lo = COMMAND_PORT_READ_TRANSFER |
+		      COMMAND_PORT_CP |
+		      COMMAND_PORT_DEV_INDEX(pos) |
+		      COMMAND_PORT_CMD(ccc->id) |
+		      COMMAND_PORT_TOC |
+		      COMMAND_PORT_ROC |
+		      COMMAND_PORT_DBP(ccc->dbp);
+
+	dev_dbg(master->dev, "%s:cmd_hi=0x%08x cmd_lo=0x%08x rx_len=%d id=%x\n",
+		__func__, cmd->cmd_hi, cmd->cmd_lo, cmd->rx_len, ccc->id);
+
+	aspeed_i3c_master_enqueue_xfer(master, xfer);
+	if (!wait_for_completion_timeout(&xfer->comp, XFER_TIMEOUT))
+		aspeed_i3c_master_dequeue_xfer(master, xfer);
+
+	ret = xfer->ret;
+	if (ret)
+		dev_err(master->dev, "xfer error: %x\n", xfer->cmds[0].error);
+	if (xfer->cmds[0].error == RESPONSE_ERROR_IBA_NACK)
+		ccc->err = I3C_ERROR_M2;
+	aspeed_i3c_master_free_xfer(xfer);
+
+	return ret;
+}
+
+static int aspeed_i3c_master_send_ccc_cmd(struct i3c_master_controller *m,
+				      struct i3c_ccc_cmd *ccc)
+{
+	struct aspeed_i3c_master *master = to_aspeed_i3c_master(m);
+	int ret = 0;
+	u32 i3c_pp_timing, i3c_od_timing;
+
+	if (ccc->id == I3C_CCC_ENTDAA)
+		return -EINVAL;
+
+	i3c_od_timing = readl(master->regs + SCL_I3C_OD_TIMING);
+	i3c_pp_timing = readl(master->regs + SCL_I3C_PP_TIMING);
+
+	dev_dbg(master->dev, "ccc-id %02x rnw=%d\n", ccc->id, ccc->rnw);
+
+	if (ccc->rnw)
+		ret = aspeed_i3c_ccc_get(master, ccc);
+	else
+		ret = aspeed_i3c_ccc_set(master, ccc);
+
+	return ret;
+}
+
+#define IS_MANUF_ID_ASPEED(x) (I3C_PID_MANUF_ID(x) == PID_MANUF_ID_ASPEED)
+#define IS_PART_ID_AST2600_SERIES(x)                                           \
+	((I3C_PID_PART_ID(x) & PID_PART_ID_AST2600_SERIES) ==                  \
+	 PID_PART_ID_AST2600_SERIES)
+#define IS_PART_ID_AST1030_A0(x)                                               \
+	((I3C_PID_PART_ID(x) & PID_PART_ID_AST1030_A0) ==                      \
+	 PID_PART_ID_AST1030_A0)
+
+static int aspeed_i3c_master_daa(struct i3c_master_controller *m)
+{
+	struct aspeed_i3c_master *master = to_aspeed_i3c_master(m);
+	struct aspeed_i3c_xfer *xfer;
+	struct aspeed_i3c_cmd *cmd;
+	u32 olddevs, newdevs, dat;
+	u8 p, last_addr = 0, last_grp = 0;
+	int ret, pos, ndevs;
+
+	olddevs = ~(master->free_pos);
+	ndevs = 0;
+
+	/* Prepare DAT before launching DAA. */
+	for (pos = 0; pos < master->maxdevs; pos++) {
+		if (olddevs & BIT(pos))
+			continue;
+
+		ret = i3c_master_get_free_addr(m, (last_grp + 1) << ADDR_GRP_SHIFT);
+		if (ret < 0)
+			break;
+
+		ndevs++;
+
+		master->addrs[pos] = ret;
+		p = even_parity(ret);
+		last_addr = ret;
+		last_grp = ADDR_GRP(last_addr);
+
+		dat = readl(master->regs +
+			    DEV_ADDR_TABLE_LOC(master->datstartaddr, pos));
+		dat &= ~(DEV_ADDR_TABLE_DYNAMIC_ADDR |
+			 DEV_ADDR_TABLE_DA_PARITY);
+		dat |= FIELD_PREP(DEV_ADDR_TABLE_DYNAMIC_ADDR, ret) |
+		       FIELD_PREP(DEV_ADDR_TABLE_DA_PARITY, p);
+		writel(dat, master->regs + DEV_ADDR_TABLE_LOC(
+						   master->datstartaddr, pos));
+	}
+
+	if (!ndevs)
+		return -ENOSPC;
+
+	xfer = aspeed_i3c_master_alloc_xfer(master, 1);
+	if (!xfer)
+		return -ENOMEM;
+
+	pos = aspeed_i3c_master_get_free_pos(master);
+	if (pos < 0) {
+		aspeed_i3c_master_free_xfer(xfer);
+		return pos;
+	}
+	cmd = &xfer->cmds[0];
+	cmd->cmd_hi = 0x1;
+	cmd->cmd_lo = COMMAND_PORT_DEV_COUNT(ndevs) |
+		      COMMAND_PORT_DEV_INDEX(pos) |
+		      COMMAND_PORT_CMD(I3C_CCC_ENTDAA) |
+		      COMMAND_PORT_ADDR_ASSGN_CMD |
+		      COMMAND_PORT_TOC |
+		      COMMAND_PORT_ROC;
+
+	aspeed_i3c_master_enqueue_xfer(master, xfer);
+	if (!wait_for_completion_timeout(&xfer->comp, XFER_TIMEOUT))
+		aspeed_i3c_master_dequeue_xfer(master, xfer);
+
+	newdevs = GENMASK(ndevs - cmd->rx_len - 1, 0) << pos;
+	for (pos = 0; pos < master->maxdevs; pos++) {
+		if (newdevs & BIT(pos)) {
+			u32 addr;
+
+			dat = GET_DAT_FROM_POS(master, pos);
+			addr = FIELD_GET(DEV_ADDR_TABLE_DYNAMIC_ADDR, dat);
+
+			aspeed_i3c_master_set_group_dat(master, addr, dat);
+			i3c_master_add_i3c_dev_locked(m, addr);
+		}
+
+		/* cleanup the free HW DATs */
+		if (master->free_pos & BIT(pos)) {
+			dat = readl(
+				master->regs +
+				DEV_ADDR_TABLE_LOC(master->datstartaddr, pos));
+			dat &= ~(DEV_ADDR_TABLE_DYNAMIC_ADDR |
+				 DEV_ADDR_TABLE_DA_PARITY);
+			dat |= FIELD_PREP(DEV_ADDR_TABLE_DA_PARITY,
+					  even_parity(0));
+			writel(dat, master->regs +
+					    DEV_ADDR_TABLE_LOC(
+						    master->datstartaddr, pos));
+		}
+	}
+
+	aspeed_i3c_master_free_xfer(xfer);
+
+	return 0;
+}
+#ifdef CONFIG_AST2600_I3C_CCC_WORKAROUND
+/*
+ * Provide an interface for sending CCC from userspace.  Especially for the
+ * transfers with PEC and direct CCC.
+ */
+static int aspeed_i3c_master_ccc_xfers(struct i3c_dev_desc *dev,
+				    struct i3c_priv_xfer *i3c_xfers,
+				    int i3c_nxfers)
+{
+	struct aspeed_i3c_i2c_dev_data *data = i3c_dev_get_master_data(dev);
+	struct i3c_master_controller *m = i3c_dev_get_master(dev);
+	struct aspeed_i3c_master *master = to_aspeed_i3c_master(m);
+	struct aspeed_i3c_xfer *xfer;
+	int i, ret = 0;
+	struct aspeed_i3c_cmd *cmd_ccc;
+
+	xfer = aspeed_i3c_master_alloc_xfer(master, i3c_nxfers);
+	if (!xfer)
+		return -ENOMEM;
+
+	/* i3c_xfers[0] handles the CCC data */
+	cmd_ccc = &xfer->cmds[0];
+	cmd_ccc->cmd_hi = COMMAND_PORT_ARG_DATA_LEN(i3c_xfers[0].len - 1) |
+			  COMMAND_PORT_TRANSFER_ARG;
+	cmd_ccc->tx_buf = i3c_xfers[0].data.out + 1;
+	cmd_ccc->tx_len = i3c_xfers[0].len - 1;
+	cmd_ccc->cmd_lo = COMMAND_PORT_SPEED(dev->info.max_write_ds);
+	cmd_ccc->cmd_lo |= COMMAND_PORT_TID(0) |
+			   COMMAND_PORT_DEV_INDEX(master->maxdevs - 1) |
+			   COMMAND_PORT_ROC;
+	if (i3c_nxfers == 1)
+		cmd_ccc->cmd_lo |= COMMAND_PORT_TOC;
+
+	dev_dbg(master->dev,
+		"%s:cmd_ccc_hi=0x%08x cmd_ccc_lo=0x%08x tx_len=%d\n", __func__,
+		cmd_ccc->cmd_hi, cmd_ccc->cmd_lo, cmd_ccc->tx_len);
+
+	for (i = 1; i < i3c_nxfers; i++) {
+		struct aspeed_i3c_cmd *cmd = &xfer->cmds[i];
+
+		cmd->cmd_hi = COMMAND_PORT_ARG_DATA_LEN(i3c_xfers[i].len) |
+			COMMAND_PORT_TRANSFER_ARG;
+
+		if (i3c_xfers[i].rnw) {
+			cmd->rx_buf = i3c_xfers[i].data.in;
+			cmd->rx_len = i3c_xfers[i].len;
+			cmd->cmd_lo = COMMAND_PORT_READ_TRANSFER |
+				      COMMAND_PORT_SPEED(dev->info.max_read_ds);
+
+		} else {
+			cmd->tx_buf = i3c_xfers[i].data.out;
+			cmd->tx_len = i3c_xfers[i].len;
+			cmd->cmd_lo =
+				COMMAND_PORT_SPEED(dev->info.max_write_ds);
+		}
+
+		cmd->cmd_lo |= COMMAND_PORT_TID(i) |
+			       COMMAND_PORT_DEV_INDEX(data->index) |
+			       COMMAND_PORT_ROC;
+
+		if (i == (i3c_nxfers - 1))
+			cmd->cmd_lo |= COMMAND_PORT_TOC;
+
+		dev_dbg(master->dev,
+			"%s:cmd_hi=0x%08x cmd_lo=0x%08x tx_len=%d rx_len=%d\n",
+			__func__, cmd->cmd_hi, cmd->cmd_lo, cmd->tx_len,
+			cmd->rx_len);
+	}
+
+	aspeed_i3c_master_enqueue_xfer(master, xfer);
+	if (!wait_for_completion_timeout(&xfer->comp, XFER_TIMEOUT))
+		aspeed_i3c_master_dequeue_xfer(master, xfer);
+
+	ret = xfer->ret;
+	aspeed_i3c_master_free_xfer(xfer);
+
+	return ret;
+}
+#endif
+static int aspeed_i3c_master_priv_xfers(struct i3c_dev_desc *dev,
+				    struct i3c_priv_xfer *i3c_xfers,
+				    int i3c_nxfers)
+{
+	struct aspeed_i3c_i2c_dev_data *data = i3c_dev_get_master_data(dev);
+	struct i3c_master_controller *m = i3c_dev_get_master(dev);
+	struct aspeed_i3c_master *master = to_aspeed_i3c_master(m);
+	unsigned int nrxwords = 0, ntxwords = 0;
+	struct aspeed_i3c_xfer *xfer;
+	int i, ret = 0;
+
+	if (!i3c_nxfers)
+		return 0;
+
+	if (i3c_nxfers > master->caps.cmdfifodepth)
+		return -ENOTSUPP;
+
+	for (i = 0; i < i3c_nxfers; i++) {
+		if (i3c_xfers[i].rnw)
+			nrxwords += DIV_ROUND_UP(i3c_xfers[i].len, 4);
+		else
+			ntxwords += DIV_ROUND_UP(i3c_xfers[i].len, 4);
+	}
+
+	if (ntxwords > master->caps.datafifodepth ||
+	    nrxwords > master->caps.datafifodepth)
+		return -ENOTSUPP;
+
+#ifdef CONFIG_AST2600_I3C_CCC_WORKAROUND
+	if (i3c_xfers[0].rnw == 0) {
+		/* write command: check if hit special address */
+		u8 tmp;
+
+		memcpy(&tmp, i3c_xfers[0].data.out, 1);
+		if (tmp == 0xff)
+			return aspeed_i3c_master_ccc_xfers(dev, i3c_xfers, i3c_nxfers);
+	}
+#endif
+
+	xfer = aspeed_i3c_master_alloc_xfer(master, i3c_nxfers);
+	if (!xfer)
+		return -ENOMEM;
+
+	data->index = aspeed_i3c_master_sync_hw_dat(master, dev->info.dyn_addr);
+
+	for (i = 0; i < i3c_nxfers; i++) {
+		struct aspeed_i3c_cmd *cmd = &xfer->cmds[i];
+
+		cmd->cmd_hi = COMMAND_PORT_ARG_DATA_LEN(i3c_xfers[i].len) |
+			COMMAND_PORT_TRANSFER_ARG;
+
+		if (i3c_xfers[i].rnw) {
+			cmd->rx_buf = i3c_xfers[i].data.in;
+			cmd->rx_len = i3c_xfers[i].len;
+			cmd->cmd_lo = COMMAND_PORT_READ_TRANSFER |
+				      COMMAND_PORT_SPEED(dev->info.max_read_ds);
+
+		} else {
+			cmd->tx_buf = i3c_xfers[i].data.out;
+			cmd->tx_len = i3c_xfers[i].len;
+			cmd->cmd_lo =
+				COMMAND_PORT_SPEED(dev->info.max_write_ds);
+		}
+
+		cmd->cmd_lo |= COMMAND_PORT_TID(i) |
+			       COMMAND_PORT_DEV_INDEX(data->index) |
+			       COMMAND_PORT_ROC;
+
+		if (i == (i3c_nxfers - 1))
+			cmd->cmd_lo |= COMMAND_PORT_TOC;
+
+		if (dev->info.pec)
+			cmd->cmd_lo |= COMMAND_PORT_PEC;
+
+		dev_dbg(master->dev,
+			"%s:cmd_hi=0x%08x cmd_lo=0x%08x tx_len=%d rx_len=%d\n",
+			__func__, cmd->cmd_hi, cmd->cmd_lo, cmd->tx_len,
+			cmd->rx_len);
+	}
+
+	aspeed_i3c_master_enqueue_xfer(master, xfer);
+	if (!wait_for_completion_timeout(&xfer->comp, XFER_TIMEOUT))
+		aspeed_i3c_master_dequeue_xfer(master, xfer);
+
+	for (i = 0; i < i3c_nxfers; i++) {
+		struct aspeed_i3c_cmd *cmd = &xfer->cmds[i];
+
+		if (i3c_xfers[i].rnw)
+			i3c_xfers[i].len = cmd->rx_len;
+	}
+
+	ret = xfer->ret;
+	if (ret)
+		dev_err(master->dev, "xfer error: %x\n", xfer->cmds[0].error);
+	aspeed_i3c_master_free_xfer(xfer);
+
+	return ret;
+}
+
+static int aspeed_i3c_master_send_hdr_cmd(struct i3c_master_controller *m,
+					  struct i3c_hdr_cmd *cmds,
+					  int ncmds)
+{
+	struct aspeed_i3c_master *master = to_aspeed_i3c_master(m);
+	u8 dat_index;
+	int ret, i, ntxwords = 0, nrxwords = 0;
+	struct aspeed_i3c_xfer *xfer;
+
+	if (ncmds < 1)
+		return 0;
+
+	dev_dbg(master->dev, "ncmds = %x", ncmds);
+
+	if (ncmds > master->caps.cmdfifodepth)
+		return -ENOTSUPP;
+
+	for (i = 0; i < ncmds; i++) {
+		dev_dbg(master->dev, "cmds[%d] mode = %x", i, cmds[i].mode);
+		if (cmds[i].mode != I3C_HDR_DDR)
+			return -ENOTSUPP;
+		if (cmds[i].code & 0x80)
+			nrxwords += DIV_ROUND_UP(cmds[i].ndatawords, 2);
+		else
+			ntxwords += DIV_ROUND_UP(cmds[i].ndatawords, 2);
+	}
+	dev_dbg(master->dev, "ntxwords = %x, nrxwords = %x", ntxwords,
+		 nrxwords);
+	if (ntxwords > master->caps.datafifodepth ||
+	    nrxwords > master->caps.datafifodepth)
+		return -ENOTSUPP;
+
+	xfer = aspeed_i3c_master_alloc_xfer(master, ncmds);
+	if (!xfer)
+		return -ENOMEM;
+
+	for (i = 0; i < ncmds; i++) {
+		struct aspeed_i3c_cmd *cmd = &xfer->cmds[i];
+
+		dat_index = aspeed_i3c_master_sync_hw_dat(master, cmds[i].addr);
+
+		cmd->cmd_hi = COMMAND_PORT_ARG_DATA_LEN(cmds[i].ndatawords << 1) |
+			      COMMAND_PORT_TRANSFER_ARG;
+
+		if (cmds[i].code & 0x80) {
+			cmd->rx_buf = cmds[i].data.in;
+			cmd->rx_len = cmds[i].ndatawords << 1;
+			cmd->cmd_lo = COMMAND_PORT_READ_TRANSFER |
+				      COMMAND_PORT_CP |
+				      COMMAND_PORT_CMD(cmds[i].code) |
+				      COMMAND_PORT_SPEED(SPEED_I3C_HDR_DDR);
+
+		} else {
+			cmd->tx_buf = cmds[i].data.out;
+			cmd->tx_len = cmds[i].ndatawords << 1;
+			cmd->cmd_lo = COMMAND_PORT_CP |
+				      COMMAND_PORT_CMD(cmds[i].code) |
+				      COMMAND_PORT_SPEED(SPEED_I3C_HDR_DDR);
+		}
+
+		cmd->cmd_lo |= COMMAND_PORT_TID(i) |
+			       COMMAND_PORT_DEV_INDEX(dat_index) |
+			       COMMAND_PORT_ROC;
+
+		if (i == (ncmds - 1))
+			cmd->cmd_lo |= COMMAND_PORT_TOC;
+
+		dev_dbg(master->dev,
+			 "%s:cmd_hi=0x%08x cmd_lo=0x%08x tx_len=%d rx_len=%d\n",
+			 __func__, cmd->cmd_hi, cmd->cmd_lo, cmd->tx_len,
+			 cmd->rx_len);
+	}
+
+	aspeed_i3c_master_enqueue_xfer(master, xfer);
+	if (!wait_for_completion_timeout(&xfer->comp, XFER_TIMEOUT))
+		aspeed_i3c_master_dequeue_xfer(master, xfer);
+
+	for (i = 0; i < ncmds; i++) {
+		struct aspeed_i3c_cmd *cmd = &xfer->cmds[i];
+
+		if (cmds[i].code & 0x80)
+			cmds[i].ndatawords = DIV_ROUND_UP(cmd->rx_len, 2);
+	}
+
+	ret = xfer->ret;
+	if (ret)
+		dev_err(master->dev, "xfer error: %x\n", xfer->cmds[0].error);
+	aspeed_i3c_master_free_xfer(xfer);
+
+	return ret;
+}
+
+static int aspeed_i3c_master_reattach_i3c_dev(struct i3c_dev_desc *dev,
+					  u8 old_dyn_addr)
+{
+	struct aspeed_i3c_i2c_dev_data *data = i3c_dev_get_master_data(dev);
+	struct i3c_master_controller *m = i3c_dev_get_master(dev);
+	struct aspeed_i3c_master *master = to_aspeed_i3c_master(m);
+
+	aspeed_i3c_master_set_group_dat(
+		master, dev->info.dyn_addr,
+		FIELD_PREP(DEV_ADDR_TABLE_DYNAMIC_ADDR, dev->info.dyn_addr));
+
+	master->addrs[data->index] = dev->info.dyn_addr;
+
+	return 0;
+}
+
+static int aspeed_i3c_master_attach_i3c_dev(struct i3c_dev_desc *dev)
+{
+	struct i3c_master_controller *m = i3c_dev_get_master(dev);
+	struct aspeed_i3c_master *master = to_aspeed_i3c_master(m);
+	struct aspeed_i3c_i2c_dev_data *data;
+	int pos;
+	u8 addr = dev->info.dyn_addr ? : dev->info.static_addr;
+
+	pos = aspeed_i3c_master_set_group_dat(
+		master, addr, FIELD_PREP(DEV_ADDR_TABLE_DYNAMIC_ADDR, addr));
+	if (pos < 0)
+		return pos;
+
+	data = kzalloc(sizeof(*data), GFP_KERNEL);
+	if (!data)
+		return -ENOMEM;
+
+	data->index = aspeed_i3c_master_get_group_hw_index(master, addr);
+	master->addrs[pos] = addr;
+	i3c_dev_set_master_data(dev, data);
+
+	if (master->base.jdec_spd)
+		dev->info.max_write_ds = dev->info.max_read_ds = 0;
+
+	return 0;
+}
+
+static void aspeed_i3c_master_detach_i3c_dev(struct i3c_dev_desc *dev)
+{
+	struct aspeed_i3c_i2c_dev_data *data = i3c_dev_get_master_data(dev);
+	struct i3c_master_controller *m = i3c_dev_get_master(dev);
+	struct aspeed_i3c_master *master = to_aspeed_i3c_master(m);
+
+	aspeed_i3c_master_set_group_dat(master, dev->info.dyn_addr, 0);
+
+	i3c_dev_set_master_data(dev, NULL);
+	master->addrs[data->index] = 0;
+	kfree(data);
+}
+
+static int aspeed_i3c_master_i2c_xfers(struct i2c_dev_desc *dev,
+				   const struct i2c_msg *i2c_xfers,
+				   int i2c_nxfers)
+{
+	struct aspeed_i3c_i2c_dev_data *data = i2c_dev_get_master_data(dev);
+	struct i3c_master_controller *m = i2c_dev_get_master(dev);
+	struct aspeed_i3c_master *master = to_aspeed_i3c_master(m);
+	unsigned int nrxwords = 0, ntxwords = 0;
+	struct aspeed_i3c_xfer *xfer;
+	int i, ret = 0;
+
+	if (!i2c_nxfers)
+		return 0;
+
+	if (i2c_nxfers > master->caps.cmdfifodepth)
+		return -ENOTSUPP;
+
+	for (i = 0; i < i2c_nxfers; i++) {
+		if (i2c_xfers[i].flags & I2C_M_RD)
+			nrxwords += DIV_ROUND_UP(i2c_xfers[i].len, 4);
+		else
+			ntxwords += DIV_ROUND_UP(i2c_xfers[i].len, 4);
+	}
+
+	if (ntxwords > master->caps.datafifodepth ||
+	    nrxwords > master->caps.datafifodepth)
+		return -ENOTSUPP;
+
+	xfer = aspeed_i3c_master_alloc_xfer(master, i2c_nxfers);
+	if (!xfer)
+		return -ENOMEM;
+
+	data->index = aspeed_i3c_master_sync_hw_dat(master, dev->addr);
+
+	for (i = 0; i < i2c_nxfers; i++) {
+		struct aspeed_i3c_cmd *cmd = &xfer->cmds[i];
+
+		cmd->cmd_hi = COMMAND_PORT_ARG_DATA_LEN(i2c_xfers[i].len) |
+			COMMAND_PORT_TRANSFER_ARG;
+
+		cmd->cmd_lo = COMMAND_PORT_TID(i) |
+			      COMMAND_PORT_DEV_INDEX(data->index) |
+			      COMMAND_PORT_ROC;
+
+		if (i2c_xfers[i].flags & I2C_M_RD) {
+			cmd->cmd_lo |= COMMAND_PORT_READ_TRANSFER;
+			cmd->rx_buf = i2c_xfers[i].buf;
+			cmd->rx_len = i2c_xfers[i].len;
+		} else {
+			cmd->tx_buf = i2c_xfers[i].buf;
+			cmd->tx_len = i2c_xfers[i].len;
+		}
+
+		if (i == (i2c_nxfers - 1))
+			cmd->cmd_lo |= COMMAND_PORT_TOC;
+
+		dev_dbg(master->dev,
+			"%s:cmd_hi=0x%08x cmd_lo=0x%08x tx_len=%d rx_len=%d\n",
+			__func__, cmd->cmd_hi, cmd->cmd_lo, cmd->tx_len,
+			cmd->rx_len);
+	}
+
+	aspeed_i3c_master_enqueue_xfer(master, xfer);
+	if (!wait_for_completion_timeout(&xfer->comp, XFER_TIMEOUT))
+		aspeed_i3c_master_dequeue_xfer(master, xfer);
+
+	ret = xfer->ret;
+	if (ret)
+		dev_err(master->dev, "xfer error: %x\n", xfer->cmds[0].error);
+	aspeed_i3c_master_free_xfer(xfer);
+
+	return ret;
+}
+
+static int aspeed_i3c_master_attach_i2c_dev(struct i2c_dev_desc *dev)
+{
+	struct i3c_master_controller *m = i2c_dev_get_master(dev);
+	struct aspeed_i3c_master *master = to_aspeed_i3c_master(m);
+	struct aspeed_i3c_i2c_dev_data *data;
+	int pos;
+
+	pos = aspeed_i3c_master_set_group_dat(
+		master, dev->addr,
+		DEV_ADDR_TABLE_LEGACY_I2C_DEV |
+			FIELD_PREP(DEV_ADDR_TABLE_STATIC_ADDR, dev->addr));
+
+	if (pos < 0)
+		return pos;
+
+	data = kzalloc(sizeof(*data), GFP_KERNEL);
+	if (!data)
+		return -ENOMEM;
+
+	data->index = aspeed_i3c_master_get_group_hw_index(master, dev->addr);
+	master->addrs[data->index] = dev->addr;
+	i2c_dev_set_master_data(dev, data);
+
+
+	return 0;
+}
+
+static void aspeed_i3c_master_detach_i2c_dev(struct i2c_dev_desc *dev)
+{
+	struct aspeed_i3c_i2c_dev_data *data = i2c_dev_get_master_data(dev);
+	struct i3c_master_controller *m = i2c_dev_get_master(dev);
+	struct aspeed_i3c_master *master = to_aspeed_i3c_master(m);
+
+	aspeed_i3c_master_set_group_dat(master, dev->addr, 0);
+
+	i2c_dev_set_master_data(dev, NULL);
+	master->addrs[data->index] = 0;
+	kfree(data);
+}
+
+static void aspeed_i3c_slave_event_handler(struct aspeed_i3c_master *master)
+{
+	u32 event = readl(master->regs + SLV_EVENT_CTRL);
+	u32 reg = readl(master->regs + PRESENT_STATE);
+	u32 cm_state = FIELD_GET(CM_TFR_STS, reg);
+
+	if (cm_state == CM_TFR_STS_SLAVE_HALT) {
+		dev_dbg(master->dev, "slave in halt state\n");
+		aspeed_i3c_master_resume(master);
+	}
+
+	dev_dbg(master->dev, "slave event=%08x\n", event);
+	if (event & SLV_EVENT_CTRL_MRL_UPD)
+		dev_dbg(master->dev, "isr: master set mrl=%d\n",
+			readl(master->regs + SLV_MAX_LEN) >> 16);
+
+	if (event & SLV_EVENT_CTRL_MWL_UPD)
+		dev_dbg(master->dev, "isr: master set mwl=%ld\n",
+			readl(master->regs + SLV_MAX_LEN) & GENMASK(15, 0));
+
+	writel(event, master->regs + SLV_EVENT_CTRL);
+}
+
+static void aspeed_i3c_slave_resp_handler(struct aspeed_i3c_master *master,
+					  u32 status)
+{
+	int i, has_error = 0;
+	u32 resp, nbytes, nresp;
+	u8 error, tid;
+	u32 *buf = master->slave_data.buf;
+	struct i3c_slave_payload payload;
+
+	nresp = readl(master->regs + QUEUE_STATUS_LEVEL);
+	nresp = QUEUE_STATUS_LEVEL_RESP(nresp);
+
+	for (i = 0; i < nresp; i++) {
+		resp = readl(master->regs + RESPONSE_QUEUE_PORT);
+		error = RESPONSE_PORT_ERR_STATUS(resp);
+		nbytes = RESPONSE_PORT_DATA_LEN(resp);
+		tid = RESPONSE_PORT_TID(resp);
+
+		if (error) {
+			has_error = 1;
+			if (error == RESPONSE_ERROR_EARLY_TERMINATE)
+				dev_dbg(master->dev,
+					"early termination, remain length %d\n",
+					nbytes);
+		}
+
+		if (!error && nbytes) {
+			aspeed_i3c_master_read_rx_fifo(master, (u8 *)buf, nbytes);
+
+			payload.len = nbytes;
+			payload.data = buf;
+			/* currently only support master write transfer */
+			if (master->slave_data.callback && (tid == TID_MASTER_WRITE_DATA))
+				master->slave_data.callback(&master->base, &payload);
+		}
+
+		if (!error && !nbytes) {
+			if (status & INTR_IBI_UPDATED_STAT && tid == TID_SLAVE_IBI_DONE)
+				complete(&master->sir_complete);
+			else if (tid == TID_MASTER_READ_DATA)
+				complete(&master->data_read_complete);
+			else
+				dev_warn(master->dev, "Unreogized response %x",
+					 resp);
+		}
+	}
+
+	if (has_error) {
+		writel(RESET_CTRL_QUEUES, master->regs + RESET_CTRL);
+		aspeed_i3c_master_resume(master);
+	}
+}
+
+static irqreturn_t aspeed_i3c_master_irq_handler(int irq, void *dev_id)
+{
+	struct aspeed_i3c_master *master = dev_id;
+	u32 status;
+
+	status = readl(master->regs + INTR_STATUS);
+
+	if (!(status & readl(master->regs + INTR_STATUS_EN))) {
+		writel(INTR_ALL, master->regs + INTR_STATUS);
+		return IRQ_NONE;
+	}
+
+	if (master->secondary) {
+		if (status & INTR_READ_REQ_RECV_STAT)
+			dev_dbg(master->dev, "read queue received\n");
+
+		if (status & INTR_RESP_READY_STAT)
+			aspeed_i3c_slave_resp_handler(master, status);
+
+		if (status & INTR_CCC_UPDATED_STAT)
+			aspeed_i3c_slave_event_handler(master);
+	} else {
+		u32 reg, cm_state, xfr_state;
+
+		if (status & INTR_RESP_READY_STAT ||
+		    status & INTR_TRANSFER_ERR_STAT) {
+			spin_lock(&master->xferqueue.lock);
+			aspeed_i3c_master_end_xfer_locked(master, status);
+			if (status & INTR_TRANSFER_ERR_STAT)
+				writel(INTR_TRANSFER_ERR_STAT, master->regs + INTR_STATUS);
+			spin_unlock(&master->xferqueue.lock);
+		}
+
+		if (status & INTR_IBI_THLD_STAT)
+			aspeed_i3c_master_demux_ibis(master);
+
+		/*
+		 * check whether the controller is in halt state and resume the
+		 * controller if it is in halt state
+		 */
+		reg = readl(master->regs + PRESENT_STATE);
+		cm_state = FIELD_GET(CM_TFR_ST_STS, reg);
+		xfr_state = FIELD_GET(CM_TFR_STS, reg);
+
+		if (cm_state == CM_TFR_ST_STS_HALT ||
+		    xfr_state == CM_TFR_STS_MASTER_HALT) {
+			dev_dbg(master->dev, "master in halt state, resume\n");
+			writel(RESET_CTRL_QUEUES, master->regs + RESET_CTRL);
+			aspeed_i3c_master_resume(master);
+		}
+	}
+
+	writel(status, master->regs + INTR_STATUS);
+
+	return IRQ_HANDLED;
+}
+
+static void aspeed_i3c_master_enable_ibi_irq(struct aspeed_i3c_master *master)
+{
+	u32 reg;
+
+	reg = readl(master->regs + INTR_STATUS_EN);
+	reg |= INTR_IBI_THLD_STAT;
+	writel(reg, master->regs + INTR_STATUS_EN);
+
+	reg = readl(master->regs + INTR_SIGNAL_EN);
+	reg |= INTR_IBI_THLD_STAT;
+	writel(reg, master->regs + INTR_SIGNAL_EN);
+}
+
+static void aspeed_i3c_master_disable_ibi_irq(struct aspeed_i3c_master *master)
+{
+	u32 reg;
+
+	reg = readl(master->regs + INTR_STATUS_EN);
+	reg &= ~INTR_IBI_THLD_STAT;
+	writel(reg, master->regs + INTR_STATUS_EN);
+
+	reg = readl(master->regs + INTR_SIGNAL_EN);
+	reg &= ~INTR_IBI_THLD_STAT;
+	writel(reg, master->regs + INTR_SIGNAL_EN);
+}
+
+static int aspeed_i3c_master_disable_ibi(struct i3c_dev_desc *dev)
+{
+	struct i3c_master_controller *m = i3c_dev_get_master(dev);
+	struct aspeed_i3c_master *master = to_aspeed_i3c_master(m);
+	struct aspeed_i3c_i2c_dev_data *data = i3c_dev_get_master_data(dev);
+	struct aspeed_i3c_dev_group *dev_grp =
+		aspeed_i3c_master_get_group(master, dev->info.dyn_addr);
+	unsigned long flags;
+	u32 sirmap, dat, hj_nack;
+	int ret, i;
+	bool ibi_enable = false;
+
+	ret = i3c_master_disec_locked(m, dev->info.dyn_addr,
+				      I3C_CCC_EVENT_SIR);
+	if (ret)
+		return ret;
+
+	spin_lock_irqsave(&master->ibi.lock, flags);
+	dat = aspeed_i3c_master_get_group_dat(master, dev->info.dyn_addr);
+	dat |= DEV_ADDR_TABLE_SIR_REJECT;
+	dat &= ~DEV_ADDR_TABLE_IBI_WITH_DATA;
+	aspeed_i3c_master_set_group_dat(master, dev->info.dyn_addr, dat);
+
+	/*
+	 * if any available device in this group still needs to enable ibi, then
+	 * just keep the hw setting until all of the devices agree to disable ibi
+	 */
+	for (i = 0; i < MAX_DEVS_IN_GROUP; i++) {
+		if ((!(dev_grp->free_pos & BIT(i))) &&
+		    (!(dev_grp->dat[i] & DEV_ADDR_TABLE_SIR_REJECT))) {
+			ibi_enable = true;
+			break;
+		}
+	}
+
+	if (!ibi_enable) {
+		sirmap = readl(master->regs + IBI_SIR_REQ_REJECT);
+		sirmap |= BIT(data->ibi);
+		writel(sirmap, master->regs + IBI_SIR_REQ_REJECT);
+
+		dev_grp->mask.clr |= DEV_ADDR_TABLE_IBI_WITH_DATA |
+				     DEV_ADDR_TABLE_IBI_ADDR_MASK;
+		dev_grp->mask.set &= ~DEV_ADDR_TABLE_IBI_WITH_DATA;
+		dev_grp->mask.set |= DEV_ADDR_TABLE_SIR_REJECT;
+	}
+
+	sirmap = readl(master->regs + IBI_SIR_REQ_REJECT);
+	hj_nack = readl(master->regs + DEVICE_CTRL) & DEV_CTRL_HOT_JOIN_NACK;
+	if (sirmap == IBI_REQ_REJECT_ALL && hj_nack)
+		aspeed_i3c_master_disable_ibi_irq(master);
+	else
+		aspeed_i3c_master_enable_ibi_irq(master);
+
+	spin_unlock_irqrestore(&master->ibi.lock, flags);
+
+	return ret;
+}
+
+static int aspeed_i3c_master_enable_ibi(struct i3c_dev_desc *dev)
+{
+	struct i3c_master_controller *m = i3c_dev_get_master(dev);
+	struct aspeed_i3c_master *master = to_aspeed_i3c_master(m);
+	struct aspeed_i3c_i2c_dev_data *data = i3c_dev_get_master_data(dev);
+	struct aspeed_i3c_dev_group *dev_grp =
+		aspeed_i3c_master_get_group(master, dev->info.dyn_addr);
+	unsigned long flags;
+	u32 sirmap, hj_nack;
+	u32 sirmap_backup, mask_clr_backup, mask_set_backup;
+	int ret;
+
+	spin_lock_irqsave(&master->ibi.lock, flags);
+	sirmap_backup = readl(master->regs + IBI_SIR_REQ_REJECT);
+	sirmap = sirmap_backup & ~BIT(data->ibi);
+	writel(sirmap, master->regs + IBI_SIR_REQ_REJECT);
+
+	mask_clr_backup = dev_grp->mask.clr;
+	mask_set_backup = dev_grp->mask.set;
+	dev_grp->mask.clr |= DEV_ADDR_TABLE_SIR_REJECT | DEV_ADDR_TABLE_IBI_ADDR_MASK;
+	dev_grp->mask.set &= ~DEV_ADDR_TABLE_SIR_REJECT;
+	dev_grp->mask.set |= FIELD_PREP(DEV_ADDR_TABLE_IBI_ADDR_MASK,
+					IBI_ADDR_MASK_LAST_3BITS);
+	if (IS_MANUF_ID_ASPEED(dev->info.pid))
+		dev_grp->mask.set |= DEV_ADDR_TABLE_IBI_PEC_EN;
+	if (dev->info.bcr & I3C_BCR_IBI_PAYLOAD)
+		dev_grp->mask.set |= DEV_ADDR_TABLE_IBI_WITH_DATA;
+
+	spin_unlock_irqrestore(&master->ibi.lock, flags);
+
+	dev_dbg(master->dev, "addr:%x, hw_index:%d, data->ibi:%d, mask: %08x %08x\n",
+		dev->info.dyn_addr, dev_grp->hw_index, data->ibi, dev_grp->mask.set,
+		dev_grp->mask.clr);
+
+	/* Dat will be synchronized before sending the CCC */
+	ret = i3c_master_enec_locked(m, dev->info.dyn_addr,
+				     I3C_CCC_EVENT_SIR);
+
+	if (ret) {
+		spin_lock_irqsave(&master->ibi.lock, flags);
+		writel(sirmap_backup, master->regs + IBI_SIR_REQ_REJECT);
+
+		dev_grp->mask.clr = mask_clr_backup;
+		dev_grp->mask.set = mask_set_backup;
+		aspeed_i3c_master_sync_hw_dat(master, dev->info.dyn_addr);
+		spin_unlock_irqrestore(&master->ibi.lock, flags);
+	}
+
+	sirmap = readl(master->regs + IBI_SIR_REQ_REJECT);
+	hj_nack = readl(master->regs + DEVICE_CTRL) & DEV_CTRL_HOT_JOIN_NACK;
+	if (sirmap == IBI_REQ_REJECT_ALL && hj_nack)
+		aspeed_i3c_master_disable_ibi_irq(master);
+	else
+		aspeed_i3c_master_enable_ibi_irq(master);
+
+	return ret;
+}
+
+static int aspeed_i3c_master_request_ibi(struct i3c_dev_desc *dev,
+				       const struct i3c_ibi_setup *req)
+{
+	struct i3c_master_controller *m = i3c_dev_get_master(dev);
+	struct aspeed_i3c_master *master = to_aspeed_i3c_master(m);
+	struct aspeed_i3c_i2c_dev_data *data = i3c_dev_get_master_data(dev);
+	unsigned long flags;
+	unsigned int i;
+
+	data->ibi_pool = i3c_generic_ibi_alloc_pool(dev, req);
+	if (IS_ERR(data->ibi_pool))
+		return PTR_ERR(data->ibi_pool);
+
+	spin_lock_irqsave(&master->ibi.lock, flags);
+	master->ibi.slots[dev->info.dyn_addr & 0x7f] = dev;
+	master->ibi.received_ibi_len[dev->info.dyn_addr & 0x7f] = 0;
+	data->ibi = aspeed_i3c_master_get_group_hw_index(master,
+							 dev->info.dyn_addr);
+	spin_unlock_irqrestore(&master->ibi.lock, flags);
+
+	if (i < MAX_DEVS)
+		return 0;
+
+	i3c_generic_ibi_free_pool(data->ibi_pool);
+	data->ibi_pool = NULL;
+
+	return -ENOSPC;
+}
+
+static void aspeed_i3c_master_free_ibi(struct i3c_dev_desc *dev)
+{
+	struct i3c_master_controller *m = i3c_dev_get_master(dev);
+	struct aspeed_i3c_master *master = to_aspeed_i3c_master(m);
+	struct aspeed_i3c_i2c_dev_data *data = i3c_dev_get_master_data(dev);
+	unsigned long flags;
+
+	spin_lock_irqsave(&master->ibi.lock, flags);
+	master->ibi.slots[dev->info.dyn_addr] = NULL;
+	data->ibi = -1;
+	spin_unlock_irqrestore(&master->ibi.lock, flags);
+
+	i3c_generic_ibi_free_pool(data->ibi_pool);
+}
+
+static void aspeed_i3c_master_recycle_ibi_slot(struct i3c_dev_desc *dev,
+					     struct i3c_ibi_slot *slot)
+{
+	struct aspeed_i3c_i2c_dev_data *data = i3c_dev_get_master_data(dev);
+
+	i3c_generic_ibi_recycle_slot(data->ibi_pool, slot);
+}
+
+static int aspeed_i3c_master_register_slave(struct i3c_master_controller *m,
+			      const struct i3c_slave_setup *req)
+{
+	struct aspeed_i3c_master *master = to_aspeed_i3c_master(m);
+	u32 *buf;
+
+	buf = kzalloc(req->max_payload_len, GFP_KERNEL);
+	if (!buf)
+		return -ENOMEM;
+
+	master->slave_data.callback = req->handler;
+	master->slave_data.buf = buf;
+
+	return 0;
+}
+
+static int aspeed_i3c_master_unregister_slave(struct i3c_master_controller *m)
+{
+	struct aspeed_i3c_master *master = to_aspeed_i3c_master(m);
+
+	master->slave_data.callback = NULL;
+	kfree(master->slave_data.buf);
+
+	return 0;
+}
+
+static int aspeed_i3c_master_send_sir(struct i3c_master_controller *m,
+				      struct i3c_slave_payload *payload)
+{
+	struct aspeed_i3c_master *master = to_aspeed_i3c_master(m);
+	uint32_t slv_event, intr_req, reg, thld_ctrl;
+	uint8_t *data = (uint8_t *)payload->data;
+
+	slv_event = readl(master->regs + SLV_EVENT_CTRL);
+	if ((slv_event & SLV_EVENT_CTRL_SIR_EN) == 0)
+		return -EPERM;
+
+	if (!payload)
+		return -ENXIO;
+
+	if (payload->len > (CONFIG_AST2600_I3C_IBI_MAX_PAYLOAD + 1)) {
+		dev_err(master->dev,
+			"input length %d exceeds max ibi payload size %d\n",
+			payload->len, CONFIG_AST2600_I3C_IBI_MAX_PAYLOAD + 1);
+		return -E2BIG;
+	}
+
+	init_completion(&master->sir_complete);
+
+	reg = readl(master->regs + DEVICE_CTRL);
+	reg &= ~DEV_CTRL_SLAVE_MDB;
+	reg |= FIELD_PREP(DEV_CTRL_SLAVE_MDB, data[0]) |
+	       FIELD_PREP(DEV_CTRL_SLAVE_PEC_EN, 1);
+	writel(reg, master->regs + DEVICE_CTRL);
+
+	aspeed_i3c_master_wr_tx_fifo(master, data, payload->len);
+
+	reg = FIELD_PREP(COMMAND_PORT_SLAVE_DATA_LEN, payload->len);
+	writel(reg, master->regs + COMMAND_QUEUE_PORT);
+
+	thld_ctrl = readl(master->regs + QUEUE_THLD_CTRL);
+	thld_ctrl &= ~QUEUE_THLD_CTRL_RESP_BUF_MASK;
+	thld_ctrl |= QUEUE_THLD_CTRL_RESP_BUF(1);
+	writel(thld_ctrl, master->regs + QUEUE_THLD_CTRL);
+
+	writel(1, master->regs + SLV_INTR_REQ);
+	if (!wait_for_completion_timeout(&master->sir_complete, XFER_TIMEOUT)) {
+		dev_err(master->dev, "send sir timeout\n");
+		writel(RESET_CTRL_RX_FIFO | RESET_CTRL_TX_FIFO |
+			       RESET_CTRL_RESP_QUEUE | RESET_CTRL_CMD_QUEUE,
+		       master->regs + RESET_CTRL);
+	}
+
+	reg = readl(master->regs + DEVICE_CTRL);
+	reg &= ~DEV_CTRL_SLAVE_PEC_EN;
+	writel(reg, master->regs + DEVICE_CTRL);
+
+	intr_req = readl(master->regs + SLV_INTR_REQ);
+	if (SLV_INTR_REQ_IBI_STS(intr_req) != SLV_IBI_STS_OK) {
+		slv_event = readl(master->regs + SLV_EVENT_CTRL);
+		if ((slv_event & SLV_EVENT_CTRL_SIR_EN) == 0)
+			pr_warn("sir is disabled by master\n");
+		return -EACCES;
+	}
+
+	return 0;
+}
+
+static int aspeed_i3c_slave_reset_queue(struct aspeed_i3c_master *master)
+{
+	u32 wait_enable_us;
+	int ret = 0;
+
+	aspeed_i3c_isolate_scl_sda(master, true);
+	aspeed_i3c_master_disable(master);
+	aspeed_i3c_toggle_scl_in(master, 8);
+	if (readl(master->regs + DEVICE_CTRL) & DEV_CTRL_ENABLE) {
+		dev_warn(master->dev,
+			 "Master read timeout: failed to disable controller");
+		ret = -EACCES;
+		goto reset_finished;
+	}
+	writel(RESET_CTRL_QUEUES, master->regs + RESET_CTRL);
+	aspeed_i3c_master_enable(master);
+	wait_enable_us = DIV_ROUND_UP(
+		master->timing.core_period *
+			FIELD_GET(GENMASK(31, 16),
+				  readl(master->regs + BUS_FREE_TIMING)),
+		NSEC_PER_USEC);
+	udelay(wait_enable_us);
+	aspeed_i3c_toggle_scl_in(master, 8);
+	if (!(readl(master->regs + DEVICE_CTRL) & DEV_CTRL_ENABLE)) {
+		dev_warn(master->dev,
+			 "Master read timeout: failed to enable controller");
+		ret = -EACCES;
+	}
+reset_finished:
+	aspeed_i3c_isolate_scl_sda(master, false);
+	return ret;
+}
+
+static int aspeed_i3c_master_put_read_data(struct i3c_master_controller *m,
+					   struct i3c_slave_payload *data,
+					   struct i3c_slave_payload *ibi_notify)
+{
+	struct aspeed_i3c_master *master = to_aspeed_i3c_master(m);
+	u32 reg, thld_ctrl;
+	u8 *buf;
+	int ret;
+
+	if (!data)
+		return -ENXIO;
+
+	if (ibi_notify) {
+		buf = (u8 *)ibi_notify->data;
+		init_completion(&master->sir_complete);
+
+		reg = readl(master->regs + DEVICE_CTRL);
+		reg &= ~DEV_CTRL_SLAVE_MDB;
+		reg |= FIELD_PREP(DEV_CTRL_SLAVE_MDB, buf[0]) |
+		       FIELD_PREP(DEV_CTRL_SLAVE_PEC_EN, 1);
+		writel(reg, master->regs + DEVICE_CTRL);
+
+		aspeed_i3c_master_wr_tx_fifo(master, buf, ibi_notify->len);
+
+		reg = FIELD_PREP(COMMAND_PORT_SLAVE_DATA_LEN, ibi_notify->len) |
+		      COMMAND_PORT_SLAVE_TID(TID_SLAVE_IBI_DONE);
+		writel(reg, master->regs + COMMAND_QUEUE_PORT);
+
+		thld_ctrl = readl(master->regs + QUEUE_THLD_CTRL);
+		thld_ctrl &= ~QUEUE_THLD_CTRL_RESP_BUF_MASK;
+		thld_ctrl |= QUEUE_THLD_CTRL_RESP_BUF(1);
+		writel(thld_ctrl, master->regs + QUEUE_THLD_CTRL);
+	}
+
+	buf = (u8 *)data->data;
+	init_completion(&master->data_read_complete);
+	aspeed_i3c_master_wr_tx_fifo(master, buf, data->len);
+
+	reg = FIELD_PREP(COMMAND_PORT_SLAVE_DATA_LEN, data->len) |
+	      COMMAND_PORT_SLAVE_TID(TID_MASTER_READ_DATA);
+	writel(reg, master->regs + COMMAND_QUEUE_PORT);
+
+	if (ibi_notify) {
+		writel(1, master->regs + SLV_INTR_REQ);
+		if (!wait_for_completion_timeout(&master->sir_complete,
+						 XFER_TIMEOUT)) {
+			dev_err(master->dev, "send sir timeout\n");
+			writel(RESET_CTRL_QUEUES, master->regs + RESET_CTRL);
+		}
+
+		reg = readl(master->regs + DEVICE_CTRL);
+		reg &= ~DEV_CTRL_SLAVE_PEC_EN;
+		writel(reg, master->regs + DEVICE_CTRL);
+	}
+
+	/* Wait data to be read */
+	if (!wait_for_completion_timeout(&master->data_read_complete,
+						 XFER_TIMEOUT)) {
+		dev_err(master->dev, "wait master read timeout\n");
+		ret = aspeed_i3c_slave_reset_queue(master);
+		if (ret) {
+			dev_err(master->dev, "i3c queue reset failed");
+			return ret;
+		}
+	}
+
+	return 0;
+}
+
+static int aspeed_i3c_master_timing_config(struct aspeed_i3c_master *master,
+					   struct device_node *np)
+{
+	u32 val, reg;
+	u32 timed_reset_scl_low_ns;
+	u32 sda_tx_hold_ns;
+
+	master->timing.core_rate = clk_get_rate(master->core_clk);
+	if (!master->timing.core_rate) {
+		dev_err(master->dev, "core clock rate not found\n");
+		return -EINVAL;
+	}
+
+	/* core_period is in nanosecond */
+	master->timing.core_period =
+		DIV_ROUND_UP(1000000000, master->timing.core_rate);
+
+	/* setup default timing configuration */
+	sda_tx_hold_ns = SDA_TX_HOLD_MIN * master->timing.core_period;
+	timed_reset_scl_low_ns = JESD403_TIMED_RESET_NS_DEF;
+
+	/* parse configurations from DT */
+	if (!of_property_read_u32(np, "i3c-pp-scl-hi-period-ns", &val))
+		master->timing.i3c_pp_scl_high = val;
+
+	if (!of_property_read_u32(np, "i3c-pp-scl-lo-period-ns", &val))
+		master->timing.i3c_pp_scl_low = val;
+
+	if (!of_property_read_u32(np, "i3c-od-scl-hi-period-ns", &val))
+		master->timing.i3c_od_scl_high = val;
+
+	if (!of_property_read_u32(np, "i3c-od-scl-lo-period-ns", &val))
+		master->timing.i3c_od_scl_low = val;
+
+	if (!of_property_read_u32(np, "sda-tx-hold-ns", &val))
+		sda_tx_hold_ns = val;
+
+	if (!of_property_read_u32(np, "timed-reset-scl-low-ns", &val))
+		timed_reset_scl_low_ns = val;
+
+	val = clamp((u32)DIV_ROUND_CLOSEST(sda_tx_hold_ns,
+					   master->timing.core_period),
+		    (u32)SDA_TX_HOLD_MIN, (u32)SDA_TX_HOLD_MAX);
+	reg = readl(master->regs + SDA_HOLD_SWITCH_DLY_TIMING);
+	reg &= ~SDA_TX_HOLD;
+	reg |= FIELD_PREP(SDA_TX_HOLD, val);
+	writel(reg, master->regs + SDA_HOLD_SWITCH_DLY_TIMING);
+
+	val = DIV_ROUND_CLOSEST(timed_reset_scl_low_ns,
+				master->timing.core_period);
+	writel(val, master->regs + SCL_LOW_MST_EXT_TIMEOUT);
+
+	return 0;
+}
+
+static const struct i3c_master_controller_ops aspeed_i3c_ops = {
+	.bus_init = aspeed_i3c_master_bus_init,
+	.bus_cleanup = aspeed_i3c_master_bus_cleanup,
+	.bus_reset = aspeed_i3c_master_bus_reset,
+	.attach_i3c_dev = aspeed_i3c_master_attach_i3c_dev,
+	.reattach_i3c_dev = aspeed_i3c_master_reattach_i3c_dev,
+	.detach_i3c_dev = aspeed_i3c_master_detach_i3c_dev,
+	.do_daa = aspeed_i3c_master_daa,
+	.supports_ccc_cmd = aspeed_i3c_master_supports_ccc_cmd,
+	.send_ccc_cmd = aspeed_i3c_master_send_ccc_cmd,
+	.send_hdr_cmds = aspeed_i3c_master_send_hdr_cmd,
+	.priv_xfers = aspeed_i3c_master_priv_xfers,
+	.attach_i2c_dev = aspeed_i3c_master_attach_i2c_dev,
+	.detach_i2c_dev = aspeed_i3c_master_detach_i2c_dev,
+	.i2c_xfers = aspeed_i3c_master_i2c_xfers,
+	.enable_ibi = aspeed_i3c_master_enable_ibi,
+	.disable_ibi = aspeed_i3c_master_disable_ibi,
+	.request_ibi = aspeed_i3c_master_request_ibi,
+	.free_ibi = aspeed_i3c_master_free_ibi,
+	.recycle_ibi_slot = aspeed_i3c_master_recycle_ibi_slot,
+	.register_slave = aspeed_i3c_master_register_slave,
+	.unregister_slave = aspeed_i3c_master_unregister_slave,
+	.send_sir = aspeed_i3c_master_send_sir,
+	.put_read_data = aspeed_i3c_master_put_read_data,
+};
+
+static void aspeed_i3c_master_hj(struct work_struct *work)
+{
+	struct aspeed_i3c_master *master =
+		container_of(work, typeof(*master), hj_work);
+
+	i3c_master_do_daa(&master->base);
+}
+
+static int aspeed_i3c_master_enable_hj(struct aspeed_i3c_master *master)
+{
+	int ret;
+
+	aspeed_i3c_master_enable_ibi_irq(master);
+	ast_clrbits(master->regs + DEVICE_CTRL, DEV_CTRL_HOT_JOIN_NACK);
+	ret = i3c_master_enable_hj(&master->base);
+
+	return ret;
+}
+
+static int aspeed_i3c_probe(struct platform_device *pdev)
+{
+	struct aspeed_i3c_master *master;
+	struct device_node *np;
+	int ret, irq;
+
+	master = devm_kzalloc(&pdev->dev, sizeof(*master), GFP_KERNEL);
+	if (!master)
+		return -ENOMEM;
+
+	master->i3cg = syscon_regmap_lookup_by_phandle(pdev->dev.of_node,
+						       "aspeed,i3cg");
+	if (IS_ERR(master->i3cg)) {
+		dev_err(master->dev,
+			"i3c controller missing 'aspeed,i3cg' property\n");
+		return PTR_ERR(master->i3cg);
+	}
+
+	master->regs = devm_platform_ioremap_resource(pdev, 0);
+	if (IS_ERR(master->regs))
+		return PTR_ERR(master->regs);
+
+	master->core_clk = devm_clk_get(&pdev->dev, NULL);
+	if (IS_ERR(master->core_clk))
+		return PTR_ERR(master->core_clk);
+
+	master->core_rst = devm_reset_control_get_optional_exclusive(&pdev->dev,
+								    "core_rst");
+	if (IS_ERR(master->core_rst))
+		return PTR_ERR(master->core_rst);
+
+	ret = clk_prepare_enable(master->core_clk);
+	if (ret)
+		goto err_disable_core_clk;
+
+	reset_control_deassert(master->core_rst);
+
+	spin_lock_init(&master->ibi.lock);
+	spin_lock_init(&master->xferqueue.lock);
+	INIT_LIST_HEAD(&master->xferqueue.list);
+
+	writel(RESET_CTRL_ALL, master->regs + RESET_CTRL);
+	while (readl(master->regs + RESET_CTRL))
+		;
+
+	writel(INTR_ALL, master->regs + INTR_STATUS);
+	irq = platform_get_irq(pdev, 0);
+	ret = devm_request_irq(&pdev->dev, irq,
+			       aspeed_i3c_master_irq_handler, 0,
+			       dev_name(&pdev->dev), master);
+	if (ret)
+		goto err_assert_rst;
+
+	platform_set_drvdata(pdev, master);
+
+	np = pdev->dev.of_node;
+	if (of_get_property(np, "secondary", NULL))
+		master->secondary = true;
+	else
+		master->secondary = false;
+
+	ret = of_property_read_u32(np, "i3c_chan", &master->channel);
+	if ((ret != 0) || (master->channel > I3C_CHANNEL_MAX)) {
+		dev_err(&pdev->dev, "no valid 'i3c_chan' %d %d configured\n", ret, master->channel);
+		return -EINVAL;
+	}
+
+	ret = aspeed_i3c_master_timing_config(master, np);
+	if (ret)
+		goto err_assert_rst;
+
+	/* Information regarding the FIFOs/QUEUEs depth */
+	ret = readl(master->regs + QUEUE_STATUS_LEVEL);
+	master->caps.cmdfifodepth = QUEUE_STATUS_LEVEL_CMD(ret);
+
+	ret = readl(master->regs + DATA_BUFFER_STATUS_LEVEL);
+	master->caps.datafifodepth = DATA_BUFFER_STATUS_LEVEL_TX(ret);
+
+	ret = readl(master->regs + DEVICE_ADDR_TABLE_POINTER);
+	master->datstartaddr = ret;
+	master->maxdevs = ret >> 16;
+	master->free_pos = GENMASK(master->maxdevs - 1, 0);
+	aspeed_i3c_master_init_group_dat(master);
+#ifdef CONFIG_AST2600_I3C_CCC_WORKAROUND
+	master->free_pos &= ~BIT(master->maxdevs - 1);
+	ret = (even_parity(I3C_BROADCAST_ADDR) << 7) | I3C_BROADCAST_ADDR;
+	master->addrs[master->maxdevs - 1] = ret;
+	writel(FIELD_PREP(DEV_ADDR_TABLE_DYNAMIC_ADDR, ret),
+	       master->regs + DEV_ADDR_TABLE_LOC(master->datstartaddr,
+						 master->maxdevs - 1));
+#endif
+	master->dev = &pdev->dev;
+	master->base.pec_supported = true;
+	INIT_WORK(&master->hj_work, aspeed_i3c_master_hj);
+	ret = i3c_master_register(&master->base, &pdev->dev,
+				  &aspeed_i3c_ops, master->secondary);
+	if (ret)
+		goto err_assert_rst;
+
+	if (!master->secondary && !master->base.jdec_spd) {
+		ret = aspeed_i3c_master_enable_hj(master);
+		if (ret)
+			goto err_master_register;
+	}
+
+	return 0;
+
+err_master_register:
+	i3c_master_unregister(&master->base);
+
+err_assert_rst:
+	reset_control_assert(master->core_rst);
+
+err_disable_core_clk:
+	clk_disable_unprepare(master->core_clk);
+
+	return ret;
+}
+
+static int aspeed_i3c_remove(struct platform_device *pdev)
+{
+	struct aspeed_i3c_master *master = platform_get_drvdata(pdev);
+	int ret;
+
+	ret = i3c_master_unregister(&master->base);
+	if (ret)
+		return ret;
+
+	reset_control_assert(master->core_rst);
+
+	clk_disable_unprepare(master->core_clk);
+
+	return 0;
+}
+
+static const struct of_device_id aspeed_i3c_master_of_match[] = {
+	{ .compatible = "aspeed,ast2600-i3c", },
+	{},
+};
+MODULE_DEVICE_TABLE(of, aspeed_i3c_master_of_match);
+
+static struct platform_driver aspeed_i3c_driver = {
+	.probe = aspeed_i3c_probe,
+	.remove = aspeed_i3c_remove,
+	.driver = {
+		.name = "ast2600-i3c-master",
+		.of_match_table = of_match_ptr(aspeed_i3c_master_of_match),
+	},
+};
+module_platform_driver(aspeed_i3c_driver);
+
+MODULE_AUTHOR("Dylan Hung <dylan_hung@aspeedtech.com>");
+MODULE_DESCRIPTION("Aspeed MIPI I3C driver");
+MODULE_LICENSE("GPL v2");
diff --git a/drivers/i3c/master/mipi-i3c-hci/core.c b/drivers/i3c/master/mipi-i3c-hci/core.c
index 1b73647cc3b1..6aef5ce43cc1 100644
--- a/drivers/i3c/master/mipi-i3c-hci/core.c
+++ b/drivers/i3c/master/mipi-i3c-hci/core.c
@@ -662,7 +662,7 @@ static int i3c_hci_init(struct i3c_hci *hci)
 
 	/* Make sure our data ordering fits the host's */
 	regval = reg_read(HC_CONTROL);
-	if (IS_ENABLED(CONFIG_BIG_ENDIAN)) {
+	if (IS_ENABLED(CONFIG_CPU_BIG_ENDIAN)) {
 		if (!(regval & HC_CONTROL_DATA_BIG_ENDIAN)) {
 			regval |= HC_CONTROL_DATA_BIG_ENDIAN;
 			reg_write(HC_CONTROL, regval);
@@ -768,13 +768,8 @@ static int i3c_hci_probe(struct platform_device *pdev)
 static int i3c_hci_remove(struct platform_device *pdev)
 {
 	struct i3c_hci *hci = platform_get_drvdata(pdev);
-	int ret;
 
-	ret = i3c_master_unregister(&hci->master);
-	if (ret)
-		return ret;
-
-	return 0;
+	return i3c_master_unregister(&hci->master);
 }
 
 static const __maybe_unused struct of_device_id i3c_hci_of_match[] = {
diff --git a/drivers/i3c/master/mipi-i3c-hci/dma.c b/drivers/i3c/master/mipi-i3c-hci/dma.c
index af873a9be050..2990ac9eaade 100644
--- a/drivers/i3c/master/mipi-i3c-hci/dma.c
+++ b/drivers/i3c/master/mipi-i3c-hci/dma.c
@@ -223,7 +223,7 @@ static int hci_dma_init(struct i3c_hci *hci)
 	}
 	if (nr_rings > XFER_RINGS)
 		nr_rings = XFER_RINGS;
-	rings = kzalloc(sizeof(*rings) + nr_rings * sizeof(*rh), GFP_KERNEL);
+	rings = kzalloc(struct_size(rings, headers, nr_rings), GFP_KERNEL);
 	if (!rings)
 		return -ENOMEM;
 	hci->io_data = rings;
diff --git a/drivers/i3c/master/mipi-i3c-hci/hci.h b/drivers/i3c/master/mipi-i3c-hci/hci.h
index 80beb1d5be8f..f109923f6c3f 100644
--- a/drivers/i3c/master/mipi-i3c-hci/hci.h
+++ b/drivers/i3c/master/mipi-i3c-hci/hci.h
@@ -98,7 +98,7 @@ struct hci_xfer {
 
 static inline struct hci_xfer *hci_alloc_xfer(unsigned int n)
 {
-	return kzalloc(sizeof(struct hci_xfer) * n, GFP_KERNEL);
+	return kcalloc(n, sizeof(struct hci_xfer), GFP_KERNEL);
 }
 
 static inline void hci_free_xfer(struct hci_xfer *xfer, unsigned int n)
diff --git a/drivers/i3c/mctp/Kconfig b/drivers/i3c/mctp/Kconfig
new file mode 100644
index 000000000000..fe635940138b
--- /dev/null
+++ b/drivers/i3c/mctp/Kconfig
@@ -0,0 +1,14 @@
+# SPDX-License-Identifier: GPL-2.0-only
+config I3C_MCTP
+    tristate "I3C Controller MCTP driver"
+    depends on I3C
+help
+    Say yes here to enable the I3C MCTP driver for I3C HW that is
+    configured as an I3C Controller Device on the I3C Bus.
+
+config I3C_TARGET_MCTP
+    tristate "I3C Target MCTP driver"
+    depends on I3C
+help
+    Say yes here to enable the I3C MCTP driver for I3C HW that is
+    configured as an I3C Target Device on the I3C Bus.
diff --git a/drivers/i3c/mctp/Makefile b/drivers/i3c/mctp/Makefile
new file mode 100644
index 000000000000..05eb78684843
--- /dev/null
+++ b/drivers/i3c/mctp/Makefile
@@ -0,0 +1,3 @@
+# SPDX-License-Identifier: GPL-2.0-only
+obj-$(CONFIG_I3C_MCTP)		+= i3c-mctp.o
+obj-$(CONFIG_I3C_TARGET_MCTP)	+= i3c-target-mctp.o
diff --git a/drivers/i3c/mctp/i3c-mctp.c b/drivers/i3c/mctp/i3c-mctp.c
new file mode 100644
index 000000000000..6291ec362e56
--- /dev/null
+++ b/drivers/i3c/mctp/i3c-mctp.c
@@ -0,0 +1,628 @@
+// SPDX-License-Identifier: GPL-2.0
+/* Copyright (C) 2022 Intel Corporation.*/
+
+#include <linux/cdev.h>
+#include <linux/fs.h>
+#include <linux/module.h>
+#include <linux/platform_device.h>
+#include <linux/poll.h>
+#include <linux/preempt.h>
+#include <linux/ptr_ring.h>
+#include <linux/slab.h>
+#include <linux/timer.h>
+#include <linux/types.h>
+#include <linux/workqueue.h>
+
+#include <linux/i3c/device.h>
+
+#include <linux/i3c/mctp/i3c-mctp.h>
+
+#define I3C_MCTP_MINORS				32
+#define CCC_DEVICE_STATUS_PENDING_INTR(x)	(((x) & GENMASK(3, 0)) >> 0)
+#define POLLING_TIMEOUT_MS			50
+#define MCTP_INTERRUPT_NUMBER			1
+#define RX_RING_COUNT				16
+#define I3C_MCTP_MIN_TRANSFER_SIZE		69
+#define I3C_MCTP_IBI_PAYLOAD_SIZE		2
+
+struct i3c_mctp {
+	struct i3c_device *i3c;
+	struct cdev cdev;
+	struct device *dev;
+	struct delayed_work polling_work;
+	struct platform_device *i3c_peci;
+	int id;
+	/*
+	 * Restrict an access to the /dev descriptor to one
+	 * user at a time.
+	 */
+	spinlock_t device_file_lock;
+	int device_open;
+	/* Currently only one userspace client is supported */
+	struct i3c_mctp_client *default_client;
+	struct i3c_mctp_client *peci_client;
+	u16 max_read_len;
+	u16 max_write_len;
+};
+
+struct i3c_mctp_client {
+	struct i3c_mctp *priv;
+	struct ptr_ring rx_queue;
+	wait_queue_head_t wait_queue;
+};
+
+static struct class *i3c_mctp_class;
+static dev_t i3c_mctp_devt;
+static DEFINE_IDA(i3c_mctp_ida);
+
+static struct kmem_cache *packet_cache;
+
+/**
+ * i3c_mctp_packet_alloc() - allocates i3c_mctp_packet
+ *
+ * @flags: the type of memory to allocate
+ *
+ * Allocates i3c_mctp_packet via slab allocation
+ * Return: pointer to the packet, NULL if some error occurred
+ */
+void *i3c_mctp_packet_alloc(gfp_t flags)
+{
+	return kmem_cache_alloc(packet_cache, flags);
+}
+EXPORT_SYMBOL_GPL(i3c_mctp_packet_alloc);
+
+/**
+ * i3c_mctp_packet_free() - frees i3c_mctp_packet
+ *
+ * @packet: pointer to the packet which should be freed
+ *
+ * Frees i3c_mctp_packet previously allocated via slab allocation
+ */
+void i3c_mctp_packet_free(void *packet)
+{
+	kmem_cache_free(packet_cache, packet);
+}
+EXPORT_SYMBOL_GPL(i3c_mctp_packet_free);
+
+static void i3c_mctp_client_free(struct i3c_mctp_client *client)
+{
+	ptr_ring_cleanup(&client->rx_queue, &i3c_mctp_packet_free);
+
+	kfree(client);
+}
+
+static struct i3c_mctp_client *i3c_mctp_client_alloc(struct i3c_mctp *priv)
+{
+	struct i3c_mctp_client *client;
+	int ret;
+
+	client = kzalloc(sizeof(*client), GFP_KERNEL);
+	if (!client)
+		goto out;
+
+	client->priv = priv;
+	ret = ptr_ring_init(&client->rx_queue, RX_RING_COUNT, GFP_KERNEL);
+	if (ret)
+		return ERR_PTR(ret);
+	init_waitqueue_head(&client->wait_queue);
+out:
+	return client;
+}
+
+static struct i3c_mctp_client *i3c_mctp_find_client(struct i3c_mctp *priv,
+						    struct i3c_mctp_packet *packet)
+{
+	u8 *msg_hdr = (u8 *)packet->data.payload;
+	u8 mctp_type = msg_hdr[MCTP_MSG_HDR_MSG_TYPE_OFFSET];
+	u16 vendor = (msg_hdr[MCTP_MSG_HDR_VENDOR_OFFSET] << 8
+		      | msg_hdr[MCTP_MSG_HDR_VENDOR_OFFSET + 1]);
+	u8 intel_msg_op_code = msg_hdr[MCTP_MSG_HDR_OPCODE_OFFSET];
+
+	if (priv->peci_client && mctp_type == MCTP_MSG_TYPE_VDM_PCI &&
+	    vendor == MCTP_VDM_PCI_INTEL_VENDOR_ID && intel_msg_op_code == MCTP_VDM_PCI_INTEL_PECI)
+		return priv->peci_client;
+
+	return priv->default_client;
+}
+
+static struct i3c_mctp_packet *i3c_mctp_read_packet(struct i3c_device *i3c)
+{
+	struct i3c_mctp *priv = dev_get_drvdata(i3cdev_to_dev(i3c));
+	struct i3c_mctp_packet *rx_packet;
+	struct i3c_priv_xfer xfers = {
+		.rnw = true,
+	};
+	int ret;
+
+	rx_packet = i3c_mctp_packet_alloc(GFP_KERNEL);
+	if (!rx_packet)
+		return ERR_PTR(-ENOMEM);
+
+	rx_packet->size = I3C_MCTP_PACKET_SIZE;
+	xfers.len = rx_packet->size;
+	xfers.data.in = &rx_packet->data;
+
+	/* Check against packet size + PEC byte to make sure that we always try to read max */
+	if (priv->max_read_len != xfers.len + 1) {
+		dev_dbg(i3cdev_to_dev(i3c), "Length mismatch. MRL = %d, xfers.len = %d",
+			priv->max_read_len, xfers.len);
+		i3c_mctp_packet_free(rx_packet);
+		return ERR_PTR(-EINVAL);
+	}
+
+	ret = i3c_device_do_priv_xfers(i3c, &xfers, 1);
+	if (ret) {
+		i3c_mctp_packet_free(rx_packet);
+		return ERR_PTR(ret);
+	}
+	rx_packet->size = xfers.len;
+
+	return rx_packet;
+}
+
+static void i3c_mctp_dispatch_packet(struct i3c_mctp *priv, struct i3c_mctp_packet *packet)
+{
+	struct i3c_mctp_client *client = i3c_mctp_find_client(priv, packet);
+	int ret;
+
+	ret = ptr_ring_produce(&client->rx_queue, packet);
+	if (ret)
+		i3c_mctp_packet_free(packet);
+	else
+		wake_up_all(&client->wait_queue);
+}
+
+static void i3c_mctp_polling_work(struct work_struct *work)
+{
+	struct i3c_mctp *priv = container_of(to_delayed_work(work), struct i3c_mctp, polling_work);
+	struct i3c_device *i3cdev = priv->i3c;
+	struct i3c_mctp_packet *rx_packet;
+	struct i3c_device_info info;
+	int ret;
+
+	i3c_device_get_info(i3cdev, &info);
+	ret = i3c_device_getstatus_ccc(i3cdev, &info);
+	if (ret)
+		return;
+
+	if (CCC_DEVICE_STATUS_PENDING_INTR(info.status) != MCTP_INTERRUPT_NUMBER)
+		return;
+
+	rx_packet = i3c_mctp_read_packet(i3cdev);
+	if (IS_ERR(rx_packet))
+		goto out;
+
+	i3c_mctp_dispatch_packet(priv, rx_packet);
+out:
+	schedule_delayed_work(&priv->polling_work, msecs_to_jiffies(POLLING_TIMEOUT_MS));
+}
+
+static ssize_t i3c_mctp_write(struct file *file, const char __user *buf, size_t count,
+			      loff_t *f_pos)
+{
+	struct i3c_mctp *priv = file->private_data;
+	struct i3c_device *i3c = priv->i3c;
+	struct i3c_priv_xfer xfers = {
+		.rnw = false,
+		.len = count,
+	};
+	u8 *data;
+	int ret;
+
+	/*
+	 * Check against packet size + PEC byte
+	 * to not send more data than it was set in the probe
+	 */
+	if (priv->max_write_len < xfers.len + 1) {
+		dev_dbg(i3cdev_to_dev(i3c), "Length mismatch. MWL = %d, xfers.len = %d",
+			priv->max_write_len, xfers.len);
+		return -EINVAL;
+	}
+
+	data = memdup_user(buf, count);
+	if (IS_ERR(data))
+		return PTR_ERR(data);
+
+	xfers.data.out = data;
+
+	ret = i3c_device_do_priv_xfers(i3c, &xfers, 1);
+	kfree(data);
+	return ret ?: count;
+}
+
+static ssize_t i3c_mctp_read(struct file *file, char __user *buf, size_t count, loff_t *f_pos)
+{
+	struct i3c_mctp *priv = file->private_data;
+	struct i3c_mctp_client *client = priv->default_client;
+	struct i3c_mctp_packet *rx_packet;
+
+	if (count > sizeof(rx_packet->data))
+		count = sizeof(rx_packet->data);
+
+	rx_packet = ptr_ring_consume(&client->rx_queue);
+	if (!rx_packet)
+		return -EAGAIN;
+
+	if (count > rx_packet->size)
+		count = rx_packet->size;
+
+	if (copy_to_user(buf, &rx_packet->data, count))
+		return -EFAULT;
+
+	i3c_mctp_packet_free(rx_packet);
+
+	return count;
+}
+
+static int i3c_mctp_open(struct inode *inode, struct file *file)
+{
+	struct i3c_mctp *priv = container_of(inode->i_cdev, struct i3c_mctp, cdev);
+
+	spin_lock(&priv->device_file_lock);
+	if (priv->device_open) {
+		spin_unlock(&priv->device_file_lock);
+		return -EBUSY;
+	}
+	priv->device_open++;
+	spin_unlock(&priv->device_file_lock);
+
+	file->private_data = priv;
+
+	return 0;
+}
+
+static int i3c_mctp_release(struct inode *inode, struct file *file)
+{
+	struct i3c_mctp *priv = file->private_data;
+
+	spin_lock(&priv->device_file_lock);
+	priv->device_open--;
+	spin_unlock(&priv->device_file_lock);
+
+	file->private_data = NULL;
+
+	return 0;
+}
+
+static __poll_t i3c_mctp_poll(struct file *file, struct poll_table_struct *pt)
+{
+	struct i3c_mctp *priv = file->private_data;
+	__poll_t ret = 0;
+
+	poll_wait(file, &priv->default_client->wait_queue, pt);
+
+	if (__ptr_ring_peek(&priv->default_client->rx_queue))
+		ret |= EPOLLIN;
+
+	return ret;
+}
+
+static const struct file_operations i3c_mctp_fops = {
+	.owner = THIS_MODULE,
+	.read = i3c_mctp_read,
+	.write = i3c_mctp_write,
+	.poll = i3c_mctp_poll,
+	.open = i3c_mctp_open,
+	.release = i3c_mctp_release,
+};
+
+/**
+ * i3c_mctp_add_peci_client() - registers PECI client
+ * @i3c: I3C device to get the PECI client for
+ *
+ * Return: pointer to PECI client, -ENOMEM - in case of client alloc fault
+ */
+struct i3c_mctp_client *i3c_mctp_add_peci_client(struct i3c_device *i3c)
+{
+	struct i3c_mctp *priv = dev_get_drvdata(i3cdev_to_dev(i3c));
+	struct i3c_mctp_client *client;
+
+	client = i3c_mctp_client_alloc(priv);
+	if (IS_ERR(client))
+		return ERR_PTR(-ENOMEM);
+
+	priv->peci_client = client;
+
+	return priv->peci_client;
+}
+EXPORT_SYMBOL_GPL(i3c_mctp_add_peci_client);
+
+/**
+ * i3c_mctp_remove_peci_client() - un-registers PECI client
+ * @client: i3c_mctp_client to be freed
+ */
+void i3c_mctp_remove_peci_client(struct i3c_mctp_client *client)
+{
+	struct i3c_mctp *priv = client->priv;
+
+	i3c_mctp_client_free(priv->peci_client);
+
+	priv->peci_client = NULL;
+}
+EXPORT_SYMBOL_GPL(i3c_mctp_remove_peci_client);
+
+static struct i3c_mctp *i3c_mctp_alloc(struct i3c_device *i3c)
+{
+	struct i3c_mctp *priv;
+	int id;
+
+	priv = devm_kzalloc(i3cdev_to_dev(i3c), sizeof(*priv), GFP_KERNEL);
+	if (!priv)
+		return ERR_PTR(-ENOMEM);
+
+	id = ida_alloc(&i3c_mctp_ida, GFP_KERNEL);
+	if (id < 0) {
+		pr_err("i3c_mctp: no minor number available!\n");
+		return ERR_PTR(id);
+	}
+
+	priv->id = id;
+	priv->i3c = i3c;
+
+	spin_lock_init(&priv->device_file_lock);
+
+	return priv;
+}
+
+static void i3c_mctp_ibi_handler(struct i3c_device *dev, const struct i3c_ibi_payload *payload)
+{
+	struct i3c_mctp *priv = dev_get_drvdata(i3cdev_to_dev(dev));
+	struct i3c_mctp_packet *rx_packet;
+
+	rx_packet = i3c_mctp_read_packet(dev);
+	if (IS_ERR(rx_packet))
+		return;
+
+	i3c_mctp_dispatch_packet(priv, rx_packet);
+}
+
+static int i3c_mctp_init(struct i3c_driver *drv)
+{
+	int ret;
+
+	packet_cache = kmem_cache_create_usercopy("mctp-i3c-packet",
+						  sizeof(struct i3c_mctp_packet), 0, 0, 0,
+						  sizeof(struct i3c_mctp_packet), NULL);
+	if (IS_ERR(packet_cache)) {
+		ret = PTR_ERR(packet_cache);
+		goto out;
+	}
+
+	/* Dynamically request unused major number */
+	ret = alloc_chrdev_region(&i3c_mctp_devt, 0, I3C_MCTP_MINORS, "i3c-mctp");
+	if (ret)
+		goto out;
+
+	/* Create a class to populate sysfs entries*/
+	i3c_mctp_class = class_create(THIS_MODULE, "i3c-mctp");
+	if (IS_ERR(i3c_mctp_class)) {
+		ret = PTR_ERR(i3c_mctp_class);
+		goto out_unreg_chrdev;
+	}
+
+	i3c_driver_register(drv);
+
+	return 0;
+
+out_unreg_chrdev:
+	unregister_chrdev_region(i3c_mctp_devt, I3C_MCTP_MINORS);
+out:
+	pr_err("i3c_mctp: driver initialisation failed\n");
+	return ret;
+}
+
+static void i3c_mctp_free(struct i3c_driver *drv)
+{
+	i3c_driver_unregister(drv);
+	class_destroy(i3c_mctp_class);
+	unregister_chrdev_region(i3c_mctp_devt, I3C_MCTP_MINORS);
+	kmem_cache_destroy(packet_cache);
+}
+
+static int i3c_mctp_enable_ibi(struct i3c_device *i3cdev)
+{
+	struct i3c_ibi_setup ibireq = {
+		.handler = i3c_mctp_ibi_handler,
+		.max_payload_len = 2,
+		.num_slots = 10,
+	};
+	int ret;
+
+	ret = i3c_device_request_ibi(i3cdev, &ibireq);
+	if (ret)
+		return ret;
+	ret = i3c_device_enable_ibi(i3cdev);
+	if (ret)
+		i3c_device_free_ibi(i3cdev);
+
+	return ret;
+}
+
+/**
+ * i3c_mctp_get_eid() - receive MCTP EID assigned to the device
+ *
+ * @client: client for the device to get the EID for
+ * @domain_id: requested domain ID
+ * @eid: pointer to store EID value
+ *
+ * Receive MCTP endpoint ID dynamically assigned by the MCTP Bus Owner
+ * Return: 0 in case of success, a negative error code otherwise.
+ */
+int i3c_mctp_get_eid(struct i3c_mctp_client *client, u8 domain_id, u8 *eid)
+{
+	/* TODO: Implement EID assignment basing on domain ID */
+	*eid = 1;
+	return 0;
+}
+EXPORT_SYMBOL_GPL(i3c_mctp_get_eid);
+
+/**
+ * i3c_mctp_send_packet() - send mctp packet
+ *
+ * @tx_packet: the allocated packet that needs to be send via I3C
+ * @i3c: i3c device to send the packet to
+ *
+ * Return: 0 in case of success, a negative error code otherwise.
+ */
+int i3c_mctp_send_packet(struct i3c_device *i3c, struct i3c_mctp_packet *tx_packet)
+{
+	struct i3c_priv_xfer xfers = {
+		.rnw = false,
+		.len = tx_packet->size,
+		.data.out = &tx_packet->data,
+	};
+
+	return i3c_device_do_priv_xfers(i3c, &xfers, 1);
+}
+EXPORT_SYMBOL_GPL(i3c_mctp_send_packet);
+
+/**
+ * i3c_mctp_receive_packet() - receive mctp packet
+ *
+ * @client: i3c_mctp_client to receive the packet from
+ * @timeout: timeout, in jiffies
+ *
+ * The function will sleep for up to @timeout if no packet is ready to read.
+ *
+ * Returns struct i3c_mctp_packet from or ERR_PTR in case of error or the
+ * timeout elapsed.
+ */
+struct i3c_mctp_packet *i3c_mctp_receive_packet(struct i3c_mctp_client *client,
+						unsigned long timeout)
+{
+	struct i3c_mctp_packet *rx_packet;
+	int ret;
+
+	ret = wait_event_interruptible_timeout(client->wait_queue,
+					       __ptr_ring_peek(&client->rx_queue), timeout);
+	if (ret < 0)
+		return ERR_PTR(ret);
+	else if (ret == 0)
+		return ERR_PTR(-ETIME);
+
+	rx_packet = ptr_ring_consume(&client->rx_queue);
+	if (!rx_packet)
+		return ERR_PTR(-EAGAIN);
+
+	return rx_packet;
+}
+EXPORT_SYMBOL_GPL(i3c_mctp_receive_packet);
+
+static int i3c_mctp_probe(struct i3c_device *i3cdev)
+{
+	int ibi_payload_size = I3C_MCTP_IBI_PAYLOAD_SIZE;
+	struct device *dev = i3cdev_to_dev(i3cdev);
+	struct i3c_device_info info;
+	struct i3c_mctp *priv;
+	int ret;
+
+	priv = i3c_mctp_alloc(i3cdev);
+	if (IS_ERR(priv))
+		return PTR_ERR(priv);
+
+	cdev_init(&priv->cdev, &i3c_mctp_fops);
+
+	priv->cdev.owner = THIS_MODULE;
+	ret = cdev_add(&priv->cdev, MKDEV(MAJOR(i3c_mctp_devt), priv->id), 1);
+	if (ret)
+		goto error_cdev;
+
+	/* register this i3c device with the driver core */
+	priv->dev = device_create(i3c_mctp_class, dev,
+				  MKDEV(MAJOR(i3c_mctp_devt), priv->id),
+				  NULL, "i3c-mctp-%d", priv->id);
+	if (IS_ERR(priv->dev)) {
+		ret = PTR_ERR(priv->dev);
+		goto error;
+	}
+
+	ret = i3c_device_control_pec(i3cdev, true);
+	if (ret)
+		goto error;
+
+	priv->default_client = i3c_mctp_client_alloc(priv);
+	if (IS_ERR(priv->default_client))
+		goto error;
+
+	dev_set_drvdata(i3cdev_to_dev(i3cdev), priv);
+
+	priv->i3c_peci = platform_device_register_data(i3cdev_to_dev(i3cdev), "peci-i3c", priv->id,
+						       NULL, 0);
+	if (IS_ERR(priv->i3c_peci))
+		dev_warn(priv->dev, "failed to register peci-i3c device\n");
+
+	if (i3c_mctp_enable_ibi(i3cdev)) {
+		INIT_DELAYED_WORK(&priv->polling_work, i3c_mctp_polling_work);
+		schedule_delayed_work(&priv->polling_work, msecs_to_jiffies(POLLING_TIMEOUT_MS));
+		ibi_payload_size = 0;
+	}
+
+	i3c_device_get_info(i3cdev, &info);
+
+	ret = i3c_device_getmrl_ccc(i3cdev, &info);
+	if (ret || info.max_read_len != I3C_MCTP_MIN_TRANSFER_SIZE)
+		ret = i3c_device_setmrl_ccc(i3cdev, &info, I3C_MCTP_MIN_TRANSFER_SIZE,
+					    ibi_payload_size);
+	if (ret && info.max_read_len != I3C_MCTP_MIN_TRANSFER_SIZE) {
+		dev_err(dev, "Failed to set MRL!, ret = %d\n", ret);
+		goto error_peci;
+	}
+	priv->max_read_len = info.max_read_len;
+
+	ret = i3c_device_getmwl_ccc(i3cdev, &info);
+	if (ret || info.max_write_len != I3C_MCTP_MIN_TRANSFER_SIZE)
+		ret = i3c_device_setmwl_ccc(i3cdev, &info, I3C_MCTP_MIN_TRANSFER_SIZE);
+	if (ret && info.max_write_len != I3C_MCTP_MIN_TRANSFER_SIZE) {
+		dev_err(dev, "Failed to set MWL!, ret = %d\n", ret);
+		goto error_peci;
+	}
+	priv->max_write_len = info.max_write_len;
+
+	return 0;
+
+error_peci:
+	platform_device_unregister(priv->i3c_peci);
+	i3c_device_disable_ibi(i3cdev);
+	i3c_device_free_ibi(i3cdev);
+error:
+	cdev_del(&priv->cdev);
+error_cdev:
+	put_device(dev);
+	return ret;
+}
+
+static void i3c_mctp_remove(struct i3c_device *i3cdev)
+{
+	struct i3c_mctp *priv = dev_get_drvdata(i3cdev_to_dev(i3cdev));
+
+	i3c_device_disable_ibi(i3cdev);
+	i3c_device_free_ibi(i3cdev);
+	i3c_mctp_client_free(priv->default_client);
+	priv->default_client = NULL;
+	platform_device_unregister(priv->i3c_peci);
+
+	device_destroy(i3c_mctp_class, MKDEV(MAJOR(i3c_mctp_devt), priv->id));
+	cdev_del(&priv->cdev);
+	ida_free(&i3c_mctp_ida, priv->id);
+}
+
+static const struct i3c_device_id i3c_mctp_ids[] = {
+	I3C_CLASS(0xCC, 0x0),
+	I3C_DEVICE(0x3f6, 0x8000, (void *)0),
+	I3C_DEVICE(0x3f6, 0x8001, (void *)0),
+	I3C_DEVICE(0x3f6, 0xA001, (void *)0),
+	I3C_DEVICE(0x3f6, 0xA003, (void *)0),
+	{ },
+};
+
+static struct i3c_driver i3c_mctp_drv = {
+	.driver.name = "i3c-mctp",
+	.id_table = i3c_mctp_ids,
+	.probe = i3c_mctp_probe,
+	.remove = i3c_mctp_remove,
+};
+
+module_driver(i3c_mctp_drv, i3c_mctp_init, i3c_mctp_free);
+MODULE_AUTHOR("Oleksandr Shulzhenko <oleksandr.shulzhenko.viktorovych@intel.com>");
+MODULE_DESCRIPTION("I3C MCTP driver");
+MODULE_LICENSE("GPL");
diff --git a/drivers/i3c/mctp/i3c-target-mctp.c b/drivers/i3c/mctp/i3c-target-mctp.c
new file mode 100644
index 000000000000..d8c767f967fe
--- /dev/null
+++ b/drivers/i3c/mctp/i3c-target-mctp.c
@@ -0,0 +1,389 @@
+// SPDX-License-Identifier: GPL-2.0
+/* Copyright (C) 2022 Intel Corporation.*/
+
+#include <linux/cdev.h>
+#include <linux/idr.h>
+#include <linux/module.h>
+#include <linux/poll.h>
+#include <linux/ptr_ring.h>
+#include <linux/workqueue.h>
+
+#include <linux/i3c/device.h>
+
+#define I3C_TARGET_MCTP_MINORS	32
+#define RX_RING_COUNT		16
+
+static struct class *i3c_target_mctp_class;
+static dev_t i3c_target_mctp_devt;
+static DEFINE_IDA(i3c_target_mctp_ida);
+
+struct mctp_client;
+
+struct i3c_target_mctp {
+	struct i3c_device *i3cdev;
+	struct cdev cdev;
+	int id;
+	struct mctp_client *client;
+	spinlock_t client_lock; /* to protect client access */
+};
+
+struct mctp_client {
+	struct kref ref;
+	struct i3c_target_mctp *priv;
+	struct ptr_ring rx_queue;
+	wait_queue_head_t wait_queue;
+};
+
+struct mctp_packet {
+	u8 *data;
+	u16 count;
+};
+
+static void *i3c_target_mctp_packet_alloc(u16 count)
+{
+	struct mctp_packet *packet;
+	u8 *data;
+
+	packet = kzalloc(sizeof(*packet), GFP_ATOMIC);
+	if (!packet)
+		return NULL;
+
+	data = kzalloc(count, GFP_ATOMIC);
+	if (!data) {
+		kfree(packet);
+		return NULL;
+	}
+
+	packet->data = data;
+	packet->count = count;
+
+	return packet;
+}
+
+static void i3c_target_mctp_packet_free(void *data)
+{
+	struct mctp_packet *packet = data;
+
+	kfree(packet->data);
+	kfree(packet);
+}
+
+static struct mctp_client *i3c_target_mctp_client_alloc(struct i3c_target_mctp *priv)
+{
+	struct mctp_client *client;
+
+	client = kzalloc(sizeof(*client), GFP_KERNEL);
+	if (!client)
+		goto out;
+
+	kref_init(&client->ref);
+	client->priv = priv;
+	ptr_ring_init(&client->rx_queue, RX_RING_COUNT, GFP_KERNEL);
+out:
+	return client;
+}
+
+static void i3c_target_mctp_client_free(struct kref *ref)
+{
+	struct mctp_client *client = container_of(ref, typeof(*client), ref);
+
+	ptr_ring_cleanup(&client->rx_queue, &i3c_target_mctp_packet_free);
+
+	kfree(client);
+}
+
+static void i3c_target_mctp_client_get(struct mctp_client *client)
+{
+	kref_get(&client->ref);
+}
+
+static void i3c_target_mctp_client_put(struct mctp_client *client)
+{
+	kref_put(&client->ref, &i3c_target_mctp_client_free);
+}
+
+static void
+i3c_target_mctp_rx_packet_enqueue(struct i3c_device *i3cdev, const u8 *data, size_t count)
+{
+	struct i3c_target_mctp *priv = dev_get_drvdata(i3cdev_to_dev(i3cdev));
+	struct mctp_client *client;
+	struct mctp_packet *packet;
+	int ret;
+
+	spin_lock(&priv->client_lock);
+	client = priv->client;
+	if (client)
+		i3c_target_mctp_client_get(client);
+	spin_unlock(&priv->client_lock);
+
+	if (!client)
+		return;
+
+	packet = i3c_target_mctp_packet_alloc(count);
+	if (!packet)
+		goto err;
+
+	memcpy(packet->data, data, count);
+
+	ret = ptr_ring_produce(&client->rx_queue, packet);
+	if (ret)
+		i3c_target_mctp_packet_free(packet);
+	else
+		wake_up_all(&client->wait_queue);
+err:
+	i3c_target_mctp_client_put(client);
+}
+
+static struct mctp_client *i3c_target_mctp_create_client(struct i3c_target_mctp *priv)
+{
+	struct mctp_client *client;
+	int ret;
+
+	/* Currently, we support just one client. */
+	spin_lock_irq(&priv->client_lock);
+	ret = priv->client ? -EBUSY : 0;
+	spin_unlock_irq(&priv->client_lock);
+
+	if (ret)
+		return ERR_PTR(ret);
+
+	client = i3c_target_mctp_client_alloc(priv);
+	if (!client)
+		return ERR_PTR(-ENOMEM);
+
+	init_waitqueue_head(&client->wait_queue);
+
+	spin_lock_irq(&priv->client_lock);
+	priv->client = client;
+	spin_unlock_irq(&priv->client_lock);
+
+	return client;
+}
+
+static void i3c_target_mctp_delete_client(struct mctp_client *client)
+{
+	struct i3c_target_mctp *priv = client->priv;
+
+	spin_lock_irq(&priv->client_lock);
+	priv->client = NULL;
+	spin_unlock_irq(&priv->client_lock);
+
+	i3c_target_mctp_client_put(client);
+}
+
+static int i3c_target_mctp_open(struct inode *inode, struct file *file)
+{
+	struct i3c_target_mctp *priv = container_of(inode->i_cdev, struct i3c_target_mctp, cdev);
+	struct mctp_client *client;
+
+	client = i3c_target_mctp_create_client(priv);
+	if (IS_ERR(client))
+		return PTR_ERR(client);
+
+	file->private_data = client;
+
+	return 0;
+}
+
+static int i3c_target_mctp_release(struct inode *inode, struct file *file)
+{
+	struct mctp_client *client = file->private_data;
+
+	i3c_target_mctp_delete_client(client);
+
+	return 0;
+}
+
+static ssize_t i3c_target_mctp_read(struct file *file, char __user *buf,
+				    size_t count, loff_t *ppos)
+{
+	struct mctp_client *client = file->private_data;
+	struct mctp_packet *rx_packet;
+
+	rx_packet = ptr_ring_consume_irq(&client->rx_queue);
+	if (!rx_packet)
+		return -EAGAIN;
+
+	if (count < rx_packet->count) {
+		count = -EINVAL;
+		goto err_free;
+	}
+	if (count > rx_packet->count)
+		count = rx_packet->count;
+
+	if (copy_to_user(buf, rx_packet->data, count))
+		count = -EFAULT;
+err_free:
+	i3c_target_mctp_packet_free(rx_packet);
+
+	return count;
+}
+
+static ssize_t i3c_target_mctp_write(struct file *file, const char __user *buf,
+				     size_t count, loff_t *ppos)
+{
+	struct mctp_client *client = file->private_data;
+	struct i3c_target_mctp *priv = client->priv;
+	struct i3c_priv_xfer xfers[1] = {};
+	u8 *tx_data;
+	int ret;
+
+	tx_data = kzalloc(count, GFP_KERNEL);
+	if (!tx_data)
+		return -ENOMEM;
+
+	if (copy_from_user(tx_data, buf, count)) {
+		ret = -EFAULT;
+		goto out_packet;
+	}
+
+	xfers[0].data.out = tx_data;
+	xfers[0].len = count;
+
+	ret = i3c_device_do_priv_xfers(priv->i3cdev, xfers, ARRAY_SIZE(xfers));
+	if (ret)
+		goto out_packet;
+	ret = count;
+
+	/*
+	 * TODO: Add support for IBI generation - it should be done only if IBI
+	 * are enabled (the Active Controller may disabled them using CCC for
+	 * that). Otherwise (if IBIs are disabled), we should make sure that when
+	 * Active Controller issues GETSTATUS CCC the return value indicates
+	 * that data is ready.
+	 */
+out_packet:
+	kfree(tx_data);
+	return ret;
+}
+
+static __poll_t i3c_target_mctp_poll(struct file *file, struct poll_table_struct *pt)
+{
+	struct mctp_client *client = file->private_data;
+	__poll_t ret = 0;
+
+	poll_wait(file, &client->wait_queue, pt);
+
+	if (__ptr_ring_peek(&client->rx_queue))
+		ret |= EPOLLIN;
+
+	/*
+	 * TODO: Add support for "write" readiness.
+	 * DW-I3C has a hardware queue that has finite number of entries.
+	 * If we try to issue more writes that space in this queue allows for,
+	 * we're in trouble. This should be handled by error from write() and
+	 * poll() blocking for write events.
+	 */
+	return ret;
+}
+
+static const struct file_operations i3c_target_mctp_fops = {
+	.owner = THIS_MODULE,
+	.open = i3c_target_mctp_open,
+	.release = i3c_target_mctp_release,
+	.read = i3c_target_mctp_read,
+	.write = i3c_target_mctp_write,
+	.poll = i3c_target_mctp_poll,
+};
+
+static struct i3c_target_read_setup i3c_target_mctp_rx_packet_setup = {
+	.handler = i3c_target_mctp_rx_packet_enqueue,
+};
+
+static int i3c_target_mctp_probe(struct i3c_device *i3cdev)
+{
+	struct device *parent = i3cdev_to_dev(i3cdev);
+	struct i3c_target_mctp *priv;
+	struct device *dev;
+	int ret;
+
+	priv = devm_kzalloc(parent, sizeof(*priv), GFP_KERNEL);
+	if (!priv)
+		return -ENOMEM;
+
+	ret = ida_alloc(&i3c_target_mctp_ida, GFP_KERNEL);
+	if (ret < 0)
+		return ret;
+	priv->id = ret;
+
+	priv->i3cdev = i3cdev;
+	spin_lock_init(&priv->client_lock);
+
+	cdev_init(&priv->cdev, &i3c_target_mctp_fops);
+	priv->cdev.owner = THIS_MODULE;
+	ret = cdev_add(&priv->cdev, i3c_target_mctp_devt, 1);
+	if (ret) {
+		ida_free(&i3c_target_mctp_ida, priv->id);
+		return ret;
+	}
+
+	dev = device_create(i3c_target_mctp_class, parent, i3c_target_mctp_devt,
+			    NULL, "i3c-mctp-target-%d", priv->id);
+	if (IS_ERR(dev)) {
+		ret = PTR_ERR(dev);
+		goto err;
+	}
+
+	i3cdev_set_drvdata(i3cdev, priv);
+
+	i3c_target_read_register(i3cdev, &i3c_target_mctp_rx_packet_setup);
+
+	return 0;
+err:
+	cdev_del(&priv->cdev);
+	ida_free(&i3c_target_mctp_ida, priv->id);
+
+	return ret;
+}
+
+static void i3c_target_mctp_remove(struct i3c_device *i3cdev)
+{
+	struct i3c_target_mctp *priv = dev_get_drvdata(i3cdev_to_dev(i3cdev));
+
+	device_destroy(i3c_target_mctp_class, i3c_target_mctp_devt);
+	cdev_del(&priv->cdev);
+	ida_free(&i3c_target_mctp_ida, priv->id);
+}
+
+static const struct i3c_device_id i3c_target_mctp_ids[] = {
+	I3C_CLASS(0xcc, 0x0),
+	{ },
+};
+
+static struct i3c_driver i3c_target_mctp_drv = {
+	.driver.name = "i3c-target-mctp",
+	.id_table = i3c_target_mctp_ids,
+	.probe = i3c_target_mctp_probe,
+	.remove = i3c_target_mctp_remove,
+	.target = true,
+};
+
+static int i3c_target_mctp_init(struct i3c_driver *drv)
+{
+	int ret;
+
+	ret = alloc_chrdev_region(&i3c_target_mctp_devt, 0,
+				  I3C_TARGET_MCTP_MINORS, "i3c-target-mctp");
+	if (ret)
+		return ret;
+
+	i3c_target_mctp_class = class_create(THIS_MODULE, "i3c-target-mctp");
+	if (IS_ERR(i3c_target_mctp_class)) {
+		unregister_chrdev_region(i3c_target_mctp_devt, I3C_TARGET_MCTP_MINORS);
+		return PTR_ERR(i3c_target_mctp_class);
+	}
+
+	return i3c_driver_register(drv);
+}
+
+static void i3c_target_mctp_fini(struct i3c_driver *drv)
+{
+	i3c_driver_unregister(drv);
+	class_destroy(i3c_target_mctp_class);
+	unregister_chrdev_region(i3c_target_mctp_devt, I3C_TARGET_MCTP_MINORS);
+}
+
+module_driver(i3c_target_mctp_drv, i3c_target_mctp_init, i3c_target_mctp_fini);
+MODULE_AUTHOR("Iwona Winiarska <iwona.winiarska@intel.com>");
+MODULE_DESCRIPTION("I3C Target MCTP driver");
+MODULE_LICENSE("GPL");
diff --git a/drivers/soc/aspeed/Kconfig b/drivers/soc/aspeed/Kconfig
index aaf4596ae4f9..1d3632c3f55b 100644
--- a/drivers/soc/aspeed/Kconfig
+++ b/drivers/soc/aspeed/Kconfig
@@ -4,6 +4,18 @@ if ARCH_ASPEED || COMPILE_TEST
 
 menu "ASPEED SoC drivers"
 
+config ASPEED_BMC_DEV
+	tristate "ASPEED BMC Device"
+
+config ASPEED_HOST_BMC_DEV
+	tristate "ASPEED Host BMC Device"
+
+config ASPEED_VIDEO
+	tristate "ASPEED Video Engine driver"
+	default n
+	help
+	  Driver for AST Video Engine
+
 config ASPEED_LPC_CTRL
 	tristate "ASPEED LPC firmware cycle control"
 	select REGMAP
@@ -24,6 +36,20 @@ config ASPEED_LPC_SNOOP
 	  allows the BMC to listen on and save the data written by
 	  the host to an arbitrary LPC I/O port.
 
+config ASPEED_SSP
+	tristate "ASPEED SSP loader"
+	default n
+	help
+	  Driver for loading secondary-service-processor binary
+
+config ASPEED_MCTP
+	tristate "Aspeed ast2600 MCTP Controller support"
+	depends on REGMAP && MFD_SYSCON
+	help
+	  Enable support for ast2600 MCTP Controller.
+	  The MCTP controller allows the BMC to communicate with devices on
+	  the host PCIe network.
+
 config ASPEED_UART_ROUTING
 	tristate "ASPEED uart routing control"
 	select REGMAP
@@ -34,6 +60,16 @@ config ASPEED_UART_ROUTING
 	  users to perform runtime configuration of the RX muxes among
 	  the UART controllers and I/O pins.
 
+config ASPEED_LPC_MAILBOX
+	tristate "ASPEED LPC mailbox support"
+	select REGMAP
+	select MFD_SYSCON
+	default ARCH_ASPEED
+	help
+	  Provides a driver to control the LPC mailbox which possesses
+	  up to 32 data registers for the communication between the Host
+	  and the BMC over LPC.
+
 config ASPEED_P2A_CTRL
 	tristate "ASPEED P2A (VGA MMIO to BMC) bridge control"
 	select REGMAP
@@ -62,6 +98,45 @@ config ASPEED_XDMA
 	  SoCs. The XDMA engine can perform PCIe DMA operations between the BMC
 	  and a host processor.
 
+config ASPEED_ESPI
+	bool "ASPEED eSPI slave driver"
+	select REGMAP
+	select MFD_SYSCON
+	default n
+	help
+	  Enable driver support for the Aspeed eSPI engine. The eSPI engine
+	  plays as a slave device in BMC to communicate with the Host over
+	  the eSPI interface. The four eSPI channels, namely peripheral,
+	  virtual wire, out-of-band, and flash are supported.
+
+config ASPEED_ESPI_MMBI
+	tristate "Aspeed eSPI MMBI Driver"
+	depends on ASPEED_ESPI
+	help
+	  Control Aspeed eSPI MMBI driver
+
+config ASPEED_UDMA
+	tristate "Aspeed UDMA Engine Driver"
+	depends on ARCH_ASPEED && REGMAP && MFD_SYSCON && HAS_DMA
+	help
+	  Enable support for the Aspeed UDMA Engine found on the Aspeed AST2XXX
+	  SOCs. The UDMA engine can perform UART DMA operations between the memory
+	  buffer and the UART/VUART devices.
+
+config ASPEED_LPC_PCC
+	tristate "Aspeed Post Code Capture support"
+	depends on ARCH_ASPEED && REGMAP && MFD_SYSCON
+	help
+	  Provides a driver to control the LPC PCC interface,
+	  allowing the BMC to snoop data bytes written by the
+	  the host to an arbitrary LPC I/O port.
+
+config ASPEED_OTP
+	tristate "Aspeed OTP Driver"
+	depends on MACH_ASPEED_G6
+
+source "drivers/soc/aspeed/rvas/Kconfig"
+
 config ASPEED_SBC
 	bool "ASPEED Secure Boot Controller driver"
 	default MACH_ASPEED_G6
diff --git a/drivers/soc/aspeed/Makefile b/drivers/soc/aspeed/Makefile
index 9e275fd1d54d..9eaab1743e68 100644
--- a/drivers/soc/aspeed/Makefile
+++ b/drivers/soc/aspeed/Makefile
@@ -1,8 +1,26 @@
 # SPDX-License-Identifier: GPL-2.0-only
+obj-$(CONFIG_ASPEED_BMC_DEV)		+= aspeed-bmc-dev.o
+obj-$(CONFIG_ASPEED_HOST_BMC_DEV)	+= aspeed-host-bmc-dev.o
 obj-$(CONFIG_ASPEED_LPC_CTRL)		+= aspeed-lpc-ctrl.o
 obj-$(CONFIG_ASPEED_LPC_SNOOP)		+= aspeed-lpc-snoop.o
 obj-$(CONFIG_ASPEED_UART_ROUTING)	+= aspeed-uart-routing.o
+obj-$(CONFIG_ASPEED_SSP)		+= aspeed-ssp.o
 obj-$(CONFIG_ASPEED_P2A_CTRL)		+= aspeed-p2a-ctrl.o
 obj-$(CONFIG_ASPEED_SOCINFO)		+= aspeed-socinfo.o
+obj-$(CONFIG_ASPEED_XDMA)		+= aspeed-xdma.o
+obj-$(CONFIG_ASPEED_VIDEO)		+= ast_video.o
+obj-$(CONFIG_ASPEED_ESPI)		+= aspeed-espi-ctrl.o \
+					   aspeed-espi-perif.o \
+					   aspeed-espi-vw.o \
+					   aspeed-espi-oob.o \
+					   aspeed-espi-flash.o
+obj-$(CONFIG_ASPEED_ESPI_MMBI)		+= aspeed-espi-mmbi.o
+obj-$(CONFIG_ASPEED_LPC_MAILBOX)	+= aspeed-lpc-mbox.o
+obj-$(CONFIG_ASPEED_UDMA)		+= aspeed-udma.o
+obj-$(CONFIG_ASPEED_LPC_PCC)		+= aspeed-lpc-pcc.o
+obj-$(CONFIG_ASPEED_RVAS)		+= rvas/
+obj-$(CONFIG_ARCH_ASPEED)		+= aspeed-usb-phy.o
+obj-$(CONFIG_ARCH_ASPEED)		+= aspeed-usb-ahp.o
+obj-$(CONFIG_ASPEED_MCTP)		+= aspeed-mctp.o
+obj-$(CONFIG_ASPEED_OTP)		+= aspeed-otp.o
 obj-$(CONFIG_ASPEED_SBC)		+= aspeed-sbc.o
-obj-$(CONFIG_ASPEED_XDMA)	+= aspeed-xdma.o
diff --git a/drivers/soc/aspeed/aspeed-bmc-dev.c b/drivers/soc/aspeed/aspeed-bmc-dev.c
new file mode 100644
index 000000000000..367bb050d99e
--- /dev/null
+++ b/drivers/soc/aspeed/aspeed-bmc-dev.c
@@ -0,0 +1,503 @@
+// SPDX-License-Identifier: GPL-2.0-or-later
+// Copyright (C) ASPEED Technology Inc.
+
+#include <linux/init.h>
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/errno.h>
+
+#include <linux/of_address.h>
+#include <linux/of_irq.h>
+#include <linux/of.h>
+#include <linux/of_platform.h>
+#include <linux/of_reserved_mem.h>
+#include <linux/platform_device.h>
+
+#include <linux/wait.h>
+#include <linux/workqueue.h>
+
+#include <linux/regmap.h>
+#include <linux/interrupt.h>
+#include <linux/mfd/syscon.h>
+#include <linux/dma-mapping.h>
+#include <linux/miscdevice.h>
+
+#define DEVICE_NAME     "bmc-device"
+#define SCU_TRIGGER_MSI
+
+struct aspeed_bmc_device {
+	unsigned char *host2bmc_base_virt;
+	struct device *dev;
+	struct miscdevice	miscdev;
+	void __iomem	*reg_base;
+	void __iomem	*bmc_mem_virt;
+	dma_addr_t bmc_mem_phy;
+	struct bin_attribute	bin0;
+	struct bin_attribute	bin1;
+
+	/* Queue waiters for idle engine */
+	wait_queue_head_t tx_wait0;
+	wait_queue_head_t tx_wait1;
+	wait_queue_head_t rx_wait0;
+	wait_queue_head_t rx_wait1;
+
+	struct regmap		*scu;
+
+//	phys_addr_t		mem_base;
+//	resource_size_t		mem_size;
+
+	struct kernfs_node	*kn0;
+	struct kernfs_node	*kn1;
+
+	int pcie2lpc;
+	unsigned int irq;
+};
+
+#define BMC_MEM_BAR_SIZE		0x100000
+/* =================== SCU Define ================================================ */
+#define ASPEED_SCU04				0x04
+#define AST2600A3_SCU04	0x05030303
+#define ASPEED_SCUC20				0xC20
+#define ASPEED_SCUC24				0xC24
+#define MSI_ROUTING_MASK		GENMASK(11, 10)
+#define PCIDEV1_INTX_MSI_HOST2BMC_EN	BIT(18)
+#define MSI_ROUTING_PCIe2LPC_PCIDEV0	(0x1 << 10)
+#define MSI_ROUTING_PCIe2LPC_PCIDEV1	(0x2 << 10)
+/* ================================================================================== */
+#define ASPEED_BMC_MEM_BAR			0xF10
+#define  PCIE2PCI_MEM_BAR_ENABLE		BIT(1)
+#define  HOST2BMC_MEM_BAR_ENABLE		BIT(0)
+#define ASPEED_BMC_MEM_BAR_REMAP	0xF18
+
+#define ASPEED_BMC_SHADOW_CTRL		0xF50
+#define  READ_ONLY_MASK					BIT(31)
+#define  MASK_BAR1						BIT(2)
+#define  MASK_BAR0						BIT(1)
+#define  SHADOW_CFG						BIT(0)
+
+#define ASPEED_BMC_HOST2BMC_Q1		0xA000
+#define ASPEED_BMC_HOST2BMC_Q2		0xA010
+#define ASPEED_BMC_BMC2HOST_Q1		0xA020
+#define ASPEED_BMC_BMC2HOST_Q2		0xA030
+#define ASPEED_BMC_BMC2HOST_STS		0xA040
+#define	 BMC2HOST_INT_STS_DOORBELL		BIT(31)
+#define	 BMC2HOST_ENABLE_INTB			BIT(30)
+/* */
+#define	 BMC2HOST_Q1_FULL				BIT(27)
+#define	 BMC2HOST_Q1_EMPTY				BIT(26)
+#define	 BMC2HOST_Q2_FULL				BIT(25)
+#define	 BMC2HOST_Q2_EMPTY				BIT(24)
+#define	 BMC2HOST_Q1_FULL_UNMASK		BIT(23)
+#define	 BMC2HOST_Q1_EMPTY_UNMASK		BIT(22)
+#define	 BMC2HOST_Q2_FULL_UNMASK		BIT(21)
+#define	 BMC2HOST_Q2_EMPTY_UNMASK		BIT(20)
+
+#define ASPEED_BMC_HOST2BMC_STS		0xA044
+#define	 HOST2BMC_INT_STS_DOORBELL		BIT(31)
+#define	 HOST2BMC_ENABLE_INTB			BIT(30)
+#define	 HOST2BMC_Q1_FULL				BIT(27)
+#define	 HOST2BMC_Q1_EMPTY				BIT(26)
+#define	 HOST2BMC_Q2_FULL				BIT(25)
+#define	 HOST2BMC_Q2_EMPTY				BIT(24)
+#define	 HOST2BMC_Q1_FULL_UNMASK		BIT(23)
+#define	 HOST2BMC_Q1_EMPTY_UNMASK		BIT(22)
+#define	 HOST2BMC_Q2_FULL_UNMASK		BIT(21)
+#define	 HOST2BMC_Q2_EMPTY_UNMASK		BIT(20)
+
+#define ASPEED_SCU_PCIE_CONF_CTRL	0xC20
+#define  SCU_PCIE_CONF_BMC_DEV_EN			 BIT(8)
+#define  SCU_PCIE_CONF_BMC_DEV_EN_MMIO		 BIT(9)
+#define  SCU_PCIE_CONF_BMC_DEV_EN_MSI		 BIT(11)
+#define  SCU_PCIE_CONF_BMC_DEV_EN_IRQ		 BIT(13)
+#define  SCU_PCIE_CONF_BMC_DEV_EN_DMA		 BIT(14)
+#define  SCU_PCIE_CONF_BMC_DEV_EN_E2L		 BIT(15)
+#define  SCU_PCIE_CONF_BMC_DEV_EN_LPC_DECODE BIT(21)
+
+#define ASPEED_SCU_BMC_DEV_CLASS	0xC68
+
+static struct aspeed_bmc_device *file_aspeed_bmc_device(struct file *file)
+{
+	return container_of(file->private_data, struct aspeed_bmc_device,
+			miscdev);
+}
+
+static int aspeed_bmc_device_mmap(struct file *file, struct vm_area_struct *vma)
+{
+	struct aspeed_bmc_device *bmc_device = file_aspeed_bmc_device(file);
+	unsigned long vsize = vma->vm_end - vma->vm_start;
+	pgprot_t prot = vma->vm_page_prot;
+
+	if (vma->vm_pgoff + vsize > bmc_device->bmc_mem_phy + 0x100000)
+		return -EINVAL;
+
+	prot = pgprot_noncached(prot);
+
+	if (remap_pfn_range(vma, vma->vm_start,
+		(bmc_device->bmc_mem_phy >> PAGE_SHIFT) + vma->vm_pgoff,
+		vsize, prot))
+		return -EAGAIN;
+
+	return 0;
+}
+
+static const struct file_operations aspeed_bmc_device_fops = {
+	.owner		= THIS_MODULE,
+	.mmap		= aspeed_bmc_device_mmap,
+};
+
+static ssize_t aspeed_host2bmc_queue1_rx(struct file *filp, struct kobject *kobj,
+		struct bin_attribute *attr, char *buf, loff_t off, size_t count)
+{
+	struct aspeed_bmc_device *bmc_device = dev_get_drvdata(container_of(kobj, struct device, kobj));
+	u32 *data = (u32 *) buf;
+	u32 scu_id;
+	int ret;
+
+	ret = wait_event_interruptible(bmc_device->rx_wait0,
+		!(readl(bmc_device->reg_base + ASPEED_BMC_HOST2BMC_STS) & HOST2BMC_Q1_EMPTY));
+	if (ret)
+		return -EINTR;
+
+	data[0] = readl(bmc_device->reg_base + ASPEED_BMC_HOST2BMC_Q1);
+	regmap_read(bmc_device->scu, ASPEED_SCU04, &scu_id);
+	if (scu_id == AST2600A3_SCU04) {
+		writel(BMC2HOST_INT_STS_DOORBELL | BMC2HOST_ENABLE_INTB, bmc_device->reg_base + ASPEED_BMC_BMC2HOST_STS);
+	} else {
+		//A0 : BIT(12) A1 : BIT(15)
+		regmap_update_bits(bmc_device->scu, 0x560, BIT(15), BIT(15));
+		regmap_update_bits(bmc_device->scu, 0x560, BIT(15), 0);
+	}
+
+	return sizeof(u32);
+}
+
+static ssize_t aspeed_host2bmc_queue2_rx(struct file *filp, struct kobject *kobj,
+		struct bin_attribute *attr, char *buf, loff_t off, size_t count)
+{
+	struct aspeed_bmc_device *bmc_device = dev_get_drvdata(container_of(kobj, struct device, kobj));
+	u32 *data = (u32 *) buf;
+	u32 scu_id;
+	int ret;
+
+	ret = wait_event_interruptible(bmc_device->rx_wait1,
+		!(readl(bmc_device->reg_base + ASPEED_BMC_HOST2BMC_STS) & HOST2BMC_Q2_EMPTY));
+	if (ret)
+		return -EINTR;
+
+	data[0] = readl(bmc_device->reg_base + ASPEED_BMC_HOST2BMC_Q2);
+	regmap_read(bmc_device->scu, ASPEED_SCU04, &scu_id);
+	if (scu_id == AST2600A3_SCU04) {
+		writel(BMC2HOST_INT_STS_DOORBELL | BMC2HOST_ENABLE_INTB, bmc_device->reg_base + ASPEED_BMC_BMC2HOST_STS);
+	} else {
+		//A0 : BIT(12) A1 : BIT(15)
+		regmap_update_bits(bmc_device->scu, 0x560, BIT(15), BIT(15));
+		regmap_update_bits(bmc_device->scu, 0x560, BIT(15), 0);
+	}
+
+	return sizeof(u32);
+}
+
+static ssize_t aspeed_bmc2host_queue1_tx(struct file *filp, struct kobject *kobj,
+		struct bin_attribute *attr, char *buf, loff_t off, size_t count)
+{
+	struct aspeed_bmc_device *bmc_device = dev_get_drvdata(container_of(kobj, struct device, kobj));
+	u32 tx_buff;
+	u32 scu_id;
+	int ret;
+
+	if (count != sizeof(u32))
+		return -EINVAL;
+
+	ret = wait_event_interruptible(bmc_device->tx_wait0,
+		!(readl(bmc_device->reg_base + ASPEED_BMC_BMC2HOST_STS) & BMC2HOST_Q1_FULL));
+	if (ret)
+		return -EINTR;
+
+
+//	if (copy_from_user((void *)&tx_buff, buf, sizeof(u32)))
+//		return -EFAULT;
+	memcpy(&tx_buff, buf, 4);
+	writel(tx_buff, bmc_device->reg_base + ASPEED_BMC_BMC2HOST_Q1);
+	/* trigger to host
+	 * Only After AST2600A3 support DoorBell MSI
+	 */
+	regmap_read(bmc_device->scu, ASPEED_SCU04, &scu_id);
+	if (scu_id == AST2600A3_SCU04) {
+		writel(BMC2HOST_INT_STS_DOORBELL | BMC2HOST_ENABLE_INTB, bmc_device->reg_base + ASPEED_BMC_BMC2HOST_STS);
+	} else {
+		//A0 : BIT(12) A1 : BIT(15)
+		regmap_update_bits(bmc_device->scu, 0x560, BIT(15), BIT(15));
+		regmap_update_bits(bmc_device->scu, 0x560, BIT(15), 0);
+	}
+
+	return sizeof(u32);
+}
+
+static ssize_t aspeed_bmc2host_queue2_tx(struct file *filp, struct kobject *kobj,
+		struct bin_attribute *attr, char *buf, loff_t off, size_t count)
+{
+	struct aspeed_bmc_device *bmc_device = dev_get_drvdata(container_of(kobj, struct device, kobj));
+	u32 tx_buff = 0;
+	u32 scu_id;
+	int ret;
+
+	if (count != sizeof(u32))
+		return -EINVAL;
+
+	ret = wait_event_interruptible(bmc_device->tx_wait0,
+		!(readl(bmc_device->reg_base + ASPEED_BMC_BMC2HOST_STS) & BMC2HOST_Q2_FULL));
+	if (ret)
+		return -EINTR;
+
+
+//	if (copy_from_user((void *)&tx_buff, buf, sizeof(u32)))
+//		return -EFAULT;
+	memcpy(&tx_buff, buf, 4);
+	writel(tx_buff, bmc_device->reg_base + ASPEED_BMC_BMC2HOST_Q2);
+	/* trigger to host
+	 * Only After AST2600A3 support DoorBell MSI
+	 */
+	regmap_read(bmc_device->scu, ASPEED_SCU04, &scu_id);
+	if (scu_id == AST2600A3_SCU04) {
+		writel(BMC2HOST_INT_STS_DOORBELL | BMC2HOST_ENABLE_INTB, bmc_device->reg_base + ASPEED_BMC_BMC2HOST_STS);
+	} else {
+		//A0 : BIT(12) A1 : BIT(15)
+		regmap_update_bits(bmc_device->scu, 0x560, BIT(15), BIT(15));
+		regmap_update_bits(bmc_device->scu, 0x560, BIT(15), 0);
+	}
+
+	return sizeof(u32);
+}
+
+static irqreturn_t aspeed_bmc_dev_isr(int irq, void *dev_id)
+{
+	struct aspeed_bmc_device *bmc_device = dev_id;
+
+	u32 host2bmc_q_sts = readl(bmc_device->reg_base + ASPEED_BMC_HOST2BMC_STS);
+
+	if (host2bmc_q_sts & HOST2BMC_INT_STS_DOORBELL)
+		writel(HOST2BMC_INT_STS_DOORBELL, bmc_device->reg_base + ASPEED_BMC_HOST2BMC_STS);
+
+	if (host2bmc_q_sts & HOST2BMC_ENABLE_INTB)
+		writel(HOST2BMC_ENABLE_INTB, bmc_device->reg_base + ASPEED_BMC_HOST2BMC_STS);
+
+	if (host2bmc_q_sts & HOST2BMC_Q1_FULL)
+		dev_info(bmc_device->dev, "Q1 Full\n");
+
+	if (host2bmc_q_sts & HOST2BMC_Q2_FULL)
+		dev_info(bmc_device->dev, "Q2 Full\n");
+
+
+	if (!(readl(bmc_device->reg_base + ASPEED_BMC_BMC2HOST_STS) & BMC2HOST_Q1_FULL))
+		wake_up_interruptible(&bmc_device->tx_wait0);
+
+	if (!(readl(bmc_device->reg_base + ASPEED_BMC_HOST2BMC_STS) & HOST2BMC_Q1_EMPTY))
+		wake_up_interruptible(&bmc_device->rx_wait0);
+
+	if (!(readl(bmc_device->reg_base + ASPEED_BMC_BMC2HOST_STS) & BMC2HOST_Q2_FULL))
+		wake_up_interruptible(&bmc_device->tx_wait1);
+
+	if (!(readl(bmc_device->reg_base + ASPEED_BMC_HOST2BMC_STS) & HOST2BMC_Q2_EMPTY))
+		wake_up_interruptible(&bmc_device->rx_wait1);
+
+	return IRQ_HANDLED;
+}
+
+static void aspeed_bmc_device_init(struct aspeed_bmc_device *bmc_device)
+{
+	u32 pcie_config_ctl = SCU_PCIE_CONF_BMC_DEV_EN_IRQ | SCU_PCIE_CONF_BMC_DEV_EN_MMIO | SCU_PCIE_CONF_BMC_DEV_EN;
+	u32 scu_id;
+
+	if (bmc_device->pcie2lpc)
+		pcie_config_ctl |= SCU_PCIE_CONF_BMC_DEV_EN_E2L | SCU_PCIE_CONF_BMC_DEV_EN_LPC_DECODE;
+
+	regmap_update_bits(bmc_device->scu, ASPEED_SCU_PCIE_CONF_CTRL, pcie_config_ctl,
+			pcie_config_ctl);
+
+	/* update class code to others as it is a MFD device */
+	regmap_write(bmc_device->scu, ASPEED_SCU_BMC_DEV_CLASS, 0xff000000);
+
+#ifdef SCU_TRIGGER_MSI
+	//SCUC24[17]: Enable PCI device 1 INTx/MSI from SCU560[15]. Will be added in next version
+	regmap_update_bits(bmc_device->scu, ASPEED_SCUC20, BIT(11) | BIT(14), BIT(11) | BIT(14));
+
+	regmap_read(bmc_device->scu, ASPEED_SCU04, &scu_id);
+	if (scu_id == AST2600A3_SCU04)
+		regmap_update_bits(bmc_device->scu, ASPEED_SCUC24,
+				PCIDEV1_INTX_MSI_HOST2BMC_EN | MSI_ROUTING_MASK,
+				PCIDEV1_INTX_MSI_HOST2BMC_EN | MSI_ROUTING_PCIe2LPC_PCIDEV1);
+	else
+		regmap_update_bits(bmc_device->scu, ASPEED_SCUC24, BIT(17) | BIT(14) | BIT(11), BIT(17) | BIT(14) | BIT(11));
+#else
+	//SCUC24[18]: Enable PCI device 1 INTx/MSI from Host-to-BMC controller. Will be added in next version
+	regmap_update_bits(bmc_device->scu, 0xc24, BIT(18) | BIT(14), BIT(18) | BIT(14));
+#endif
+
+	writel(~(BMC_MEM_BAR_SIZE - 1) | HOST2BMC_MEM_BAR_ENABLE, bmc_device->reg_base + ASPEED_BMC_MEM_BAR);
+	writel(bmc_device->bmc_mem_phy, bmc_device->reg_base + ASPEED_BMC_MEM_BAR_REMAP);
+
+	//Setting BMC to Host Q register
+	writel(BMC2HOST_Q2_FULL_UNMASK | BMC2HOST_Q1_FULL_UNMASK | BMC2HOST_ENABLE_INTB, bmc_device->reg_base + ASPEED_BMC_BMC2HOST_STS);
+	writel(HOST2BMC_Q2_FULL_UNMASK | HOST2BMC_Q1_FULL_UNMASK | HOST2BMC_ENABLE_INTB, bmc_device->reg_base + ASPEED_BMC_HOST2BMC_STS);
+}
+
+static const struct of_device_id aspeed_bmc_device_of_matches[] = {
+	{ .compatible = "aspeed,ast2600-bmc-device", },
+	{},
+};
+MODULE_DEVICE_TABLE(of, aspeed_bmc_device_of_matches);
+
+static int aspeed_bmc_device_probe(struct platform_device *pdev)
+{
+	struct aspeed_bmc_device *bmc_device;
+	struct device *dev = &pdev->dev;
+	int ret = 0;
+
+	bmc_device = devm_kzalloc(&pdev->dev, sizeof(struct aspeed_bmc_device), GFP_KERNEL);
+	if (!bmc_device)
+		return -ENOMEM;
+
+	init_waitqueue_head(&bmc_device->tx_wait0);
+	init_waitqueue_head(&bmc_device->tx_wait1);
+	init_waitqueue_head(&bmc_device->rx_wait0);
+	init_waitqueue_head(&bmc_device->rx_wait1);
+
+	bmc_device->dev = dev;
+	bmc_device->reg_base = devm_platform_ioremap_resource(pdev, 0);
+	if (IS_ERR(bmc_device->reg_base))
+		goto out_region;
+
+	bmc_device->scu = syscon_regmap_lookup_by_phandle(dev->of_node, "aspeed,scu");
+	if (IS_ERR(bmc_device->scu)) {
+		dev_err(&pdev->dev, "failed to find SCU regmap\n");
+		goto out_region;
+	}
+
+	if (of_property_read_bool(dev->of_node, "pcie2lpc"))
+		bmc_device->pcie2lpc = 1;
+
+	if (of_reserved_mem_device_init(dev))
+		dev_err(dev, "can't get reserved memory\n");
+
+	dma_set_mask_and_coherent(dev, DMA_BIT_MASK(32));
+
+	bmc_device->bmc_mem_virt = dma_alloc_coherent(&pdev->dev, BMC_MEM_BAR_SIZE, &bmc_device->bmc_mem_phy, GFP_KERNEL);
+	memset(bmc_device->bmc_mem_virt, 0, BMC_MEM_BAR_SIZE);
+
+	sysfs_bin_attr_init(&bmc_device->bin0);
+	sysfs_bin_attr_init(&bmc_device->bin1);
+
+	bmc_device->bin0.attr.name = "bmc-dev-queue1";
+	bmc_device->bin0.attr.mode = 0600;
+	bmc_device->bin0.read = aspeed_host2bmc_queue1_rx;
+	bmc_device->bin0.write = aspeed_bmc2host_queue1_tx;
+	bmc_device->bin0.size = 4;
+
+	ret = sysfs_create_bin_file(&pdev->dev.kobj, &bmc_device->bin0);
+	if (ret) {
+		dev_err(dev, "error for bin file\n");
+		goto out_dma;
+	}
+
+	bmc_device->kn0 = kernfs_find_and_get(dev->kobj.sd, bmc_device->bin0.attr.name);
+	if (!bmc_device->kn0) {
+		sysfs_remove_bin_file(&dev->kobj, &bmc_device->bin0);
+		goto out_dma;
+	}
+
+	bmc_device->bin1.attr.name = "bmc-dev-queue2";
+	bmc_device->bin1.attr.mode = 0600;
+	bmc_device->bin1.read = aspeed_host2bmc_queue2_rx;
+	bmc_device->bin1.write = aspeed_bmc2host_queue2_tx;
+	bmc_device->bin1.size = 4;
+
+	ret = sysfs_create_bin_file(&pdev->dev.kobj, &bmc_device->bin1);
+	if (ret) {
+		dev_err(dev, "error for bin file ");
+		goto out_dma;
+	}
+
+	bmc_device->kn1 = kernfs_find_and_get(dev->kobj.sd, bmc_device->bin1.attr.name);
+	if (!bmc_device->kn1) {
+		sysfs_remove_bin_file(&dev->kobj, &bmc_device->bin1);
+		goto out_dma;
+	}
+
+	dev_set_drvdata(dev, bmc_device);
+
+	aspeed_bmc_device_init(bmc_device);
+
+	bmc_device->irq =  platform_get_irq(pdev, 0);
+	if (bmc_device->irq < 0) {
+		dev_err(&pdev->dev, "platform get of irq[=%d] failed!\n", bmc_device->irq);
+		goto out_unmap;
+	}
+
+	ret = devm_request_irq(&pdev->dev, bmc_device->irq, aspeed_bmc_dev_isr,
+							0, dev_name(&pdev->dev), bmc_device);
+	if (ret) {
+		dev_err(dev, "aspeed bmc device Unable to get IRQ");
+		goto out_unmap;
+	}
+
+	bmc_device->miscdev.minor = MISC_DYNAMIC_MINOR;
+	bmc_device->miscdev.name = DEVICE_NAME;
+	bmc_device->miscdev.fops = &aspeed_bmc_device_fops;
+	bmc_device->miscdev.parent = dev;
+	ret = misc_register(&bmc_device->miscdev);
+	if (ret) {
+		dev_err(dev, "Unable to register device\n");
+		goto out_irq;
+	}
+
+	dev_info(dev, "aspeed bmc device: driver successfully loaded.\n");
+
+	return 0;
+
+out_irq:
+	devm_free_irq(&pdev->dev, bmc_device->irq, bmc_device);
+
+out_unmap:
+	iounmap(bmc_device->reg_base);
+
+out_dma:
+	dma_free_coherent(&pdev->dev, BMC_MEM_BAR_SIZE, bmc_device->bmc_mem_virt, bmc_device->bmc_mem_phy);
+
+out_region:
+	devm_kfree(&pdev->dev, bmc_device);
+	dev_warn(dev, "aspeed bmc device: driver init failed (ret=%d)!\n", ret);
+	return ret;
+}
+
+static int  aspeed_bmc_device_remove(struct platform_device *pdev)
+{
+	struct aspeed_bmc_device *bmc_device = platform_get_drvdata(pdev);
+
+	misc_deregister(&bmc_device->miscdev);
+
+	devm_free_irq(&pdev->dev, bmc_device->irq, bmc_device);
+
+	iounmap(bmc_device->reg_base);
+
+	dma_free_coherent(&pdev->dev, BMC_MEM_BAR_SIZE, bmc_device->bmc_mem_virt, bmc_device->bmc_mem_phy);
+
+	devm_kfree(&pdev->dev, bmc_device);
+
+	return 0;
+}
+
+
+static struct platform_driver aspeed_bmc_device_driver = {
+	.probe		= aspeed_bmc_device_probe,
+	.remove		= aspeed_bmc_device_remove,
+	.driver		= {
+		.name	= KBUILD_MODNAME,
+		.of_match_table = aspeed_bmc_device_of_matches,
+	},
+};
+
+module_platform_driver(aspeed_bmc_device_driver);
+
+MODULE_AUTHOR("Ryan Chen <ryan_chen@aspeedtech.com>");
+MODULE_DESCRIPTION("ASPEED BMC DEVICE Driver");
+MODULE_LICENSE("GPL");
diff --git a/drivers/soc/aspeed/aspeed-espi-ctrl.c b/drivers/soc/aspeed/aspeed-espi-ctrl.c
new file mode 100644
index 000000000000..297a19df504f
--- /dev/null
+++ b/drivers/soc/aspeed/aspeed-espi-ctrl.c
@@ -0,0 +1,237 @@
+// SPDX-License-Identifier: GPL-2.0+
+/*
+ * Copyright 2021 Aspeed Technology Inc.
+ */
+#include <linux/io.h>
+#include <linux/irq.h>
+#include <linux/clk.h>
+#include <linux/module.h>
+#include <linux/of_device.h>
+#include <linux/interrupt.h>
+#include <linux/platform_device.h>
+#include <linux/miscdevice.h>
+#include <linux/dma-mapping.h>
+#include <linux/mfd/syscon.h>
+#include <linux/regmap.h>
+#include <linux/uaccess.h>
+#include <linux/vmalloc.h>
+
+#include "aspeed-espi-ioc.h"
+#include "aspeed-espi-ctrl.h"
+#include "aspeed-espi-perif.h"
+#include "aspeed-espi-vw.h"
+#include "aspeed-espi-oob.h"
+#include "aspeed-espi-flash.h"
+
+#define DEVICE_NAME "aspeed-espi-ctrl"
+
+static irqreturn_t aspeed_espi_ctrl_isr(int irq, void *arg)
+{
+	uint32_t sts;
+	struct aspeed_espi_ctrl *espi_ctrl = (struct aspeed_espi_ctrl *)arg;
+
+	regmap_read(espi_ctrl->map, ESPI_INT_STS, &sts);
+
+	if (sts & ESPI_INT_STS_PERIF_BITS) {
+		aspeed_espi_perif_event(sts, espi_ctrl->perif);
+		regmap_write(espi_ctrl->map, ESPI_INT_STS, sts & ESPI_INT_STS_PERIF_BITS);
+	}
+
+	if (sts & ESPI_INT_STS_VW_BITS) {
+		aspeed_espi_vw_event(sts, espi_ctrl->vw);
+		regmap_write(espi_ctrl->map, ESPI_INT_STS, sts & ESPI_INT_STS_VW_BITS);
+	}
+
+	if (sts & (ESPI_INT_STS_OOB_BITS)) {
+		aspeed_espi_oob_event(sts, espi_ctrl->oob);
+		regmap_write(espi_ctrl->map, ESPI_INT_STS, sts & ESPI_INT_STS_OOB_BITS);
+	}
+
+	if (sts & ESPI_INT_STS_FLASH_BITS) {
+		aspeed_espi_flash_event(sts, espi_ctrl->flash);
+		regmap_write(espi_ctrl->map, ESPI_INT_STS, sts & ESPI_INT_STS_FLASH_BITS);
+	}
+
+	if (sts & ESPI_INT_STS_HW_RST_DEASSERT) {
+		aspeed_espi_perif_enable(espi_ctrl->perif);
+		aspeed_espi_vw_enable(espi_ctrl->vw);
+		aspeed_espi_oob_enable(espi_ctrl->oob);
+		aspeed_espi_flash_enable(espi_ctrl->flash);
+
+		regmap_write(espi_ctrl->map, ESPI_SYSEVT_INT_T0, 0x0);
+		regmap_write(espi_ctrl->map, ESPI_SYSEVT_INT_T1, 0x0);
+		regmap_write(espi_ctrl->map, ESPI_SYSEVT_INT_EN, 0xffffffff);
+
+		regmap_write(espi_ctrl->map, ESPI_SYSEVT1_INT_T0, 0x1);
+		regmap_write(espi_ctrl->map, ESPI_SYSEVT1_INT_EN, 0x1);
+
+		if (espi_ctrl->model->version == ESPI_AST2500)
+			regmap_write(espi_ctrl->map, ESPI_SYSEVT_INT_T2,
+				     ESPI_SYSEVT_INT_T2_HOST_RST_WARN |
+				     ESPI_SYSEVT_INT_T2_OOB_RST_WARN);
+
+		regmap_update_bits(espi_ctrl->map, ESPI_INT_EN,
+				   ESPI_INT_EN_HW_RST_DEASSERT,
+				   ESPI_INT_EN_HW_RST_DEASSERT);
+
+		regmap_update_bits(espi_ctrl->map, ESPI_SYSEVT,
+				   ESPI_SYSEVT_SLV_BOOT_STS | ESPI_SYSEVT_SLV_BOOT_DONE,
+				   ESPI_SYSEVT_SLV_BOOT_STS | ESPI_SYSEVT_SLV_BOOT_DONE);
+
+		regmap_write(espi_ctrl->map, ESPI_INT_STS, ESPI_INT_STS_HW_RST_DEASSERT);
+	}
+
+	return IRQ_HANDLED;
+}
+
+static int aspeed_espi_ctrl_probe(struct platform_device *pdev)
+{
+	int rc = 0;
+	uint32_t reg;
+	struct aspeed_espi_ctrl *espi_ctrl;
+	struct device *dev = &pdev->dev;
+	struct regmap *scu;
+
+	espi_ctrl = devm_kzalloc(dev, sizeof(*espi_ctrl), GFP_KERNEL);
+	if (!espi_ctrl)
+		return -ENOMEM;
+
+	espi_ctrl->model = of_device_get_match_data(dev);
+
+	scu = syscon_regmap_lookup_by_phandle(dev->of_node, "aspeed,scu");
+	if (IS_ERR(scu)) {
+		dev_err(dev, "cannot to find SCU regmap\n");
+		return -ENODEV;
+	}
+
+	if (espi_ctrl->model->version == ESPI_AST2500) {
+		regmap_read(scu, 0x70, &reg);
+		if ((reg & 0x2000000) == 0)
+			return -EPERM;
+	} else if (espi_ctrl->model->version == ESPI_AST2600) {
+		regmap_read(scu, 0x510, &reg);
+		if (reg & 0x40)
+			return -EPERM;
+	} else {
+		dev_err(dev, "unknown eSPI version\n");
+		return -EINVAL;
+	}
+
+	espi_ctrl->map = syscon_node_to_regmap(dev->parent->of_node);
+	if (IS_ERR(espi_ctrl->map)) {
+		dev_err(dev, "cannot get remap\n");
+		return -ENODEV;
+	}
+
+	espi_ctrl->irq = platform_get_irq(pdev, 0);
+	if (espi_ctrl->irq < 0)
+		return espi_ctrl->irq;
+
+	espi_ctrl->clk = devm_clk_get(dev, NULL);
+	if (IS_ERR(espi_ctrl->clk)) {
+		dev_err(dev, "cannot get clock\n");
+		return -ENODEV;
+	}
+
+	rc = clk_prepare_enable(espi_ctrl->clk);
+	if (rc) {
+		dev_err(dev, "cannot enable clock\n");
+		return rc;
+	}
+
+	espi_ctrl->perif = aspeed_espi_perif_alloc(dev, espi_ctrl);
+	if (IS_ERR(espi_ctrl->perif)) {
+		dev_err(dev, "failed to allocate peripheral channel\n");
+		return PTR_ERR(espi_ctrl->perif);
+	}
+
+	espi_ctrl->vw = aspeed_espi_vw_alloc(dev, espi_ctrl);
+	if (IS_ERR(espi_ctrl->vw)) {
+		dev_err(dev, "failed to allocate virtual wire channel\n");
+		return PTR_ERR(espi_ctrl->vw);
+	}
+
+	espi_ctrl->oob = aspeed_espi_oob_alloc(dev, espi_ctrl);
+	if (IS_ERR(espi_ctrl->oob)) {
+		dev_err(dev, "failed to allocate out-of-band channel\n");
+		return PTR_ERR(espi_ctrl->oob);
+	}
+
+	espi_ctrl->flash = aspeed_espi_flash_alloc(dev, espi_ctrl);
+	if (rc) {
+		dev_err(dev, "failed to allocate flash channel\n");
+		return PTR_ERR(espi_ctrl->flash);
+	}
+
+	regmap_update_bits(espi_ctrl->map, ESPI_CTRL2, BIT(30), 0);
+
+	regmap_write(espi_ctrl->map, ESPI_SYSEVT_INT_T0, 0x0);
+	regmap_write(espi_ctrl->map, ESPI_SYSEVT_INT_T1, 0x0);
+	regmap_write(espi_ctrl->map, ESPI_SYSEVT_INT_EN, 0xffffffff);
+
+	regmap_write(espi_ctrl->map, ESPI_SYSEVT1_INT_T0, 0x1);
+	regmap_write(espi_ctrl->map, ESPI_SYSEVT1_INT_EN, 0x1);
+
+	rc = devm_request_irq(dev, espi_ctrl->irq,
+			      aspeed_espi_ctrl_isr,
+			      0, DEVICE_NAME, espi_ctrl);
+	if (rc) {
+		dev_err(dev, "failed to request IRQ\n");
+		return rc;
+	}
+
+	regmap_update_bits(espi_ctrl->map, ESPI_INT_EN,
+			   ESPI_INT_EN_HW_RST_DEASSERT,
+			   ESPI_INT_EN_HW_RST_DEASSERT);
+
+	dev_set_drvdata(dev, espi_ctrl);
+
+	dev_info(dev, "module loaded\n");
+
+	return 0;
+}
+
+static int aspeed_espi_ctrl_remove(struct platform_device *pdev)
+{
+	struct device *dev = &pdev->dev;
+	struct aspeed_espi_ctrl *espi_ctrl = dev_get_drvdata(dev);
+
+	aspeed_espi_perif_free(dev, espi_ctrl->perif);
+	aspeed_espi_vw_free(dev, espi_ctrl->vw);
+	aspeed_espi_oob_free(dev, espi_ctrl->oob);
+	aspeed_espi_flash_free(dev, espi_ctrl->flash);
+
+	return 0;
+}
+
+static const struct aspeed_espi_model ast2500_model = {
+	.version = ESPI_AST2500,
+};
+
+static const struct aspeed_espi_model ast2600_model = {
+	.version = ESPI_AST2600,
+};
+
+static const struct of_device_id aspeed_espi_ctrl_of_matches[] = {
+	{ .compatible = "aspeed,ast2500-espi-ctrl",
+	  .data = &ast2500_model },
+	{ .compatible = "aspeed,ast2600-espi-ctrl",
+	  .data = &ast2600_model },
+	{ },
+};
+
+static struct platform_driver aspeed_espi_ctrl_driver = {
+	.driver = {
+		.name = DEVICE_NAME,
+		.of_match_table = aspeed_espi_ctrl_of_matches,
+	},
+	.probe = aspeed_espi_ctrl_probe,
+	.remove = aspeed_espi_ctrl_remove,
+};
+
+module_platform_driver(aspeed_espi_ctrl_driver);
+
+MODULE_AUTHOR("Chia-Wei Wang <chiawei_wang@aspeedtech.com>");
+MODULE_AUTHOR("Ryan Chen <ryan_chen@aspeedtech.com>");
+MODULE_DESCRIPTION("Control of Aspeed eSPI Slave Device");
+MODULE_LICENSE("GPL v2");
diff --git a/drivers/soc/aspeed/aspeed-espi-ctrl.h b/drivers/soc/aspeed/aspeed-espi-ctrl.h
new file mode 100644
index 000000000000..247cb6ce46fb
--- /dev/null
+++ b/drivers/soc/aspeed/aspeed-espi-ctrl.h
@@ -0,0 +1,308 @@
+/* SPDX-License-Identifier: GPL-2.0+ */
+/*
+ * Copyright 2021 Aspeed Technology Inc.
+ */
+#ifndef _ASPEED_ESPI_CTRL_H_
+#define _ASPEED_ESPI_CTRL_H_
+
+#include <linux/bits.h>
+
+enum aspeed_espi_version {
+	ESPI_AST2500,
+	ESPI_AST2600,
+};
+
+struct aspeed_espi_model {
+	uint32_t version;
+};
+
+struct aspeed_espi_ctrl {
+	struct device *dev;
+
+	struct regmap *map;
+	struct clk *clk;
+
+	int irq;
+
+	struct aspeed_espi_perif *perif;
+	struct aspeed_espi_vw *vw;
+	struct aspeed_espi_oob *oob;
+	struct aspeed_espi_flash *flash;
+
+	const struct aspeed_espi_model *model;
+};
+
+/* eSPI register offset */
+#define ESPI_CTRL		0x000
+#define   ESPI_CTRL_OOB_RX_SW_RST		BIT(28)
+#define   ESPI_CTRL_FLASH_TX_DMA_EN		BIT(23)
+#define   ESPI_CTRL_FLASH_RX_DMA_EN		BIT(22)
+#define   ESPI_CTRL_OOB_TX_DMA_EN		BIT(21)
+#define   ESPI_CTRL_OOB_RX_DMA_EN		BIT(20)
+#define   ESPI_CTRL_PERIF_NP_TX_DMA_EN		BIT(19)
+#define   ESPI_CTRL_PERIF_PC_TX_DMA_EN		BIT(17)
+#define   ESPI_CTRL_PERIF_PC_RX_DMA_EN		BIT(16)
+#define   ESPI_CTRL_FLASH_SW_MODE_MASK		GENMASK(11, 10)
+#define   ESPI_CTRL_FLASH_SW_MODE_SHIFT		10
+#define   ESPI_CTRL_PERIF_PC_RX_DMA_EN		BIT(16)
+#define   ESPI_CTRL_FLASH_SW_RDY		BIT(7)
+#define   ESPI_CTRL_OOB_SW_RDY			BIT(4)
+#define   ESPI_CTRL_VW_SW_RDY			BIT(3)
+#define   ESPI_CTRL_PERIF_SW_RDY		BIT(1)
+#define ESPI_STS		0x004
+#define ESPI_INT_STS		0x008
+#define   ESPI_INT_STS_HW_RST_DEASSERT		BIT(31)
+#define   ESPI_INT_STS_OOB_RX_TMOUT		BIT(23)
+#define   ESPI_INT_STS_VW_SYSEVT1		BIT(22)
+#define   ESPI_INT_STS_FLASH_TX_ERR		BIT(21)
+#define   ESPI_INT_STS_OOB_TX_ERR		BIT(20)
+#define   ESPI_INT_STS_FLASH_TX_ABT		BIT(19)
+#define   ESPI_INT_STS_OOB_TX_ABT		BIT(18)
+#define   ESPI_INT_STS_PERIF_NP_TX_ABT		BIT(17)
+#define   ESPI_INT_STS_PERIF_PC_TX_ABT		BIT(16)
+#define   ESPI_INT_STS_FLASH_RX_ABT		BIT(15)
+#define   ESPI_INT_STS_OOB_RX_ABT		BIT(14)
+#define   ESPI_INT_STS_PERIF_NP_RX_ABT		BIT(13)
+#define   ESPI_INT_STS_PERIF_PC_RX_ABT		BIT(12)
+#define   ESPI_INT_STS_PERIF_NP_TX_ERR		BIT(11)
+#define   ESPI_INT_STS_PERIF_PC_TX_ERR		BIT(10)
+#define   ESPI_INT_STS_VW_GPIOEVT		BIT(9)
+#define   ESPI_INT_STS_VW_SYSEVT		BIT(8)
+#define   ESPI_INT_STS_FLASH_TX_CMPLT		BIT(7)
+#define   ESPI_INT_STS_FLASH_RX_CMPLT		BIT(6)
+#define   ESPI_INT_STS_OOB_TX_CMPLT		BIT(5)
+#define   ESPI_INT_STS_OOB_RX_CMPLT		BIT(4)
+#define   ESPI_INT_STS_PERIF_NP_TX_CMPLT	BIT(3)
+#define   ESPI_INT_STS_PERIF_PC_TX_CMPLT	BIT(1)
+#define   ESPI_INT_STS_PERIF_PC_RX_CMPLT	BIT(0)
+#define ESPI_INT_EN		0x00c
+#define   ESPI_INT_EN_HW_RST_DEASSERT		BIT(31)
+#define   ESPI_INT_EN_OOB_RX_TMOUT		BIT(23)
+#define   ESPI_INT_EN_VW_SYSEVT1		BIT(22)
+#define   ESPI_INT_EN_FLASH_TX_ERR		BIT(21)
+#define   ESPI_INT_EN_OOB_TX_ERR		BIT(20)
+#define   ESPI_INT_EN_FLASH_TX_ABT		BIT(19)
+#define   ESPI_INT_EN_OOB_TX_ABT		BIT(18)
+#define   ESPI_INT_EN_PERIF_NP_TX_ABT		BIT(17)
+#define   ESPI_INT_EN_PERIF_PC_TX_ABT		BIT(16)
+#define   ESPI_INT_EN_FLASH_RX_ABT		BIT(15)
+#define   ESPI_INT_EN_OOB_RX_ABT		BIT(14)
+#define   ESPI_INT_EN_PERIF_NP_RX_ABT		BIT(13)
+#define   ESPI_INT_EN_PERIF_PC_RX_ABT		BIT(12)
+#define   ESPI_INT_EN_PERIF_NP_TX_ERR		BIT(11)
+#define   ESPI_INT_EN_PERIF_PC_TX_ERR		BIT(10)
+#define   ESPI_INT_EN_VW_GPIOEVT		BIT(9)
+#define   ESPI_INT_EN_VW_SYSEVT			BIT(8)
+#define   ESPI_INT_EN_FLASH_TX_CMPLT		BIT(7)
+#define   ESPI_INT_EN_FLASH_RX_CMPLT		BIT(6)
+#define   ESPI_INT_EN_OOB_TX_CMPLT		BIT(5)
+#define   ESPI_INT_EN_OOB_RX_CMPLT		BIT(4)
+#define   ESPI_INT_EN_PERIF_NP_TX_CMPLT		BIT(3)
+#define   ESPI_INT_EN_PERIF_PC_TX_CMPLT		BIT(1)
+#define   ESPI_INT_EN_PERIF_PC_RX_CMPLT		BIT(0)
+#define ESPI_PERIF_PC_RX_DMA	0x010
+#define ESPI_PERIF_PC_RX_CTRL	0x014
+#define   ESPI_PERIF_PC_RX_CTRL_PEND_SERV	BIT(31)
+#define   ESPI_PERIF_PC_RX_CTRL_LEN_MASK	GENMASK(23, 12)
+#define   ESPI_PERIF_PC_RX_CTRL_LEN_SHIFT	12
+#define   ESPI_PERIF_PC_RX_CTRL_TAG_MASK	GENMASK(11, 8)
+#define   ESPI_PERIF_PC_RX_CTRL_TAG_SHIFT	8
+#define   ESPI_PERIF_PC_RX_CTRL_CYC_MASK	GENMASK(7, 0)
+#define   ESPI_PERIF_PC_RX_CTRL_CYC_SHIFT	0
+#define ESPI_PERIF_PC_RX_PORT	0x018
+#define ESPI_PERIF_PC_TX_DMA	0x020
+#define ESPI_PERIF_PC_TX_CTRL	0x024
+#define	  ESPI_PERIF_PC_TX_CTRL_TRIGGER		BIT(31)
+#define	  ESPI_PERIF_PC_TX_CTRL_LEN_MASK	GENMASK(23, 12)
+#define	  ESPI_PERIF_PC_TX_CTRL_LEN_SHIFT	12
+#define	  ESPI_PERIF_PC_TX_CTRL_TAG_MASK	GENMASK(11, 8)
+#define	  ESPI_PERIF_PC_TX_CTRL_TAG_SHIFT	8
+#define	  ESPI_PERIF_PC_TX_CTRL_CYC_MASK	GENMASK(7, 0)
+#define	  ESPI_PERIF_PC_TX_CTRL_CYC_SHIFT	0
+#define ESPI_PERIF_PC_TX_PORT	0x028
+#define ESPI_PERIF_NP_TX_DMA	0x030
+#define ESPI_PERIF_NP_TX_CTRL	0x034
+#define   ESPI_PERIF_NP_TX_CTRL_TRIGGER		BIT(31)
+#define	  ESPI_PERIF_NP_TX_CTRL_LEN_MASK	GENMASK(23, 12)
+#define	  ESPI_PERIF_NP_TX_CTRL_LEN_SHIFT	12
+#define	  ESPI_PERIF_NP_TX_CTRL_TAG_MASK	GENMASK(11, 8)
+#define	  ESPI_PERIF_NP_TX_CTRL_TAG_SHIFT	8
+#define	  ESPI_PERIF_NP_TX_CTRL_CYC_MASK	GENMASK(7, 0)
+#define	  ESPI_PERIF_NP_TX_CTRL_CYC_SHIFT	0
+#define ESPI_PERIF_NP_TX_PORT	0x038
+#define ESPI_OOB_RX_DMA		0x040
+#define ESPI_OOB_RX_CTRL	0x044
+#define	  ESPI_OOB_RX_CTRL_PEND_SERV		BIT(31)
+#define	  ESPI_OOB_RX_CTRL_LEN_MASK		GENMASK(23, 12)
+#define	  ESPI_OOB_RX_CTRL_LEN_SHIFT		12
+#define	  ESPI_OOB_RX_CTRL_TAG_MASK		GENMASK(11, 8)
+#define	  ESPI_OOB_RX_CTRL_TAG_SHIFT		8
+#define	  ESPI_OOB_RX_CTRL_CYC_MASK		GENMASK(7, 0)
+#define	  ESPI_OOB_RX_CTRL_CYC_SHIFT		0
+#define ESPI_OOB_RX_PORT	0x048
+#define ESPI_OOB_TX_DMA		0x050
+#define ESPI_OOB_TX_CTRL	0x054
+#define	  ESPI_OOB_TX_CTRL_TRIGGER		BIT(31)
+#define	  ESPI_OOB_TX_CTRL_LEN_MASK		GENMASK(23, 12)
+#define	  ESPI_OOB_TX_CTRL_LEN_SHIFT		12
+#define	  ESPI_OOB_TX_CTRL_TAG_MASK		GENMASK(11, 8)
+#define	  ESPI_OOB_TX_CTRL_TAG_SHIFT		8
+#define	  ESPI_OOB_TX_CTRL_CYC_MASK		GENMASK(7, 0)
+#define	  ESPI_OOB_TX_CTRL_CYC_SHIFT		0
+#define ESPI_OOB_TX_PORT	0x058
+#define ESPI_FLASH_RX_DMA	0x060
+#define ESPI_FLASH_RX_CTRL	0x064
+#define	  ESPI_FLASH_RX_CTRL_PEND_SERV		BIT(31)
+#define	  ESPI_FLASH_RX_CTRL_LEN_MASK		GENMASK(23, 12)
+#define	  ESPI_FLASH_RX_CTRL_LEN_SHIFT		12
+#define	  ESPI_FLASH_RX_CTRL_TAG_MASK		GENMASK(11, 8)
+#define	  ESPI_FLASH_RX_CTRL_TAG_SHIFT		8
+#define	  ESPI_FLASH_RX_CTRL_CYC_MASK		GENMASK(7, 0)
+#define	  ESPI_FLASH_RX_CTRL_CYC_SHIFT		0
+#define ESPI_FLASH_RX_PORT	0x068
+#define ESPI_FLASH_TX_DMA	0x070
+#define ESPI_FLASH_TX_CTRL	0x074
+#define	  ESPI_FLASH_TX_CTRL_TRIGGER		BIT(31)
+#define	  ESPI_FLASH_TX_CTRL_LEN_MASK		GENMASK(23, 12)
+#define	  ESPI_FLASH_TX_CTRL_LEN_SHIFT		12
+#define	  ESPI_FLASH_TX_CTRL_TAG_MASK		GENMASK(11, 8)
+#define	  ESPI_FLASH_TX_CTRL_TAG_SHIFT		8
+#define	  ESPI_FLASH_TX_CTRL_CYC_MASK		GENMASK(7, 0)
+#define	  ESPI_FLASH_TX_CTRL_CYC_SHIFT		0
+#define ESPI_FLASH_TX_PORT	0x078
+#define ESPI_CTRL2		0x080
+#define   ESPI_CTRL2_MEMCYC_RD_DIS		BIT(6)
+#define   ESPI_CTRL2_MEMCYC_WR_DIS		BIT(4)
+#define ESPI_PERIF_PC_RX_SADDR	0x084
+#define ESPI_PERIF_PC_RX_TADDR	0x088
+#define ESPI_PERIF_PC_RX_MASK	0x08c
+#define   ESPI_PERIF_PC_RX_MASK_CFG_WP		BIT(0)
+#define ESPI_SYSEVT_INT_EN	0x094
+#define ESPI_SYSEVT		0x098
+#define   ESPI_SYSEVT_HOST_RST_ACK		BIT(27)
+#define   ESPI_SYSEVT_RST_CPU_INIT		BIT(26)
+#define   ESPI_SYSEVT_SLV_BOOT_STS		BIT(23)
+#define   ESPI_SYSEVT_NON_FATAL_ERR		BIT(22)
+#define   ESPI_SYSEVT_FATAL_ERR			BIT(21)
+#define   ESPI_SYSEVT_SLV_BOOT_DONE		BIT(20)
+#define   ESPI_SYSEVT_OOB_RST_ACK		BIT(16)
+#define   ESPI_SYSEVT_NMI_OUT			BIT(10)
+#define   ESPI_SYSEVT_SMI_OUT			BIT(9)
+#define   ESPI_SYSEVT_HOST_RST_WARN		BIT(8)
+#define   ESPI_SYSEVT_OOB_RST_WARN		BIT(6)
+#define   ESPI_SYSEVT_PLTRSTN			BIT(5)
+#define   ESPI_SYSEVT_SUSPEND			BIT(4)
+#define   ESPI_SYSEVT_S5_SLEEP			BIT(2)
+#define   ESPI_SYSEVT_S4_SLEEP			BIT(1)
+#define   ESPI_SYSEVT_S3_SLEEP			BIT(0)
+#define ESPI_VW_GPIO_VAL	0x09c
+#define ESPI_GEN_CAP_N_CONF	0x0a0
+#define ESPI_CH0_CAP_N_CONF	0x0a4
+#define ESPI_CH1_CAP_N_CONF	0x0a8
+#define ESPI_CH2_CAP_N_CONF	0x0ac
+#define ESPI_CH3_CAP_N_CONF	0x0b0
+#define ESPI_CH3_CAP_N_CONF2	0x0b4
+#define ESPI_SYSEVT1_INT_EN	0x100
+#define ESPI_SYSEVT1		0x104
+#define   ESPI_SYSEVT1_SUSPEND_ACK		BIT(20)
+#define   ESPI_SYSEVT1_SUSPEND_WARN		BIT(0)
+#define ESPI_SYSEVT_INT_T0	0x110
+#define ESPI_SYSEVT_INT_T1	0x114
+#define ESPI_SYSEVT_INT_T2	0x118
+#define   ESPI_SYSEVT_INT_T2_HOST_RST_WARN	ESPI_SYSEVT_HOST_RST_WARN
+#define   ESPI_SYSEVT_INT_T2_OOB_RST_WARN	ESPI_SYSEVT_OOB_RST_WARN
+#define ESPI_SYSEVT_INT_STS	0x11c
+#define   ESPI_SYSEVT_INT_STS_NMI_OUT		ESPI_SYSEVT_NMI_OUT
+#define   ESPI_SYSEVT_INT_STS_SMI_OUT		ESPI_SYSEVT_SMI_OUT
+#define   ESPI_SYSEVT_INT_STS_HOST_RST_WARN	ESPI_SYSEVT_HOST_RST_WARN
+#define   ESPI_SYSEVT_INT_STS_OOB_RST_WARN	ESPI_SYSEVT_OOB_RST_WARN
+#define   ESPI_SYSEVT_INT_STS_PLTRSTN		ESPI_SYSEVT_PLTRSTN
+#define   ESPI_SYSEVT_INT_STS_SUSPEND		ESPI_SYSEVT_SUSPEND
+#define   ESPI_SYSEVT_INT_STS_S5_SLEEP		ESPI_SYSEVT_INT_S5_SLEEP
+#define   ESPI_SYSEVT_INT_STS_S4_SLEEP		ESPI_SYSEVT_INT_S4_SLEEP
+#define   ESPI_SYSEVT_INT_STS_S3_SLEEP		ESPI_SYSEVT_INT_S3_SLEEP
+#define ESPI_SYSEVT1_INT_T0	0x120
+#define ESPI_SYSEVT1_INT_T1	0x124
+#define ESPI_SYSEVT1_INT_T2	0x128
+#define ESPI_SYSEVT1_INT_STS	0x12c
+#define   ESPI_SYSEVT1_INT_STS_SUSPEND_WARN	ESPI_SYSEVT1_SUSPEND_WARN
+#define ESPI_OOB_RX_DMA_RB_SIZE	0x130
+#define ESPI_OOB_RX_DMA_RD_PTR	0x134
+#define	  ESPI_OOB_RX_DMA_RD_PTR_UPDATE		BIT(31)
+#define ESPI_OOB_RX_DMA_WS_PTR	0x138
+#define   ESPI_OOB_RX_DMA_WS_PTR_RECV_EN	BIT(31)
+#define   ESPI_OOB_RX_DMA_WS_PTR_SP_MASK	GENMASK(27, 16)
+#define   ESPI_OOB_RX_DMA_WS_PTR_SP_SHIFT	16
+#define   ESPI_OOB_RX_DMA_WS_PTR_WP_MASK	GENMASK(11, 0)
+#define   ESPI_OOB_RX_DMA_WS_PTR_WP_SHIFT	0
+#define ESPI_OOB_TX_DMA_RB_SIZE	0x140
+#define ESPI_OOB_TX_DMA_RD_PTR	0x144
+#define	  ESPI_OOB_TX_DMA_RD_PTR_UPDATE		BIT(31)
+#define ESPI_OOB_TX_DMA_WR_PTR	0x148
+#define	  ESPI_OOB_TX_DMA_WR_PTR_SEND_EN	BIT(31)
+
+/* collect ESPI_INT_STS bits of eSPI channels for convenience */
+#define ESPI_INT_STS_PERIF_BITS			\
+	(ESPI_INT_STS_PERIF_NP_TX_ABT |		\
+	 ESPI_INT_STS_PERIF_PC_TX_ABT |		\
+	 ESPI_INT_STS_PERIF_NP_RX_ABT |		\
+	 ESPI_INT_STS_PERIF_PC_RX_ABT |		\
+	 ESPI_INT_STS_PERIF_NP_TX_ERR |		\
+	 ESPI_INT_STS_PERIF_PC_TX_ERR |		\
+	 ESPI_INT_STS_PERIF_NP_TX_CMPLT |	\
+	 ESPI_INT_STS_PERIF_PC_TX_CMPLT |	\
+	 ESPI_INT_STS_PERIF_PC_RX_CMPLT)
+
+#define ESPI_INT_STS_VW_BITS		\
+	(ESPI_INT_STS_VW_SYSEVT1 |	\
+	 ESPI_INT_STS_VW_GPIOEVT |	\
+	 ESPI_INT_STS_VW_SYSEVT)
+
+#define ESPI_INT_STS_OOB_BITS		\
+	(ESPI_INT_STS_OOB_RX_TMOUT |	\
+	 ESPI_INT_STS_OOB_TX_ERR |	\
+	 ESPI_INT_STS_OOB_TX_ABT |	\
+	 ESPI_INT_STS_OOB_RX_ABT |	\
+	 ESPI_INT_STS_OOB_TX_CMPLT |	\
+	 ESPI_INT_STS_OOB_RX_CMPLT)
+
+#define ESPI_INT_STS_FLASH_BITS		\
+	(ESPI_INT_STS_FLASH_TX_ERR |	\
+	 ESPI_INT_STS_FLASH_TX_ABT |	\
+	 ESPI_INT_STS_FLASH_RX_ABT |	\
+	 ESPI_INT_STS_FLASH_TX_CMPLT |	\
+	 ESPI_INT_STS_FLASH_RX_CMPLT)
+
+/* collect ESPI_INT_EN bits of eSPI channels for convenience */
+#define ESPI_INT_EN_PERIF_BITS			\
+	(ESPI_INT_EN_PERIF_NP_TX_ABT |		\
+	 ESPI_INT_EN_PERIF_PC_TX_ABT |		\
+	 ESPI_INT_EN_PERIF_NP_RX_ABT |		\
+	 ESPI_INT_EN_PERIF_PC_RX_ABT |		\
+	 ESPI_INT_EN_PERIF_NP_TX_ERR |		\
+	 ESPI_INT_EN_PERIF_PC_TX_ERR |		\
+	 ESPI_INT_EN_PERIF_NP_TX_CMPLT |	\
+	 ESPI_INT_EN_PERIF_PC_TX_CMPLT |	\
+	 ESPI_INT_EN_PERIF_PC_RX_CMPLT)
+
+#define ESPI_INT_EN_VW_BITS		\
+	(ESPI_INT_EN_VW_SYSEVT1 |	\
+	 ESPI_INT_EN_VW_GPIOEVT |	\
+	 ESPI_INT_EN_VW_SYSEVT)
+
+#define ESPI_INT_EN_OOB_BITS		\
+	(ESPI_INT_EN_OOB_RX_TMOUT |	\
+	 ESPI_INT_EN_OOB_TX_ERR |	\
+	 ESPI_INT_EN_OOB_TX_ABT |	\
+	 ESPI_INT_EN_OOB_RX_ABT |	\
+	 ESPI_INT_EN_OOB_TX_CMPLT |	\
+	 ESPI_INT_EN_OOB_RX_CMPLT)
+
+#define ESPI_INT_EN_FLASH_BITS		\
+	(ESPI_INT_EN_FLASH_TX_ERR |	\
+	 ESPI_INT_EN_FLASH_TX_ABT |	\
+	 ESPI_INT_EN_FLASH_RX_ABT |	\
+	 ESPI_INT_EN_FLASH_TX_CMPLT |	\
+	 ESPI_INT_EN_FLASH_RX_CMPLT)
+
+#endif
diff --git a/drivers/soc/aspeed/aspeed-espi-flash.c b/drivers/soc/aspeed/aspeed-espi-flash.c
new file mode 100644
index 000000000000..d6990259a9dd
--- /dev/null
+++ b/drivers/soc/aspeed/aspeed-espi-flash.c
@@ -0,0 +1,355 @@
+// SPDX-License-Identifier: GPL-2.0+
+/*
+ * Copyright 2021 ASPEED Technology Inc.
+ */
+#include <linux/fs.h>
+#include <linux/of_device.h>
+#include <linux/miscdevice.h>
+#include <linux/mfd/syscon.h>
+#include <linux/regmap.h>
+#include <linux/uaccess.h>
+#include <linux/vmalloc.h>
+#include <linux/miscdevice.h>
+#include <linux/dma-mapping.h>
+
+#include "aspeed-espi-ioc.h"
+#include "aspeed-espi-ctrl.h"
+#include "aspeed-espi-flash.h"
+
+#define FLASH_MDEV_NAME	"aspeed-espi-flash"
+
+static long aspeed_espi_flash_get_rx(struct file *fp,
+				     struct aspeed_espi_ioc *ioc,
+				     struct aspeed_espi_flash *espi_flash)
+{
+	int i, rc = 0;
+	unsigned long flags;
+	uint32_t reg;
+	uint32_t cyc, tag, len;
+	uint8_t *pkt;
+	uint32_t pkt_len;
+	struct espi_comm_hdr *hdr;
+	struct aspeed_espi_ctrl *espi_ctrl = espi_flash->ctrl;
+
+	if (fp->f_flags & O_NONBLOCK) {
+		if (!mutex_trylock(&espi_flash->get_rx_mtx))
+			return -EAGAIN;
+
+		if (!espi_flash->rx_ready) {
+			rc = -ENODATA;
+			goto unlock_mtx_n_out;
+		}
+	} else {
+		mutex_lock(&espi_flash->get_rx_mtx);
+
+		if (!espi_flash->rx_ready) {
+			rc = wait_event_interruptible(espi_flash->wq,
+						      espi_flash->rx_ready);
+			if (rc == -ERESTARTSYS) {
+				rc = -EINTR;
+				goto unlock_mtx_n_out;
+			}
+		}
+	}
+
+	/* common header (i.e. cycle type, tag, and length) is taken by HW */
+	regmap_read(espi_ctrl->map, ESPI_FLASH_RX_CTRL, &reg);
+	cyc = (reg & ESPI_FLASH_RX_CTRL_CYC_MASK) >> ESPI_FLASH_RX_CTRL_CYC_SHIFT;
+	tag = (reg & ESPI_FLASH_RX_CTRL_TAG_MASK) >> ESPI_FLASH_RX_CTRL_TAG_SHIFT;
+	len = (reg & ESPI_FLASH_RX_CTRL_LEN_MASK) >> ESPI_FLASH_RX_CTRL_LEN_SHIFT;
+
+	/*
+	 * calculate the length of the rest part of the
+	 * eSPI packet to be read from HW and copied to
+	 * user space.
+	 */
+	switch (cyc) {
+	case ESPI_FLASH_WRITE:
+		pkt_len = ((len) ? len : ESPI_PLD_LEN_MAX) +
+			  sizeof(struct espi_flash_rwe);
+		break;
+	case ESPI_FLASH_READ:
+	case ESPI_FLASH_ERASE:
+		pkt_len = sizeof(struct espi_flash_rwe);
+		break;
+	case ESPI_FLASH_SUC_CMPLT_D_MIDDLE:
+	case ESPI_FLASH_SUC_CMPLT_D_FIRST:
+	case ESPI_FLASH_SUC_CMPLT_D_LAST:
+	case ESPI_FLASH_SUC_CMPLT_D_ONLY:
+		pkt_len = ((len) ? len : ESPI_PLD_LEN_MAX) +
+			  sizeof(struct espi_flash_cmplt);
+		break;
+	case ESPI_FLASH_SUC_CMPLT:
+	case ESPI_FLASH_UNSUC_CMPLT:
+		pkt_len = sizeof(struct espi_flash_cmplt);
+		break;
+	default:
+		rc = -EFAULT;
+		goto unlock_mtx_n_out;
+	}
+
+	if (ioc->pkt_len < pkt_len) {
+		rc = -EINVAL;
+		goto unlock_mtx_n_out;
+	}
+
+	pkt = vmalloc(pkt_len);
+	if (!pkt) {
+		rc = -ENOMEM;
+		goto unlock_mtx_n_out;
+	}
+
+	hdr = (struct espi_comm_hdr *)pkt;
+	hdr->cyc = cyc;
+	hdr->tag = tag;
+	hdr->len_h = len >> 8;
+	hdr->len_l = len & 0xff;
+
+	if (espi_flash->dma_mode) {
+		memcpy(hdr + 1, espi_flash->dma.rx_virt,
+		       pkt_len - sizeof(*hdr));
+	} else {
+		for (i = sizeof(*hdr); i < pkt_len; ++i) {
+			regmap_read(espi_ctrl->map,
+				    ESPI_FLASH_RX_PORT, &reg);
+			pkt[i] = reg & 0xff;
+		}
+	}
+
+	if (copy_to_user((void __user *)ioc->pkt, pkt, pkt_len)) {
+		rc = -EFAULT;
+		goto free_n_out;
+	}
+
+	spin_lock_irqsave(&espi_flash->lock, flags);
+
+	regmap_write_bits(espi_ctrl->map, ESPI_FLASH_RX_CTRL,
+			  ESPI_FLASH_RX_CTRL_PEND_SERV,
+			  ESPI_FLASH_RX_CTRL_PEND_SERV);
+
+	espi_flash->rx_ready = 0;
+
+	spin_unlock_irqrestore(&espi_flash->lock, flags);
+
+free_n_out:
+	vfree(pkt);
+
+unlock_mtx_n_out:
+	mutex_unlock(&espi_flash->get_rx_mtx);
+
+	return rc;
+}
+
+static long aspeed_espi_flash_put_tx(struct file *fp,
+				     struct aspeed_espi_ioc *ioc,
+				     struct aspeed_espi_flash *espi_flash)
+{
+	int i, rc = 0;
+	uint32_t reg;
+	uint32_t cyc, tag, len;
+	uint8_t *pkt;
+	struct espi_comm_hdr *hdr;
+	struct aspeed_espi_ctrl *espi_ctrl = espi_flash->ctrl;
+
+	if (!mutex_trylock(&espi_flash->put_tx_mtx))
+		return -EAGAIN;
+
+	regmap_read(espi_ctrl->map, ESPI_FLASH_TX_CTRL, &reg);
+	if (reg & ESPI_FLASH_TX_CTRL_TRIGGER) {
+		rc = -EBUSY;
+		goto unlock_mtx_n_out;
+	}
+
+	pkt = vmalloc(ioc->pkt_len);
+	if (!pkt) {
+		rc = -ENOMEM;
+		goto unlock_mtx_n_out;
+	}
+
+	hdr = (struct espi_comm_hdr *)pkt;
+
+	if (copy_from_user(pkt, (void __user *)ioc->pkt, ioc->pkt_len)) {
+		rc = -EFAULT;
+		goto free_n_out;
+	}
+
+	/*
+	 * common header (i.e. cycle type, tag, and length)
+	 * part is written to HW registers
+	 */
+	if (espi_flash->dma_mode) {
+		memcpy(espi_flash->dma.tx_virt, hdr + 1,
+		       ioc->pkt_len - sizeof(*hdr));
+		dma_wmb();
+	} else {
+		for (i = sizeof(*hdr); i < ioc->pkt_len; ++i)
+			regmap_write(espi_ctrl->map,
+				     ESPI_FLASH_TX_PORT, pkt[i]);
+	}
+
+	cyc = hdr->cyc;
+	tag = hdr->tag;
+	len = (hdr->len_h << 8) | (hdr->len_l & 0xff);
+
+	reg = ((cyc << ESPI_FLASH_TX_CTRL_CYC_SHIFT) & ESPI_FLASH_TX_CTRL_CYC_MASK)
+		| ((tag << ESPI_FLASH_TX_CTRL_TAG_SHIFT) & ESPI_FLASH_TX_CTRL_TAG_MASK)
+		| ((len << ESPI_FLASH_TX_CTRL_LEN_SHIFT) & ESPI_FLASH_TX_CTRL_LEN_MASK)
+		| ESPI_FLASH_TX_CTRL_TRIGGER;
+
+	regmap_write(espi_ctrl->map, ESPI_FLASH_TX_CTRL, reg);
+
+free_n_out:
+	vfree(pkt);
+
+unlock_mtx_n_out:
+	mutex_unlock(&espi_flash->put_tx_mtx);
+
+	return rc;
+}
+
+static long aspeed_espi_flash_ioctl(struct file *fp, unsigned int cmd, unsigned long arg)
+{
+	struct aspeed_espi_ioc ioc;
+	struct aspeed_espi_flash *espi_flash = container_of(
+			fp->private_data,
+			struct aspeed_espi_flash,
+			mdev);
+
+	if (copy_from_user(&ioc, (void __user *)arg, sizeof(ioc)))
+		return -EFAULT;
+
+	if (ioc.pkt_len > ESPI_PKT_LEN_MAX)
+		return -EINVAL;
+
+	switch (cmd) {
+	case ASPEED_ESPI_FLASH_GET_RX:
+		return aspeed_espi_flash_get_rx(fp, &ioc, espi_flash);
+	case ASPEED_ESPI_FLASH_PUT_TX:
+		return aspeed_espi_flash_put_tx(fp, &ioc, espi_flash);
+	};
+
+	return -EINVAL;
+}
+
+void aspeed_espi_flash_event(uint32_t sts, struct aspeed_espi_flash *espi_flash)
+{
+	unsigned long flags;
+
+	if (sts & ESPI_INT_STS_FLASH_RX_CMPLT) {
+		spin_lock_irqsave(&espi_flash->lock, flags);
+		espi_flash->rx_ready = 1;
+		spin_unlock_irqrestore(&espi_flash->lock, flags);
+		wake_up_interruptible(&espi_flash->wq);
+	}
+}
+
+void aspeed_espi_flash_enable(struct aspeed_espi_flash *espi_flash)
+{
+	struct aspeed_espi_flash_dma *dma = &espi_flash->dma;
+	struct aspeed_espi_ctrl *espi_ctrl = espi_flash->ctrl;
+
+	regmap_update_bits(espi_ctrl->map, ESPI_CTRL,
+			   ESPI_CTRL_FLASH_SW_MODE_MASK,
+			   (espi_flash->safs_mode << ESPI_CTRL_FLASH_SW_MODE_SHIFT));
+
+	if (espi_flash->dma_mode) {
+		regmap_write(espi_ctrl->map, ESPI_FLASH_TX_DMA, dma->tx_addr);
+		regmap_write(espi_ctrl->map, ESPI_FLASH_RX_DMA, dma->rx_addr);
+		regmap_update_bits(espi_ctrl->map, ESPI_CTRL,
+				   ESPI_CTRL_FLASH_TX_DMA_EN | ESPI_CTRL_FLASH_RX_DMA_EN,
+				   ESPI_CTRL_FLASH_TX_DMA_EN | ESPI_CTRL_FLASH_RX_DMA_EN);
+	}
+
+	regmap_write(espi_ctrl->map, ESPI_INT_STS,
+		     ESPI_INT_STS_FLASH_BITS);
+
+	regmap_update_bits(espi_ctrl->map, ESPI_INT_EN,
+			   ESPI_INT_EN_FLASH_BITS,
+			   ESPI_INT_EN_FLASH_BITS);
+
+	regmap_update_bits(espi_ctrl->map, ESPI_CTRL,
+			   ESPI_CTRL_FLASH_SW_RDY,
+			   ESPI_CTRL_FLASH_SW_RDY);
+}
+
+static const struct file_operations aspeed_espi_flash_fops = {
+	.owner = THIS_MODULE,
+	.unlocked_ioctl = aspeed_espi_flash_ioctl,
+};
+
+void *aspeed_espi_flash_alloc(struct device *dev, struct aspeed_espi_ctrl *espi_ctrl)
+{
+	int rc = 0;
+	struct aspeed_espi_flash *espi_flash;
+	struct aspeed_espi_flash_dma *dma;
+
+	espi_flash = devm_kzalloc(dev, sizeof(*espi_flash), GFP_KERNEL);
+	if (!espi_flash)
+		return ERR_PTR(-ENOMEM);
+
+	espi_flash->ctrl = espi_ctrl;
+	espi_flash->safs_mode = SAFS_MODE_HW;
+
+	init_waitqueue_head(&espi_flash->wq);
+
+	spin_lock_init(&espi_flash->lock);
+
+	mutex_init(&espi_flash->put_tx_mtx);
+	mutex_init(&espi_flash->get_rx_mtx);
+
+	if (of_property_read_bool(dev->of_node, "flash,dma-mode"))
+		espi_flash->dma_mode = 1;
+
+	of_property_read_u32(dev->of_node, "flash,safs-mode", &espi_flash->safs_mode);
+	if (espi_flash->safs_mode >= SAFS_MODES) {
+		dev_err(dev, "invalid SAFS mode\n");
+		return ERR_PTR(-EINVAL);
+	}
+
+	if (espi_flash->dma_mode) {
+		dma = &espi_flash->dma;
+
+		dma->tx_virt = dma_alloc_coherent(dev, PAGE_SIZE,
+						  &dma->tx_addr, GFP_KERNEL);
+		if (!dma->tx_virt) {
+			dev_err(dev, "cannot allocate DMA TX buffer\n");
+			return ERR_PTR(-ENOMEM);
+		}
+
+		dma->rx_virt = dma_alloc_coherent(dev, PAGE_SIZE,
+						  &dma->rx_addr, GFP_KERNEL);
+		if (!dma->rx_virt) {
+			dev_err(dev, "cannot allocate DMA RX buffer\n");
+			return ERR_PTR(-ENOMEM);
+		}
+	}
+
+	espi_flash->mdev.parent = dev;
+	espi_flash->mdev.minor = MISC_DYNAMIC_MINOR;
+	espi_flash->mdev.name = devm_kasprintf(dev, GFP_KERNEL, "%s", FLASH_MDEV_NAME);
+	espi_flash->mdev.fops = &aspeed_espi_flash_fops;
+	rc = misc_register(&espi_flash->mdev);
+	if (rc) {
+		dev_err(dev, "cannot register device\n");
+		return ERR_PTR(rc);
+	}
+
+	aspeed_espi_flash_enable(espi_flash);
+
+	return espi_flash;
+}
+
+void aspeed_espi_flash_free(struct device *dev, struct aspeed_espi_flash *espi_flash)
+{
+	struct aspeed_espi_flash_dma *dma = &espi_flash->dma;
+
+	if (espi_flash->dma_mode) {
+		dma_free_coherent(dev, PAGE_SIZE, dma->tx_virt, dma->tx_addr);
+		dma_free_coherent(dev, PAGE_SIZE, dma->rx_virt, dma->rx_addr);
+	}
+
+	mutex_destroy(&espi_flash->put_tx_mtx);
+	mutex_destroy(&espi_flash->get_rx_mtx);
+
+	misc_deregister(&espi_flash->mdev);
+}
diff --git a/drivers/soc/aspeed/aspeed-espi-flash.h b/drivers/soc/aspeed/aspeed-espi-flash.h
new file mode 100644
index 000000000000..bd5177329e50
--- /dev/null
+++ b/drivers/soc/aspeed/aspeed-espi-flash.h
@@ -0,0 +1,45 @@
+/* SPDX-License-Identifier: GPL-2.0+ */
+/*
+ * Copyright 2021 ASPEED Technology Inc.
+ */
+#ifndef _ASPEED_ESPI_FLASH_H_
+#define _ASPEED_ESPI_FLASH_H_
+
+enum aspeed_espi_flash_safs_mode {
+	SAFS_MODE_MIX,
+	SAFS_MODE_SW,
+	SAFS_MODE_HW,
+	SAFS_MODES,
+};
+
+struct aspeed_espi_flash_dma {
+	void *tx_virt;
+	dma_addr_t tx_addr;
+	void *rx_virt;
+	dma_addr_t rx_addr;
+};
+
+struct aspeed_espi_flash {
+	uint32_t safs_mode;
+
+	uint32_t dma_mode;
+	struct aspeed_espi_flash_dma dma;
+
+	uint32_t rx_ready;
+	wait_queue_head_t wq;
+
+	struct mutex get_rx_mtx;
+	struct mutex put_tx_mtx;
+
+	spinlock_t lock;
+
+	struct miscdevice mdev;
+	struct aspeed_espi_ctrl *ctrl;
+};
+
+void aspeed_espi_flash_event(uint32_t sts, struct aspeed_espi_flash *espi_flash);
+void aspeed_espi_flash_enable(struct aspeed_espi_flash *espi_flash);
+void *aspeed_espi_flash_alloc(struct device *dev, struct aspeed_espi_ctrl *espi_ctrl);
+void aspeed_espi_flash_free(struct device *dev, struct aspeed_espi_flash *espi_flash);
+
+#endif
diff --git a/drivers/soc/aspeed/aspeed-espi-ioc.h b/drivers/soc/aspeed/aspeed-espi-ioc.h
new file mode 100644
index 000000000000..a78f1069841f
--- /dev/null
+++ b/drivers/soc/aspeed/aspeed-espi-ioc.h
@@ -0,0 +1,195 @@
+/* SPDX-License-Identifier: GPL-2.0+ */
+/*
+ * Copyright 2021 Aspeed Technology Inc.
+ */
+#ifndef _ASPEED_ESPI_IOC_H
+#define _ASPEED_ESPI_IOC_H
+
+#include <linux/ioctl.h>
+#include <linux/types.h>
+
+/*
+ * eSPI cycle type encoding
+ *
+ * Section 5.1 Cycle Types and Packet Format,
+ * Intel eSPI Interface Base Specification, Rev 1.0, Jan. 2016.
+ */
+#define ESPI_PERIF_MEMRD32		0x00
+#define ESPI_PERIF_MEMRD64		0x02
+#define ESPI_PERIF_MEMWR32		0x01
+#define ESPI_PERIF_MEMWR64		0x03
+#define ESPI_PERIF_MSG			0x10
+#define ESPI_PERIF_MSG_D		0x11
+#define ESPI_PERIF_SUC_CMPLT		0x06
+#define ESPI_PERIF_SUC_CMPLT_D_MIDDLE	0x09
+#define ESPI_PERIF_SUC_CMPLT_D_FIRST	0x0b
+#define ESPI_PERIF_SUC_CMPLT_D_LAST	0x0d
+#define ESPI_PERIF_SUC_CMPLT_D_ONLY	0x0f
+#define ESPI_PERIF_UNSUC_CMPLT		0x0c
+#define ESPI_OOB_MSG			0x21
+#define ESPI_FLASH_READ			0x00
+#define ESPI_FLASH_WRITE		0x01
+#define ESPI_FLASH_ERASE		0x02
+#define ESPI_FLASH_SUC_CMPLT		0x06
+#define ESPI_FLASH_SUC_CMPLT_D_MIDDLE	0x09
+#define ESPI_FLASH_SUC_CMPLT_D_FIRST	0x0b
+#define ESPI_FLASH_SUC_CMPLT_D_LAST	0x0d
+#define ESPI_FLASH_SUC_CMPLT_D_ONLY	0x0f
+#define ESPI_FLASH_UNSUC_CMPLT		0x0c
+
+/*
+ * eSPI packet format structure
+ *
+ * Section 5.1 Cycle Types and Packet Format,
+ * Intel eSPI Interface Base Specification, Rev 1.0, Jan. 2016.
+ */
+struct espi_comm_hdr {
+	uint8_t cyc;
+	uint8_t len_h : 4;
+	uint8_t tag : 4;
+	uint8_t len_l;
+};
+
+struct espi_perif_mem32 {
+	uint8_t cyc;
+	uint8_t len_h : 4;
+	uint8_t tag : 4;
+	uint8_t len_l;
+	uint32_t addr_be;
+	uint8_t data[];
+} __packed;
+
+struct espi_perif_mem64 {
+	uint8_t cyc;
+	uint8_t len_h : 4;
+	uint8_t tag : 4;
+	uint8_t len_l;
+	uint32_t addr_be;
+	uint8_t data[];
+} __packed;
+
+struct espi_perif_msg {
+	uint8_t cyc;
+	uint8_t len_h : 4;
+	uint8_t tag : 4;
+	uint8_t len_l;
+	uint8_t msg_code;
+	uint8_t msg_byte[4];
+	uint8_t data[];
+} __packed;
+
+struct espi_perif_cmplt {
+	uint8_t cyc;
+	uint8_t len_h : 4;
+	uint8_t tag : 4;
+	uint8_t len_l;
+	uint8_t data[];
+} __packed;
+
+struct espi_oob_msg {
+	uint8_t cyc;
+	uint8_t len_h : 4;
+	uint8_t tag : 4;
+	uint8_t len_l;
+	uint8_t data[];
+};
+
+struct espi_flash_rwe {
+	uint8_t cyc;
+	uint8_t len_h : 4;
+	uint8_t tag : 4;
+	uint8_t len_l;
+	uint32_t addr_be;
+	uint8_t data[];
+} __packed;
+
+struct espi_flash_cmplt {
+	uint8_t cyc;
+	uint8_t len_h : 4;
+	uint8_t tag : 4;
+	uint8_t len_l;
+	uint8_t data[];
+} __packed;
+
+struct aspeed_espi_ioc {
+	uint32_t pkt_len;
+	uint8_t *pkt;
+};
+
+/*
+ * we choose the longest header and the max payload size
+ * based on the Intel specification to define the maximum
+ * eSPI packet length
+ */
+#define ESPI_PLD_LEN_MIN	(1UL << 6)
+#define ESPI_PLD_LEN_MAX	(1UL << 12)
+#define ESPI_PKT_LEN_MAX	(sizeof(struct espi_perif_msg) + ESPI_PLD_LEN_MAX)
+
+#define __ASPEED_ESPI_IOCTL_MAGIC	0xb8
+
+/*
+ * The IOCTL-based interface works in the eSPI packet in/out paradigm.
+ *
+ * Only the virtual wire IOCTL is a special case which does not send
+ * or receive an eSPI packet. However, to keep a more consisten use from
+ * userspace, we make all of the four channel drivers serve through the
+ * IOCTL interface.
+ *
+ * For the eSPI packet format, refer to
+ *   Section 5.1 Cycle Types and Packet Format,
+ *   Intel eSPI Interface Base Specification, Rev 1.0, Jan. 2016.
+ *
+ * For the example user apps using these IOCTL, refer to
+ *   https://github.com/AspeedTech-BMC/aspeed_app/tree/master/espi_test
+ */
+
+/*
+ * Peripheral Channel (CH0)
+ *  - ASPEED_ESPI_PERIF_PC_GET_RX
+ *      Receive an eSPI Posted/Completion packet
+ *  - ASPEED_ESPI_PERIF_PC_PUT_TX
+ *      Transmit an eSPI Posted/Completion packet
+ *  - ASPEED_ESPI_PERIF_NP_PUT_TX
+ *      Transmit an eSPI Non-Posted packet
+ */
+#define ASPEED_ESPI_PERIF_PC_GET_RX	_IOR(__ASPEED_ESPI_IOCTL_MAGIC, \
+					     0x00, struct aspeed_espi_ioc)
+#define ASPEED_ESPI_PERIF_PC_PUT_TX	_IOW(__ASPEED_ESPI_IOCTL_MAGIC, \
+					     0x01, struct aspeed_espi_ioc)
+#define ASPEED_ESPI_PERIF_NP_PUT_TX	_IOW(__ASPEED_ESPI_IOCTL_MAGIC, \
+					     0x02, struct aspeed_espi_ioc)
+/*
+ * Virtual Wire Channel (CH1)
+ *  - ASPEED_ESPI_VW_GET_GPIO_VAL
+ *      Read the input value of GPIO over the VW channel
+ *  - ASPEED_ESPI_VW_PUT_GPIO_VAL
+ *      Write the output value of GPIO over the VW channel
+ */
+#define ASPEED_ESPI_VW_GET_GPIO_VAL	_IOR(__ASPEED_ESPI_IOCTL_MAGIC, \
+					     0x10, uint8_t)
+#define ASPEED_ESPI_VW_PUT_GPIO_VAL	_IOW(__ASPEED_ESPI_IOCTL_MAGIC, \
+					     0x11, uint8_t)
+/*
+ * Out-of-band Channel (CH2)
+ *  - ASPEED_ESPI_OOB_GET_RX
+ *      Receive an eSPI OOB packet
+ *  - ASPEED_ESPI_OOB_PUT_TX
+ *      Transmit an eSPI OOB packet
+ */
+#define ASPEED_ESPI_OOB_GET_RX		_IOR(__ASPEED_ESPI_IOCTL_MAGIC, \
+					     0x20, struct aspeed_espi_ioc)
+#define ASPEED_ESPI_OOB_PUT_TX		_IOW(__ASPEED_ESPI_IOCTL_MAGIC, \
+					     0x21, struct aspeed_espi_ioc)
+/*
+ * Flash Channel (CH3)
+ *  - ASPEED_ESPI_FLASH_GET_RX
+ *      Receive an eSPI flash packet
+ *  - ASPEED_ESPI_FLASH_PUT_TX
+ *      Transmit an eSPI flash packet
+ */
+#define ASPEED_ESPI_FLASH_GET_RX	_IOR(__ASPEED_ESPI_IOCTL_MAGIC, \
+					     0x30, struct aspeed_espi_ioc)
+#define ASPEED_ESPI_FLASH_PUT_TX	_IOW(__ASPEED_ESPI_IOCTL_MAGIC, \
+					     0x31, struct aspeed_espi_ioc)
+
+#endif
diff --git a/drivers/soc/aspeed/aspeed-espi-mmbi.c b/drivers/soc/aspeed/aspeed-espi-mmbi.c
new file mode 100644
index 000000000000..81ac46b8a9af
--- /dev/null
+++ b/drivers/soc/aspeed/aspeed-espi-mmbi.c
@@ -0,0 +1,343 @@
+// SPDX-License-Identifier: GPL-2.0-or-later
+/*
+ * Copyright 2021 Aspeed Technology Inc.
+ */
+#include <linux/io.h>
+#include <linux/irq.h>
+#include <linux/clk.h>
+#include <linux/module.h>
+#include <linux/of_device.h>
+#include <linux/interrupt.h>
+#include <linux/platform_device.h>
+#include <linux/miscdevice.h>
+#include <linux/dma-mapping.h>
+#include <linux/mfd/syscon.h>
+#include <linux/regmap.h>
+#include <linux/uaccess.h>
+#include <linux/vmalloc.h>
+#include <linux/poll.h>
+
+#include "aspeed-espi-ctrl.h"
+#include "aspeed-espi-perif.h"
+
+#define DEVICE_NAME "aspeed-espi-mmbi"
+
+#define MMBI_CTRL		0x800
+#define MMBI_CTRL_INST_SZ_MASK		GENMASK(10, 8)
+#define MMBI_CTRL_INST_SZ_SHIFT		8
+#define MMBI_CTRL_TOTAL_SZ_MASK		GENMASK(6, 4)
+#define MMBI_CTRL_TOTAL_SZ_SHIFT	4
+#define MMBI_CTRL_EN			BIT(0)
+#define MMBI_INT_STS		0x808
+#define MMBI_INT_EN		0x80c
+#define MMBI_HOST_RWP(x)	(0x810 + (x << 3))
+
+#define MMBI_INST_NUM	8
+
+enum aspeed_espi_mmbi_inst_size {
+	MMBI_INST_SIZE_8KB = 0x0,
+	MMBI_INST_SIZE_16KB,
+	MMBI_INST_SIZE_32KB,
+	MMBI_INST_SIZE_64KB,
+	MMBI_INST_SIZE_128KB,
+	MMBI_INST_SIZE_256KB,
+	MMBI_INST_SIZE_512KB,
+	MMBI_INST_SIZE_1024KB,
+	MMBI_INST_SIZE_TYPES,
+};
+
+struct aspeed_espi_mmbi_instance {
+	uint32_t idx;
+	dma_addr_t b2h_addr;
+	dma_addr_t h2b_addr;
+	struct miscdevice mdev_b2h;
+	struct miscdevice mdev_h2b;
+	bool host_rwp_updated;
+	wait_queue_head_t wq;
+	struct aspeed_espi_mmbi *mmbi;
+};
+
+struct aspeed_espi_mmbi {
+	struct device *dev;
+	struct regmap *map;
+	int irq;
+
+	uint32_t inst_sz;
+
+	void *virt;
+	dma_addr_t addr;
+	uint32_t src_addr;
+
+	struct aspeed_espi_mmbi_instance inst[MMBI_INST_NUM];
+	struct aspeed_espi_ctrl *espi_ctrl;
+};
+
+
+static int aspeed_espi_mmbi_b2h_mmap(struct file *fp, struct vm_area_struct *vma)
+{
+	struct aspeed_espi_mmbi_instance *mmbi_inst = container_of(fp->private_data,
+								   struct aspeed_espi_mmbi_instance, mdev_b2h);
+	struct aspeed_espi_mmbi *espi_mmbi = mmbi_inst->mmbi;
+	unsigned long vm_size = vma->vm_end - vma->vm_start;
+	pgprot_t prot = vma->vm_page_prot;
+
+	if (((vma->vm_pgoff << PAGE_SHIFT) + vm_size) > (0x1000 << espi_mmbi->inst_sz))
+		return -EINVAL;
+
+	prot = pgprot_noncached(prot);
+
+	if (remap_pfn_range(vma, vma->vm_start,
+			    (mmbi_inst->b2h_addr >> PAGE_SHIFT) + vma->vm_pgoff,
+			    vm_size, prot))
+		return -EAGAIN;
+
+	return 0;
+}
+
+static int aspeed_espi_mmbi_h2b_mmap(struct file *fp, struct vm_area_struct *vma)
+{
+	struct aspeed_espi_mmbi_instance *mmbi_inst = container_of(fp->private_data,
+								   struct aspeed_espi_mmbi_instance, mdev_h2b);
+	struct aspeed_espi_mmbi *espi_mmbi = mmbi_inst->mmbi;
+	unsigned long vm_size = vma->vm_end - vma->vm_start;
+	pgprot_t prot = vma->vm_page_prot;
+
+	if (((vma->vm_pgoff << PAGE_SHIFT) + vm_size) > (0x1000 << espi_mmbi->inst_sz))
+		return -EINVAL;
+
+	prot = pgprot_noncached(prot);
+
+	if (remap_pfn_range(vma, vma->vm_start,
+			    (mmbi_inst->h2b_addr >> PAGE_SHIFT) + vma->vm_pgoff,
+			    vm_size, prot))
+		return -EAGAIN;
+
+	return 0;
+}
+
+static __poll_t aspeed_espi_mmbi_h2b_poll(struct file *fp, struct poll_table_struct *pt)
+{
+	struct aspeed_espi_mmbi_instance *mmbi_inst = container_of(fp->private_data,
+			struct aspeed_espi_mmbi_instance, mdev_h2b);
+
+	poll_wait(fp, &mmbi_inst->wq, pt);
+
+	if (!mmbi_inst->host_rwp_updated)
+		return 0;
+
+	mmbi_inst->host_rwp_updated = false;
+
+	return EPOLLIN;
+}
+
+static irqreturn_t aspeed_espi_mmbi_isr(int irq, void *arg)
+{
+	int i;
+	uint32_t sts, tmp;
+	struct aspeed_espi_mmbi *espi_mmbi = (struct aspeed_espi_mmbi *)arg;
+
+	regmap_read(espi_mmbi->map, MMBI_INT_STS, &sts);
+
+	tmp = sts;
+	for (i = 0; i < MMBI_INST_NUM; ++i, tmp >>= 2) {
+		if (!(tmp & 0x3))
+		    continue;
+
+		regmap_read(espi_mmbi->map, MMBI_HOST_RWP(i),
+			    espi_mmbi->virt + (0x1000 << espi_mmbi->inst_sz) * (i + MMBI_INST_NUM));
+		regmap_read(espi_mmbi->map, MMBI_HOST_RWP(i) + 4,
+			    espi_mmbi->virt + (0x1000 << espi_mmbi->inst_sz) * (i + MMBI_INST_NUM) + 4);
+
+		espi_mmbi->inst[i].host_rwp_updated = true;
+		wake_up_interruptible(&espi_mmbi->inst[i].wq);
+	}
+
+	regmap_write(espi_mmbi->map, MMBI_INT_STS, sts);
+
+	return IRQ_HANDLED;
+}
+
+static const struct file_operations aspeed_espi_mmbi_b2h_fops = {
+	.owner = THIS_MODULE,
+	.mmap = aspeed_espi_mmbi_b2h_mmap,
+};
+
+static const struct file_operations aspeed_espi_mmbi_h2b_fops = {
+	.owner = THIS_MODULE,
+	.mmap = aspeed_espi_mmbi_h2b_mmap,
+	.poll = aspeed_espi_mmbi_h2b_poll,
+};
+
+static int aspeed_espi_mmbi_enable(struct aspeed_espi_mmbi *espi_mmbi)
+{
+	int i, rc;
+	uint32_t reg;
+	struct aspeed_espi_mmbi_instance *mmbi_inst;
+
+	espi_mmbi->virt = dma_alloc_coherent(espi_mmbi->dev,
+					     (0x2000 << espi_mmbi->inst_sz) * MMBI_INST_NUM,
+					     &espi_mmbi->addr, GFP_KERNEL);
+	if (!espi_mmbi->virt)
+		return -ENOMEM;
+
+	for (i = 0; i < MMBI_INST_NUM; ++i) {
+		mmbi_inst = &espi_mmbi->inst[i];
+		mmbi_inst->idx = i;
+		mmbi_inst->b2h_addr = espi_mmbi->addr +
+				      ((0x1000 << espi_mmbi->inst_sz) * i);
+		mmbi_inst->h2b_addr = espi_mmbi->addr +
+				      ((0x1000 << espi_mmbi->inst_sz) * (i + MMBI_INST_NUM));
+
+		mmbi_inst->mdev_b2h.parent = espi_mmbi->dev;
+		mmbi_inst->mdev_b2h.minor = MISC_DYNAMIC_MINOR;
+		mmbi_inst->mdev_b2h.name = devm_kasprintf(espi_mmbi->dev, GFP_KERNEL, "%s-b2h%d", DEVICE_NAME, i);
+		mmbi_inst->mdev_b2h.fops = &aspeed_espi_mmbi_b2h_fops;
+		rc = misc_register(&mmbi_inst->mdev_b2h);
+		if (rc) {
+			dev_err(espi_mmbi->dev, "cannot register device %s\n", mmbi_inst->mdev_b2h.name);
+			return rc;
+		}
+
+		mmbi_inst->mdev_h2b.parent = espi_mmbi->dev;
+		mmbi_inst->mdev_h2b.minor = MISC_DYNAMIC_MINOR;
+		mmbi_inst->mdev_h2b.name = devm_kasprintf(espi_mmbi->dev, GFP_KERNEL, "%s-h2b%d", DEVICE_NAME, i);
+		mmbi_inst->mdev_h2b.fops = &aspeed_espi_mmbi_h2b_fops;
+		rc = misc_register(&mmbi_inst->mdev_h2b);
+		if (rc) {
+			dev_err(espi_mmbi->dev, "cannot register device %s\n", mmbi_inst->mdev_h2b.name);
+			return rc;
+		}
+
+		init_waitqueue_head(&mmbi_inst->wq);
+
+		mmbi_inst->host_rwp_updated = false;
+		mmbi_inst->mmbi = espi_mmbi;
+	}
+
+	rc = devm_request_irq(espi_mmbi->dev, espi_mmbi->irq,
+			      aspeed_espi_mmbi_isr,
+			      0, DEVICE_NAME, espi_mmbi);
+	if (rc) {
+		dev_err(espi_mmbi->dev, "failed to request IRQ\n");
+		return rc;
+	}
+
+	regmap_write(espi_mmbi->map, MMBI_INT_EN, 0);
+	regmap_write(espi_mmbi->map, MMBI_INT_STS, 0xffff);
+
+	regmap_update_bits(espi_mmbi->map, ESPI_CTRL2,
+			ESPI_CTRL2_MEMCYC_RD_DIS | ESPI_CTRL2_MEMCYC_WR_DIS, 0);
+	regmap_write(espi_mmbi->map, ESPI_PERIF_PC_RX_MASK,
+			~(((0x2000 << espi_mmbi->inst_sz) * MMBI_INST_NUM) - 1));
+	regmap_write(espi_mmbi->map, ESPI_PERIF_PC_RX_SADDR, espi_mmbi->src_addr);
+	regmap_write(espi_mmbi->map, ESPI_PERIF_PC_RX_TADDR, espi_mmbi->addr);
+
+	reg = ((espi_mmbi->inst_sz << MMBI_CTRL_INST_SZ_SHIFT) & MMBI_CTRL_INST_SZ_MASK) |
+	      ((espi_mmbi->inst_sz << MMBI_CTRL_TOTAL_SZ_SHIFT) & MMBI_CTRL_TOTAL_SZ_MASK) |
+	      MMBI_CTRL_EN;
+
+	regmap_write(espi_mmbi->map, MMBI_CTRL, reg);
+	regmap_write(espi_mmbi->map, MMBI_INT_EN, 0xffff);
+
+	return 0;
+}
+
+static int aspeed_espi_mmbi_disable(struct aspeed_espi_mmbi *espi_mmbi)
+{
+	int i;
+
+	for (i = 0; i < MMBI_INST_NUM; ++i) {
+		misc_deregister(&espi_mmbi->inst[i].mdev_b2h);
+		misc_deregister(&espi_mmbi->inst[i].mdev_h2b);
+	}
+
+	if (espi_mmbi->virt)
+		dma_free_coherent(espi_mmbi->dev,
+				  (0x2000 << espi_mmbi->inst_sz) * MMBI_INST_NUM,
+				  espi_mmbi->virt, espi_mmbi->addr);
+
+	return 0;
+}
+
+static int aspeed_espi_mmbi_probe(struct platform_device *pdev)
+{
+	int rc;
+	struct device *dev = &pdev->dev;
+	struct aspeed_espi_mmbi *espi_mmbi;
+	uint32_t reg;
+
+	espi_mmbi = devm_kzalloc(dev, sizeof(*espi_mmbi), GFP_KERNEL);
+	if (!espi_mmbi)
+		return -ENOMEM;
+
+	espi_mmbi->map = syscon_node_to_regmap(dev->parent->of_node);
+	if (IS_ERR(espi_mmbi->map)) {
+		dev_err(dev, "cannot get remap\n");
+		return -ENODEV;
+	}
+
+	regmap_read(espi_mmbi->map, ESPI_CTRL, &reg);
+	if (!(reg & ESPI_CTRL_PERIF_SW_RDY))
+		return -EPROBE_DEFER;
+
+	espi_mmbi->irq = platform_get_irq(pdev, 0);
+	if (espi_mmbi->irq < 0)
+		return espi_mmbi->irq;
+
+	rc = of_property_read_u32(dev->of_node, "host-src-addr", &espi_mmbi->src_addr);
+	if (rc) {
+		dev_err(dev, "cannot get Host source address\n");
+		return -ENODEV;
+	}
+
+	rc = of_property_read_u32(dev->of_node, "instance-size", &espi_mmbi->inst_sz);
+	if (rc) {
+		dev_err(dev, "cannot get instance size\n");
+		return -ENODEV;
+	}
+
+	if (espi_mmbi->inst_sz >= MMBI_INST_SIZE_TYPES) {
+		dev_err(dev, "invalid MMBI instance size\n");
+		return -EINVAL;
+	}
+
+	espi_mmbi->dev = dev;
+
+	rc = aspeed_espi_mmbi_enable(espi_mmbi);
+	if (rc)
+	    return rc;
+
+	dev_info(dev, "module loaded\n");
+
+	return 0;
+}
+
+static int aspeed_espi_mmbi_remove(struct platform_device *pdev)
+{
+	struct device *dev = &pdev->dev;
+	struct aspeed_espi_mmbi *espi_mmbi = dev_get_drvdata(dev);
+
+	aspeed_espi_mmbi_disable(espi_mmbi);
+
+	return 0;
+}
+
+static const struct of_device_id aspeed_espi_mmbi_of_matches[] = {
+	{ .compatible = "aspeed,ast2600-espi-mmbi" },
+	{ },
+};
+
+static struct platform_driver aspeed_espi_mmbi_driver = {
+	.driver = {
+		.name = DEVICE_NAME,
+		.of_match_table = aspeed_espi_mmbi_of_matches,
+	},
+	.probe = aspeed_espi_mmbi_probe,
+	.remove = aspeed_espi_mmbi_remove,
+};
+
+module_platform_driver(aspeed_espi_mmbi_driver);
+
+MODULE_AUTHOR("Chia-Wei Wang <chiawei_wang@aspeedtech.com>");
+MODULE_DESCRIPTION("Control of Aspeed eSPI MMBI Device");
+MODULE_LICENSE("GPL v2");
diff --git a/drivers/soc/aspeed/aspeed-espi-oob.c b/drivers/soc/aspeed/aspeed-espi-oob.c
new file mode 100644
index 000000000000..f9ba2870b685
--- /dev/null
+++ b/drivers/soc/aspeed/aspeed-espi-oob.c
@@ -0,0 +1,558 @@
+// SPDX-License-Identifier: GPL-2.0+
+/*
+ * Copyright 2021 Aspeed Technology Inc.
+ */
+#include <linux/fs.h>
+#include <linux/of_device.h>
+#include <linux/miscdevice.h>
+#include <linux/mfd/syscon.h>
+#include <linux/regmap.h>
+#include <linux/uaccess.h>
+#include <linux/vmalloc.h>
+#include <linux/miscdevice.h>
+#include <linux/dma-mapping.h>
+
+#include "aspeed-espi-ioc.h"
+#include "aspeed-espi-ctrl.h"
+#include "aspeed-espi-oob.h"
+
+#define OOB_MDEV_NAME	"aspeed-espi-oob"
+
+/* DMA descriptor is supported since AST2600 */
+#define OOB_DMA_DESC_MAX_NUM	1024
+#define OOB_DMA_TX_DESC_CUST	0x04
+
+/* descriptor-based RX DMA handling */
+static long aspeed_espi_oob_dma_desc_get_rx(struct file *fp,
+					    struct aspeed_espi_ioc *ioc,
+					    struct aspeed_espi_oob *espi_oob)
+{
+	int rc = 0;
+	unsigned long flags;
+	uint32_t reg;
+	uint32_t wptr, sptr;
+	uint8_t *pkt;
+	uint32_t pkt_len;
+	struct espi_comm_hdr *hdr;
+	struct oob_rx_dma_desc *d;
+	struct aspeed_espi_ctrl *espi_ctrl = espi_oob->ctrl;
+
+	regmap_read(espi_ctrl->map, ESPI_OOB_RX_DMA_WS_PTR, &reg);
+	wptr = (reg & ESPI_OOB_RX_DMA_WS_PTR_WP_MASK) >> ESPI_OOB_RX_DMA_WS_PTR_WP_SHIFT;
+	sptr = (reg & ESPI_OOB_RX_DMA_WS_PTR_SP_MASK) >> ESPI_OOB_RX_DMA_WS_PTR_SP_SHIFT;
+
+	d = &espi_oob->dma.rx_desc[sptr];
+
+	if (!d->dirty)
+		return -EFAULT;
+
+	pkt_len = ((d->len) ? d->len : 0x1000) + sizeof(struct espi_comm_hdr);
+
+	if (ioc->pkt_len < pkt_len)
+		return -EINVAL;
+
+	pkt = vmalloc(pkt_len);
+	if (!pkt)
+		return -ENOMEM;
+
+	hdr = (struct espi_comm_hdr *)pkt;
+	hdr->cyc = d->cyc;
+	hdr->tag = d->tag;
+	hdr->len_h = d->len >> 8;
+	hdr->len_l = d->len & 0xff;
+	memcpy(hdr + 1, espi_oob->dma.rx_virt + (PAGE_SIZE * sptr), pkt_len - sizeof(*hdr));
+
+	if (copy_to_user((void __user *)ioc->pkt, pkt, pkt_len)) {
+		rc = -EFAULT;
+		goto free_n_out;
+	}
+
+	spin_lock_irqsave(&espi_oob->lock, flags);
+
+	/* make current descriptor available again */
+	d->dirty = 0;
+
+	sptr = (sptr + 1) % espi_oob->dma.rx_desc_num;
+	wptr = (wptr + 1) % espi_oob->dma.rx_desc_num;
+
+	reg = ((wptr << ESPI_OOB_RX_DMA_WS_PTR_WP_SHIFT) & ESPI_OOB_RX_DMA_WS_PTR_WP_MASK)
+		| ((sptr << ESPI_OOB_RX_DMA_WS_PTR_SP_SHIFT) & ESPI_OOB_RX_DMA_WS_PTR_SP_MASK)
+		| ESPI_OOB_RX_DMA_WS_PTR_RECV_EN;
+	regmap_write(espi_ctrl->map, ESPI_OOB_RX_DMA_WS_PTR, reg);
+
+	/* set ready flag base on the next RX descriptor */
+	espi_oob->rx_ready = espi_oob->dma.rx_desc[sptr].dirty;
+
+	spin_unlock_irqrestore(&espi_oob->lock, flags);
+
+free_n_out:
+	vfree(pkt);
+
+	return rc;
+}
+
+static long aspeed_espi_oob_get_rx(struct file *fp,
+				   struct aspeed_espi_ioc *ioc,
+				   struct aspeed_espi_oob *espi_oob)
+{
+	int i, rc = 0;
+	unsigned long flags;
+	uint32_t reg;
+	uint32_t cyc, tag, len;
+	uint8_t *pkt;
+	uint32_t pkt_len;
+	struct espi_comm_hdr *hdr;
+	struct aspeed_espi_ctrl *espi_ctrl = espi_oob->ctrl;
+
+	if (fp->f_flags & O_NONBLOCK) {
+		if (!mutex_trylock(&espi_oob->get_rx_mtx))
+			return -EAGAIN;
+
+		if (!espi_oob->rx_ready) {
+			rc = -ENODATA;
+			goto unlock_mtx_n_out;
+		}
+	} else {
+		mutex_lock(&espi_oob->get_rx_mtx);
+
+		if (!espi_oob->rx_ready) {
+			rc = wait_event_interruptible(espi_oob->wq,
+						      espi_oob->rx_ready);
+			if (rc == -ERESTARTSYS) {
+				rc = -EINTR;
+				goto unlock_mtx_n_out;
+			}
+		}
+	}
+
+	if (espi_oob->dma_mode && espi_ctrl->model->version != ESPI_AST2500) {
+		rc = aspeed_espi_oob_dma_desc_get_rx(fp, ioc, espi_oob);
+		goto unlock_mtx_n_out;
+	}
+
+	/* common header (i.e. cycle type, tag, and length) is taken by HW */
+	regmap_read(espi_ctrl->map, ESPI_OOB_RX_CTRL, &reg);
+	cyc = (reg & ESPI_OOB_RX_CTRL_CYC_MASK) >> ESPI_OOB_RX_CTRL_CYC_SHIFT;
+	tag = (reg & ESPI_OOB_RX_CTRL_TAG_MASK) >> ESPI_OOB_RX_CTRL_TAG_SHIFT;
+	len = (reg & ESPI_OOB_RX_CTRL_LEN_MASK) >> ESPI_OOB_RX_CTRL_LEN_SHIFT;
+
+	/*
+	 * calculate the length of the rest part of the
+	 * eSPI packet to be read from HW and copied to
+	 * user space.
+	 */
+	pkt_len = ((len) ? len : ESPI_PLD_LEN_MAX) + sizeof(struct espi_comm_hdr);
+
+	if (ioc->pkt_len < pkt_len) {
+		rc = -EINVAL;
+		goto unlock_mtx_n_out;
+	}
+
+	pkt = vmalloc(pkt_len);
+	if (!pkt) {
+		rc = -ENOMEM;
+		goto unlock_mtx_n_out;
+	}
+
+	hdr = (struct espi_comm_hdr *)pkt;
+	hdr->cyc = cyc;
+	hdr->tag = tag;
+	hdr->len_h = len >> 8;
+	hdr->len_l = len & 0xff;
+
+	if (espi_oob->dma_mode) {
+		memcpy(hdr + 1, espi_oob->dma.rx_virt,
+		       pkt_len - sizeof(*hdr));
+	} else {
+		for (i = sizeof(*hdr); i < pkt_len; ++i) {
+			regmap_read(espi_ctrl->map,
+				    ESPI_OOB_RX_PORT, &reg);
+			pkt[i] = reg & 0xff;
+		}
+	}
+
+	if (copy_to_user((void __user *)ioc->pkt, pkt, pkt_len)) {
+		rc = -EFAULT;
+		goto free_n_out;
+	}
+
+	spin_lock_irqsave(&espi_oob->lock, flags);
+
+	regmap_write_bits(espi_ctrl->map, ESPI_OOB_RX_CTRL,
+			  ESPI_OOB_RX_CTRL_PEND_SERV,
+			  ESPI_OOB_RX_CTRL_PEND_SERV);
+
+	espi_oob->rx_ready = 0;
+
+	spin_unlock_irqrestore(&espi_oob->lock, flags);
+
+free_n_out:
+	vfree(pkt);
+
+unlock_mtx_n_out:
+	mutex_unlock(&espi_oob->get_rx_mtx);
+
+	return rc;
+}
+
+/* descriptor-based TX DMA handling */
+static long aspeed_espi_oob_dma_desc_put_tx(struct file *fp,
+					    struct aspeed_espi_ioc *ioc,
+					    struct aspeed_espi_oob *espi_oob)
+{
+	int rc = 0;
+	uint32_t rptr, wptr;
+	uint8_t *pkt;
+	struct espi_comm_hdr *hdr;
+	struct oob_tx_dma_desc *d;
+	struct aspeed_espi_ctrl *espi_ctrl = espi_oob->ctrl;
+
+	pkt = vzalloc(ioc->pkt_len);
+	if (!pkt)
+		return -ENOMEM;
+
+	hdr = (struct espi_comm_hdr *)pkt;
+
+	if (copy_from_user(pkt, (void __user *)ioc->pkt, ioc->pkt_len)) {
+		rc = -EFAULT;
+		goto free_n_out;
+	}
+
+	/* kick HW to reflect the up-to-date read/write pointer */
+	regmap_write(espi_ctrl->map, ESPI_OOB_TX_DMA_RD_PTR,
+		     ESPI_OOB_TX_DMA_RD_PTR_UPDATE);
+
+	regmap_read(espi_ctrl->map, ESPI_OOB_TX_DMA_RD_PTR, &rptr);
+	regmap_read(espi_ctrl->map, ESPI_OOB_TX_DMA_WR_PTR, &wptr);
+
+	if (((wptr + 1) % espi_oob->dma.tx_desc_num) == rptr) {
+		rc = -EBUSY;
+		goto free_n_out;
+	}
+
+	d = &espi_oob->dma.tx_desc[wptr];
+	d->cyc = hdr->cyc;
+	d->tag = hdr->tag;
+	d->len = (hdr->len_h << 8) | (hdr->len_l & 0xff);
+	d->msg_type = OOB_DMA_TX_DESC_CUST;
+
+	memcpy(espi_oob->dma.tx_virt + (PAGE_SIZE * wptr), hdr + 1,
+	       ioc->pkt_len - sizeof(*hdr));
+
+	dma_wmb();
+
+	wptr = (wptr + 1) % espi_oob->dma.tx_desc_num;
+	wptr |= ESPI_OOB_TX_DMA_WR_PTR_SEND_EN;
+	regmap_write(espi_ctrl->map, ESPI_OOB_TX_DMA_WR_PTR, wptr);
+
+free_n_out:
+	vfree(pkt);
+
+	return rc;
+}
+
+static long aspeed_espi_oob_put_tx(struct file *fp,
+				   struct aspeed_espi_ioc *ioc,
+				   struct aspeed_espi_oob *espi_oob)
+{
+	int i, rc = 0;
+	uint32_t reg;
+	uint32_t cyc, tag, len;
+	uint8_t *pkt;
+	struct espi_comm_hdr *hdr;
+	struct aspeed_espi_ctrl *espi_ctrl = espi_oob->ctrl;
+
+	if (!mutex_trylock(&espi_oob->put_tx_mtx))
+		return -EAGAIN;
+
+	if (espi_oob->dma_mode && espi_ctrl->model->version != ESPI_AST2500) {
+		rc = aspeed_espi_oob_dma_desc_put_tx(fp, ioc, espi_oob);
+		goto unlock_mtx_n_out;
+	}
+
+	regmap_read(espi_ctrl->map, ESPI_OOB_TX_CTRL, &reg);
+	if (reg & ESPI_OOB_TX_CTRL_TRIGGER) {
+		rc = -EBUSY;
+		goto unlock_mtx_n_out;
+	}
+
+	if (ioc->pkt_len > ESPI_PKT_LEN_MAX) {
+		rc = -EINVAL;
+		goto unlock_mtx_n_out;
+	}
+
+	pkt = vmalloc(ioc->pkt_len);
+	if (!pkt) {
+		rc = -ENOMEM;
+		goto unlock_mtx_n_out;
+	}
+
+	hdr = (struct espi_comm_hdr *)pkt;
+
+	if (copy_from_user(pkt, (void __user *)ioc->pkt, ioc->pkt_len)) {
+		rc = -EFAULT;
+		goto free_n_out;
+	}
+
+	/*
+	 * common header (i.e. cycle type, tag, and length)
+	 * part is written to HW registers
+	 */
+	if (espi_oob->dma_mode) {
+		memcpy(espi_oob->dma.tx_virt, hdr + 1,
+		       ioc->pkt_len - sizeof(*hdr));
+		dma_wmb();
+	} else {
+		for (i = sizeof(*hdr); i < ioc->pkt_len; ++i)
+			regmap_write(espi_ctrl->map,
+				     ESPI_OOB_TX_PORT, pkt[i]);
+	}
+
+	cyc = hdr->cyc;
+	tag = hdr->tag;
+	len = (hdr->len_h << 8) | (hdr->len_l & 0xff);
+
+	reg = ((cyc << ESPI_OOB_TX_CTRL_CYC_SHIFT) & ESPI_OOB_TX_CTRL_CYC_MASK)
+		| ((tag << ESPI_OOB_TX_CTRL_TAG_SHIFT) & ESPI_OOB_TX_CTRL_TAG_MASK)
+		| ((len << ESPI_OOB_TX_CTRL_LEN_SHIFT) & ESPI_OOB_TX_CTRL_LEN_MASK)
+		| ESPI_OOB_TX_CTRL_TRIGGER;
+
+	regmap_write(espi_ctrl->map, ESPI_OOB_TX_CTRL, reg);
+
+free_n_out:
+	vfree(pkt);
+
+unlock_mtx_n_out:
+	mutex_unlock(&espi_oob->put_tx_mtx);
+
+	return rc;
+}
+
+static long aspeed_espi_oob_ioctl(struct file *fp, unsigned int cmd, unsigned long arg)
+{
+	struct aspeed_espi_ioc ioc;
+	struct aspeed_espi_oob *espi_oob = container_of(
+			fp->private_data,
+			struct aspeed_espi_oob,
+			mdev);
+
+	if (copy_from_user(&ioc, (void __user *)arg, sizeof(ioc)))
+		return -EFAULT;
+
+	if (ioc.pkt_len > ESPI_PKT_LEN_MAX)
+		return -EINVAL;
+
+	switch (cmd) {
+	case ASPEED_ESPI_OOB_GET_RX:
+		return aspeed_espi_oob_get_rx(fp, &ioc, espi_oob);
+	case ASPEED_ESPI_OOB_PUT_TX:
+		return aspeed_espi_oob_put_tx(fp, &ioc, espi_oob);
+	};
+
+	return -EINVAL;
+}
+
+void aspeed_espi_oob_event(uint32_t sts, struct aspeed_espi_oob *espi_oob)
+{
+	unsigned long flags;
+
+	if (sts & ESPI_INT_STS_OOB_RX_CMPLT) {
+		spin_lock_irqsave(&espi_oob->lock, flags);
+		espi_oob->rx_ready = 1;
+		spin_unlock_irqrestore(&espi_oob->lock, flags);
+
+		wake_up_interruptible(&espi_oob->wq);
+	}
+}
+
+void aspeed_espi_oob_enable(struct aspeed_espi_oob *espi_oob)
+{
+	int i;
+	struct aspeed_espi_oob_dma *dma = &espi_oob->dma;
+	struct aspeed_espi_ctrl *espi_ctrl = espi_oob->ctrl;
+
+	regmap_update_bits(espi_ctrl->map, ESPI_CTRL,
+			   ESPI_CTRL_OOB_SW_RDY | ESPI_CTRL_OOB_RX_SW_RST, 0);
+
+	if (espi_oob->dma_mode)
+		regmap_update_bits(espi_ctrl->map, ESPI_CTRL,
+				   ESPI_CTRL_OOB_TX_DMA_EN | ESPI_CTRL_OOB_RX_DMA_EN, 0);
+	else
+		regmap_write(espi_ctrl->map, ESPI_OOB_RX_CTRL, ESPI_OOB_RX_CTRL_PEND_SERV);
+
+	/*
+	 * cleanup OOB RX FIFO to get rid of the data
+	 * of OOB early init side-effect
+	 */
+	regmap_update_bits(espi_ctrl->map, ESPI_CTRL,
+			   ESPI_CTRL_OOB_RX_SW_RST, ESPI_CTRL_OOB_RX_SW_RST);
+
+	regmap_write(espi_ctrl->map, ESPI_OOB_RX_CTRL,
+		     ESPI_OOB_RX_CTRL_PEND_SERV);
+
+	if (espi_oob->dma_mode) {
+		regmap_update_bits(espi_ctrl->map, ESPI_CTRL,
+				   ESPI_CTRL_OOB_TX_DMA_EN | ESPI_CTRL_OOB_RX_DMA_EN,
+				   ESPI_CTRL_OOB_TX_DMA_EN | ESPI_CTRL_OOB_RX_DMA_EN);
+
+		if (espi_ctrl->model->version == ESPI_AST2500) {
+			regmap_write(espi_ctrl->map, ESPI_OOB_TX_DMA, dma->tx_addr);
+			regmap_write(espi_ctrl->map, ESPI_OOB_RX_DMA, dma->rx_addr);
+		} else {
+			for (i = 0; i < dma->tx_desc_num; ++i)
+				dma->tx_desc[i].data_addr = dma->tx_addr + (i * PAGE_SIZE);
+
+			for (i = 0; i < dma->rx_desc_num; ++i) {
+				dma->rx_desc[i].data_addr = dma->rx_addr + (i * PAGE_SIZE);
+				dma->rx_desc[i].dirty = 0;
+			}
+
+			regmap_write(espi_ctrl->map, ESPI_OOB_TX_DMA, dma->tx_desc_addr);
+			regmap_write(espi_ctrl->map, ESPI_OOB_TX_DMA_RB_SIZE, dma->tx_desc_num);
+
+			regmap_write(espi_ctrl->map, ESPI_OOB_RX_DMA, dma->rx_desc_addr);
+			regmap_write(espi_ctrl->map, ESPI_OOB_RX_DMA_RB_SIZE, dma->rx_desc_num);
+			regmap_update_bits(espi_ctrl->map, ESPI_OOB_RX_DMA_WS_PTR,
+					   ESPI_OOB_RX_DMA_WS_PTR_RECV_EN,
+					   ESPI_OOB_RX_DMA_WS_PTR_RECV_EN);
+		}
+	}
+
+	regmap_write(espi_ctrl->map, ESPI_INT_STS,
+		     ESPI_INT_STS_OOB_BITS);
+
+	regmap_update_bits(espi_ctrl->map, ESPI_INT_EN,
+			   ESPI_INT_EN_OOB_BITS,
+			   ESPI_INT_EN_OOB_BITS);
+
+	regmap_update_bits(espi_ctrl->map, ESPI_CTRL,
+			   ESPI_CTRL_OOB_SW_RDY,
+			   ESPI_CTRL_OOB_SW_RDY);
+}
+
+static const struct file_operations aspeed_espi_oob_fops = {
+	.owner = THIS_MODULE,
+	.unlocked_ioctl = aspeed_espi_oob_ioctl,
+};
+
+void *aspeed_espi_oob_alloc(struct device *dev, struct aspeed_espi_ctrl *espi_ctrl)
+{
+	int rc = 0;
+	struct aspeed_espi_oob *espi_oob;
+	struct aspeed_espi_oob_dma *dma;
+
+	espi_oob = devm_kzalloc(dev, sizeof(*espi_oob), GFP_KERNEL);
+	if (!espi_oob)
+		return ERR_PTR(-ENOMEM);
+
+	espi_oob->ctrl = espi_ctrl;
+
+	init_waitqueue_head(&espi_oob->wq);
+
+	spin_lock_init(&espi_oob->lock);
+
+	mutex_init(&espi_oob->put_tx_mtx);
+	mutex_init(&espi_oob->get_rx_mtx);
+
+	if (of_property_read_bool(dev->of_node, "oob,dma-mode"))
+		espi_oob->dma_mode = 1;
+
+	if (espi_oob->dma_mode) {
+		dma = &espi_oob->dma;
+
+		/* Descriptor based OOB DMA is supported since AST2600 */
+		if (espi_ctrl->model->version != ESPI_AST2500) {
+			of_property_read_u32(dev->of_node, "oob,dma-tx-desc-num",
+					     &dma->tx_desc_num);
+			of_property_read_u32(dev->of_node, "oob,dma-rx-desc-num",
+					     &dma->rx_desc_num);
+
+			if (!dma->tx_desc_num || !dma->rx_desc_num) {
+				dev_err(dev, "invalid zero number of DMA channels\n");
+				return ERR_PTR(-EINVAL);
+			}
+
+			if (dma->tx_desc_num >= OOB_DMA_DESC_MAX_NUM ||
+			    dma->rx_desc_num >= OOB_DMA_DESC_MAX_NUM) {
+				dev_err(dev, "too many number of DMA channels\n");
+				return ERR_PTR(-EINVAL);
+			}
+
+			dma->tx_desc = dma_alloc_coherent(dev,
+							  sizeof(*dma->tx_desc) * dma->tx_desc_num,
+							  &dma->tx_desc_addr, GFP_KERNEL);
+			if (!dma->tx_desc) {
+				dev_err(dev, "cannot allocate DMA TX descriptor\n");
+				return ERR_PTR(-ENOMEM);
+			}
+
+			dma->rx_desc = dma_alloc_coherent(dev,
+							  sizeof(*dma->rx_desc) * dma->rx_desc_num,
+							  &dma->rx_desc_addr, GFP_KERNEL);
+			if (!dma->rx_desc) {
+				dev_err(dev, "cannot allocate DMA RX descriptor\n");
+				return ERR_PTR(-ENOMEM);
+			}
+		}
+
+		/*
+		 * DMA descriptors are consumed in the circular
+		 * queue paradigm. Therefore, one dummy slot is
+		 * reserved to detect the full condition.
+		 *
+		 * For AST2500 without DMA descriptors supported,
+		 * the number of the queue slot should be 1 here.
+		 */
+		dma->tx_desc_num += 1;
+		dma->rx_desc_num += 1;
+
+		dma->tx_virt = dma_alloc_coherent(dev, PAGE_SIZE * dma->tx_desc_num,
+						  &dma->tx_addr, GFP_KERNEL);
+		if (!dma->tx_virt) {
+			dev_err(dev, "cannot allocate DMA TX buffer\n");
+			return ERR_PTR(-ENOMEM);
+		}
+
+		dma->rx_virt = dma_alloc_coherent(dev, PAGE_SIZE * dma->rx_desc_num,
+						  &dma->rx_addr, GFP_KERNEL);
+		if (!dma->rx_virt) {
+			dev_err(dev, "cannot allocate DMA RX buffer\n");
+			return ERR_PTR(-ENOMEM);
+		}
+	}
+
+	espi_oob->mdev.parent = dev;
+	espi_oob->mdev.minor = MISC_DYNAMIC_MINOR;
+	espi_oob->mdev.name = devm_kasprintf(dev, GFP_KERNEL, "%s", OOB_MDEV_NAME);
+	espi_oob->mdev.fops = &aspeed_espi_oob_fops;
+	rc = misc_register(&espi_oob->mdev);
+	if (rc) {
+		dev_err(dev, "cannot register device\n");
+		return ERR_PTR(rc);
+	}
+
+	aspeed_espi_oob_enable(espi_oob);
+
+	return espi_oob;
+}
+
+void aspeed_espi_oob_free(struct device *dev, struct aspeed_espi_oob *espi_oob)
+{
+	struct aspeed_espi_oob_dma *dma = &espi_oob->dma;
+
+	if (espi_oob->dma_mode) {
+		dma_free_coherent(dev, sizeof(*dma->tx_desc) * dma->tx_desc_num,
+				  dma->tx_desc, dma->tx_desc_addr);
+		dma_free_coherent(dev, sizeof(*dma->rx_desc) * dma->rx_desc_num,
+				  dma->rx_desc, dma->rx_desc_addr);
+		dma_free_coherent(dev, PAGE_SIZE * dma->tx_desc_num,
+				  dma->tx_virt, dma->tx_addr);
+		dma_free_coherent(dev, PAGE_SIZE * dma->rx_desc_num,
+				  dma->rx_virt, dma->rx_addr);
+	}
+
+	mutex_destroy(&espi_oob->put_tx_mtx);
+	mutex_destroy(&espi_oob->get_rx_mtx);
+
+	misc_deregister(&espi_oob->mdev);
+}
diff --git a/drivers/soc/aspeed/aspeed-espi-oob.h b/drivers/soc/aspeed/aspeed-espi-oob.h
new file mode 100644
index 000000000000..03d74ef39e8b
--- /dev/null
+++ b/drivers/soc/aspeed/aspeed-espi-oob.h
@@ -0,0 +1,70 @@
+/* SPDX-License-Identifier: GPL-2.0+ */
+/*
+ * Copyright 2021 Aspeed Technology Inc.
+ */
+#ifndef _ASPEED_ESPI_OOB_H_
+#define _ASPEED_ESPI_OOB_H_
+
+struct oob_tx_dma_desc {
+	uint32_t data_addr;
+	uint8_t cyc;
+	uint16_t tag : 4;
+	uint16_t len : 12;
+	uint8_t msg_type : 3;
+	uint8_t raz0 : 1;
+	uint8_t pec : 1;
+	uint8_t int_en : 1;
+	uint8_t pause : 1;
+	uint8_t raz1 : 1;
+	uint32_t raz2;
+	uint32_t raz3;
+} __packed;
+
+struct oob_rx_dma_desc {
+	uint32_t data_addr;
+	uint8_t cyc;
+	uint16_t tag : 4;
+	uint16_t len : 12;
+	uint8_t raz : 7;
+	uint8_t dirty : 1;
+} __packed;
+
+struct aspeed_espi_oob_dma {
+	uint32_t tx_desc_num;
+	uint32_t rx_desc_num;
+
+	struct oob_tx_dma_desc *tx_desc;
+	dma_addr_t tx_desc_addr;
+
+	struct oob_rx_dma_desc *rx_desc;
+	dma_addr_t rx_desc_addr;
+
+	void *tx_virt;
+	dma_addr_t tx_addr;
+
+	void *rx_virt;
+	dma_addr_t rx_addr;
+};
+
+struct aspeed_espi_oob {
+	uint32_t dma_mode;
+	struct aspeed_espi_oob_dma dma;
+
+	uint32_t rx_ready;
+	wait_queue_head_t wq;
+
+	struct mutex get_rx_mtx;
+	struct mutex put_tx_mtx;
+
+	spinlock_t lock;
+
+	struct miscdevice mdev;
+	struct aspeed_espi_ctrl *ctrl;
+};
+
+void aspeed_espi_oob_event(uint32_t sts, struct aspeed_espi_oob *espi_oob);
+void aspeed_espi_oob_enable(struct aspeed_espi_oob *espi_oob);
+void *aspeed_espi_oob_alloc(struct device *dev, struct aspeed_espi_ctrl *espi_ctrl);
+void aspeed_espi_oob_free(struct device *dev, struct aspeed_espi_oob *espi_oob);
+
+#endif
diff --git a/drivers/soc/aspeed/aspeed-espi-perif.c b/drivers/soc/aspeed/aspeed-espi-perif.c
new file mode 100644
index 000000000000..0f9d6df83199
--- /dev/null
+++ b/drivers/soc/aspeed/aspeed-espi-perif.c
@@ -0,0 +1,520 @@
+// SPDX-License-Identifier: GPL-2.0+
+/*
+ * Copyright 2021 ASPEED Technology Inc.
+ */
+#include <linux/fs.h>
+#include <linux/of_device.h>
+#include <linux/miscdevice.h>
+#include <linux/mfd/syscon.h>
+#include <linux/regmap.h>
+#include <linux/uaccess.h>
+#include <linux/vmalloc.h>
+#include <linux/miscdevice.h>
+#include <linux/dma-mapping.h>
+
+#include "aspeed-espi-ioc.h"
+#include "aspeed-espi-ctrl.h"
+#include "aspeed-espi-perif.h"
+
+#define PERIF_MDEV_NAME		"aspeed-espi-peripheral"
+#define PERIF_MEMCYC_UNLOCK_KEY	0xfedc756e
+#define PERIF_MEMCYC_SIZE_MIN	0x10000
+
+static long aspeed_espi_perif_pc_get_rx(struct file *fp,
+					struct aspeed_espi_ioc *ioc,
+					struct aspeed_espi_perif *espi_perif)
+{
+	int i, rc = 0;
+	uint32_t reg;
+	uint32_t cyc, tag, len;
+	uint8_t *pkt;
+	uint32_t pkt_len;
+	struct espi_comm_hdr *hdr;
+	unsigned long flags;
+	struct aspeed_espi_ctrl *espi_ctrl = espi_perif->ctrl;
+
+	if (fp->f_flags & O_NONBLOCK) {
+		if (!mutex_trylock(&espi_perif->pc_rx_mtx))
+			return -EAGAIN;
+
+		if (!espi_perif->rx_ready) {
+			rc = -ENODATA;
+			goto unlock_mtx_n_out;
+		}
+	} else {
+		mutex_lock(&espi_perif->pc_rx_mtx);
+
+		if (!espi_perif->rx_ready) {
+			rc = wait_event_interruptible(espi_perif->wq,
+						      espi_perif->rx_ready);
+			if (rc == -ERESTARTSYS) {
+				rc = -EINTR;
+				goto unlock_mtx_n_out;
+			}
+		}
+	}
+
+	/* common header (i.e. cycle type, tag, and length) is taken by HW */
+	regmap_read(espi_ctrl->map, ESPI_PERIF_PC_RX_CTRL, &reg);
+	cyc = (reg & ESPI_PERIF_PC_RX_CTRL_CYC_MASK) >> ESPI_PERIF_PC_RX_CTRL_CYC_SHIFT;
+	tag = (reg & ESPI_PERIF_PC_RX_CTRL_TAG_MASK) >> ESPI_PERIF_PC_RX_CTRL_TAG_SHIFT;
+	len = (reg & ESPI_PERIF_PC_RX_CTRL_LEN_MASK) >> ESPI_PERIF_PC_RX_CTRL_LEN_SHIFT;
+
+	/*
+	 * calculate the length of the rest part of the
+	 * eSPI packet to be read from HW and copied to
+	 * user space.
+	 */
+	switch (cyc) {
+	case ESPI_PERIF_MSG:
+		pkt_len = sizeof(struct espi_perif_msg);
+		break;
+	case ESPI_PERIF_MSG_D:
+		pkt_len = ((len) ? len : ESPI_PLD_LEN_MAX) +
+			  sizeof(struct espi_perif_msg);
+		break;
+	case ESPI_PERIF_SUC_CMPLT_D_MIDDLE:
+	case ESPI_PERIF_SUC_CMPLT_D_FIRST:
+	case ESPI_PERIF_SUC_CMPLT_D_LAST:
+	case ESPI_PERIF_SUC_CMPLT_D_ONLY:
+		pkt_len = ((len) ? len : ESPI_PLD_LEN_MAX) +
+			  sizeof(struct espi_perif_cmplt);
+		break;
+	case ESPI_PERIF_SUC_CMPLT:
+	case ESPI_PERIF_UNSUC_CMPLT:
+		pkt_len = sizeof(struct espi_perif_cmplt);
+		break;
+	default:
+		rc = -EFAULT;
+		goto unlock_mtx_n_out;
+	}
+
+	if (ioc->pkt_len < pkt_len) {
+		rc = -EINVAL;
+		goto unlock_mtx_n_out;
+	}
+
+	pkt = vmalloc(pkt_len);
+	if (!pkt) {
+		rc = -ENOMEM;
+		goto unlock_mtx_n_out;
+	}
+
+	hdr = (struct espi_comm_hdr *)pkt;
+	hdr->cyc = cyc;
+	hdr->tag = tag;
+	hdr->len_h = len >> 8;
+	hdr->len_l = len & 0xff;
+
+	if (espi_perif->dma_mode) {
+		memcpy(hdr + 1, espi_perif->dma.pc_rx_virt,
+		       pkt_len - sizeof(*hdr));
+	} else {
+		for (i = sizeof(*hdr); i < pkt_len; ++i) {
+			regmap_read(espi_ctrl->map,
+				    ESPI_PERIF_PC_RX_PORT, &reg);
+			pkt[i] = reg & 0xff;
+		}
+	}
+
+	if (copy_to_user((void __user *)ioc->pkt, pkt, pkt_len)) {
+		rc = -EFAULT;
+		goto free_n_out;
+	}
+
+	spin_lock_irqsave(&espi_perif->lock, flags);
+
+	regmap_write_bits(espi_ctrl->map, ESPI_PERIF_PC_RX_CTRL,
+			  ESPI_PERIF_PC_RX_CTRL_PEND_SERV,
+			  ESPI_PERIF_PC_RX_CTRL_PEND_SERV);
+
+	espi_perif->rx_ready = 0;
+
+	spin_unlock_irqrestore(&espi_perif->lock, flags);
+
+free_n_out:
+	vfree(pkt);
+
+unlock_mtx_n_out:
+	mutex_unlock(&espi_perif->pc_rx_mtx);
+
+	return rc;
+}
+
+static long aspeed_espi_perif_pc_put_tx(struct file *fp,
+					struct aspeed_espi_ioc *ioc,
+					struct aspeed_espi_perif *espi_perif)
+{
+	int i, rc = 0;
+	uint32_t reg;
+	uint32_t cyc, tag, len;
+	uint8_t *pkt;
+	struct espi_comm_hdr *hdr;
+	struct aspeed_espi_ctrl *espi_ctrl = espi_perif->ctrl;
+
+	if (!mutex_trylock(&espi_perif->pc_tx_mtx))
+		return -EAGAIN;
+
+	regmap_read(espi_ctrl->map, ESPI_PERIF_PC_TX_CTRL, &reg);
+	if (reg & ESPI_PERIF_PC_TX_CTRL_TRIGGER) {
+		rc = -EBUSY;
+		goto unlock_n_out;
+	}
+
+	pkt = vmalloc(ioc->pkt_len);
+	if (!pkt) {
+		rc = -ENOMEM;
+		goto unlock_n_out;
+	}
+
+	hdr = (struct espi_comm_hdr *)pkt;
+
+	if (copy_from_user(pkt, (void __user *)ioc->pkt, ioc->pkt_len)) {
+		rc = -EFAULT;
+		goto free_n_out;
+	}
+
+	/*
+	 * common header (i.e. cycle type, tag, and length)
+	 * part is written to HW registers
+	 */
+	if (espi_perif->dma_mode) {
+		memcpy(espi_perif->dma.pc_tx_virt, hdr + 1,
+		       ioc->pkt_len - sizeof(*hdr));
+		dma_wmb();
+	} else {
+		for (i = sizeof(*hdr); i < ioc->pkt_len; ++i)
+			regmap_write(espi_ctrl->map,
+				     ESPI_PERIF_PC_TX_PORT, pkt[i]);
+	}
+
+	cyc = hdr->cyc;
+	tag = hdr->tag;
+	len = (hdr->len_h << 8) | (hdr->len_l & 0xff);
+
+	reg = ((cyc << ESPI_PERIF_PC_TX_CTRL_CYC_SHIFT) & ESPI_PERIF_PC_TX_CTRL_CYC_MASK)
+		| ((tag << ESPI_PERIF_PC_TX_CTRL_TAG_SHIFT) & ESPI_PERIF_PC_TX_CTRL_TAG_MASK)
+		| ((len << ESPI_PERIF_PC_TX_CTRL_LEN_SHIFT) & ESPI_PERIF_PC_TX_CTRL_LEN_MASK)
+		| ESPI_PERIF_PC_TX_CTRL_TRIGGER;
+
+	regmap_write(espi_ctrl->map, ESPI_PERIF_PC_TX_CTRL, reg);
+
+free_n_out:
+	vfree(pkt);
+
+unlock_n_out:
+	mutex_unlock(&espi_perif->pc_tx_mtx);
+
+	return rc;
+}
+
+static long aspeed_espi_perif_np_put_tx(struct file *fp,
+					struct aspeed_espi_ioc *ioc,
+					struct aspeed_espi_perif *espi_perif)
+{
+	int i, rc = 0;
+	uint32_t reg;
+	uint32_t cyc, tag, len;
+	uint8_t *pkt;
+	struct espi_comm_hdr *hdr;
+	struct aspeed_espi_ctrl *espi_ctrl = espi_perif->ctrl;
+
+	if (!mutex_trylock(&espi_perif->np_tx_mtx))
+		return -EAGAIN;
+
+	regmap_read(espi_ctrl->map, ESPI_PERIF_NP_TX_CTRL, &reg);
+	if (reg & ESPI_PERIF_NP_TX_CTRL_TRIGGER) {
+		rc = -EBUSY;
+		goto unlock_n_out;
+	}
+
+	pkt = vmalloc(ioc->pkt_len);
+	if (!pkt) {
+		rc = -ENOMEM;
+		goto unlock_n_out;
+	}
+
+	hdr = (struct espi_comm_hdr *)pkt;
+
+	if (copy_from_user(pkt, (void __user *)ioc->pkt, ioc->pkt_len)) {
+		rc = -EFAULT;
+		goto free_n_out;
+	}
+
+	/*
+	 * common header (i.e. cycle type, tag, and length)
+	 * part is written to HW registers
+	 */
+	if (espi_perif->dma_mode) {
+		memcpy(espi_perif->dma.np_tx_virt, hdr + 1,
+		       ioc->pkt_len - sizeof(*hdr));
+		dma_wmb();
+	} else {
+		for (i = sizeof(*hdr); i < ioc->pkt_len; ++i)
+			regmap_write(espi_ctrl->map,
+				     ESPI_PERIF_NP_TX_PORT, pkt[i]);
+	}
+
+	cyc = hdr->cyc;
+	tag = hdr->tag;
+	len = (hdr->len_h << 8) | (hdr->len_l & 0xff);
+
+	reg = ((cyc << ESPI_PERIF_NP_TX_CTRL_CYC_SHIFT) & ESPI_PERIF_NP_TX_CTRL_CYC_MASK)
+		| ((tag << ESPI_PERIF_NP_TX_CTRL_TAG_SHIFT) & ESPI_PERIF_NP_TX_CTRL_TAG_MASK)
+		| ((len << ESPI_PERIF_NP_TX_CTRL_LEN_SHIFT) & ESPI_PERIF_NP_TX_CTRL_LEN_MASK)
+		| ESPI_PERIF_NP_TX_CTRL_TRIGGER;
+
+	regmap_write(espi_ctrl->map, ESPI_PERIF_NP_TX_CTRL, reg);
+
+free_n_out:
+	vfree(pkt);
+
+unlock_n_out:
+	mutex_unlock(&espi_perif->np_tx_mtx);
+
+	return rc;
+
+}
+
+static long aspeed_espi_perif_ioctl(struct file *fp, unsigned int cmd, unsigned long arg)
+{
+	struct aspeed_espi_ioc ioc;
+	struct aspeed_espi_perif *espi_perif = container_of(
+			fp->private_data,
+			struct aspeed_espi_perif,
+			mdev);
+
+	if (copy_from_user(&ioc, (void __user *)arg, sizeof(ioc)))
+		return -EFAULT;
+
+	if (ioc.pkt_len > ESPI_PKT_LEN_MAX)
+		return -EINVAL;
+
+	switch (cmd) {
+	case ASPEED_ESPI_PERIF_PC_GET_RX:
+		return aspeed_espi_perif_pc_get_rx(fp, &ioc, espi_perif);
+	case ASPEED_ESPI_PERIF_PC_PUT_TX:
+		return aspeed_espi_perif_pc_put_tx(fp, &ioc, espi_perif);
+	case ASPEED_ESPI_PERIF_NP_PUT_TX:
+		return aspeed_espi_perif_np_put_tx(fp, &ioc, espi_perif);
+	};
+
+	return -EINVAL;
+}
+
+static int aspeed_espi_perif_mmap(struct file *fp, struct vm_area_struct *vma)
+{
+	struct aspeed_espi_perif *espi_perif = container_of(
+			fp->private_data,
+			struct aspeed_espi_perif,
+			mdev);
+	unsigned long vm_size = vma->vm_end - vma->vm_start;
+	pgprot_t prot = vma->vm_page_prot;
+
+	if (!espi_perif->mcyc_enable)
+		return -EPERM;
+
+	if (((vma->vm_pgoff << PAGE_SHIFT) + vm_size) > espi_perif->mcyc_size)
+		return -EINVAL;
+
+	prot = pgprot_noncached(prot);
+
+	if (remap_pfn_range(vma, vma->vm_start,
+			    (espi_perif->mcyc_taddr >> PAGE_SHIFT) + vma->vm_pgoff,
+			    vm_size, prot))
+		return -EAGAIN;
+
+	return 0;
+}
+
+void aspeed_espi_perif_event(uint32_t sts, struct aspeed_espi_perif *espi_perif)
+{
+	unsigned long flags;
+
+	if (sts & ESPI_INT_STS_PERIF_PC_RX_CMPLT) {
+		spin_lock_irqsave(&espi_perif->lock, flags);
+		espi_perif->rx_ready = 1;
+		spin_unlock_irqrestore(&espi_perif->lock, flags);
+
+		wake_up_interruptible(&espi_perif->wq);
+	}
+}
+
+void aspeed_espi_perif_enable(struct aspeed_espi_perif *espi_perif)
+{
+	struct aspeed_espi_perif_dma *dma = &espi_perif->dma;
+	struct aspeed_espi_ctrl *espi_ctrl = espi_perif->ctrl;
+
+	if (espi_perif->mcyc_enable) {
+		if (espi_ctrl->model->version == ESPI_AST2500) {
+			regmap_write(espi_ctrl->map, ESPI_PERIF_PC_RX_MASK,
+				     PERIF_MEMCYC_UNLOCK_KEY);
+			regmap_write(espi_ctrl->map, ESPI_PERIF_PC_RX_MASK,
+				     espi_perif->mcyc_mask);
+		} else {
+			regmap_write(espi_ctrl->map, ESPI_PERIF_PC_RX_MASK,
+				     espi_perif->mcyc_mask | ESPI_PERIF_PC_RX_MASK_CFG_WP);
+			regmap_update_bits(espi_ctrl->map, ESPI_CTRL2,
+					   ESPI_CTRL2_MEMCYC_RD_DIS | ESPI_CTRL2_MEMCYC_WR_DIS, 0);
+		}
+
+		regmap_write(espi_ctrl->map, ESPI_PERIF_PC_RX_SADDR, espi_perif->mcyc_saddr);
+		regmap_write(espi_ctrl->map, ESPI_PERIF_PC_RX_TADDR, espi_perif->mcyc_taddr);
+	}
+
+	if (espi_perif->dma_mode) {
+		regmap_write(espi_ctrl->map, ESPI_PERIF_PC_RX_DMA, dma->pc_rx_addr);
+		regmap_write(espi_ctrl->map, ESPI_PERIF_PC_TX_DMA, dma->pc_tx_addr);
+		regmap_write(espi_ctrl->map, ESPI_PERIF_NP_TX_DMA, dma->np_tx_addr);
+
+		regmap_update_bits(espi_ctrl->map, ESPI_CTRL,
+				   ESPI_CTRL_PERIF_NP_TX_DMA_EN |
+				   ESPI_CTRL_PERIF_PC_TX_DMA_EN |
+				   ESPI_CTRL_PERIF_PC_RX_DMA_EN,
+				   ESPI_CTRL_PERIF_NP_TX_DMA_EN |
+				   ESPI_CTRL_PERIF_PC_TX_DMA_EN |
+				   ESPI_CTRL_PERIF_PC_RX_DMA_EN);
+	}
+
+	regmap_write(espi_ctrl->map, ESPI_INT_STS,
+		     ESPI_INT_STS_PERIF_BITS);
+
+	regmap_update_bits(espi_ctrl->map, ESPI_INT_EN,
+			   ESPI_INT_EN_PERIF_BITS,
+			   ESPI_INT_EN_PERIF_BITS);
+
+	regmap_update_bits(espi_ctrl->map, ESPI_CTRL,
+			   ESPI_CTRL_PERIF_SW_RDY,
+			   ESPI_CTRL_PERIF_SW_RDY);
+}
+
+static const struct file_operations aspeed_espi_perif_fops = {
+	.owner = THIS_MODULE,
+	.mmap = aspeed_espi_perif_mmap,
+	.unlocked_ioctl = aspeed_espi_perif_ioctl,
+};
+
+void *aspeed_espi_perif_alloc(struct device *dev, struct aspeed_espi_ctrl *espi_ctrl)
+{
+	int rc;
+	struct aspeed_espi_perif *espi_perif;
+	struct aspeed_espi_perif_dma *dma;
+
+	espi_perif = devm_kzalloc(dev, sizeof(*espi_perif), GFP_KERNEL);
+	if (!espi_perif)
+		return ERR_PTR(-ENOMEM);
+
+	espi_perif->ctrl = espi_ctrl;
+
+	init_waitqueue_head(&espi_perif->wq);
+
+	spin_lock_init(&espi_perif->lock);
+
+	mutex_init(&espi_perif->pc_rx_mtx);
+	mutex_init(&espi_perif->pc_tx_mtx);
+	mutex_init(&espi_perif->np_tx_mtx);
+
+	espi_perif->mcyc_enable = of_property_read_bool(dev->of_node, "perif,memcyc-enable");
+
+	do {
+		if (!espi_perif->mcyc_enable)
+			break;
+
+		if (of_device_is_available(of_parse_phandle(dev->of_node, "aspeed,espi-mmbi", 0))) {
+			dev_warn(dev, "memory cycle is occupied by MMBI\n");
+			break;
+		}
+
+		rc = of_property_read_u32(dev->of_node, "perif,memcyc-src-addr",
+					  &espi_perif->mcyc_saddr);
+		if (rc) {
+			dev_err(dev, "cannot get Host source address for memory cycle\n");
+			return ERR_PTR(-ENODEV);
+		}
+
+		rc = of_property_read_u32(dev->of_node, "perif,memcyc-size",
+					  &espi_perif->mcyc_size);
+		if (rc) {
+			dev_err(dev, "cannot get size for memory cycle\n");
+			return ERR_PTR(-ENODEV);
+		}
+
+		if (espi_perif->mcyc_size < PERIF_MEMCYC_SIZE_MIN)
+			espi_perif->mcyc_size = PERIF_MEMCYC_SIZE_MIN;
+		else
+			espi_perif->mcyc_size = roundup_pow_of_two(espi_perif->mcyc_size);
+
+		espi_perif->mcyc_mask = ~(espi_perif->mcyc_size - 1);
+		espi_perif->mcyc_virt = dma_alloc_coherent(dev, espi_perif->mcyc_size,
+							   &espi_perif->mcyc_taddr, GFP_KERNEL);
+		if (!espi_perif->mcyc_virt) {
+			dev_err(dev, "cannot allocate memory cycle region\n");
+			return ERR_PTR(-ENOMEM);
+		}
+	} while (0);
+
+	if (of_property_read_bool(dev->of_node, "perif,dma-mode")) {
+		dma = &espi_perif->dma;
+
+		dma->pc_tx_virt = dma_alloc_coherent(dev, PAGE_SIZE,
+						     &dma->pc_tx_addr, GFP_KERNEL);
+		if (!dma->pc_tx_virt) {
+			dev_err(dev, "cannot allocate posted TX DMA buffer\n");
+			return ERR_PTR(-ENOMEM);
+		}
+
+		dma->pc_rx_virt = dma_alloc_coherent(dev, PAGE_SIZE,
+						     &dma->pc_rx_addr, GFP_KERNEL);
+		if (!dma->pc_rx_virt) {
+			dev_err(dev, "cannot allocate posted RX DMA buffer\n");
+			return ERR_PTR(-ENOMEM);
+		}
+
+		dma->np_tx_virt = dma_alloc_coherent(dev, PAGE_SIZE,
+				&dma->np_tx_addr, GFP_KERNEL);
+		if (!dma->np_tx_virt) {
+			dev_err(dev, "cannot allocate non-posted TX DMA buffer\n");
+			return ERR_PTR(-ENOMEM);
+		}
+
+		espi_perif->dma_mode = 1;
+	}
+
+	espi_perif->mdev.parent = dev;
+	espi_perif->mdev.minor = MISC_DYNAMIC_MINOR;
+	espi_perif->mdev.name = devm_kasprintf(dev, GFP_KERNEL, "%s", PERIF_MDEV_NAME);
+	espi_perif->mdev.fops = &aspeed_espi_perif_fops;
+	rc = misc_register(&espi_perif->mdev);
+	if (rc) {
+		dev_err(dev, "cannot register device\n");
+		return ERR_PTR(rc);
+	}
+
+	aspeed_espi_perif_enable(espi_perif);
+
+	return espi_perif;
+}
+
+void aspeed_espi_perif_free(struct device *dev, struct aspeed_espi_perif *espi_perif)
+{
+	struct aspeed_espi_perif_dma *dma = &espi_perif->dma;
+
+	if (espi_perif->mcyc_virt)
+		dma_free_coherent(dev, espi_perif->mcyc_size,
+				  espi_perif->mcyc_virt,
+				  espi_perif->mcyc_taddr);
+
+	if (espi_perif->dma_mode) {
+		dma_free_coherent(dev, PAGE_SIZE, dma->pc_tx_virt,
+				  dma->pc_tx_addr);
+		dma_free_coherent(dev, PAGE_SIZE, dma->pc_rx_virt,
+				  dma->pc_rx_addr);
+		dma_free_coherent(dev, PAGE_SIZE, dma->np_tx_virt,
+				  dma->np_tx_addr);
+	}
+
+	mutex_destroy(&espi_perif->pc_tx_mtx);
+	mutex_destroy(&espi_perif->np_tx_mtx);
+
+	misc_deregister(&espi_perif->mdev);
+}
diff --git a/drivers/soc/aspeed/aspeed-espi-perif.h b/drivers/soc/aspeed/aspeed-espi-perif.h
new file mode 100644
index 000000000000..1b964e4680f5
--- /dev/null
+++ b/drivers/soc/aspeed/aspeed-espi-perif.h
@@ -0,0 +1,45 @@
+/* SPDX-License-Identifier: GPL-2.0+ */
+/*
+ * Copyright 2021 ASPEED Technology Inc.
+ */
+#ifndef _ASPEED_ESPI_PERIF_H_
+#define _ASPEED_ESPI_PERIF_H_
+
+struct aspeed_espi_perif_dma {
+	void *pc_tx_virt;
+	dma_addr_t pc_tx_addr;
+	void *pc_rx_virt;
+	dma_addr_t pc_rx_addr;
+	void *np_tx_virt;
+	dma_addr_t np_tx_addr;
+};
+
+struct aspeed_espi_perif {
+	uint32_t mcyc_enable;
+	void *mcyc_virt;
+	uint32_t mcyc_saddr;
+	phys_addr_t mcyc_taddr;
+	uint32_t mcyc_size;
+	uint32_t mcyc_mask;
+
+	uint32_t dma_mode;
+	struct aspeed_espi_perif_dma dma;
+
+	uint32_t rx_ready;
+	wait_queue_head_t wq;
+
+	spinlock_t lock;
+	struct mutex pc_rx_mtx;
+	struct mutex pc_tx_mtx;
+	struct mutex np_tx_mtx;
+
+	struct miscdevice mdev;
+	struct aspeed_espi_ctrl *ctrl;
+};
+
+void aspeed_espi_perif_event(uint32_t sts, struct aspeed_espi_perif *espi_perif);
+void aspeed_espi_perif_enable(struct aspeed_espi_perif *espi_perif);
+void *aspeed_espi_perif_alloc(struct device *dev, struct aspeed_espi_ctrl *espi_ctrl);
+void aspeed_espi_perif_free(struct device *dev, struct aspeed_espi_perif *espi_perif);
+
+#endif
diff --git a/drivers/soc/aspeed/aspeed-espi-vw.c b/drivers/soc/aspeed/aspeed-espi-vw.c
new file mode 100644
index 000000000000..819b3ae2f7ea
--- /dev/null
+++ b/drivers/soc/aspeed/aspeed-espi-vw.c
@@ -0,0 +1,137 @@
+// SPDX-License-Identifier: GPL-2.0+
+/*
+ * Copyright 2021 ASPEED Technology Inc.
+ */
+#include <linux/fs.h>
+#include <linux/of_device.h>
+#include <linux/miscdevice.h>
+#include <linux/mfd/syscon.h>
+#include <linux/regmap.h>
+#include <linux/uaccess.h>
+#include <linux/vmalloc.h>
+#include <linux/miscdevice.h>
+#include <linux/dma-mapping.h>
+
+#include "aspeed-espi-ioc.h"
+#include "aspeed-espi-ctrl.h"
+#include "aspeed-espi-vw.h"
+
+#define VW_MDEV_NAME	"aspeed-espi-vw"
+
+static long aspeed_espi_vw_ioctl(struct file *fp, unsigned int cmd, unsigned long arg)
+{
+	uint32_t val;
+
+	struct aspeed_espi_vw *espi_vw = container_of(fp->private_data,
+						      struct aspeed_espi_vw,
+						      mdev);
+	struct aspeed_espi_ctrl *espi_ctrl = espi_vw->ctrl;
+
+	switch (cmd) {
+	case ASPEED_ESPI_VW_GET_GPIO_VAL:
+		regmap_read(espi_ctrl->map, ESPI_VW_GPIO_VAL, &val);
+		if (put_user(val, (uint32_t __user *)arg))
+			return -EFAULT;
+		break;
+
+	case ASPEED_ESPI_VW_PUT_GPIO_VAL:
+		if (get_user(val, (uint32_t __user *)arg))
+			return -EFAULT;
+		regmap_write(espi_ctrl->map, ESPI_VW_GPIO_VAL, val);
+		break;
+
+	default:
+		return -EINVAL;
+	};
+
+	return 0;
+}
+
+void aspeed_espi_vw_event(uint32_t sts, struct aspeed_espi_vw *espi_vw)
+{
+	uint32_t sysevt_sts;
+	struct aspeed_espi_ctrl *espi_ctrl = espi_vw->ctrl;
+
+	regmap_read(espi_ctrl->map, ESPI_INT_STS, &sts);
+
+	if (sts & ESPI_INT_STS_VW_SYSEVT) {
+		regmap_read(espi_ctrl->map, ESPI_SYSEVT_INT_STS, &sysevt_sts);
+
+		if (espi_ctrl->model->version == ESPI_AST2500) {
+			if (sysevt_sts & ESPI_SYSEVT_INT_STS_HOST_RST_WARN)
+				regmap_update_bits(espi_ctrl->map, ESPI_SYSEVT,
+						   ESPI_SYSEVT_HOST_RST_ACK,
+						   ESPI_SYSEVT_HOST_RST_ACK);
+
+			if (sysevt_sts & ESPI_SYSEVT_INT_STS_OOB_RST_WARN)
+				regmap_update_bits(espi_ctrl->map, ESPI_SYSEVT,
+						   ESPI_SYSEVT_OOB_RST_ACK,
+						   ESPI_SYSEVT_OOB_RST_ACK);
+		}
+
+		regmap_write(espi_ctrl->map, ESPI_SYSEVT_INT_STS, sysevt_sts);
+	}
+
+	if (sts & ESPI_INT_STS_VW_SYSEVT1) {
+		regmap_read(espi_ctrl->map, ESPI_SYSEVT1_INT_STS, &sysevt_sts);
+
+		if (sysevt_sts & ESPI_SYSEVT1_INT_STS_SUSPEND_WARN)
+			regmap_update_bits(espi_ctrl->map, ESPI_SYSEVT1,
+					   ESPI_SYSEVT1_SUSPEND_ACK,
+					   ESPI_SYSEVT1_SUSPEND_ACK);
+
+		regmap_write(espi_ctrl->map, ESPI_SYSEVT1_INT_STS, sysevt_sts);
+	}
+}
+
+void aspeed_espi_vw_enable(struct aspeed_espi_vw *espi_vw)
+{
+	struct aspeed_espi_ctrl *espi_ctrl = espi_vw->ctrl;
+
+	regmap_write(espi_ctrl->map, ESPI_INT_STS,
+		     ESPI_INT_STS_VW_BITS);
+
+	regmap_update_bits(espi_ctrl->map, ESPI_INT_EN,
+			   ESPI_INT_EN_VW_BITS,
+			   ESPI_INT_EN_VW_BITS);
+
+	regmap_update_bits(espi_ctrl->map, ESPI_CTRL,
+			   ESPI_CTRL_VW_SW_RDY,
+			   ESPI_CTRL_VW_SW_RDY);
+}
+
+static const struct file_operations aspeed_espi_vw_fops = {
+	.owner = THIS_MODULE,
+	.unlocked_ioctl = aspeed_espi_vw_ioctl,
+};
+
+void *aspeed_espi_vw_alloc(struct device *dev, struct aspeed_espi_ctrl *espi_ctrl)
+{
+	int rc;
+	struct aspeed_espi_vw *espi_vw;
+
+	espi_vw = devm_kzalloc(dev, sizeof(*espi_vw), GFP_KERNEL);
+	if (!espi_vw)
+		return ERR_PTR(-ENOMEM);
+
+	espi_vw->ctrl = espi_ctrl;
+
+	espi_vw->mdev.parent = dev;
+	espi_vw->mdev.minor = MISC_DYNAMIC_MINOR;
+	espi_vw->mdev.name = devm_kasprintf(dev, GFP_KERNEL, "%s", VW_MDEV_NAME);
+	espi_vw->mdev.fops = &aspeed_espi_vw_fops;
+	rc = misc_register(&espi_vw->mdev);
+	if (rc) {
+		dev_err(dev, "cannot register device\n");
+		return ERR_PTR(rc);
+	}
+
+	aspeed_espi_vw_enable(espi_vw);
+
+	return espi_vw;
+}
+
+void aspeed_espi_vw_free(struct device *dev, struct aspeed_espi_vw *espi_vw)
+{
+	misc_deregister(&espi_vw->mdev);
+}
diff --git a/drivers/soc/aspeed/aspeed-espi-vw.h b/drivers/soc/aspeed/aspeed-espi-vw.h
new file mode 100644
index 000000000000..aba9c414ac1b
--- /dev/null
+++ b/drivers/soc/aspeed/aspeed-espi-vw.h
@@ -0,0 +1,21 @@
+/* SPDX-License-Identifier: GPL-2.0+ */
+/*
+ * Copyright 2021 ASPEED Technology Inc.
+ */
+#ifndef _ASPEED_ESPI_VW_H_
+#define _ASPEED_ESPI_VW_H_
+
+struct aspeed_espi_vw {
+	int irq;
+	int irq_reset;
+
+	struct miscdevice mdev;
+	struct aspeed_espi_ctrl *ctrl;
+};
+
+void aspeed_espi_vw_event(uint32_t sts, struct aspeed_espi_vw *espi_vw);
+void aspeed_espi_vw_enable(struct aspeed_espi_vw *espi_vw);
+void *aspeed_espi_vw_alloc(struct device *dev, struct aspeed_espi_ctrl *espi_ctrl);
+void aspeed_espi_vw_free(struct device *dev, struct aspeed_espi_vw *espi_vw);
+
+#endif
diff --git a/drivers/soc/aspeed/aspeed-host-bmc-dev.c b/drivers/soc/aspeed/aspeed-host-bmc-dev.c
new file mode 100644
index 000000000000..414a26a4edd9
--- /dev/null
+++ b/drivers/soc/aspeed/aspeed-host-bmc-dev.c
@@ -0,0 +1,519 @@
+// SPDX-License-Identifier: GPL-2.0-or-later
+// Copyright (C) ASPEED Technology Inc.
+
+#include <linux/init.h>
+#include <linux/version.h>
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/errno.h>
+
+#include <linux/pci.h>
+#include <linux/file.h>
+#include <linux/fs.h>
+#include <linux/interrupt.h>
+#include <linux/wait.h>
+#include <linux/workqueue.h>
+
+
+#include <linux/miscdevice.h>
+#include <linux/module.h>
+#include <linux/serial_core.h>
+#include <linux/serial_8250.h>
+
+#define ASPEED_PCI_BMC_HOST2BMC_Q1		0x30000
+#define ASPEED_PCI_BMC_HOST2BMC_Q2		0x30010
+#define ASPEED_PCI_BMC_BMC2HOST_Q1		0x30020
+#define ASPEED_PCI_BMC_BMC2HOST_Q2		0x30030
+#define ASPEED_PCI_BMC_BMC2HOST_STS		0x30040
+#define	 BMC2HOST_INT_STS_DOORBELL		BIT(31)
+#define	 BMC2HOST_ENABLE_INTB			BIT(30)
+/* */
+#define	 BMC2HOST_Q1_FULL				BIT(27)
+#define	 BMC2HOST_Q1_EMPTY				BIT(26)
+#define	 BMC2HOST_Q2_FULL				BIT(25)
+#define	 BMC2HOST_Q2_EMPTY				BIT(24)
+#define	 BMC2HOST_Q1_FULL_UNMASK		BIT(23)
+#define	 BMC2HOST_Q1_EMPTY_UNMASK		BIT(22)
+#define	 BMC2HOST_Q2_FULL_UNMASK		BIT(21)
+#define	 BMC2HOST_Q2_EMPTY_UNMASK		BIT(20)
+
+#define ASPEED_PCI_BMC_HOST2BMC_STS		0x30044
+#define	 HOST2BMC_INT_STS_DOORBELL		BIT(31)
+#define	 HOST2BMC_ENABLE_INTB			BIT(30)
+/* */
+#define	 HOST2BMC_Q1_FULL				BIT(27)
+#define	 HOST2BMC_Q1_EMPTY				BIT(26)
+#define	 HOST2BMC_Q2_FULL				BIT(25)
+#define	 HOST2BMC_Q2_EMPTY				BIT(24)
+#define	 HOST2BMC_Q1_FULL_UNMASK		BIT(23)
+#define	 HOST2BMC_Q1_EMPTY_UNMASK		BIT(22)
+#define	 HOST2BMC_Q2_FULL_UNMASK		BIT(21)
+#define	 HOST2BMC_Q2_EMPTY_UNMASK		BIT(20)
+
+struct aspeed_pci_bmc_dev {
+	struct device *dev;
+	struct miscdevice miscdev;
+
+	unsigned long mem_bar_base;
+	unsigned long mem_bar_size;
+	void __iomem *mem_bar_reg;
+
+	unsigned long message_bar_base;
+	unsigned long message_bar_size;
+	void __iomem *msg_bar_reg;
+
+	struct bin_attribute	bin0;
+	struct bin_attribute	bin1;
+
+	struct kernfs_node	*kn0;
+	struct kernfs_node	*kn1;
+
+	/* Queue waiters for idle engine */
+	wait_queue_head_t tx_wait0;
+	wait_queue_head_t tx_wait1;
+	wait_queue_head_t rx_wait0;
+	wait_queue_head_t rx_wait1;
+
+	void __iomem *sio_mbox_reg;
+	int sio_mbox_irq;
+
+	u8 IntLine;
+	int legency_irq;
+};
+
+#define HOST_BMC_QUEUE_SIZE			(16 * 4)
+#define PCIE_DEVICE_SIO_ADDR		(0x2E * 4)
+#define BMC_MULTI_MSI	32
+
+#define DRIVER_NAME "ASPEED BMC DEVICE"
+
+#define VUART_MAX_PARMS		2
+static uint16_t vuart_ioport[VUART_MAX_PARMS];
+static uint16_t vuart_sirq[VUART_MAX_PARMS];
+
+static struct aspeed_pci_bmc_dev *file_aspeed_bmc_device(struct file *file)
+{
+	return container_of(file->private_data, struct aspeed_pci_bmc_dev,
+			miscdev);
+}
+
+static int aspeed_pci_bmc_dev_mmap(struct file *file, struct vm_area_struct *vma)
+{
+	struct aspeed_pci_bmc_dev *pci_bmc_dev = file_aspeed_bmc_device(file);
+	unsigned long vsize = vma->vm_end - vma->vm_start;
+	pgprot_t prot = vma->vm_page_prot;
+
+	if (vma->vm_pgoff + vsize > pci_bmc_dev->mem_bar_base + 0x100000)
+		return -EINVAL;
+
+	prot = pgprot_noncached(prot);
+
+	if (remap_pfn_range(vma, vma->vm_start,
+		(pci_bmc_dev->mem_bar_base >> PAGE_SHIFT) + vma->vm_pgoff,
+		vsize, prot))
+		return -EAGAIN;
+
+	return 0;
+}
+
+static const struct file_operations aspeed_pci_bmc_dev_fops = {
+	.owner		= THIS_MODULE,
+	.mmap		= aspeed_pci_bmc_dev_mmap,
+};
+
+static ssize_t aspeed_pci_bmc_dev_queue1_rx(struct file *filp, struct kobject *kobj,
+		struct bin_attribute *attr, char *buf, loff_t off, size_t count)
+{
+	struct aspeed_pci_bmc_dev *pci_bmc_device = dev_get_drvdata(container_of(kobj, struct device, kobj));
+	u32 *data = (u32 *) buf;
+	int ret;
+
+	ret = wait_event_interruptible(pci_bmc_device->rx_wait0,
+		!(readl(pci_bmc_device->msg_bar_reg + ASPEED_PCI_BMC_BMC2HOST_STS) & BMC2HOST_Q1_EMPTY));
+	if (ret)
+		return -EINTR;
+
+	data[0] = readl(pci_bmc_device->msg_bar_reg + ASPEED_PCI_BMC_BMC2HOST_Q1);
+	writel(HOST2BMC_INT_STS_DOORBELL | HOST2BMC_ENABLE_INTB, pci_bmc_device->msg_bar_reg + ASPEED_PCI_BMC_HOST2BMC_STS);
+	return sizeof(u32);
+}
+
+static ssize_t aspeed_pci_bmc_dev_queue2_rx(struct file *filp, struct kobject *kobj,
+		struct bin_attribute *attr, char *buf, loff_t off, size_t count)
+{
+	struct aspeed_pci_bmc_dev *pci_bmc_device = dev_get_drvdata(container_of(kobj, struct device, kobj));
+	u32 *data = (u32 *) buf;
+	int ret;
+
+	ret = wait_event_interruptible(pci_bmc_device->rx_wait1,
+		!(readl(pci_bmc_device->msg_bar_reg + ASPEED_PCI_BMC_BMC2HOST_STS) & BMC2HOST_Q2_EMPTY));
+	if (ret)
+		return -EINTR;
+
+	data[0] = readl(pci_bmc_device->msg_bar_reg + ASPEED_PCI_BMC_BMC2HOST_Q2);
+	writel(HOST2BMC_INT_STS_DOORBELL | HOST2BMC_ENABLE_INTB, pci_bmc_device->msg_bar_reg + ASPEED_PCI_BMC_HOST2BMC_STS);
+	return sizeof(u32);
+}
+
+static ssize_t aspeed_pci_bmc_dev_queue1_tx(struct file *filp, struct kobject *kobj,
+		struct bin_attribute *attr, char *buf, loff_t off, size_t count)
+{
+	struct aspeed_pci_bmc_dev *pci_bmc_device = dev_get_drvdata(container_of(kobj, struct device, kobj));
+	u32 tx_buff;
+	int ret;
+
+	if (count != sizeof(u32))
+		return -EINVAL;
+
+	ret = wait_event_interruptible(pci_bmc_device->tx_wait0,
+		!(readl(pci_bmc_device->msg_bar_reg + ASPEED_PCI_BMC_HOST2BMC_STS) & HOST2BMC_Q1_FULL));
+	if (ret)
+		return -EINTR;
+
+	memcpy(&tx_buff, buf, 4);
+	writel(tx_buff, pci_bmc_device->msg_bar_reg + ASPEED_PCI_BMC_HOST2BMC_Q1);
+	//trigger to host
+	writel(HOST2BMC_INT_STS_DOORBELL | HOST2BMC_ENABLE_INTB, pci_bmc_device->msg_bar_reg + ASPEED_PCI_BMC_HOST2BMC_STS);
+
+	return sizeof(u32);
+}
+
+static ssize_t aspeed_pci_bmc_dev_queue2_tx(struct file *filp, struct kobject *kobj,
+		struct bin_attribute *attr, char *buf, loff_t off, size_t count)
+{
+	struct aspeed_pci_bmc_dev *pci_bmc_device = dev_get_drvdata(container_of(kobj, struct device, kobj));
+	u32 tx_buff = 0;
+	int ret;
+
+	if (count != sizeof(u32))
+		return -EINVAL;
+
+	ret = wait_event_interruptible(pci_bmc_device->tx_wait0,
+		!(readl(pci_bmc_device->msg_bar_reg + ASPEED_PCI_BMC_HOST2BMC_STS) & HOST2BMC_Q2_FULL));
+	if (ret)
+		return -EINTR;
+
+	memcpy(&tx_buff, buf, 4);
+	writel(tx_buff, pci_bmc_device->msg_bar_reg + ASPEED_PCI_BMC_HOST2BMC_Q2);
+	//trigger to host
+	writel(HOST2BMC_INT_STS_DOORBELL | HOST2BMC_ENABLE_INTB, pci_bmc_device->msg_bar_reg + ASPEED_PCI_BMC_HOST2BMC_STS);
+
+	return sizeof(u32);
+}
+
+irqreturn_t aspeed_pci_host_bmc_device_interrupt(int irq, void *dev_id)
+{
+	struct aspeed_pci_bmc_dev *pci_bmc_device = dev_id;
+	u32 bmc2host_q_sts = readl(pci_bmc_device->msg_bar_reg + ASPEED_PCI_BMC_BMC2HOST_STS);
+
+	if (bmc2host_q_sts & BMC2HOST_INT_STS_DOORBELL)
+		writel(BMC2HOST_INT_STS_DOORBELL, pci_bmc_device->msg_bar_reg + ASPEED_PCI_BMC_BMC2HOST_STS);
+
+	if (bmc2host_q_sts & BMC2HOST_ENABLE_INTB)
+		writel(BMC2HOST_ENABLE_INTB, pci_bmc_device->msg_bar_reg + ASPEED_PCI_BMC_BMC2HOST_STS);
+
+	if (bmc2host_q_sts & BMC2HOST_Q1_FULL)
+		dev_info(pci_bmc_device->dev, "Q1 Full\n");
+
+	if (bmc2host_q_sts & BMC2HOST_Q2_FULL)
+		dev_info(pci_bmc_device->dev, "Q2 Full\n");
+
+
+	//check q1
+	if (!(readl(pci_bmc_device->msg_bar_reg + ASPEED_PCI_BMC_HOST2BMC_STS) & HOST2BMC_Q1_FULL))
+		wake_up_interruptible(&pci_bmc_device->tx_wait0);
+
+	if (!(readl(pci_bmc_device->msg_bar_reg + ASPEED_PCI_BMC_BMC2HOST_STS) & BMC2HOST_Q1_EMPTY))
+		wake_up_interruptible(&pci_bmc_device->rx_wait0);
+	//chech q2
+	if (!(readl(pci_bmc_device->msg_bar_reg + ASPEED_PCI_BMC_HOST2BMC_STS) & HOST2BMC_Q2_FULL))
+		wake_up_interruptible(&pci_bmc_device->tx_wait1);
+
+	if (!(readl(pci_bmc_device->msg_bar_reg + ASPEED_PCI_BMC_BMC2HOST_STS) & BMC2HOST_Q2_EMPTY))
+		wake_up_interruptible(&pci_bmc_device->rx_wait1);
+
+	return IRQ_HANDLED;
+
+}
+
+irqreturn_t aspeed_pci_host_mbox_interrupt(int irq, void *dev_id)
+{
+	struct aspeed_pci_bmc_dev *pci_bmc_device = dev_id;
+	u32 isr = readl(pci_bmc_device->sio_mbox_reg + 0x94);
+
+	if (isr & BIT(7))
+		writel(BIT(7), pci_bmc_device->sio_mbox_reg + 0x94);
+
+	return IRQ_HANDLED;
+
+}
+
+#define BMC_MSI_IDX_BASE	4
+static int aspeed_pci_host_bmc_device_probe(struct pci_dev *pdev, const struct pci_device_id *ent)
+{
+	struct uart_8250_port uart[VUART_MAX_PARMS];
+	struct aspeed_pci_bmc_dev *pci_bmc_dev;
+	struct device *dev = &pdev->dev;
+	void __iomem *pcie_sio_decode_addr;
+	u16 config_cmd_val;
+	int nr_entries;
+	int rc = 0;
+	int i = 0;
+
+	pr_info("ASPEED BMC PCI ID %04x:%04x, IRQ=%u\n", pdev->vendor, pdev->device, pdev->irq);
+
+	pci_bmc_dev = kzalloc(sizeof(*pci_bmc_dev), GFP_KERNEL);
+	if (!pci_bmc_dev) {
+		rc = -ENOMEM;
+		dev_err(&pdev->dev, "kmalloc() returned NULL memory.\n");
+		goto out_err;
+	}
+
+	rc = pci_enable_device(pdev);
+	if (rc != 0) {
+		dev_err(&pdev->dev, "pci_enable_device() returned error %d\n", rc);
+		goto out_err;
+	}
+
+	/* set PCI host mastering  */
+	pci_set_master(pdev);
+
+	nr_entries = pci_alloc_irq_vectors(pdev, 1, BMC_MULTI_MSI,
+				PCI_IRQ_MSIX | PCI_IRQ_MSI);
+	if (nr_entries < 0) {
+		pci_bmc_dev->legency_irq = 1;
+		pci_read_config_word(pdev, PCI_COMMAND, &config_cmd_val);
+		config_cmd_val &= ~PCI_COMMAND_INTX_DISABLE;
+		pci_write_config_word((struct pci_dev *)pdev, PCI_COMMAND, config_cmd_val);
+
+	} else {
+		pci_bmc_dev->legency_irq = 0;
+		pci_read_config_word(pdev, PCI_COMMAND, &config_cmd_val);
+		config_cmd_val |= PCI_COMMAND_INTX_DISABLE;
+		pci_write_config_word((struct pci_dev *)pdev, PCI_COMMAND, config_cmd_val);
+		pdev->irq = pci_irq_vector(pdev, BMC_MSI_IDX_BASE);
+	}
+
+	pr_info("ASPEED BMC PCI ID %04x:%04x, IRQ=%u\n", pdev->vendor, pdev->device, pdev->irq);
+
+	init_waitqueue_head(&pci_bmc_dev->tx_wait0);
+	init_waitqueue_head(&pci_bmc_dev->tx_wait1);
+	init_waitqueue_head(&pci_bmc_dev->rx_wait0);
+	init_waitqueue_head(&pci_bmc_dev->rx_wait1);
+
+	//Get MEM bar
+	pci_bmc_dev->mem_bar_base = pci_resource_start(pdev, 0);
+	pci_bmc_dev->mem_bar_size = pci_resource_len(pdev, 0);
+
+	pr_info("BAR0 I/O Mapped Base Address is: %08lx End %08lx\n", pci_bmc_dev->mem_bar_base, pci_bmc_dev->mem_bar_size);
+
+	pci_bmc_dev->mem_bar_reg = pci_ioremap_bar(pdev, 0);
+	if (!pci_bmc_dev->mem_bar_reg) {
+		rc = -ENOMEM;
+		goto out_free0;
+	}
+
+    //Get MSG BAR info
+	pci_bmc_dev->message_bar_base = pci_resource_start(pdev, 1);
+	pci_bmc_dev->message_bar_size = pci_resource_len(pdev, 1);
+
+	pr_info("MSG BAR1 Memory Mapped Base Address is: %08lx End %08lx\n", pci_bmc_dev->message_bar_base, pci_bmc_dev->message_bar_size);
+
+	pci_bmc_dev->msg_bar_reg = pci_ioremap_bar(pdev, 1);
+	if (!pci_bmc_dev->msg_bar_reg) {
+		rc = -ENOMEM;
+		goto out_free1;
+	}
+
+	/* ERRTA40: dummy read */
+	(void)__raw_readl((void __iomem *)pci_bmc_dev->msg_bar_reg);
+
+	sysfs_bin_attr_init(&pci_bmc_dev->bin0);
+	sysfs_bin_attr_init(&pci_bmc_dev->bin1);
+
+	pci_bmc_dev->bin0.attr.name = "pci-bmc-dev-queue1";
+	pci_bmc_dev->bin0.attr.mode = 0600;
+	pci_bmc_dev->bin0.read = aspeed_pci_bmc_dev_queue1_rx;
+	pci_bmc_dev->bin0.write = aspeed_pci_bmc_dev_queue1_tx;
+	pci_bmc_dev->bin0.size = 4;
+
+	rc = sysfs_create_bin_file(&pdev->dev.kobj, &pci_bmc_dev->bin0);
+	if (rc) {
+		pr_err("error for bin file ");
+		goto out_free1;
+	}
+
+	pci_bmc_dev->kn0 = kernfs_find_and_get(dev->kobj.sd, pci_bmc_dev->bin0.attr.name);
+	if (!pci_bmc_dev->kn0) {
+		sysfs_remove_bin_file(&dev->kobj, &pci_bmc_dev->bin0);
+		goto out_free1;
+	}
+
+	pci_bmc_dev->bin1.attr.name = "pci-bmc-dev-queue2";
+	pci_bmc_dev->bin1.attr.mode = 0600;
+	pci_bmc_dev->bin1.read = aspeed_pci_bmc_dev_queue2_rx;
+	pci_bmc_dev->bin1.write = aspeed_pci_bmc_dev_queue2_tx;
+	pci_bmc_dev->bin1.size = 4;
+
+	rc = sysfs_create_bin_file(&pdev->dev.kobj, &pci_bmc_dev->bin1);
+	if (rc) {
+		sysfs_remove_bin_file(&dev->kobj, &pci_bmc_dev->bin1);
+		goto out_free1;
+	}
+
+	pci_bmc_dev->kn1 = kernfs_find_and_get(dev->kobj.sd, pci_bmc_dev->bin1.attr.name);
+	if (!pci_bmc_dev->kn1) {
+		sysfs_remove_bin_file(&dev->kobj, &pci_bmc_dev->bin1);
+		goto out_free1;
+	}
+
+	pci_bmc_dev->miscdev.minor = MISC_DYNAMIC_MINOR;
+	pci_bmc_dev->miscdev.name = DRIVER_NAME;
+	pci_bmc_dev->miscdev.fops = &aspeed_pci_bmc_dev_fops;
+	pci_bmc_dev->miscdev.parent = dev;
+
+	rc = misc_register(&pci_bmc_dev->miscdev);
+	if (rc) {
+		pr_err("host bmc register fail %d\n", rc);
+		goto out_free;
+	}
+
+	pci_set_drvdata(pdev, pci_bmc_dev);
+
+	rc = request_irq(pdev->irq, aspeed_pci_host_bmc_device_interrupt, IRQF_SHARED, "ASPEED BMC DEVICE", pci_bmc_dev);
+	if (rc) {
+		pr_err("host bmc device Unable to get IRQ %d\n", rc);
+		goto out_unreg;
+	}
+
+	/* setup mbox */
+	pcie_sio_decode_addr = pci_bmc_dev->msg_bar_reg + PCIE_DEVICE_SIO_ADDR;
+	writel(0xaa, pcie_sio_decode_addr);
+	writel(0xa5, pcie_sio_decode_addr);
+	writel(0xa5, pcie_sio_decode_addr);
+	writel(0x07, pcie_sio_decode_addr);
+	writel(0x0e, pcie_sio_decode_addr + 0x04);
+	/* disable */
+	writel(0x30, pcie_sio_decode_addr);
+	writel(0x00, pcie_sio_decode_addr + 0x04);
+	/* set decode address 0x100 */
+	writel(0x60, pcie_sio_decode_addr);
+	writel(0x01, pcie_sio_decode_addr + 0x04);
+	writel(0x61, pcie_sio_decode_addr);
+	writel(0x00, pcie_sio_decode_addr + 0x04);
+	/* enable */
+	writel(0x30, pcie_sio_decode_addr);
+	writel(0x01, pcie_sio_decode_addr + 0x04);
+	pci_bmc_dev->sio_mbox_reg = pci_bmc_dev->msg_bar_reg + 0x400;
+
+	if (pci_bmc_dev->legency_irq)
+		pci_bmc_dev->sio_mbox_irq = pdev->irq;
+	else
+		pci_bmc_dev->sio_mbox_irq = pci_irq_vector(pdev, 0x10 + 9 - BMC_MSI_IDX_BASE);
+
+	rc = request_irq(pci_bmc_dev->sio_mbox_irq, aspeed_pci_host_mbox_interrupt, IRQF_SHARED, "ASPEED SIO MBOX", pci_bmc_dev);
+	if (rc)
+		pr_err("host bmc device Unable to get IRQ %d\n", rc);
+
+	/* setup VUART */
+	memset(uart, 0, sizeof(uart));
+
+	for (i = 0; i < VUART_MAX_PARMS; i++) {
+		vuart_ioport[i] = 0x3F8 - (i * 0x100);
+		vuart_sirq[i] = 0x10 + 4 - i - BMC_MSI_IDX_BASE;
+		uart[i].port.flags = UPF_SKIP_TEST | UPF_BOOT_AUTOCONF | UPF_SHARE_IRQ;
+		uart[i].port.uartclk = 115200 * 16;
+
+		if (pci_bmc_dev->legency_irq)
+			uart[i].port.irq = pdev->irq;
+		else
+			uart[i].port.irq = pci_irq_vector(pdev, vuart_sirq[i]);
+		uart[i].port.dev = &pdev->dev;
+		uart[i].port.iotype = UPIO_MEM32;
+		uart[i].port.iobase = 0;
+		uart[i].port.mapbase = pci_bmc_dev->message_bar_base + (vuart_ioport[i] << 2);
+		uart[i].port.membase = 0;
+		uart[i].port.type = PORT_16550A;
+		uart[i].port.flags |= (UPF_IOREMAP | UPF_FIXED_PORT | UPF_FIXED_TYPE);
+		uart[i].port.regshift = 2;
+
+		rc = serial8250_register_8250_port(&uart[i]);
+		if (rc < 0) {
+			dev_err(dev, "cannot setup VUART@%xh over PCIe, rc=%d\n", vuart_ioport[i], rc);
+			goto out_unreg;
+		}
+	}
+
+	return 0;
+
+out_unreg:
+	misc_deregister(&pci_bmc_dev->miscdev);
+out_free1:
+	pci_release_region(pdev, 1);
+out_free0:
+	pci_release_region(pdev, 0);
+out_free:
+	kfree(pci_bmc_dev);
+out_err:
+	pci_disable_device(pdev);
+
+	return rc;
+
+}
+
+static void aspeed_pci_host_bmc_device_remove(struct pci_dev *pdev)
+{
+	struct aspeed_pci_bmc_dev *pci_bmc_dev = pci_get_drvdata(pdev);
+
+	free_irq(pdev->irq, pdev);
+	misc_deregister(&pci_bmc_dev->miscdev);
+	pci_release_regions(pdev);
+	kfree(pci_bmc_dev);
+	pci_disable_device(pdev);
+}
+
+/**
+ * This table holds the list of (VendorID,DeviceID) supported by this driver
+ *
+ */
+static struct pci_device_id aspeed_host_bmc_dev_pci_ids[] = {
+	{ PCI_DEVICE(0x1A03, 0x2402), },
+	{ 0, }
+};
+
+MODULE_DEVICE_TABLE(pci, aspeed_host_bmc_dev_pci_ids);
+
+static struct pci_driver aspeed_host_bmc_dev_driver = {
+	.name		= DRIVER_NAME,
+	.id_table	= aspeed_host_bmc_dev_pci_ids,
+	.probe		= aspeed_pci_host_bmc_device_probe,
+	.remove		= aspeed_pci_host_bmc_device_remove,
+};
+
+static int __init aspeed_host_bmc_device_init(void)
+{
+	int ret;
+
+	/* register pci driver */
+	ret = pci_register_driver(&aspeed_host_bmc_dev_driver);
+	if (ret < 0) {
+		pr_err("pci-driver: can't register pci driver\n");
+		return ret;
+	}
+
+	return 0;
+
+}
+
+static void aspeed_host_bmc_device_exit(void)
+{
+	/* unregister pci driver */
+	pci_unregister_driver(&aspeed_host_bmc_dev_driver);
+}
+
+late_initcall(aspeed_host_bmc_device_init);
+module_exit(aspeed_host_bmc_device_exit);
+
+MODULE_AUTHOR("Ryan Chen <ryan_chen@aspeedtech.com>");
+MODULE_DESCRIPTION("ASPEED Host BMC DEVICE Driver");
+MODULE_LICENSE("GPL");
diff --git a/drivers/soc/aspeed/aspeed-lpc-mbox.c b/drivers/soc/aspeed/aspeed-lpc-mbox.c
new file mode 100644
index 000000000000..a09ca6a175f7
--- /dev/null
+++ b/drivers/soc/aspeed/aspeed-lpc-mbox.c
@@ -0,0 +1,418 @@
+// SPDX-License-Identifier: GPL-2.0-or-later
+/*
+ * Copyright 2017 IBM Corporation
+ * Copyright 2021 Aspeed Technology Inc.
+ */
+#include <linux/interrupt.h>
+#include <linux/mfd/syscon.h>
+#include <linux/miscdevice.h>
+#include <linux/module.h>
+#include <linux/of_irq.h>
+#include <linux/of_device.h>
+#include <linux/platform_device.h>
+#include <linux/poll.h>
+#include <linux/regmap.h>
+#include <linux/slab.h>
+
+#define DEVICE_NAME	"aspeed-mbox"
+
+#define ASPEED_MBOX_DR(dr, n)	(dr + (n * 4))
+#define ASPEED_MBOX_STR(str, n)	(str + (n / 8) * 4)
+#define ASPEED_MBOX_BIE(bie, n)	(bie + (n / 8) * 4)
+#define ASPEED_MBOX_HIE(hie, n) (hie + (n / 8) * 4)
+
+#define ASPEED_MBOX_BCR_RECV	BIT(7)
+#define ASPEED_MBOX_BCR_MASK	BIT(1)
+#define ASPEED_MBOX_BCR_SEND	BIT(0)
+
+/* ioctl code */
+#define ASPEED_MBOX_IOCTL		0xA3
+#define ASPEED_MBOX_IOCTL_GET_SIZE	\
+	_IOR(ASPEED_MBOX_IOCTL, 0, struct aspeed_mbox_ioctl_data)
+
+struct aspeed_mbox_ioctl_data {
+	unsigned int data;
+};
+
+struct aspeed_mbox_model {
+	unsigned int dr_num;
+
+	/* offsets to the MBOX registers */
+	unsigned int dr;
+	unsigned int str;
+	unsigned int bcr;
+	unsigned int hcr;
+	unsigned int bie;
+	unsigned int hie;
+};
+
+struct aspeed_mbox {
+	struct miscdevice miscdev;
+	struct regmap *map;
+	unsigned int base;
+	wait_queue_head_t queue;
+	struct mutex mutex;
+	const struct aspeed_mbox_model *model;
+};
+
+static atomic_t aspeed_mbox_open_count = ATOMIC_INIT(0);
+
+static u8 aspeed_mbox_inb(struct aspeed_mbox *mbox, int reg)
+{
+	/*
+	 * The mbox registers are actually only one byte but are addressed
+	 * four bytes apart. The other three bytes are marked 'reserved',
+	 * they *should* be zero but lets not rely on it.
+	 * I am going to rely on the fact we can casually read/write to them...
+	 */
+	unsigned int val = 0xff; /* If regmap throws an error return 0xff */
+	int rc = regmap_read(mbox->map, mbox->base + reg, &val);
+
+	if (rc)
+		dev_err(mbox->miscdev.parent, "regmap_read() failed with "
+			"%d (reg: 0x%08x)\n", rc, reg);
+
+	return val & 0xff;
+}
+
+static void aspeed_mbox_outb(struct aspeed_mbox *mbox, u8 data, int reg)
+{
+	int rc = regmap_write(mbox->map, mbox->base + reg, data);
+
+	if (rc)
+		dev_err(mbox->miscdev.parent, "regmap_write() failed with "
+			"%d (data: %u reg: 0x%08x)\n", rc, data, reg);
+}
+
+static struct aspeed_mbox *file_mbox(struct file *file)
+{
+	return container_of(file->private_data, struct aspeed_mbox, miscdev);
+}
+
+static int aspeed_mbox_open(struct inode *inode, struct file *file)
+{
+	struct aspeed_mbox *mbox = file_mbox(file);
+	const struct aspeed_mbox_model *model = mbox->model;
+
+	if (atomic_inc_return(&aspeed_mbox_open_count) == 1) {
+		/*
+		 * Clear the interrupt status bit if it was left on and unmask
+		 * interrupts.
+		 * ASPEED_MBOX_BCR_RECV bit is W1C, this also unmasks in 1 step
+		 */
+		aspeed_mbox_outb(mbox, ASPEED_MBOX_BCR_RECV, model->bcr);
+		return 0;
+	}
+
+	atomic_dec(&aspeed_mbox_open_count);
+	return -EBUSY;
+}
+
+static ssize_t aspeed_mbox_read(struct file *file, char __user *buf,
+				size_t count, loff_t *ppos)
+{
+	struct aspeed_mbox *mbox = file_mbox(file);
+	const struct aspeed_mbox_model *model = mbox->model;
+	char __user *p = buf;
+	ssize_t ret;
+	int i;
+
+	if (!access_ok(buf, count))
+		return -EFAULT;
+
+	if (count + *ppos > model->dr_num)
+		return -EINVAL;
+
+	if (file->f_flags & O_NONBLOCK) {
+		if (!(aspeed_mbox_inb(mbox, model->bcr) &
+				ASPEED_MBOX_BCR_RECV))
+			return -EAGAIN;
+	} else if (wait_event_interruptible(mbox->queue,
+				aspeed_mbox_inb(mbox, model->bcr) &
+				ASPEED_MBOX_BCR_RECV)) {
+		return -ERESTARTSYS;
+	}
+
+	mutex_lock(&mbox->mutex);
+
+	for (i = *ppos; count > 0 && i < model->dr_num; i++) {
+		uint8_t reg = aspeed_mbox_inb(mbox, ASPEED_MBOX_DR(model->dr, i));
+
+		ret = __put_user(reg, p);
+		if (ret)
+			goto out_unlock;
+
+		p++;
+		count--;
+	}
+
+	/* ASPEED_MBOX_BCR_RECV bit is write to clear, this also unmasks in 1 step */
+	aspeed_mbox_outb(mbox, ASPEED_MBOX_BCR_RECV, model->bcr);
+	ret = p - buf;
+
+out_unlock:
+	mutex_unlock(&mbox->mutex);
+	return ret;
+}
+
+static ssize_t aspeed_mbox_write(struct file *file, const char __user *buf,
+				size_t count, loff_t *ppos)
+{
+	struct aspeed_mbox *mbox = file_mbox(file);
+	const struct aspeed_mbox_model *model = mbox->model;
+	const char __user *p = buf;
+	ssize_t ret;
+	char c;
+	int i;
+
+	if (!access_ok(buf, count))
+		return -EFAULT;
+
+	if (count + *ppos > model->dr_num)
+		return -EINVAL;
+
+	mutex_lock(&mbox->mutex);
+
+	for (i = *ppos; count > 0 && i < model->dr_num; i++) {
+		ret = __get_user(c, p);
+		if (ret)
+			goto out_unlock;
+
+		aspeed_mbox_outb(mbox, c, ASPEED_MBOX_DR(model->dr, i));
+		p++;
+		count--;
+	}
+
+	aspeed_mbox_outb(mbox, ASPEED_MBOX_BCR_SEND, model->bcr);
+	ret = p - buf;
+
+out_unlock:
+	mutex_unlock(&mbox->mutex);
+	return ret;
+}
+
+static __poll_t aspeed_mbox_poll(struct file *file, poll_table *wait)
+{
+	struct aspeed_mbox *mbox = file_mbox(file);
+	const struct aspeed_mbox_model *model = mbox->model;
+	__poll_t mask = 0;
+
+	poll_wait(file, &mbox->queue, wait);
+
+	if (aspeed_mbox_inb(mbox, model->bcr) & ASPEED_MBOX_BCR_RECV)
+		mask |= POLLIN;
+
+	return mask;
+}
+
+static int aspeed_mbox_release(struct inode *inode, struct file *file)
+{
+	atomic_dec(&aspeed_mbox_open_count);
+	return 0;
+}
+
+static long aspeed_mbox_ioctl(struct file *file, unsigned int cmd,
+				 unsigned long param)
+{
+	long ret = 0;
+	struct aspeed_mbox *mbox = file_mbox(file);
+	const struct aspeed_mbox_model *model = mbox->model;
+	struct aspeed_mbox_ioctl_data data;
+
+	switch (cmd) {
+	case ASPEED_MBOX_IOCTL_GET_SIZE:
+		data.data = model->dr_num;
+		if (copy_to_user((void __user *)param, &data, sizeof(data)))
+			ret = -EFAULT;
+		break;
+	default:
+		ret = -ENOTTY;
+		break;
+	}
+
+	return ret;
+}
+
+static const struct file_operations aspeed_mbox_fops = {
+	.owner		= THIS_MODULE,
+	.llseek		= no_seek_end_llseek,
+	.read		= aspeed_mbox_read,
+	.write		= aspeed_mbox_write,
+	.open		= aspeed_mbox_open,
+	.release	= aspeed_mbox_release,
+	.poll		= aspeed_mbox_poll,
+	.unlocked_ioctl	= aspeed_mbox_ioctl,
+};
+
+static irqreturn_t aspeed_mbox_irq(int irq, void *arg)
+{
+	struct aspeed_mbox *mbox = arg;
+	const struct aspeed_mbox_model *model = mbox->model;
+
+	if (!(aspeed_mbox_inb(mbox, model->bcr) & ASPEED_MBOX_BCR_RECV))
+		return IRQ_NONE;
+
+	/*
+	 * Leave the status bit set so that we know the data is for us,
+	 * clear it once it has been read.
+	 */
+
+	/* Mask it off, we'll clear it when we the data gets read */
+	aspeed_mbox_outb(mbox, ASPEED_MBOX_BCR_MASK, model->bcr);
+
+	wake_up(&mbox->queue);
+	return IRQ_HANDLED;
+}
+
+static int aspeed_mbox_config_irq(struct aspeed_mbox *mbox,
+		struct platform_device *pdev)
+{
+	const struct aspeed_mbox_model *model = mbox->model;
+	struct device *dev = &pdev->dev;
+	int i, rc, irq;
+
+	irq = irq_of_parse_and_map(dev->of_node, 0);
+	if (!irq)
+		return -ENODEV;
+
+	rc = devm_request_irq(dev, irq, aspeed_mbox_irq,
+			IRQF_SHARED, DEVICE_NAME, mbox);
+	if (rc < 0) {
+		dev_err(dev, "Unable to request IRQ %d\n", irq);
+		return rc;
+	}
+
+	/*
+	 * Disable all register based interrupts.
+	 */
+	for (i = 0; i < model->dr_num / 8; ++i)
+		aspeed_mbox_outb(mbox, 0x00, ASPEED_MBOX_BIE(model->bie, i));
+
+	/* These registers are write one to clear. Clear them. */
+	for (i = 0; i < model->dr_num / 8; ++i)
+		aspeed_mbox_outb(mbox, 0xff, ASPEED_MBOX_STR(model->str, i));
+
+	aspeed_mbox_outb(mbox, ASPEED_MBOX_BCR_RECV, model->bcr);
+	return 0;
+}
+
+static int aspeed_mbox_probe(struct platform_device *pdev)
+{
+	struct aspeed_mbox *mbox;
+	struct device *dev;
+	int rc;
+
+	dev = &pdev->dev;
+
+	mbox = devm_kzalloc(dev, sizeof(*mbox), GFP_KERNEL);
+	if (!mbox)
+		return -ENOMEM;
+
+	dev_set_drvdata(&pdev->dev, mbox);
+
+	rc = of_property_read_u32(dev->of_node, "reg", &mbox->base);
+	if (rc) {
+		dev_err(dev, "Couldn't read reg device tree property\n");
+		return rc;
+	}
+
+	mbox->model = of_device_get_match_data(dev);
+	if (IS_ERR(mbox->model)) {
+		dev_err(dev, "Couldn't get model data\n");
+		return -ENODEV;
+	}
+
+	mbox->map = syscon_node_to_regmap(
+			pdev->dev.parent->of_node);
+	if (IS_ERR(mbox->map)) {
+		dev_err(dev, "Couldn't get regmap\n");
+		return -ENODEV;
+	}
+
+	mutex_init(&mbox->mutex);
+	init_waitqueue_head(&mbox->queue);
+
+	mbox->miscdev.minor = MISC_DYNAMIC_MINOR;
+	mbox->miscdev.name = DEVICE_NAME;
+	mbox->miscdev.fops = &aspeed_mbox_fops;
+	mbox->miscdev.parent = dev;
+	rc = misc_register(&mbox->miscdev);
+	if (rc) {
+		dev_err(dev, "Unable to register device\n");
+		return rc;
+	}
+
+	rc = aspeed_mbox_config_irq(mbox, pdev);
+	if (rc) {
+		dev_err(dev, "Failed to configure IRQ\n");
+		misc_deregister(&mbox->miscdev);
+		return rc;
+	}
+
+	return 0;
+}
+
+static int aspeed_mbox_remove(struct platform_device *pdev)
+{
+	struct aspeed_mbox *mbox = dev_get_drvdata(&pdev->dev);
+
+	misc_deregister(&mbox->miscdev);
+
+	return 0;
+}
+
+static const struct aspeed_mbox_model ast2400_model = {
+	.dr_num = 16,
+	.dr	= 0x0,
+	.str = 0x40,
+	.bcr = 0x48,
+	.hcr = 0x4c,
+	.bie = 0x50,
+	.hie = 0x58,
+};
+
+static const struct aspeed_mbox_model ast2500_model = {
+	.dr_num = 16,
+	.dr	= 0x0,
+	.str = 0x40,
+	.bcr = 0x48,
+	.hcr = 0x4c,
+	.bie = 0x50,
+	.hie = 0x58,
+};
+
+static const struct aspeed_mbox_model ast2600_model = {
+	.dr_num = 32,
+	.dr	= 0x0,
+	.str = 0x80,
+	.bcr = 0x90,
+	.hcr = 0x94,
+	.bie = 0xa0,
+	.hie = 0xb0,
+};
+
+static const struct of_device_id aspeed_mbox_match[] = {
+	{ .compatible = "aspeed,ast2400-mbox",
+	  .data = &ast2400_model },
+	{ .compatible = "aspeed,ast2500-mbox",
+	  .data = &ast2500_model },
+	{ .compatible = "aspeed,ast2600-mbox",
+	  .data = &ast2600_model },
+	{ },
+};
+
+static struct platform_driver aspeed_mbox_driver = {
+	.driver = {
+		.name		= DEVICE_NAME,
+		.of_match_table = aspeed_mbox_match,
+	},
+	.probe = aspeed_mbox_probe,
+	.remove = aspeed_mbox_remove,
+};
+
+module_platform_driver(aspeed_mbox_driver);
+MODULE_DEVICE_TABLE(of, aspeed_mbox_match);
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Cyril Bur <cyrilbur@gmail.com>");
+MODULE_AUTHOR("Chia-Wei Wang <chiawei_wang@aspeedtech.com");
+MODULE_DESCRIPTION("Aspeed mailbox device driver");
diff --git a/drivers/soc/aspeed/aspeed-lpc-pcc.c b/drivers/soc/aspeed/aspeed-lpc-pcc.c
new file mode 100644
index 000000000000..88f08553b610
--- /dev/null
+++ b/drivers/soc/aspeed/aspeed-lpc-pcc.c
@@ -0,0 +1,636 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Copyright (C) ASPEED Technology Inc.
+ */
+#include <linux/bitops.h>
+#include <linux/interrupt.h>
+#include <linux/fs.h>
+#include <linux/kfifo.h>
+#include <linux/mfd/syscon.h>
+#include <linux/miscdevice.h>
+#include <linux/module.h>
+#include <linux/of.h>
+#include <linux/of_device.h>
+#include <linux/of_address.h>
+#include <linux/platform_device.h>
+#include <linux/poll.h>
+#include <linux/regmap.h>
+#include <linux/dma-mapping.h>
+
+#define DEVICE_NAME "aspeed-lpc-pcc"
+
+#define LHCR5	0x0b4
+#define LHCR6	0x0b8
+#define PCCR6	0x0c4
+#define LHCRA	0x0c8
+#define   LHCRA_PAT_A_LEN_MASK		GENMASK(18, 17)
+#define   LHCRA_PAT_A_LEN_SHIFT		17
+#define   LHCRA_PAT_A_WRITE		BIT(16)
+#define   LHCRA_PAT_A_ADDR_MASK		GENMASK(15, 0)
+#define   LHCRA_PAT_A_ADDR_SHIFT	0
+#define LHCRB	0x0cc
+#define   LHCRB_PAT_B_LEN_MASK		GENMASK(18, 17)
+#define   LHCRB_PAT_B_LEN_SHIFT		17
+#define   LHCRB_PAT_B_WRITE		BIT(16)
+#define   LHCRB_PAT_B_ADDR_MASK		GENMASK(15, 0)
+#define   LHCRB_PAT_B_ADDR_SHIFT	0
+#define PCCR4	0x0d0
+#define PCCR5	0x0d4
+#define PCCR0	0x130
+#define   PCCR0_EN_DMA_INT		BIT(31)
+#define   PCCR0_EN_PAT_B_INT		BIT(23)
+#define   PCCR0_EN_PAT_B		BIT(22)
+#define   PCCR0_EN_PAT_A_INT		BIT(21)
+#define   PCCR0_EN_PAT_A		BIT(20)
+#define   PCCR0_EN_DMA_MODE		BIT(14)
+#define   PCCR0_ADDR_SEL_MASK		GENMASK(13, 12)
+#define   PCCR0_ADDR_SEL_SHIFT		12
+#define   PCCR0_RX_TRIG_LVL_MASK	GENMASK(10, 8)
+#define   PCCR0_RX_TRIG_LVL_SHIFT	8
+#define   PCCR0_CLR_RX_FIFO		BIT(7)
+#define   PCCR0_MODE_SEL_MASK		GENMASK(5, 4)
+#define   PCCR0_MODE_SEL_SHIFT		4
+#define   PCCR0_EN_RX_OVR_INT		BIT(3)
+#define   PCCR0_EN_RX_TMOUT_INT		BIT(2)
+#define   PCCR0_EN_RX_AVAIL_INT		BIT(1)
+#define   PCCR0_EN			BIT(0)
+#define PCCR1	0x134
+#define   PCCR1_BASE_ADDR_MASK		GENMASK(15, 0)
+#define   PCCR1_BASE_ADDR_SHIFT		0
+#define   PCCR1_DONT_CARE_BITS_MASK	GENMASK(21, 16)
+#define   PCCR1_DONT_CARE_BITS_SHIFT	16
+#define PCCR2	0x138
+#define   PCCR2_PAT_B_RST		BIT(17)
+#define   PCCR2_PAT_B_INT		BIT(16)
+#define   PCCR2_PAT_A_RST		BIT(9)
+#define   PCCR2_PAT_A_INT		BIT(8)
+#define   PCCR2_DMA_DONE		BIT(4)
+#define   PCCR2_DATA_RDY		PCCR2_DMA_DONE
+#define   PCCR2_RX_OVR_INT		BIT(3)
+#define   PCCR2_RX_TMOUT_INT		BIT(2)
+#define   PCCR2_RX_AVAIL_INT		BIT(1)
+#define PCCR3	0x13c
+#define   PCCR3_FIFO_DATA_MASK		GENMASK(7, 0)
+
+#define PCC_DMA_MAX_BUFSZ	(PAGE_SIZE)
+#define PCC_MAX_PATNM		2
+
+enum pcc_fifo_threshold {
+	PCC_FIFO_THR_1_BYTE,
+	PCC_FIFO_THR_1_EIGHTH,
+	PCC_FIFO_THR_2_EIGHTH,
+	PCC_FIFO_THR_3_EIGHTH,
+	PCC_FIFO_THR_4_EIGHTH,
+	PCC_FIFO_THR_5_EIGHTH,
+	PCC_FIFO_THR_6_EIGHTH,
+	PCC_FIFO_THR_7_EIGHTH,
+	PCC_FIFO_THR_8_EIGHTH,
+};
+
+enum pcc_record_mode {
+	PCC_REC_1B,
+	PCC_REC_2B,
+	PCC_REC_4B,
+	PCC_REC_FULL,
+};
+
+enum pcc_port_hbits_select {
+	PCC_PORT_HBITS_SEL_NONE,
+	PCC_PORT_HBITS_SEL_45,
+	PCC_PORT_HBITS_SEL_67,
+	PCC_PORT_HBITS_SEL_89,
+};
+
+struct pcc_pattern {
+	u32 enable;
+	u32 pattern;
+	u32 len;
+	u32 write;
+	u32 port;
+};
+
+struct aspeed_pcc_dma {
+	u32 idx;
+	u32 addr;
+	u8 *virt;
+	u32 size;
+	u32 static_mem;
+	struct tasklet_struct tasklet;
+};
+
+struct aspeed_pcc {
+	struct device *dev;
+	struct regmap *regmap;
+	int irq;
+
+	u32 rec_mode;
+
+	u32 port;
+	u32 port_xbits;
+	u32 port_hbits_select;
+
+	u32 dma_mode;
+	struct aspeed_pcc_dma dma;
+
+	struct pcc_pattern pat_search[PCC_MAX_PATNM];
+
+	struct kfifo fifo;
+	wait_queue_head_t wq;
+
+	struct miscdevice misc_dev;
+};
+
+static inline bool is_pcc_enabled(struct aspeed_pcc *pcc)
+{
+	u32 reg;
+	if (regmap_read(pcc->regmap, PCCR0, &reg))
+		return false;
+	return (reg & PCCR0_EN) ? true : false;
+}
+
+static inline bool is_valid_rec_mode(u32 mode)
+{
+	return (mode > PCC_REC_FULL) ? false : true;
+}
+
+static inline bool is_valid_high_bits_select(u32 select)
+{
+	return (select > PCC_PORT_HBITS_SEL_89) ? false : true;
+}
+
+static ssize_t aspeed_pcc_file_read(struct file *file, char __user *buffer,
+		size_t count, loff_t *ppos)
+{
+	int rc;
+	ssize_t copied;
+
+	struct aspeed_pcc *pcc = container_of(
+			file->private_data,
+			struct aspeed_pcc,
+			misc_dev);
+
+	if (kfifo_is_empty(&pcc->fifo)) {
+		if (file->f_flags & O_NONBLOCK)
+			return -EAGAIN;
+		rc = wait_event_interruptible(pcc->wq,
+				!kfifo_is_empty(&pcc->fifo));
+		if (rc == -ERESTARTSYS)
+			return -EINTR;
+	}
+
+	rc = kfifo_to_user(&pcc->fifo, buffer, count, &copied);
+	return rc ? rc : copied;
+}
+
+static __poll_t aspeed_pcc_file_poll(struct file *file,
+		struct poll_table_struct *pt)
+{
+	struct aspeed_pcc *pcc = container_of(
+			file->private_data,
+			struct aspeed_pcc,
+			misc_dev);
+
+	poll_wait(file, &pcc->wq, pt);
+	return !kfifo_is_empty(&pcc->fifo) ? POLLIN : 0;
+}
+
+static const struct file_operations pcc_fops = {
+	.owner = THIS_MODULE,
+	.read = aspeed_pcc_file_read,
+	.poll = aspeed_pcc_file_poll,
+};
+
+static void aspeed_pcc_dma_tasklet(unsigned long arg)
+{
+	u32 reg;
+	u32 pre_dma_idx;
+	u32 cur_dma_idx;
+	u8 has_data = 0;
+
+	struct aspeed_pcc *pcc = (struct aspeed_pcc*)arg;
+	struct kfifo *fifo = &pcc->fifo;
+
+	if (!kfifo_initialized(fifo))
+		return;
+
+	if (regmap_read(pcc->regmap, PCCR6, &reg))
+		return;
+
+	cur_dma_idx = reg & (PCC_DMA_MAX_BUFSZ - 1);
+	pre_dma_idx = pcc->dma.idx;
+	has_data = (pre_dma_idx == cur_dma_idx) ? false : true;
+
+	do {
+		/* kick the oldest one if full */
+		if (kfifo_is_full(fifo))
+			kfifo_skip(fifo);
+		kfifo_put(fifo, pcc->dma.virt[pre_dma_idx]);
+		pre_dma_idx = (pre_dma_idx + 1) % PCC_DMA_MAX_BUFSZ;
+	} while (pre_dma_idx != cur_dma_idx);
+
+	if (has_data)
+		wake_up_interruptible(&pcc->wq);
+
+	pcc->dma.idx = cur_dma_idx;
+}
+
+static irqreturn_t aspeed_pcc_isr(int irq, void *arg)
+{
+	u32 val;
+	irqreturn_t ret = IRQ_NONE;
+	struct aspeed_pcc *pcc = (struct aspeed_pcc*)arg;
+
+	if (regmap_read(pcc->regmap, PCCR2, &val))
+		return ret;
+
+	if (val & PCCR2_PAT_B_INT) {
+		dev_info(pcc->dev, "pattern search B interrupt\n");
+		regmap_write_bits(pcc->regmap, PCCR2,
+			PCCR2_PAT_B_INT, PCCR2_PAT_B_INT);
+		ret = IRQ_HANDLED;
+	}
+
+	if (val & PCCR2_PAT_A_INT) {
+		dev_info(pcc->dev, "pattern search A interrupt\n");
+		regmap_write_bits(pcc->regmap, PCCR2,
+			PCCR2_PAT_A_INT, PCCR2_PAT_A_INT);
+		ret = IRQ_HANDLED;
+	}
+
+	if (val & PCCR2_RX_OVR_INT) {
+		dev_warn(pcc->dev, "RX FIFO overrun\n");
+		regmap_write_bits(pcc->regmap, PCCR2,
+			PCCR2_RX_OVR_INT, PCCR2_RX_OVR_INT);
+		ret = IRQ_HANDLED;
+	}
+
+	if (val & (PCCR2_DMA_DONE | PCCR2_RX_TMOUT_INT | PCCR2_RX_AVAIL_INT)) {
+		if (pcc->dma_mode) {
+			regmap_write_bits(pcc->regmap, PCCR2,
+					PCCR2_DMA_DONE, PCCR2_DMA_DONE);
+			tasklet_schedule(&pcc->dma.tasklet);
+		}
+		else {
+			do {
+				if (regmap_read(pcc->regmap, PCCR3, &val))
+					break;
+				if (kfifo_is_full(&pcc->fifo))
+					kfifo_skip(&pcc->fifo);
+				kfifo_put(&pcc->fifo, val & PCCR3_FIFO_DATA_MASK);
+
+				if (regmap_read(pcc->regmap, PCCR2, &val))
+					break;
+			} while (val & PCCR2_DATA_RDY);
+
+			wake_up_interruptible(&pcc->wq);
+		}
+		ret = IRQ_HANDLED;
+	}
+
+	return ret;
+}
+
+static void aspeed_pcc_config(struct aspeed_pcc *pcc)
+{
+	struct pcc_pattern* pat_search = pcc->pat_search;
+
+	/* record mode */
+	regmap_update_bits(pcc->regmap, PCCR0,
+			PCCR0_MODE_SEL_MASK,
+			pcc->rec_mode << PCCR0_MODE_SEL_SHIFT);
+
+	/* port address */
+	regmap_update_bits(pcc->regmap, PCCR1,
+			PCCR1_BASE_ADDR_MASK,
+			pcc->port << PCCR1_BASE_ADDR_SHIFT);
+
+	/* port address high bits selection or parser control */
+	regmap_update_bits(pcc->regmap, PCCR0,
+			PCCR0_ADDR_SEL_MASK,
+			pcc->port_hbits_select << PCCR0_ADDR_SEL_SHIFT);
+
+	/* port address dont care bits */
+	regmap_update_bits(pcc->regmap, PCCR1,
+			PCCR1_DONT_CARE_BITS_MASK,
+			pcc->port_xbits << PCCR1_DONT_CARE_BITS_SHIFT);
+
+	/* pattern search state reset */
+	regmap_write_bits(pcc->regmap, PCCR2,
+			PCCR2_PAT_B_RST | PCCR2_PAT_A_RST,
+			PCCR2_PAT_B_RST | PCCR2_PAT_A_RST);
+
+	/* pattern A to search */
+	regmap_write(pcc->regmap, LHCR5, pat_search[0].pattern);
+	regmap_update_bits(pcc->regmap, LHCRA,
+			LHCRA_PAT_A_LEN_MASK,
+			(pat_search[0].len - 1) << LHCRA_PAT_A_LEN_SHIFT);
+	regmap_update_bits(pcc->regmap, LHCRA,
+			LHCRA_PAT_A_WRITE,
+			(pat_search[0].write) ? LHCRA_PAT_A_WRITE : 0);
+	regmap_update_bits(pcc->regmap, LHCRA,
+			LHCRA_PAT_A_ADDR_MASK,
+			pat_search[0].port << LHCRA_PAT_A_ADDR_SHIFT);
+	regmap_update_bits(pcc->regmap, PCCR0,
+			PCCR0_EN_PAT_A_INT | PCCR0_EN_PAT_A,
+			(pat_search[0].enable) ? PCCR0_EN_PAT_A_INT | PCCR0_EN_PAT_A : 0);
+
+	/* pattern B to search */
+	regmap_write(pcc->regmap, LHCR6, pat_search[1].pattern);
+	regmap_update_bits(pcc->regmap, LHCRB,
+			LHCRB_PAT_B_LEN_MASK,
+			(pat_search[1].len - 1) << LHCRB_PAT_B_LEN_SHIFT);
+	regmap_update_bits(pcc->regmap, LHCRB,
+			LHCRB_PAT_B_WRITE,
+			(pat_search[1].write) ? LHCRB_PAT_B_WRITE : 0);
+	regmap_update_bits(pcc->regmap, LHCRB,
+			LHCRB_PAT_B_ADDR_MASK,
+			pat_search[1].port << LHCRB_PAT_B_ADDR_SHIFT);
+	regmap_update_bits(pcc->regmap, PCCR0,
+			PCCR0_EN_PAT_B_INT | PCCR0_EN_PAT_B,
+			PCCR0_EN_PAT_B_INT | PCCR0_EN_PAT_B);
+	regmap_update_bits(pcc->regmap, PCCR0,
+			PCCR0_EN_PAT_B_INT | PCCR0_EN_PAT_B,
+			(pat_search[1].enable) ? PCCR0_EN_PAT_B_INT | PCCR0_EN_PAT_B : 0);
+
+	/* DMA address and size (4-bytes unit) */
+	if (pcc->dma_mode) {
+		regmap_write(pcc->regmap, PCCR4, pcc->dma.addr);
+		regmap_write(pcc->regmap, PCCR5, pcc->dma.size / 4);
+	}
+}
+
+static int aspeed_pcc_enable(struct aspeed_pcc *pcc, struct device *dev)
+{
+	int rc;
+
+	if (pcc->dma_mode) {
+		/* map reserved memory or allocate a new one for DMA use */
+		if (pcc->dma.static_mem) {
+			if (pcc->dma.size > PCC_DMA_MAX_BUFSZ) {
+				rc = -EINVAL;
+				goto err_ret;
+			}
+
+			pcc->dma.virt = ioremap(pcc->dma.addr,
+							  pcc->dma.size);
+			if (pcc->dma.virt == NULL) {
+				rc = -ENOMEM;
+				goto err_ret;
+			}
+		}
+		else {
+			pcc->dma.size = PCC_DMA_MAX_BUFSZ;
+			pcc->dma.virt = dma_alloc_coherent(dev,
+					pcc->dma.size,
+					&pcc->dma.addr,
+					GFP_KERNEL);
+			if (pcc->dma.virt == NULL) {
+				rc = -ENOMEM;
+				goto err_ret;
+			}
+		}
+	}
+
+	rc = kfifo_alloc(&pcc->fifo, PAGE_SIZE, GFP_KERNEL);
+	if (rc)
+		goto err_free_dma;
+
+	pcc->misc_dev.parent = dev;
+	pcc->misc_dev.name = devm_kasprintf(dev, GFP_KERNEL, "%s", DEVICE_NAME);
+	pcc->misc_dev.fops = &pcc_fops;
+	rc = misc_register(&pcc->misc_dev);
+	if (rc)
+		goto err_free_kfifo;
+
+	aspeed_pcc_config(pcc);
+
+	/* skip FIFO cleanup if already enabled */
+	if (!is_pcc_enabled(pcc))
+		regmap_write_bits(pcc->regmap, PCCR0,
+				PCCR0_CLR_RX_FIFO, PCCR0_CLR_RX_FIFO);
+
+	if (pcc->dma_mode) {
+		regmap_update_bits(pcc->regmap, PCCR0,
+			PCCR0_EN_DMA_INT | PCCR0_EN_DMA_MODE,
+			PCCR0_EN_DMA_INT | PCCR0_EN_DMA_MODE);
+	}
+	else {
+		regmap_update_bits(pcc->regmap, PCCR0,
+			PCCR0_RX_TRIG_LVL_MASK,
+			PCC_FIFO_THR_4_EIGHTH << PCCR0_RX_TRIG_LVL_SHIFT);
+		regmap_update_bits(pcc->regmap, PCCR0,
+			PCCR0_EN_RX_OVR_INT | PCCR0_EN_RX_TMOUT_INT | PCCR0_EN_RX_AVAIL_INT,
+			PCCR0_EN_RX_OVR_INT | PCCR0_EN_RX_TMOUT_INT | PCCR0_EN_RX_AVAIL_INT);
+	}
+
+	regmap_update_bits(pcc->regmap, PCCR0, PCCR0_EN, PCCR0_EN);
+	return 0;
+
+err_free_kfifo:
+	kfifo_free(&pcc->fifo);
+err_free_dma:
+	if (pcc->dma_mode) {
+		if (pcc->dma.static_mem)
+			iounmap(pcc->dma.virt);
+		else
+			dma_free_coherent(dev, pcc->dma.size,
+					pcc->dma.virt, pcc->dma.addr);
+	}
+err_ret:
+	return rc;
+}
+
+static int aspeed_pcc_disable(struct aspeed_pcc *pcc, struct device *dev)
+{
+	regmap_update_bits(pcc->regmap, PCCR0,
+		PCCR0_EN_DMA_INT
+		| PCCR0_EN_RX_OVR_INT
+		| PCCR0_EN_RX_TMOUT_INT
+		| PCCR0_EN_RX_AVAIL_INT
+		| PCCR0_EN_DMA_MODE
+		| PCCR0_EN,
+		0);
+
+	if (pcc->dma.static_mem)
+		iounmap(pcc->dma.virt);
+	else
+		dma_free_coherent(dev, pcc->dma.size,
+				pcc->dma.virt, pcc->dma.addr);
+
+	misc_deregister(&pcc->misc_dev);
+	kfifo_free(&pcc->fifo);
+
+	return 0;
+}
+
+static int aspeed_pcc_probe(struct platform_device *pdev)
+{
+	int rc;
+
+	struct aspeed_pcc *pcc;
+
+	struct device *dev = &pdev->dev;
+	struct device_node *node;
+
+	struct resource res;
+
+	pcc = devm_kzalloc(&pdev->dev, sizeof(*pcc), GFP_KERNEL);
+	if (!pcc) {
+		dev_err(dev, "failed to allocate memory\n");
+		return -ENOMEM;
+	}
+
+	pcc->regmap = syscon_node_to_regmap(pdev->dev.parent->of_node);
+	if (IS_ERR(pcc->regmap)) {
+		dev_err(dev, "failed to get regmap\n");
+		return -ENODEV;
+	}
+
+	rc = of_property_read_u32(dev->of_node, "port-addr", &pcc->port);
+	if (rc) {
+		dev_err(dev, "failed to get port base address\n");
+		return rc;
+	}
+
+	pcc->dma_mode = of_property_read_bool(dev->of_node, "dma-mode");
+	if (pcc->dma_mode) {
+		/*
+		 * optional, reserved memory for the DMA buffer
+		 * if not specified, the DMA buffer is allocated
+		 * dynamically.
+		 */
+		node = of_parse_phandle(dev->of_node, "memory-region", 0);
+		if (node) {
+			rc = of_address_to_resource(node, 0, &res);
+			if (rc) {
+				dev_err(dev, "failed to get reserved memory region\n");
+				return -ENOMEM;
+			}
+			pcc->dma.addr = res.start;
+			pcc->dma.size = resource_size(&res);
+			pcc->dma.static_mem = 1;
+			of_node_put(node);
+		}
+	}
+
+	/* optional, by default: 0 -> 1-Byte mode */
+	of_property_read_u32(dev->of_node, "rec-mode", &pcc->rec_mode);
+	if (!is_valid_rec_mode(pcc->rec_mode)) {
+		dev_err(dev, "invalid record mode: %u\n",
+				pcc->rec_mode);
+		return -EINVAL;
+	}
+
+	/* optional, by default: 0 -> no don't care bits */
+	of_property_read_u32(dev->of_node, "port-addr-xbits", &pcc->port_xbits);
+
+	/*
+	 * optional, by default: 0 -> no high address bits
+	 *
+	 * Note that when record mode is set to 1-Byte, this
+	 * property is ignored and the corresponding HW bits
+	 * behave as read/write cycle parser control with the
+	 * value set to 0b11
+	 */
+	if (pcc->rec_mode) {
+		of_property_read_u32(dev->of_node, "port-addr-hbits-select", &pcc->port_hbits_select);
+		if (!is_valid_high_bits_select(pcc->port_hbits_select)) {
+			dev_err(dev, "invalid high address bits selection: %u\n",
+				pcc->port_hbits_select);
+			return -EINVAL;
+		}
+	}
+	else
+		pcc->port_hbits_select = 0x3;
+
+	/* optional, pattern search A */
+	if (of_property_read_bool(dev->of_node, "pattern-a-en")) {
+		of_property_read_u32(dev->of_node, "pattern-a", &pcc->pat_search[0].pattern);
+		of_property_read_u32(dev->of_node, "pattern-a-len", &pcc->pat_search[0].len);
+		of_property_read_u32(dev->of_node, "pattern-a-write", &pcc->pat_search[0].write);
+		of_property_read_u32(dev->of_node, "pattern-a-port", &pcc->pat_search[0].port);
+		pcc->pat_search[0].enable = 1;
+	}
+
+	/* optional, pattern search B */
+	if (of_property_read_bool(dev->of_node, "pattern-b-en")) {
+		of_property_read_u32(dev->of_node, "pattern-b", &pcc->pat_search[1].pattern);
+		of_property_read_u32(dev->of_node, "pattern-b-len", &pcc->pat_search[1].len);
+		of_property_read_u32(dev->of_node, "pattern-b-write", &pcc->pat_search[1].write);
+		of_property_read_u32(dev->of_node, "pattern-b-port", &pcc->pat_search[1].port);
+		pcc->pat_search[1].enable = 1;
+	}
+
+	pcc->irq = platform_get_irq(pdev, 0);
+	if (!pcc->irq) {
+		dev_err(dev, "failed to get IRQ\n");
+		return -ENODEV;
+	}
+
+	/*
+	 * as PCC may have been enabled in early stages, we
+	 * need to disable interrupts before requesting IRQ
+	 * to prevent kernel crash
+	 */
+	regmap_update_bits(pcc->regmap, PCCR0,
+			PCCR0_EN_DMA_INT
+			| PCCR0_EN_PAT_A_INT
+			| PCCR0_EN_PAT_B_INT
+			| PCCR0_EN_RX_OVR_INT
+			| PCCR0_EN_RX_TMOUT_INT
+			| PCCR0_EN_RX_AVAIL_INT,
+			0);
+
+	rc = devm_request_irq(dev, pcc->irq, aspeed_pcc_isr,
+			IRQF_SHARED, DEVICE_NAME, pcc);
+	if (rc < 0) {
+		dev_err(dev, "failed to request IRQ handler\n");
+		return rc;
+	}
+
+	tasklet_init(&pcc->dma.tasklet, aspeed_pcc_dma_tasklet,
+			(unsigned long)pcc);
+
+	init_waitqueue_head(&pcc->wq);
+
+	rc = aspeed_pcc_enable(pcc, dev);
+	if (rc) {
+		dev_err(dev, "failed to enable PCC\n");
+		return rc;
+	}
+
+	pcc->dev = dev;
+	dev_set_drvdata(&pdev->dev, pcc);
+
+	dev_info(dev, "module loaded\n");
+
+	return 0;
+}
+
+static int aspeed_pcc_remove(struct platform_device *pdev)
+{
+	struct device *dev = &pdev->dev;
+	struct aspeed_pcc *pcc = dev_get_drvdata(dev);
+	aspeed_pcc_disable(pcc, dev);
+	return 0;
+}
+
+static const struct of_device_id aspeed_pcc_table[] = {
+	{ .compatible = "aspeed,ast2500-lpc-pcc" },
+	{ .compatible = "aspeed,ast2600-lpc-pcc" },
+};
+
+static struct platform_driver aspeed_pcc_driver = {
+	.driver = {
+		.name = "aspeed-pcc",
+		.of_match_table = aspeed_pcc_table,
+	},
+	.probe = aspeed_pcc_probe,
+	.remove = aspeed_pcc_remove,
+};
+
+module_platform_driver(aspeed_pcc_driver);
+
+MODULE_AUTHOR("Chia-Wei Wang <chiawei_wang@aspeedtech.com>");
+MODULE_LICENSE("GPL");
+MODULE_DESCRIPTION("Driver for Aspeed Post Code Capture");
diff --git a/drivers/soc/aspeed/aspeed-lpc-snoop.c b/drivers/soc/aspeed/aspeed-lpc-snoop.c
index eceeaf8dfbeb..833654e82e78 100644
--- a/drivers/soc/aspeed/aspeed-lpc-snoop.c
+++ b/drivers/soc/aspeed/aspeed-lpc-snoop.c
@@ -11,7 +11,6 @@
  */
 
 #include <linux/bitops.h>
-#include <linux/clk.h>
 #include <linux/interrupt.h>
 #include <linux/fs.h>
 #include <linux/kfifo.h>
@@ -67,7 +66,6 @@ struct aspeed_lpc_snoop_channel {
 struct aspeed_lpc_snoop {
 	struct regmap		*regmap;
 	int			irq;
-	struct clk		*clk;
 	struct aspeed_lpc_snoop_channel chan[NUM_SNOOP_CHANNELS];
 };
 
@@ -167,7 +165,7 @@ static int aspeed_lpc_snoop_config_irq(struct aspeed_lpc_snoop *lpc_snoop,
 	int rc;
 
 	lpc_snoop->irq = platform_get_irq(pdev, 0);
-	if (!lpc_snoop->irq)
+	if (lpc_snoop->irq < 0)
 		return -ENODEV;
 
 	rc = devm_request_irq(dev, lpc_snoop->irq,
@@ -293,19 +291,6 @@ static int aspeed_lpc_snoop_probe(struct platform_device *pdev)
 		return -ENODEV;
 	}
 
-	lpc_snoop->clk = devm_clk_get(dev, NULL);
-	if (IS_ERR(lpc_snoop->clk)) {
-		rc = PTR_ERR(lpc_snoop->clk);
-		if (rc != -EPROBE_DEFER)
-			dev_err(dev, "couldn't get clock\n");
-		return rc;
-	}
-	rc = clk_prepare_enable(lpc_snoop->clk);
-	if (rc) {
-		dev_err(dev, "couldn't enable clock\n");
-		return rc;
-	}
-
 	rc = aspeed_lpc_snoop_config_irq(lpc_snoop, pdev);
 	if (rc)
 		goto err;
@@ -327,8 +312,6 @@ static int aspeed_lpc_snoop_probe(struct platform_device *pdev)
 	return 0;
 
 err:
-	clk_disable_unprepare(lpc_snoop->clk);
-
 	return rc;
 }
 
@@ -340,8 +323,6 @@ static int aspeed_lpc_snoop_remove(struct platform_device *pdev)
 	aspeed_lpc_disable_snoop(lpc_snoop, 0);
 	aspeed_lpc_disable_snoop(lpc_snoop, 1);
 
-	clk_disable_unprepare(lpc_snoop->clk);
-
 	return 0;
 }
 
diff --git a/drivers/soc/aspeed/aspeed-mctp.c b/drivers/soc/aspeed/aspeed-mctp.c
new file mode 100644
index 000000000000..d1c4b94951ee
--- /dev/null
+++ b/drivers/soc/aspeed/aspeed-mctp.c
@@ -0,0 +1,2298 @@
+// SPDX-License-Identifier: GPL-2.0
+// Copyright (c) 2020, Intel Corporation.
+
+#include <linux/aspeed-mctp.h>
+#include <linux/bitfield.h>
+#include <linux/dma-mapping.h>
+#include <linux/interrupt.h>
+#include <linux/init.h>
+#include <linux/io.h>
+#include <linux/kernel.h>
+#include <linux/list.h>
+#include <linux/list_sort.h>
+#include <linux/mfd/syscon.h>
+#include <linux/miscdevice.h>
+#include <linux/mm.h>
+#include <linux/module.h>
+#include <linux/mutex.h>
+#include <linux/of_platform.h>
+#include <linux/pci.h>
+#include <linux/poll.h>
+#include <linux/ptr_ring.h>
+#include <linux/regmap.h>
+#include <linux/reset.h>
+#include <linux/slab.h>
+#include <linux/swab.h>
+#include <linux/uaccess.h>
+#include <linux/workqueue.h>
+
+#include <uapi/linux/aspeed-mctp.h>
+
+/* AST2600 MCTP Controller registers */
+#define ASPEED_MCTP_CTRL	0x000
+#define  TX_CMD_TRIGGER		BIT(0)
+#define  RX_CMD_READY		BIT(4)
+#define  MATCHING_EID		BIT(9)
+
+#define ASPEED_MCTP_TX_CMD	0x004
+#define ASPEED_MCTP_RX_CMD	0x008
+
+#define ASPEED_MCTP_INT_STS	0x00c
+#define ASPEED_MCTP_INT_EN	0x010
+#define  TX_CMD_SENT_INT	BIT(0)
+#define  TX_CMD_LAST_INT	BIT(1)
+#define  TX_CMD_WRONG_INT	BIT(2)
+#define  RX_CMD_RECEIVE_INT	BIT(8)
+#define  RX_CMD_NO_MORE_INT	BIT(9)
+
+#define ASPEED_MCTP_EID		0x014
+#define  MEMORY_SPACE_MAPPING	GENMASK(31, 28)
+#define ASPEED_MCTP_OBFF_CTRL	0x018
+
+#define ASPEED_MCTP_ENGINE_CTRL		0x01c
+#define  TX_MAX_PAYLOAD_SIZE_SHIFT	0
+#define  TX_MAX_PAYLOAD_SIZE_MASK	GENMASK(1, TX_MAX_PAYLOAD_SIZE_SHIFT)
+#define  TX_MAX_PAYLOAD_SIZE(x) \
+	(((x) << TX_MAX_PAYLOAD_SIZE_SHIFT) & TX_MAX_PAYLOAD_SIZE_MASK)
+#define  RX_MAX_PAYLOAD_SIZE_SHIFT	4
+#define  RX_MAX_PAYLOAD_SIZE_MASK	GENMASK(5, RX_MAX_PAYLOAD_SIZE_SHIFT)
+#define  RX_MAX_PAYLOAD_SIZE(x) \
+	(((x) << RX_MAX_PAYLOAD_SIZE_SHIFT) & RX_MAX_PAYLOAD_SIZE_MASK)
+#define  FIFO_LAYOUT_SHIFT		8
+#define  FIFO_LAYOUT_MASK		GENMASK(9, FIFO_LAYOUT_SHIFT)
+#define  FIFO_LAYOUT(x) \
+	(((x) << FIFO_LAYOUT_SHIFT) & FIFO_LAYOUT_MASK)
+
+#define ASPEED_MCTP_RX_BUF_ADDR		0x08
+#define ASPEED_MCTP_RX_BUF_SIZE		0x024
+#define ASPEED_MCTP_RX_BUF_RD_PTR	0x028
+#define  UPDATE_RX_RD_PTR		BIT(31)
+#define  RX_BUF_RD_PTR_MASK		GENMASK(11, 0)
+#define ASPEED_MCTP_RX_BUF_WR_PTR	0x02c
+#define  RX_BUF_WR_PTR_MASK		GENMASK(11, 0)
+
+#define ASPEED_MCTP_TX_BUF_ADDR		0x04
+#define ASPEED_MCTP_TX_BUF_SIZE		0x034
+#define ASPEED_MCTP_TX_BUF_RD_PTR	0x038
+#define  UPDATE_TX_RD_PTR		BIT(31)
+#define  TX_BUF_RD_PTR_MASK		GENMASK(11, 0)
+#define ASPEED_MCTP_TX_BUF_WR_PTR	0x03c
+#define  TX_BUF_WR_PTR_MASK		GENMASK(11, 0)
+
+#define ADDR_LEN	GENMASK(26, 0)
+#define DATA_ADDR(x)	(((x) >> 4) & ADDR_LEN)
+
+/* TX command */
+#define TX_LAST_CMD		BIT(31)
+#define TX_DATA_ADDR_SHIFT	4
+#define TX_DATA_ADDR_MASK	GENMASK(30, TX_DATA_ADDR_SHIFT)
+#define TX_DATA_ADDR(x) \
+	((DATA_ADDR(x) << TX_DATA_ADDR_SHIFT) & TX_DATA_ADDR_MASK)
+#define TX_RESERVED_1_MASK	GENMASK(1, 0) /* must be 1 */
+#define TX_RESERVED_1		1
+#define TX_STOP_AFTER_CMD	BIT(16)
+#define TX_INTERRUPT_AFTER_CMD	BIT(15)
+#define TX_PACKET_SIZE_SHIFT	2
+#define TX_PACKET_SIZE_MASK	GENMASK(12, TX_PACKET_SIZE_SHIFT)
+#define TX_PACKET_SIZE(x) \
+	(((x) << TX_PACKET_SIZE_SHIFT) & TX_PACKET_SIZE_MASK)
+#define TX_RESERVED_0_MASK	GENMASK(1, 0) /* MBZ */
+#define TX_RESERVED_0		0
+
+/* RX command */
+#define RX_INTERRUPT_AFTER_CMD	BIT(2)
+#define RX_DATA_ADDR_SHIFT	4
+#define RX_DATA_ADDR_MASK	GENMASK(30, RX_DATA_ADDR_SHIFT)
+#define RX_DATA_ADDR(x) \
+	((DATA_ADDR(x) << RX_DATA_ADDR_SHIFT) & RX_DATA_ADDR_MASK)
+
+#define ADDR_LEN_2500	GENMASK(23, 0)
+#define DATA_ADDR_2500(x)	(((x) >> 7) & ADDR_LEN_2500)
+
+/* TX command for ast2500 */
+#define TX_DATA_ADDR_MASK_2500	GENMASK(30, 8)
+#define TX_DATA_ADDR_2500(x) \
+	FIELD_PREP(TX_DATA_ADDR_MASK_2500, DATA_ADDR_2500(x))
+#define TX_PACKET_SIZE_2500(x) \
+	FIELD_PREP(GENMASK(11, 2), DATA_ADDR_2500(x))
+#define TX_PACKET_DEST_EID	GENMASK(7, 0)
+#define TX_PACKET_TARGET_ID	GENMASK(31, 16)
+#define TX_PACKET_ROUTING_TYPE	BIT(14)
+#define TX_PACKET_TAG_OWNER	BIT(13)
+#define TX_PACKET_PADDING_LEN	GENMASK(1, 0)
+
+/* Rx command for ast2500 */
+#define RX_LAST_CMD		BIT(31)
+#define RX_DATA_ADDR_MASK_2500	GENMASK(29, 7)
+#define RX_DATA_ADDR_2500(x) \
+	FIELD_PREP(RX_DATA_ADDR_MASK_2500, DATA_ADDR_2500(x))
+#define RX_PACKET_SIZE		GENMASK(30, 24)
+#define RX_PACKET_SRC_EID	GENMASK(23, 16)
+#define RX_PACKET_ROUTING_TYPE	GENMASK(15, 14)
+#define RX_PACKET_TAG_OWNER	BIT(13)
+#define RX_PACKET_SEQ_NUMBER	GENMASK(12, 11)
+#define RX_PACKET_MSG_TAG	GENMASK(10, 8)
+#define RX_PACKET_SOM		BIT(7)
+#define RX_PACKET_EOM		BIT(6)
+#define RX_PACKET_PADDING_LEN	GENMASK(5, 4)
+
+/* HW buffer sizes */
+#define TX_PACKET_COUNT		48
+#define RX_PACKET_COUNT		96
+#if (RX_PACKET_COUNT % 4 != 0)
+#error The Rx buffer size should be 4-aligned.
+#error 1.Make runaway wrap boundary can be determined in Ast2600 A1/A2.
+#error 2.Fix the runaway read pointer bug in Ast2600 A3.
+#endif
+#define TX_MAX_PACKET_COUNT	(TX_BUF_RD_PTR_MASK + 1)
+#define RX_MAX_PACKET_COUNT	(RX_BUF_RD_PTR_MASK + 1)
+
+#define TX_CMD_BUF_SIZE \
+	PAGE_ALIGN(TX_PACKET_COUNT * sizeof(struct aspeed_mctp_tx_cmd))
+
+/* Per client packet cache sizes */
+#define RX_RING_COUNT		64
+#define TX_RING_COUNT		64
+
+/* PCIe Host Controller registers */
+#define ASPEED_PCIE_MISC_STS_1	0x0c4
+
+/* PCI address definitions */
+#define PCI_DEV_NUM_MASK	GENMASK(4, 0)
+#define PCI_BUS_NUM_SHIFT	5
+#define PCI_BUS_NUM_MASK	GENMASK(12, PCI_BUS_NUM_SHIFT)
+#define GET_PCI_DEV_NUM(x)	((x) & PCI_DEV_NUM_MASK)
+#define GET_PCI_BUS_NUM(x)	(((x) & PCI_BUS_NUM_MASK) >> PCI_BUS_NUM_SHIFT)
+
+/* MCTP header definitions */
+#define MCTP_HDR_SRC_EID_OFFSET		14
+#define MCTP_HDR_TAG_OFFSET		15
+#define MCTP_HDR_SOM			BIT(7)
+#define MCTP_HDR_EOM			BIT(6)
+#define MCTP_HDR_SOM_EOM		(MCTP_HDR_SOM | MCTP_HDR_EOM)
+#define MCTP_HDR_TYPE_OFFSET		16
+#define MCTP_HDR_TYPE_CONTROL		0
+#define MCTP_HDR_TYPE_VDM_PCI		0x7e
+#define MCTP_HDR_TYPE_SPDM		0x5
+#define MCTP_HDR_TYPE_BASE_LAST		MCTP_HDR_TYPE_SPDM
+#define MCTP_HDR_VENDOR_OFFSET		17
+#define MCTP_HDR_VDM_TYPE_OFFSET	19
+
+/* MCTP header DW little endian mask definitions */
+/* 0th DW */
+#define MCTP_HDR_DW_LE_ROUTING_TYPE	GENMASK(26, 24)
+#define MCTP_HDR_DW_LE_PACKET_SIZE	GENMASK(9, 0)
+/* 1st DW */
+#define MCTP_HDR_DW_LE_PADDING_LEN	GENMASK(13, 12)
+/* 2nd DW */
+#define MCTP_HDR_DW_LE_TARGET_ID	GENMASK(31, 16)
+/* 3rd DW */
+#define MCTP_HDR_DW_LE_TAG_OWNER	BIT(3)
+#define MCTP_HDR_DW_LE_DEST_EID		GENMASK(23, 16)
+
+#define ASPEED_MCTP_2600		0
+#define ASPEED_MCTP_2600A3		1
+
+#define ASPEED_REVISION_ID0		0x04
+#define ASPEED_REVISION_ID1		0x14
+#define ID0_AST2600A0			0x05000303
+#define ID1_AST2600A0			0x05000303
+#define ID0_AST2600A1			0x05010303
+#define ID1_AST2600A1			0x05010303
+#define ID0_AST2600A2			0x05010303
+#define ID1_AST2600A2			0x05020303
+#define ID0_AST2600A3			0x05030303
+#define ID1_AST2600A3			0x05030303
+#define ID0_AST2620A1			0x05010203
+#define ID1_AST2620A1			0x05010203
+#define ID0_AST2620A2			0x05010203
+#define ID1_AST2620A2			0x05020203
+#define ID0_AST2620A3			0x05030203
+#define ID1_AST2620A3			0x05030203
+#define ID0_AST2605A2			0x05010103
+#define ID1_AST2605A2			0x05020103
+#define ID0_AST2605A3			0x05030103
+#define ID1_AST2605A3			0x05030103
+#define ID0_AST2625A3			0x05030403
+#define ID1_AST2625A3			0x05030403
+
+struct aspeed_mctp_match_data {
+	u32 rx_cmd_size;
+	u32 packet_unit_size;
+	bool need_address_mapping;
+	bool vdm_hdr_direct_xfer;
+	bool fifo_auto_surround;
+};
+
+struct aspeed_mctp_rx_cmd {
+	u32 rx_lo;
+	u32 rx_hi;
+};
+
+struct aspeed_mctp_tx_cmd {
+	u32 tx_lo;
+	u32 tx_hi;
+};
+
+struct mctp_buffer {
+	void *vaddr;
+	dma_addr_t dma_handle;
+};
+
+struct mctp_channel {
+	struct mctp_buffer data;
+	struct mctp_buffer cmd;
+	struct tasklet_struct tasklet;
+	u32 buffer_count;
+	u32 rd_ptr;
+	u32 wr_ptr;
+	bool stopped;
+};
+
+struct aspeed_mctp {
+	struct device *dev;
+	struct miscdevice mctp_miscdev;
+	const struct aspeed_mctp_match_data *match_data;
+	struct regmap *map;
+	struct reset_control *reset;
+	/*
+	 * The reset of the dma block in the MCTP-RC is connected to
+	 * another reset pin.
+	 */
+	struct reset_control *reset_dma;
+	struct mctp_channel tx;
+	struct mctp_channel rx;
+	struct list_head clients;
+	struct mctp_client *default_client;
+	struct list_head mctp_type_handlers;
+	/*
+	 * clients_lock protects list of clients, list of type handlers
+	 * and default client
+	 */
+	spinlock_t clients_lock;
+	struct list_head endpoints;
+	size_t endpoints_count;
+	/*
+	 * endpoints_lock protects list of endpoints
+	 */
+	struct mutex endpoints_lock;
+	struct {
+		struct regmap *map;
+		struct delayed_work rst_dwork;
+		bool need_uevent;
+	} pcie;
+	struct {
+		bool enable;
+		bool first_loop;
+		int packet_counter;
+	} rx_runaway_wa;
+	bool rx_warmup;
+	u8 eid;
+	struct platform_device *peci_mctp;
+	/* Use the flag to identify RC or EP */
+	bool rc_f;
+	/* Use the flag to identify the support of MCTP interrupt */
+	bool miss_mctp_int;
+	/* Rx hardware buffer size */
+	u32 rx_packet_count;
+	/* Rx pointer ring size */
+	u32 rx_ring_count;
+	/* Tx pointer ring size */
+	u32 tx_ring_count;
+};
+
+struct mctp_client {
+	struct kref ref;
+	struct aspeed_mctp *priv;
+	struct ptr_ring tx_queue;
+	struct ptr_ring rx_queue;
+	struct list_head link;
+	wait_queue_head_t wait_queue;
+};
+
+struct mctp_type_handler {
+	u8 mctp_type;
+	u16 pci_vendor_id;
+	u16 vdm_type;
+	u16 vdm_mask;
+	struct mctp_client *client;
+	struct list_head link;
+};
+
+union aspeed_mctp_eid_data_info {
+	struct aspeed_mctp_eid_info eid_info;
+	struct aspeed_mctp_eid_ext_info eid_ext_info;
+};
+
+enum mctp_address_type {
+	ASPEED_MCTP_GENERIC_ADDR_FORMAT = 0,
+	ASPEED_MCTP_EXTENDED_ADDR_FORMAT = 1
+};
+
+struct aspeed_mctp_endpoint {
+	union  aspeed_mctp_eid_data_info data;
+	struct list_head link;
+};
+
+struct kmem_cache *packet_cache;
+
+void data_dump(struct aspeed_mctp *priv, struct mctp_pcie_packet_data *data)
+{
+	int i;
+
+	dev_dbg(priv->dev, "Address %08x", (u32)data);
+	dev_dbg(priv->dev, "VDM header:");
+	for (i = 0; i < PCIE_VDM_HDR_SIZE_DW; i++) {
+		dev_dbg(priv->dev, "%02x %02x %02x %02x", data->hdr[i] & 0xff,
+		       (data->hdr[i] >> 8) & 0xff,
+		       (data->hdr[i] >> 16) & 0xff,
+		       (data->hdr[i] >> 24) & 0xff);
+	}
+	dev_dbg(priv->dev, "Data payload:");
+	for (i = 0; i < PCIE_VDM_DATA_SIZE_DW; i++) {
+		dev_dbg(priv->dev, "%02x %02x %02x %02x", data->payload[i] & 0xff,
+		       (data->payload[i] >> 8) & 0xff,
+		       (data->payload[i] >> 16) & 0xff,
+		       (data->payload[i] >> 24) & 0xff);
+	}
+}
+
+void *aspeed_mctp_packet_alloc(gfp_t flags)
+{
+	return kmem_cache_alloc(packet_cache, flags);
+}
+EXPORT_SYMBOL_GPL(aspeed_mctp_packet_alloc);
+
+void aspeed_mctp_packet_free(void *packet)
+{
+	kmem_cache_free(packet_cache, packet);
+}
+EXPORT_SYMBOL_GPL(aspeed_mctp_packet_free);
+
+static u16 _get_bdf(struct aspeed_mctp *priv)
+{
+	u32 reg;
+	u16 bdf;
+
+	regmap_read(priv->pcie.map, ASPEED_PCIE_MISC_STS_1, &reg);
+
+	reg = reg & (PCI_BUS_NUM_MASK | PCI_DEV_NUM_MASK);
+	bdf = PCI_DEVID(GET_PCI_BUS_NUM(reg), GET_PCI_DEV_NUM(reg));
+
+	return bdf;
+}
+
+static uint32_t chip_version(struct device *dev)
+{
+	struct regmap *scu;
+	u32 revid0, revid1;
+
+	scu = syscon_regmap_lookup_by_phandle(dev->of_node, "aspeed,scu");
+	if (IS_ERR(scu)) {
+		dev_err(dev, "failed to find 2600 SCU regmap\n");
+		return PTR_ERR(scu);
+	}
+	regmap_read(scu, ASPEED_REVISION_ID0, &revid0);
+	regmap_read(scu, ASPEED_REVISION_ID1, &revid1);
+	if (revid0 == ID0_AST2600A3 && revid1 == ID1_AST2600A3) {
+		/* AST2600-A3 */
+		return ASPEED_MCTP_2600A3;
+	} else if (revid0 == ID0_AST2620A3 && revid1 == ID1_AST2620A3) {
+		/* AST2620-A3 */
+		return ASPEED_MCTP_2600A3;
+	} else if (revid0 == ID0_AST2605A3 && revid1 == ID1_AST2605A3) {
+		/* AST2605-A3 */
+		return ASPEED_MCTP_2600A3;
+	} else if (revid0 == ID0_AST2625A3 && revid1 == ID1_AST2625A3) {
+		/* AST2605-A3 */
+		return ASPEED_MCTP_2600A3;
+	}
+	return ASPEED_MCTP_2600;
+}
+
+/*
+ * HW produces and expects VDM header in little endian and payload in network order.
+ * To allow userspace to use network order for the whole packet, PCIe VDM header needs
+ * to be swapped.
+ */
+static void aspeed_mctp_swap_pcie_vdm_hdr(struct mctp_pcie_packet_data *data)
+{
+	int i;
+
+	for (i = 0; i < PCIE_VDM_HDR_SIZE_DW; i++)
+		data->hdr[i] = swab32(data->hdr[i]);
+}
+
+static void aspeed_mctp_rx_trigger(struct mctp_channel *rx)
+{
+	struct aspeed_mctp *priv = container_of(rx, typeof(*priv), rx);
+
+	/*
+	 * Even though rx_buf_addr doesn't change, if we don't do the write
+	 * here, the HW doesn't trigger RX. We're also clearing the
+	 * RX_CMD_READY bit, otherwise we're observing a rare case where
+	 * trigger isn't registered by the HW, and we're ending up with stuck
+	 * HW (not reacting to wr_ptr writes).
+	 * Also, note that we're writing 0 as wr_ptr. If we're writing other
+	 * value, the HW behaves in a bizarre way that's hard to explain...
+	 */
+	regmap_update_bits(priv->map, ASPEED_MCTP_CTRL, RX_CMD_READY, 0);
+	regmap_write(priv->map, ASPEED_MCTP_RX_BUF_ADDR, rx->cmd.dma_handle);
+	regmap_write(priv->map, ASPEED_MCTP_RX_BUF_WR_PTR, 0);
+
+	/* After re-enabling RX we need to restart WA logic */
+	if (priv->rx_runaway_wa.enable)
+		priv->rx.buffer_count = priv->rx_packet_count;
+	/*
+	 * When Rx warmup MCTP controller may store first packet into the 0th to the
+	 * 3rd cmd. In ast2600 A3, If the packet isn't stored in the 0th cmd we need
+	 * to change the rx buffer size to avoid rx runaway in first loop. In ast2600
+	 * A1/A2, after first loop hardware is guaranteed to use (RX_PACKET_COUNT - 4)
+	 * buffers.
+	 */
+	priv->rx_warmup = true;
+	priv->rx_runaway_wa.first_loop = true;
+	priv->rx_runaway_wa.packet_counter = 0;
+
+	regmap_update_bits(priv->map, ASPEED_MCTP_CTRL, RX_CMD_READY,
+			   RX_CMD_READY);
+}
+
+static void aspeed_mctp_tx_trigger(struct mctp_channel *tx, bool notify)
+{
+	struct aspeed_mctp *priv = container_of(tx, typeof(*priv), tx);
+
+	if (notify) {
+		struct aspeed_mctp_tx_cmd *last_cmd;
+
+		last_cmd = (struct aspeed_mctp_tx_cmd *)tx->cmd.vaddr +
+			   (tx->wr_ptr - 1) % TX_PACKET_COUNT;
+		last_cmd->tx_lo |= TX_INTERRUPT_AFTER_CMD;
+	}
+	if (priv->match_data->fifo_auto_surround)
+		regmap_write(priv->map, ASPEED_MCTP_TX_BUF_WR_PTR, tx->wr_ptr);
+	regmap_update_bits(priv->map, ASPEED_MCTP_CTRL, TX_CMD_TRIGGER,
+			   TX_CMD_TRIGGER);
+}
+
+static void aspeed_mctp_tx_cmd_prep(u32 *tx_hdr, struct aspeed_mctp_tx_cmd *tx_cmd)
+{
+	u32 packet_size, target_id;
+	u8 dest_eid, padding_len, routing_type, tag_owner;
+
+	packet_size = FIELD_GET(MCTP_HDR_DW_LE_PACKET_SIZE, tx_hdr[0]);
+	routing_type = FIELD_GET(MCTP_HDR_DW_LE_ROUTING_TYPE, tx_hdr[0]);
+	routing_type = routing_type ? routing_type - 1 : 0;
+	padding_len = FIELD_GET(MCTP_HDR_DW_LE_PADDING_LEN, tx_hdr[1]);
+	target_id = FIELD_GET(MCTP_HDR_DW_LE_TARGET_ID, tx_hdr[2]);
+	tag_owner = FIELD_GET(MCTP_HDR_DW_LE_TAG_OWNER, tx_hdr[3]);
+	dest_eid = FIELD_GET(MCTP_HDR_DW_LE_DEST_EID, tx_hdr[3]);
+
+	tx_cmd->tx_hi = FIELD_PREP(TX_PACKET_DEST_EID, dest_eid);
+	tx_cmd->tx_lo = FIELD_PREP(TX_PACKET_TARGET_ID, target_id) |
+			TX_INTERRUPT_AFTER_CMD |
+			FIELD_PREP(TX_PACKET_ROUTING_TYPE, routing_type) |
+			FIELD_PREP(TX_PACKET_TAG_OWNER, tag_owner) |
+			TX_PACKET_SIZE_2500(packet_size) |
+			FIELD_PREP(TX_PACKET_PADDING_LEN, padding_len);
+}
+
+static void aspeed_mctp_emit_tx_cmd(struct mctp_channel *tx,
+				    struct mctp_pcie_packet *packet)
+{
+	struct aspeed_mctp *priv = container_of(tx, typeof(*priv), tx);
+	struct aspeed_mctp_tx_cmd *tx_cmd =
+		(struct aspeed_mctp_tx_cmd *)tx->cmd.vaddr + tx->wr_ptr;
+	u32 packet_sz_dw = packet->size / sizeof(u32) -
+		sizeof(packet->data.hdr) / sizeof(u32);
+	u32 offset;
+
+	data_dump(priv, &packet->data);
+	aspeed_mctp_swap_pcie_vdm_hdr(&packet->data);
+
+	if (priv->match_data->vdm_hdr_direct_xfer) {
+		offset = tx->wr_ptr * sizeof(packet->data);
+		memcpy((u8 *)tx->data.vaddr + offset, &packet->data,
+		sizeof(packet->data));
+
+		tx_cmd->tx_lo = TX_PACKET_SIZE(packet_sz_dw);
+		tx_cmd->tx_hi = TX_RESERVED_1;
+		tx_cmd->tx_hi |= TX_DATA_ADDR(tx->data.dma_handle + offset);
+	} else {
+		offset = tx->wr_ptr * sizeof(struct mctp_pcie_packet_data_2500);
+		memcpy((u8 *)tx->data.vaddr + offset, packet->data.payload,
+		       sizeof(packet->data.payload));
+		aspeed_mctp_tx_cmd_prep(packet->data.hdr, tx_cmd);
+		tx_cmd->tx_hi |= TX_DATA_ADDR_2500(tx->data.dma_handle + offset);
+		if (tx->wr_ptr == TX_PACKET_COUNT - 1)
+			tx_cmd->tx_hi |= TX_LAST_CMD;
+	}
+	dev_dbg(priv->dev, "tx->wr_prt: %d, tx_cmd: hi:%08x lo:%08x\n",
+		 tx->wr_ptr, tx_cmd->tx_hi, tx_cmd->tx_lo);
+
+	tx->wr_ptr = (tx->wr_ptr + 1) % TX_PACKET_COUNT;
+}
+
+static struct mctp_client *aspeed_mctp_client_alloc(struct aspeed_mctp *priv)
+{
+	struct mctp_client *client;
+
+	client = kzalloc(sizeof(*client), GFP_KERNEL);
+	if (!client)
+		goto out;
+
+	kref_init(&client->ref);
+	client->priv = priv;
+	ptr_ring_init(&client->tx_queue, priv->tx_ring_count, GFP_KERNEL);
+	ptr_ring_init(&client->rx_queue, priv->rx_ring_count, GFP_ATOMIC);
+
+out:
+	return client;
+}
+
+static void aspeed_mctp_client_free(struct kref *ref)
+{
+	struct mctp_client *client = container_of(ref, typeof(*client), ref);
+
+	ptr_ring_cleanup(&client->rx_queue, &aspeed_mctp_packet_free);
+	ptr_ring_cleanup(&client->tx_queue, &aspeed_mctp_packet_free);
+
+	kfree(client);
+}
+
+static void aspeed_mctp_client_get(struct mctp_client *client)
+{
+	lockdep_assert_held(&client->priv->clients_lock);
+
+	kref_get(&client->ref);
+}
+
+static void aspeed_mctp_client_put(struct mctp_client *client)
+{
+	kref_put(&client->ref, &aspeed_mctp_client_free);
+}
+
+static struct mctp_client *
+aspeed_mctp_find_handler(struct aspeed_mctp *priv,
+			 struct mctp_pcie_packet *packet)
+{
+	struct mctp_type_handler *handler;
+	u8 *hdr = (u8 *)packet->data.hdr;
+	struct mctp_client *client = NULL;
+	u8 mctp_type, som_eom;
+	u16 vendor = 0;
+	u16 vdm_type = 0;
+
+	lockdep_assert_held(&priv->clients_lock);
+
+	/*
+	 * Middle and EOM fragments cannot be matched to MCTP type.
+	 * For consistency do not match type for any fragmented messages.
+	 */
+	som_eom = hdr[MCTP_HDR_TAG_OFFSET] & MCTP_HDR_SOM_EOM;
+	if (som_eom != MCTP_HDR_SOM_EOM)
+		return NULL;
+
+	mctp_type = hdr[MCTP_HDR_TYPE_OFFSET];
+	if (mctp_type == MCTP_HDR_TYPE_VDM_PCI) {
+		vendor = *((u16 *)&hdr[MCTP_HDR_VENDOR_OFFSET]);
+		vdm_type = *((u16 *)&hdr[MCTP_HDR_VDM_TYPE_OFFSET]);
+	}
+
+	list_for_each_entry(handler, &priv->mctp_type_handlers, link) {
+		if (handler->mctp_type == mctp_type &&
+		    handler->pci_vendor_id == vendor &&
+		    handler->vdm_type == (vdm_type & handler->vdm_mask)) {
+			dev_dbg(priv->dev, "Found client for type %x vdm %x\n",
+				mctp_type, handler->vdm_type);
+			client = handler->client;
+			break;
+		}
+	}
+	return client;
+}
+
+static void aspeed_mctp_dispatch_packet(struct aspeed_mctp *priv,
+					struct mctp_pcie_packet *packet)
+{
+	struct mctp_client *client;
+	int ret;
+
+	spin_lock(&priv->clients_lock);
+
+	client = aspeed_mctp_find_handler(priv, packet);
+
+	if (!client)
+		client = priv->default_client;
+
+	if (client)
+		aspeed_mctp_client_get(client);
+
+	spin_unlock(&priv->clients_lock);
+
+	if (client) {
+		ret = ptr_ring_produce(&client->rx_queue, packet);
+		if (ret) {
+			/*
+			 * This can happen if client process does not
+			 * consume packets fast enough
+			 */
+			dev_dbg(priv->dev, "Failed to store packet in client RX queue\n");
+			aspeed_mctp_packet_free(packet);
+		} else {
+			wake_up_all(&client->wait_queue);
+		}
+		aspeed_mctp_client_put(client);
+	} else {
+		dev_dbg(priv->dev, "Failed to dispatch RX packet\n");
+		aspeed_mctp_packet_free(packet);
+	}
+}
+
+static void aspeed_mctp_tx_tasklet(unsigned long data)
+{
+	struct mctp_channel *tx = (struct mctp_channel *)data;
+	struct aspeed_mctp *priv = container_of(tx, typeof(*priv), tx);
+	struct mctp_client *client;
+	bool trigger = false;
+	bool full = false;
+	u32 rd_ptr;
+
+	if (priv->match_data->fifo_auto_surround) {
+		regmap_write(priv->map, ASPEED_MCTP_TX_BUF_RD_PTR, UPDATE_RX_RD_PTR);
+		regmap_read(priv->map, ASPEED_MCTP_TX_BUF_RD_PTR, &rd_ptr);
+		rd_ptr &= TX_BUF_RD_PTR_MASK;
+	} else {
+		rd_ptr = tx->rd_ptr;
+	}
+
+	spin_lock(&priv->clients_lock);
+
+	list_for_each_entry(client, &priv->clients, link) {
+		while (!(full = (tx->wr_ptr + 1) % TX_PACKET_COUNT == rd_ptr)) {
+			struct mctp_pcie_packet *packet;
+
+			packet = ptr_ring_consume(&client->tx_queue);
+			if (!packet)
+				break;
+
+			aspeed_mctp_emit_tx_cmd(tx, packet);
+			aspeed_mctp_packet_free(packet);
+			trigger = true;
+		}
+	}
+
+	spin_unlock(&priv->clients_lock);
+
+	if (trigger)
+		aspeed_mctp_tx_trigger(tx, full);
+}
+
+void aspeed_mctp_rx_hdr_prep(struct aspeed_mctp *priv, u8 *hdr, u32 rx_lo)
+{
+	u16 bdf;
+	u8 routing_type;
+
+	/*
+	 * MCTP controller will map the routing type to reduce one bit
+	 * 0 (Route to RC) -> 0,
+	 * 2 (Route by ID) -> 1,
+	 * 3 (Broadcast from RC) -> 2
+	 */
+	routing_type = FIELD_GET(RX_PACKET_ROUTING_TYPE, rx_lo);
+	routing_type = routing_type ? routing_type + 1 : 0;
+	bdf = _get_bdf(priv);
+	/* Length[7:0] */
+	hdr[0] = FIELD_GET(RX_PACKET_SIZE, rx_lo);
+	/* TD:EP:ATTR[1:0]:R or AT[1:0]:Length[9:8] */
+	hdr[1] = 0;
+	/* R or T9:TC[2:0]:R[3:0] */
+	hdr[2] = 0;
+	/* R or Fmt[2]:Fmt[1:0]=b'11:Type[4:3]=b'10:Type[2:0] */
+	hdr[3] = 0x70 | routing_type;
+	/* VDM message code = 0x7f */
+	hdr[4] = 0x7f;
+	/* R[1:0]:Pad len[1:0]:MCTP VDM Code[3:0] */
+	hdr[5] = FIELD_GET(RX_PACKET_PADDING_LEN, rx_lo) << 4;
+	/* TODO: PCI Requester ID: HW didn't get this information */
+	hdr[6] = 0;
+	hdr[7] = 5;
+	/* Vendor ID: 0x1AB4 */
+	hdr[8] = 0xb4;
+	hdr[9] = 0x1a;
+	/* PCI Target ID */
+	hdr[10] = bdf & 0xff;
+	hdr[11] = bdf >> 8 & 0xff;
+	/* SOM:EOM:Pkt Seq#[1:0]:TO:Msg Tag[2:0]*/
+	hdr[12] = FIELD_GET(RX_PACKET_SOM, rx_lo) << 7 |
+		  FIELD_GET(RX_PACKET_EOM, rx_lo) << 6 |
+		  FIELD_GET(RX_PACKET_SEQ_NUMBER, rx_lo) << 4 |
+		  FIELD_GET(RX_PACKET_TAG_OWNER, rx_lo) << 3 |
+		  FIELD_GET(RX_PACKET_MSG_TAG, rx_lo);
+	/* Source Endpoint ID */
+	hdr[13] = FIELD_GET(RX_PACKET_SRC_EID, rx_lo);
+	/* Destination Endpoint ID: HW didn't get this information*/
+	hdr[14] = priv->eid;
+	/* TODO: R[3:0]: header version[3:0] */
+	hdr[15] = 1;
+}
+
+static void aspeed_mctp_rx_tasklet(unsigned long data)
+{
+	struct mctp_channel *rx = (struct mctp_channel *)data;
+	struct aspeed_mctp *priv = container_of(rx, typeof(*priv), rx);
+	struct mctp_pcie_packet *rx_packet;
+	struct aspeed_mctp_rx_cmd *rx_cmd;
+	u32 hw_read_ptr;
+	u32 *hdr, *payload;
+
+	if (priv->match_data->vdm_hdr_direct_xfer && priv->match_data->fifo_auto_surround) {
+		struct mctp_pcie_packet_data *rx_buf;
+		u32 residual_cmds = 0;
+
+		/* Trigger HW read pointer update, must be done before RX loop */
+		regmap_write(priv->map, ASPEED_MCTP_RX_BUF_RD_PTR, UPDATE_RX_RD_PTR);
+
+		/*
+		 * XXX: Using rd_ptr obtained from HW is unreliable so we need to
+		 * maintain the state of buffer on our own by peeking into the buffer
+		 * and checking where the packet was written.
+		 */
+		rx_buf = (struct mctp_pcie_packet_data *)rx->data.vaddr;
+		hdr = (u32 *)&rx_buf[rx->wr_ptr];
+		if ((priv->rx_warmup || priv->rx_runaway_wa.first_loop) && !*hdr) {
+			u32 tmp_wr_ptr = rx->wr_ptr;
+
+			/*
+			 * HACK: Right after start the RX hardware can put received
+			 * packet into an unexpected offset - in order to locate
+			 * received packet driver has to scan all RX data buffers.
+			 */
+			do {
+				tmp_wr_ptr = (tmp_wr_ptr + 1) % priv->rx_packet_count;
+
+				hdr = (u32 *)&rx_buf[tmp_wr_ptr];
+			} while (!*hdr && tmp_wr_ptr != rx->wr_ptr);
+
+			if (tmp_wr_ptr != rx->wr_ptr) {
+				dev_warn(priv->dev, "Runaway RX packet found %d -> %d\n",
+					rx->wr_ptr, tmp_wr_ptr);
+				residual_cmds = abs(tmp_wr_ptr - rx->wr_ptr);
+				rx->wr_ptr = tmp_wr_ptr;
+				if (!priv->rx_runaway_wa.enable &&
+				    priv->rx_warmup)
+					regmap_write(priv->map,
+						     ASPEED_MCTP_RX_BUF_SIZE,
+						     rx->buffer_count -
+							     residual_cmds);
+				priv->rx_warmup = false;
+			}
+		}
+
+		if (priv->rx_runaway_wa.packet_counter > priv->rx_packet_count &&
+		    priv->rx_runaway_wa.first_loop) {
+			if (priv->rx_runaway_wa.enable)
+				/*
+				 * Once we receive RX_PACKET_COUNT packets, hardware is
+				 * guaranteed to use (RX_PACKET_COUNT - 4) buffers. Decrease
+				 * buffer count by 4, then we can turn off scanning of RX
+				 * buffers. RX buffer scanning should be enabled every time
+				 * RX hardware is started.
+				 * This is just a performance optimization - we could keep
+				 * scanning RX buffers forever, but under heavy traffic it is
+				 * fairly common that rx_tasklet is executed while RX buffer
+				 * ring is empty.
+				 */
+				rx->buffer_count = priv->rx_packet_count - 4;
+			else
+				/*
+				 * Once we receive RX_PACKET_COUNT packets, we need to restore the
+				 * RX buffer size to 4 byte aligned value to avoid rx runaway.
+				 */
+				regmap_write(priv->map, ASPEED_MCTP_RX_BUF_SIZE,
+				     rx->buffer_count);
+			priv->rx_runaway_wa.first_loop = false;
+		}
+
+		while (*hdr != 0) {
+			rx_packet = aspeed_mctp_packet_alloc(GFP_ATOMIC);
+			if (rx_packet) {
+				memcpy(&rx_packet->data, hdr, sizeof(rx_packet->data));
+
+				aspeed_mctp_swap_pcie_vdm_hdr(&rx_packet->data);
+
+				aspeed_mctp_dispatch_packet(priv, rx_packet);
+			} else {
+				dev_dbg(priv->dev, "Failed to allocate RX packet\n");
+			}
+			data_dump(priv, &rx_packet->data);
+			*hdr = 0;
+			rx->wr_ptr = (rx->wr_ptr + 1) % rx->buffer_count;
+			hdr = (u32 *)&rx_buf[rx->wr_ptr];
+
+			priv->rx_runaway_wa.packet_counter++;
+		}
+
+		/*
+		 * Update HW write pointer, this can be done only after driver consumes
+		 * packets from RX ring.
+		 */
+		regmap_read(priv->map, ASPEED_MCTP_RX_BUF_RD_PTR, &hw_read_ptr);
+		hw_read_ptr &= RX_BUF_RD_PTR_MASK;
+		regmap_write(priv->map, ASPEED_MCTP_RX_BUF_WR_PTR, (hw_read_ptr));
+
+		dev_dbg(priv->dev, "RX hw ptr %02d, sw ptr %2d\n",
+			hw_read_ptr, rx->wr_ptr);
+	} else {
+		struct mctp_pcie_packet_data_2500 *rx_buf;
+
+		rx_buf = (struct mctp_pcie_packet_data_2500 *)rx->data.vaddr;
+		payload = (u32 *)&rx_buf[rx->wr_ptr];
+		rx_cmd = (struct aspeed_mctp_rx_cmd *)rx->cmd.vaddr;
+		hdr = (u32 *)&((rx_cmd + rx->wr_ptr)->rx_lo);
+
+		while (*hdr != 0) {
+			rx_packet = aspeed_mctp_packet_alloc(GFP_ATOMIC);
+			if (rx_packet) {
+				memcpy(rx_packet->data.payload, payload,
+				       sizeof(rx_packet->data.payload));
+
+				aspeed_mctp_rx_hdr_prep(priv, (u8 *)rx_packet->data.hdr, *hdr);
+
+				aspeed_mctp_swap_pcie_vdm_hdr(&rx_packet->data);
+
+				aspeed_mctp_dispatch_packet(priv, rx_packet);
+			} else {
+				dev_dbg(priv->dev, "Failed to allocate RX packet\n");
+			}
+			dev_dbg(priv->dev,
+				"rx->wr_ptr = %d, rx_cmd->rx_lo = %08x",
+				rx->wr_ptr, *hdr);
+			data_dump(priv, &rx_packet->data);
+			*hdr = 0;
+			rx->wr_ptr = (rx->wr_ptr + 1) % rx->buffer_count;
+			payload = (u32 *)&rx_buf[rx->wr_ptr];
+			hdr = (u32 *)&((rx_cmd + rx->wr_ptr)->rx_lo);
+		}
+	}
+
+	/* Kick RX if it was stopped due to ring full condition */
+	if (rx->stopped) {
+		regmap_update_bits(priv->map, ASPEED_MCTP_CTRL, RX_CMD_READY,
+				   RX_CMD_READY);
+		rx->stopped = false;
+	}
+}
+
+static void aspeed_mctp_rx_chan_init(struct mctp_channel *rx)
+{
+	struct aspeed_mctp *priv = container_of(rx, typeof(*priv), rx);
+	u32 *rx_cmd = (u32 *)rx->cmd.vaddr;
+	struct aspeed_mctp_rx_cmd *rx_cmd_64 =
+		(struct aspeed_mctp_rx_cmd *)rx->cmd.vaddr;
+	u32 data_size = priv->match_data->packet_unit_size;
+	u32 hw_rx_count = priv->rx_packet_count;
+	struct mctp_pcie_packet_data *rx_buf = (struct mctp_pcie_packet_data *)rx->data.vaddr;
+	int i;
+
+	if (priv->match_data->vdm_hdr_direct_xfer) {
+		for (i = 0; i < priv->rx_packet_count; i++) {
+			*rx_cmd = RX_DATA_ADDR(rx->data.dma_handle + data_size * i);
+			*rx_cmd |= RX_INTERRUPT_AFTER_CMD;
+			rx_cmd++;
+		}
+	} else {
+		for (i = 0; i < priv->rx_packet_count; i++) {
+			rx_cmd_64->rx_hi = RX_DATA_ADDR_2500(
+				rx->data.dma_handle + data_size * i);
+			rx_cmd_64->rx_lo = 0;
+			if (i == priv->rx_packet_count - 1)
+				rx_cmd_64->rx_hi |= RX_LAST_CMD;
+			rx_cmd_64++;
+		}
+	}
+	/* Clear the header of rx data */
+	for (i = 0; i < priv->rx_packet_count; i++)
+		*(u32 *)&rx_buf[i] = 0;
+	rx->wr_ptr = 0;
+	rx->buffer_count = priv->rx_packet_count;
+	if (priv->match_data->fifo_auto_surround) {
+		/*
+		 * TODO: Once read pointer runaway bug is fixed in some future AST2x00
+		 * stepping then add chip revision detection and turn on this
+		 * workaround only when needed
+		 */
+		priv->rx_runaway_wa.enable =
+			(chip_version(priv->dev) == ASPEED_MCTP_2600) ? true : false;
+
+		/*
+		 * Hardware does not wrap around ASPEED_MCTP_RX_BUF_SIZE
+		 * correctly - we have to set number of buffers to n/4 -1
+		 */
+		if (priv->rx_runaway_wa.enable)
+			hw_rx_count = (priv->rx_packet_count / 4 - 1);
+
+		regmap_write(priv->map, ASPEED_MCTP_RX_BUF_SIZE, hw_rx_count);
+	}
+}
+
+static void aspeed_mctp_tx_chan_init(struct mctp_channel *tx)
+{
+	struct aspeed_mctp *priv = container_of(tx, typeof(*priv), tx);
+
+	tx->wr_ptr = 0;
+	tx->rd_ptr = 0;
+	regmap_update_bits(priv->map, ASPEED_MCTP_CTRL, TX_CMD_TRIGGER, 0);
+	regmap_write(priv->map, ASPEED_MCTP_TX_BUF_ADDR, tx->cmd.dma_handle);
+	if (priv->match_data->fifo_auto_surround) {
+		regmap_write(priv->map, ASPEED_MCTP_TX_BUF_SIZE, TX_PACKET_COUNT);
+		regmap_write(priv->map, ASPEED_MCTP_TX_BUF_WR_PTR, 0);
+	}
+}
+
+struct mctp_client *aspeed_mctp_create_client(struct aspeed_mctp *priv)
+{
+	struct mctp_client *client;
+
+	client = aspeed_mctp_client_alloc(priv);
+	if (!client)
+		return NULL;
+
+	init_waitqueue_head(&client->wait_queue);
+
+	spin_lock_bh(&priv->clients_lock);
+	list_add_tail(&client->link, &priv->clients);
+	spin_unlock_bh(&priv->clients_lock);
+
+	return client;
+}
+EXPORT_SYMBOL_GPL(aspeed_mctp_create_client);
+
+static int aspeed_mctp_open(struct inode *inode, struct file *file)
+{
+	struct miscdevice *misc = file->private_data;
+	struct platform_device *pdev = to_platform_device(misc->parent);
+	struct aspeed_mctp *priv = platform_get_drvdata(pdev);
+	struct mctp_client *client;
+
+	client = aspeed_mctp_create_client(priv);
+	if (!client)
+		return -ENOMEM;
+
+	file->private_data = client;
+
+	return 0;
+}
+
+void aspeed_mctp_delete_client(struct mctp_client *client)
+{
+	struct aspeed_mctp *priv = client->priv;
+	struct mctp_type_handler *handler, *tmp;
+
+	spin_lock_bh(&priv->clients_lock);
+
+	list_del(&client->link);
+
+	if (priv->default_client == client)
+		priv->default_client = NULL;
+
+	list_for_each_entry_safe(handler, tmp, &priv->mctp_type_handlers,
+				 link) {
+		if (handler->client == client) {
+			list_del(&handler->link);
+			kfree(handler);
+		}
+	}
+	spin_unlock_bh(&priv->clients_lock);
+
+	/* Disable the tasklet to appease lockdep */
+	local_bh_disable();
+	aspeed_mctp_client_put(client);
+	local_bh_enable();
+}
+EXPORT_SYMBOL_GPL(aspeed_mctp_delete_client);
+
+static int aspeed_mctp_release(struct inode *inode, struct file *file)
+{
+	struct mctp_client *client = file->private_data;
+
+	aspeed_mctp_delete_client(client);
+
+	return 0;
+}
+
+#define LEN_MASK_HI GENMASK(9, 8)
+#define LEN_MASK_LO GENMASK(7, 0)
+#define PCI_VDM_HDR_LEN_MASK_LO GENMASK(31, 24)
+#define PCI_VDM_HDR_LEN_MASK_HI GENMASK(17, 16)
+#define PCIE_VDM_HDR_REQUESTER_BDF_MASK GENMASK(31, 16)
+
+int aspeed_mctp_send_packet(struct mctp_client *client,
+			    struct mctp_pcie_packet *packet)
+{
+	struct aspeed_mctp *priv = client->priv;
+	u32 *hdr_dw = (u32 *)packet->data.hdr;
+	u8 *hdr = (u8 *)packet->data.hdr;
+	u16 packet_data_sz_dw;
+	u16 pci_data_len_dw;
+	int ret;
+	u16 bdf;
+
+	bdf = _get_bdf(priv);
+	if (bdf == 0)
+		return -EIO;
+
+	/*
+	 * If the data size is different from contents of PCIe VDM header,
+	 * aspeed_mctp_tx_cmd will be programmed incorrectly. This may cause
+	 * MCTP HW to stop working.
+	 */
+	pci_data_len_dw = FIELD_PREP(LEN_MASK_LO, FIELD_GET(PCI_VDM_HDR_LEN_MASK_LO, hdr_dw[0])) |
+			FIELD_PREP(LEN_MASK_HI, FIELD_GET(PCI_VDM_HDR_LEN_MASK_HI, hdr_dw[0]));
+	if (pci_data_len_dw == 0) /* According to PCIe Spec, 0 means 1024 DW */
+		pci_data_len_dw = SZ_1K;
+
+	packet_data_sz_dw = packet->size / sizeof(u32) - sizeof(packet->data.hdr) / sizeof(u32);
+	if (packet_data_sz_dw != pci_data_len_dw)
+		return -EINVAL;
+
+	be32p_replace_bits(&hdr_dw[1], bdf, PCIE_VDM_HDR_REQUESTER_BDF_MASK);
+
+	/*
+	 * XXX Don't update EID for MCTP Control messages - old EID may
+	 * interfere with MCTP discovery flow.
+	 */
+	if (priv->eid && hdr[MCTP_HDR_TYPE_OFFSET] != MCTP_HDR_TYPE_CONTROL)
+		hdr[MCTP_HDR_SRC_EID_OFFSET] = priv->eid;
+
+	ret = ptr_ring_produce_bh(&client->tx_queue, packet);
+	if (!ret)
+		tasklet_hi_schedule(&priv->tx.tasklet);
+
+	return ret;
+}
+EXPORT_SYMBOL_GPL(aspeed_mctp_send_packet);
+
+struct mctp_pcie_packet *aspeed_mctp_receive_packet(struct mctp_client *client,
+						    unsigned long timeout)
+{
+	struct aspeed_mctp *priv = client->priv;
+	u16 bdf = _get_bdf(priv);
+	int ret;
+
+	if (bdf == 0)
+		return ERR_PTR(-EIO);
+
+	ret = wait_event_interruptible_timeout(client->wait_queue,
+					       __ptr_ring_peek(&client->rx_queue),
+					       timeout);
+	if (ret < 0)
+		return ERR_PTR(ret);
+	else if (ret == 0)
+		return ERR_PTR(-ETIME);
+
+	return ptr_ring_consume_bh(&client->rx_queue);
+}
+EXPORT_SYMBOL_GPL(aspeed_mctp_receive_packet);
+
+void aspeed_mctp_flush_rx_queue(struct mctp_client *client)
+{
+	struct mctp_pcie_packet *packet;
+
+	while ((packet = ptr_ring_consume_bh(&client->rx_queue)))
+		aspeed_mctp_packet_free(packet);
+}
+EXPORT_SYMBOL_GPL(aspeed_mctp_flush_rx_queue);
+
+static ssize_t aspeed_mctp_read(struct file *file, char __user *buf,
+				size_t count, loff_t *ppos)
+{
+	struct mctp_client *client = file->private_data;
+	struct aspeed_mctp *priv = client->priv;
+	struct mctp_pcie_packet *rx_packet;
+	u32 mctp_ctrl;
+	u32 mctp_int_sts;
+
+	if (count < PCIE_MCTP_MIN_PACKET_SIZE)
+		return -EINVAL;
+
+	if (count > sizeof(rx_packet->data))
+		count = sizeof(rx_packet->data);
+
+	if (priv->miss_mctp_int) {
+		regmap_read(priv->map, ASPEED_MCTP_CTRL, &mctp_ctrl);
+		if (!(mctp_ctrl & RX_CMD_READY))
+			priv->rx.stopped = true;
+		/* Polling the RX_CMD_RECEIVE_INT to ensure rx_tasklet can find the data */
+		regmap_read(priv->map, ASPEED_MCTP_INT_STS, &mctp_int_sts);
+		if (mctp_int_sts & RX_CMD_RECEIVE_INT) {
+			regmap_write(priv->map, ASPEED_MCTP_INT_STS,
+				     mctp_int_sts);
+			tasklet_hi_schedule(&priv->rx.tasklet);
+		}
+	}
+
+	rx_packet = ptr_ring_consume_bh(&client->rx_queue);
+	if (!rx_packet)
+		return -EAGAIN;
+
+	if (copy_to_user(buf, &rx_packet->data, count)) {
+		dev_err(priv->dev, "copy to user failed\n");
+		count = -EFAULT;
+	}
+
+	aspeed_mctp_packet_free(rx_packet);
+
+	return count;
+}
+
+static void aspeed_mctp_flush_tx_queue(struct mctp_client *client)
+{
+	struct mctp_pcie_packet *packet;
+
+	while ((packet = ptr_ring_consume_bh(&client->tx_queue)))
+		aspeed_mctp_packet_free(packet);
+}
+
+static void aspeed_mctp_flush_all_tx_queues(struct aspeed_mctp *priv)
+{
+	struct mctp_client *client;
+
+	spin_lock_bh(&priv->clients_lock);
+	list_for_each_entry(client, &priv->clients, link)
+		aspeed_mctp_flush_tx_queue(client);
+	spin_unlock_bh(&priv->clients_lock);
+}
+
+static ssize_t aspeed_mctp_write(struct file *file, const char __user *buf,
+				 size_t count, loff_t *ppos)
+{
+	struct mctp_client *client = file->private_data;
+	struct aspeed_mctp *priv = client->priv;
+	struct mctp_pcie_packet *tx_packet;
+	int ret;
+
+	if (count < PCIE_MCTP_MIN_PACKET_SIZE)
+		return -EINVAL;
+
+	if (count > sizeof(tx_packet->data))
+		return -ENOSPC;
+
+	tx_packet = aspeed_mctp_packet_alloc(GFP_KERNEL);
+	if (!tx_packet) {
+		ret = -ENOMEM;
+		goto out;
+	}
+
+	if (copy_from_user(&tx_packet->data, buf, count)) {
+		dev_err(priv->dev, "copy from user failed\n");
+		ret = -EFAULT;
+		goto out_packet;
+	}
+
+	tx_packet->size = count;
+
+	ret = aspeed_mctp_send_packet(client, tx_packet);
+	if (ret)
+		goto out_packet;
+
+	return count;
+
+out_packet:
+	aspeed_mctp_packet_free(tx_packet);
+out:
+	return ret;
+}
+
+int aspeed_mctp_add_type_handler(struct mctp_client *client, u8 mctp_type,
+				 u16 pci_vendor_id, u16 vdm_type, u16 vdm_mask)
+{
+	struct aspeed_mctp *priv = client->priv;
+	struct mctp_type_handler *handler, *new_handler;
+	int ret = 0;
+
+	if (mctp_type <= MCTP_HDR_TYPE_BASE_LAST) {
+		/* Vendor, type and type mask must be zero for types 0-5 */
+		if (pci_vendor_id != 0 || vdm_type != 0 || vdm_mask != 0)
+			return -EINVAL;
+	} else if (mctp_type == MCTP_HDR_TYPE_VDM_PCI) {
+		/* For Vendor Defined PCI type the the vendor ID must be nonzero */
+		if (pci_vendor_id == 0 || pci_vendor_id == 0xffff)
+			return -EINVAL;
+	} else {
+		return -EINVAL;
+	}
+
+	new_handler = kzalloc(sizeof(*new_handler), GFP_KERNEL);
+	if (!new_handler)
+		return -ENOMEM;
+	new_handler->mctp_type = mctp_type;
+	new_handler->pci_vendor_id = pci_vendor_id;
+	new_handler->vdm_type = vdm_type & vdm_mask;
+	new_handler->vdm_mask = vdm_mask;
+	new_handler->client = client;
+
+	spin_lock_bh(&priv->clients_lock);
+	list_for_each_entry(handler, &priv->mctp_type_handlers, link) {
+		if (handler->mctp_type == new_handler->mctp_type &&
+		    handler->pci_vendor_id == new_handler->pci_vendor_id &&
+		    handler->vdm_type == new_handler->vdm_type) {
+			if (handler->client != new_handler->client)
+				ret = -EBUSY;
+			kfree(new_handler);
+			goto out_unlock;
+		}
+	}
+	list_add_tail(&new_handler->link, &priv->mctp_type_handlers);
+out_unlock:
+	spin_unlock_bh(&priv->clients_lock);
+
+	return ret;
+}
+EXPORT_SYMBOL_GPL(aspeed_mctp_add_type_handler);
+
+int aspeed_mctp_remove_type_handler(struct mctp_client *client,
+				    u8 mctp_type, u16 pci_vendor_id,
+				    u16 vdm_type, u16 vdm_mask)
+{
+	struct aspeed_mctp *priv = client->priv;
+	struct mctp_type_handler *handler, *tmp;
+	int ret = -EINVAL;
+
+	vdm_type &= vdm_mask;
+
+	spin_lock_bh(&priv->clients_lock);
+	list_for_each_entry_safe(handler, tmp, &priv->mctp_type_handlers,
+				 link) {
+		if (handler->client == client &&
+		    handler->mctp_type == mctp_type &&
+		    handler->pci_vendor_id == pci_vendor_id &&
+		    handler->vdm_type == vdm_type) {
+			list_del(&handler->link);
+			kfree(handler);
+			ret = 0;
+			break;
+		}
+	}
+	spin_unlock_bh(&priv->clients_lock);
+	return ret;
+}
+
+static int aspeed_mctp_register_default_handler(struct mctp_client *client)
+{
+	struct aspeed_mctp *priv = client->priv;
+	int ret = 0;
+
+	spin_lock_bh(&priv->clients_lock);
+
+	if (!priv->default_client)
+		priv->default_client = client;
+	else if (priv->default_client != client)
+		ret = -EBUSY;
+
+	spin_unlock_bh(&priv->clients_lock);
+
+	return ret;
+}
+
+static int
+aspeed_mctp_register_type_handler(struct mctp_client *client,
+				  void __user *userbuf)
+{
+	struct aspeed_mctp *priv = client->priv;
+	struct aspeed_mctp_type_handler_ioctl handler;
+
+	if (copy_from_user(&handler, userbuf, sizeof(handler))) {
+		dev_err(priv->dev, "copy from user failed\n");
+		return -EFAULT;
+	}
+
+	return aspeed_mctp_add_type_handler(client, handler.mctp_type,
+					    handler.pci_vendor_id,
+					    handler.vendor_type,
+					    handler.vendor_type_mask);
+}
+
+static int
+aspeed_mctp_unregister_type_handler(struct mctp_client *client,
+				    void __user *userbuf)
+{
+	struct aspeed_mctp *priv = client->priv;
+	struct aspeed_mctp_type_handler_ioctl handler;
+
+	if (copy_from_user(&handler, userbuf, sizeof(handler))) {
+		dev_err(priv->dev, "copy from user failed\n");
+		return -EFAULT;
+	}
+
+	return aspeed_mctp_remove_type_handler(client, handler.mctp_type,
+					       handler.pci_vendor_id,
+					       handler.vendor_type,
+					       handler.vendor_type_mask);
+}
+
+static int
+aspeed_mctp_filter_eid(struct aspeed_mctp *priv, void __user *userbuf)
+{
+	struct aspeed_mctp_filter_eid eid;
+
+	if (copy_from_user(&eid, userbuf, sizeof(eid))) {
+		dev_err(priv->dev, "copy from user failed\n");
+		return -EFAULT;
+	}
+
+	if (eid.enable) {
+		regmap_write(priv->map, ASPEED_MCTP_EID, eid.eid);
+		regmap_update_bits(priv->map, ASPEED_MCTP_CTRL,
+				   MATCHING_EID, MATCHING_EID);
+	} else {
+		regmap_update_bits(priv->map, ASPEED_MCTP_CTRL,
+				   MATCHING_EID, 0);
+	}
+	return 0;
+}
+
+static int aspeed_mctp_get_bdf(struct aspeed_mctp *priv, void __user *userbuf)
+{
+	struct aspeed_mctp_get_bdf bdf = { _get_bdf(priv) };
+
+	if (copy_to_user(userbuf, &bdf, sizeof(bdf))) {
+		dev_err(priv->dev, "copy to user failed\n");
+		return -EFAULT;
+	}
+	return 0;
+}
+
+static int
+aspeed_mctp_get_medium_id(struct aspeed_mctp *priv, void __user *userbuf)
+{
+	struct aspeed_mctp_get_medium_id id = { 0x09 }; /* PCIe revision 2.0 */
+
+	if (copy_to_user(userbuf, &id, sizeof(id))) {
+		dev_err(priv->dev, "copy to user failed\n");
+		return -EFAULT;
+	}
+	return 0;
+}
+
+static int
+aspeed_mctp_get_mtu(struct aspeed_mctp *priv, void __user *userbuf)
+{
+	struct aspeed_mctp_get_mtu id = { ASPEED_MCTP_MTU };
+
+	if (copy_to_user(userbuf, &id, sizeof(id))) {
+		dev_err(priv->dev, "copy to user failed\n");
+		return -EFAULT;
+	}
+	return 0;
+}
+
+int aspeed_mctp_get_eid_bdf(struct mctp_client *client, u8 eid, u16 *bdf)
+{
+	struct aspeed_mctp_endpoint *endpoint;
+	int ret = -ENOENT;
+
+	mutex_lock(&client->priv->endpoints_lock);
+	list_for_each_entry(endpoint, &client->priv->endpoints, link) {
+		if (endpoint->data.eid_info.eid == eid) {
+			*bdf = endpoint->data.eid_info.bdf;
+			ret = 0;
+			break;
+		}
+	}
+	mutex_unlock(&client->priv->endpoints_lock);
+
+	return ret;
+}
+EXPORT_SYMBOL_GPL(aspeed_mctp_get_eid_bdf);
+
+int aspeed_mctp_get_eid(struct mctp_client *client, u16 bdf,
+			u8 domain_id, u8 *eid)
+{
+	struct aspeed_mctp_endpoint *endpoint;
+	int ret = -ENOENT;
+
+	mutex_lock(&client->priv->endpoints_lock);
+
+	list_for_each_entry(endpoint, &client->priv->endpoints, link) {
+		if (endpoint->data.eid_ext_info.domain_id == domain_id &&
+		    endpoint->data.eid_ext_info.bdf == bdf) {
+			*eid = endpoint->data.eid_ext_info.eid;
+			ret = 0;
+			break;
+		}
+	}
+
+	mutex_unlock(&client->priv->endpoints_lock);
+	return ret;
+}
+EXPORT_SYMBOL_GPL(aspeed_mctp_get_eid);
+
+static int
+aspeed_mctp_get_eid_info(struct aspeed_mctp *priv, void __user *userbuf,
+			 enum mctp_address_type addr_format)
+{
+	int count = 0;
+	int ret = 0;
+	struct aspeed_mctp_get_eid_info get_eid;
+	struct aspeed_mctp_endpoint *endpoint;
+	void *user_ptr;
+	size_t count_to_copy;
+
+	if (copy_from_user(&get_eid, userbuf, sizeof(get_eid))) {
+		dev_err(priv->dev, "copy from user failed\n");
+		return -EFAULT;
+	}
+
+	mutex_lock(&priv->endpoints_lock);
+
+	if (get_eid.count == 0) {
+		count = priv->endpoints_count;
+		goto out_unlock;
+	}
+
+	user_ptr = u64_to_user_ptr(get_eid.ptr);
+	count_to_copy = get_eid.count > priv->endpoints_count ?
+					priv->endpoints_count : get_eid.count;
+	list_for_each_entry(endpoint, &priv->endpoints, link) {
+		if (endpoint->data.eid_info.eid < get_eid.start_eid)
+			continue;
+		if (count >= count_to_copy)
+			break;
+
+		if (addr_format == ASPEED_MCTP_EXTENDED_ADDR_FORMAT)
+			ret = copy_to_user(&(((struct aspeed_mctp_eid_ext_info *)
+								user_ptr)[count]),
+							&endpoint->data,
+							sizeof(struct aspeed_mctp_eid_ext_info));
+		else
+			ret = copy_to_user(&(((struct aspeed_mctp_eid_info *)
+								user_ptr)[count]),
+							&endpoint->data,
+							sizeof(struct aspeed_mctp_eid_info));
+
+		if (ret) {
+			dev_err(priv->dev, "copy to user failed\n");
+			ret = -EFAULT;
+			goto out_unlock;
+		}
+		count++;
+	}
+
+out_unlock:
+	get_eid.count = count;
+	if (copy_to_user(userbuf, &get_eid, sizeof(get_eid))) {
+		dev_err(priv->dev, "copy to user failed\n");
+		ret = -EFAULT;
+	}
+
+	mutex_unlock(&priv->endpoints_lock);
+	return ret;
+}
+
+static int
+eid_info_cmp(void *priv, const struct list_head *a, const struct list_head *b)
+{
+	struct aspeed_mctp_endpoint *endpoint_a;
+	struct aspeed_mctp_endpoint *endpoint_b;
+
+	if (a == b)
+		return 0;
+
+	endpoint_a = list_entry(a, typeof(*endpoint_a), link);
+	endpoint_b = list_entry(b, typeof(*endpoint_b), link);
+
+	if (endpoint_a->data.eid_info.eid < endpoint_b->data.eid_info.eid)
+		return -1;
+	else if (endpoint_a->data.eid_info.eid > endpoint_b->data.eid_info.eid)
+		return 1;
+
+	return 0;
+}
+
+static void aspeed_mctp_eid_info_list_remove(struct list_head *list)
+{
+	struct aspeed_mctp_endpoint *endpoint;
+	struct aspeed_mctp_endpoint *tmp;
+
+	list_for_each_entry_safe(endpoint, tmp, list, link) {
+		list_del(&endpoint->link);
+		kfree(endpoint);
+	}
+}
+
+static bool
+aspeed_mctp_eid_info_list_valid(struct list_head *list)
+{
+	struct aspeed_mctp_endpoint *endpoint;
+	struct aspeed_mctp_endpoint *next;
+
+	list_for_each_entry(endpoint, list, link) {
+		next = list_next_entry(endpoint, link);
+		if (&next->link == list)
+			break;
+
+		/* duplicted eids */
+		if (next->data.eid_info.eid == endpoint->data.eid_info.eid)
+			return false;
+	}
+
+	return true;
+}
+
+static int
+aspeed_mctp_set_eid_info(struct aspeed_mctp *priv, void __user *userbuf,
+			 enum mctp_address_type addr_format)
+{
+	struct list_head list = LIST_HEAD_INIT(list);
+	struct aspeed_mctp_set_eid_info set_eid;
+	void *user_ptr;
+	struct aspeed_mctp_endpoint *endpoint;
+	int ret = 0;
+	u8 eid = 0;
+	size_t i;
+
+	if (copy_from_user(&set_eid, userbuf, sizeof(set_eid))) {
+		dev_err(priv->dev, "copy from user failed\n");
+		return -EFAULT;
+	}
+
+	if (set_eid.count > ASPEED_MCTP_EID_INFO_MAX)
+		return -EINVAL;
+
+	user_ptr = u64_to_user_ptr(set_eid.ptr);
+	for (i = 0; i < set_eid.count; i++) {
+		endpoint = kzalloc(sizeof(*endpoint), GFP_KERNEL);
+		if (!endpoint) {
+			ret = -ENOMEM;
+			goto out;
+		}
+		memset(endpoint, 0, sizeof(*endpoint));
+
+		if (addr_format == ASPEED_MCTP_EXTENDED_ADDR_FORMAT)
+			ret = copy_from_user(&endpoint->data,
+					     &(((struct aspeed_mctp_eid_ext_info *)
+								user_ptr)[i]),
+					     sizeof(struct aspeed_mctp_eid_ext_info));
+		else
+			ret = copy_from_user(&endpoint->data,
+					     &(((struct aspeed_mctp_eid_info *)
+								user_ptr)[i]),
+						 sizeof(struct aspeed_mctp_eid_info));
+
+		if (ret) {
+			dev_err(priv->dev, "copy from user failed\n");
+			kfree(endpoint);
+			ret = -EFAULT;
+			goto out;
+		}
+
+		/* Detect self EID */
+		if (_get_bdf(priv) == endpoint->data.eid_info.bdf) {
+			/*
+			 * XXX Use smallest EID with matching BDF.
+			 * On some platforms there could be multiple endpoints
+			 * with same BDF in routing table.
+			 */
+			if (eid == 0 || endpoint->data.eid_info.eid < eid)
+				eid = endpoint->data.eid_info.eid;
+		}
+
+	list_add_tail(&endpoint->link, &list);
+	}
+
+	list_sort(NULL, &list, eid_info_cmp);
+	if (!aspeed_mctp_eid_info_list_valid(&list)) {
+		ret = -EINVAL;
+		goto out;
+	}
+
+	mutex_lock(&priv->endpoints_lock);
+	if (list_empty(&priv->endpoints))
+		list_splice_init(&list, &priv->endpoints);
+	else
+		list_swap(&list, &priv->endpoints);
+	priv->endpoints_count = set_eid.count;
+	priv->eid = eid;
+	mutex_unlock(&priv->endpoints_lock);
+out:
+	aspeed_mctp_eid_info_list_remove(&list);
+	return ret;
+}
+
+static int aspeed_mctp_set_own_eid(struct aspeed_mctp *priv, void __user *userbuf)
+{
+	struct aspeed_mctp_set_own_eid data;
+
+	if (copy_from_user(&data, userbuf, sizeof(data))) {
+		dev_err(priv->dev, "copy from user failed\n");
+		return -EFAULT;
+	}
+
+	priv->eid = data.eid;
+
+	return 0;
+}
+
+static long
+aspeed_mctp_ioctl(struct file *file, unsigned int cmd, unsigned long arg)
+{
+	struct mctp_client *client = file->private_data;
+	struct aspeed_mctp *priv = client->priv;
+	void __user *userbuf = (void __user *)arg;
+	int ret;
+
+	switch (cmd) {
+	case ASPEED_MCTP_IOCTL_FILTER_EID:
+		ret = aspeed_mctp_filter_eid(priv, userbuf);
+	break;
+
+	case ASPEED_MCTP_IOCTL_GET_BDF:
+		ret = aspeed_mctp_get_bdf(priv, userbuf);
+	break;
+
+	case ASPEED_MCTP_IOCTL_GET_MEDIUM_ID:
+		ret = aspeed_mctp_get_medium_id(priv, userbuf);
+	break;
+
+	case ASPEED_MCTP_IOCTL_GET_MTU:
+		ret = aspeed_mctp_get_mtu(priv, userbuf);
+	break;
+
+	case ASPEED_MCTP_IOCTL_REGISTER_DEFAULT_HANDLER:
+		ret = aspeed_mctp_register_default_handler(client);
+	break;
+
+	case ASPEED_MCTP_IOCTL_REGISTER_TYPE_HANDLER:
+		ret = aspeed_mctp_register_type_handler(client, userbuf);
+	break;
+
+	case ASPEED_MCTP_IOCTL_UNREGISTER_TYPE_HANDLER:
+		ret = aspeed_mctp_unregister_type_handler(client, userbuf);
+	break;
+
+	case ASPEED_MCTP_IOCTL_GET_EID_INFO:
+		ret = aspeed_mctp_get_eid_info(priv, userbuf, ASPEED_MCTP_GENERIC_ADDR_FORMAT);
+	break;
+
+	case ASPEED_MCTP_IOCTL_GET_EID_EXT_INFO:
+		ret = aspeed_mctp_get_eid_info(priv, userbuf, ASPEED_MCTP_EXTENDED_ADDR_FORMAT);
+	break;
+
+	case ASPEED_MCTP_IOCTL_SET_EID_INFO:
+		ret = aspeed_mctp_set_eid_info(priv, userbuf, ASPEED_MCTP_GENERIC_ADDR_FORMAT);
+	break;
+
+	case ASPEED_MCTP_IOCTL_SET_EID_EXT_INFO:
+		ret = aspeed_mctp_set_eid_info(priv, userbuf, ASPEED_MCTP_EXTENDED_ADDR_FORMAT);
+	break;
+
+	case ASPEED_MCTP_IOCTL_SET_OWN_EID:
+		ret = aspeed_mctp_set_own_eid(priv, userbuf);
+	break;
+
+	default:
+		dev_err(priv->dev, "Command not found\n");
+		ret = -ENOTTY;
+	}
+
+	return ret;
+}
+
+static __poll_t aspeed_mctp_poll(struct file *file,
+				 struct poll_table_struct *pt)
+{
+	struct mctp_client *client = file->private_data;
+	__poll_t ret = 0;
+	struct aspeed_mctp *priv = client->priv;
+	struct mctp_channel *rx = &priv->rx;
+	u32 mctp_ctrl;
+	u32 mctp_int_sts;
+
+	if (priv->miss_mctp_int) {
+		regmap_read(priv->map, ASPEED_MCTP_CTRL, &mctp_ctrl);
+		if (!(mctp_ctrl & RX_CMD_READY))
+			rx->stopped = true;
+		/* Polling the RX_CMD_RECEIVE_INT to ensure rx_tasklet can find the data */
+		regmap_read(priv->map, ASPEED_MCTP_INT_STS, &mctp_int_sts);
+		if (mctp_int_sts & RX_CMD_RECEIVE_INT) {
+			regmap_write(priv->map, ASPEED_MCTP_INT_STS,
+				     mctp_int_sts);
+			tasklet_hi_schedule(&priv->rx.tasklet);
+		}
+	}
+
+	poll_wait(file, &client->wait_queue, pt);
+
+	if (!ptr_ring_full_bh(&client->tx_queue))
+		ret |= EPOLLOUT;
+
+	if (__ptr_ring_peek(&client->rx_queue))
+		ret |= EPOLLIN;
+
+	return ret;
+}
+
+static const struct file_operations aspeed_mctp_fops = {
+	.owner = THIS_MODULE,
+	.open = aspeed_mctp_open,
+	.release = aspeed_mctp_release,
+	.read = aspeed_mctp_read,
+	.write = aspeed_mctp_write,
+	.unlocked_ioctl = aspeed_mctp_ioctl,
+	.poll = aspeed_mctp_poll,
+};
+
+static const struct regmap_config aspeed_mctp_regmap_cfg = {
+	.reg_bits	= 32,
+	.reg_stride	= 4,
+	.val_bits	= 32,
+	.max_register	= ASPEED_MCTP_TX_BUF_WR_PTR,
+};
+
+struct device_type aspeed_mctp_type = {
+	.name		= "aspeed-mctp",
+};
+
+static void aspeed_mctp_send_pcie_uevent(struct kobject *kobj, bool ready)
+{
+	char *pcie_not_ready_event[] = { ASPEED_MCTP_READY "=0", NULL };
+	char *pcie_ready_event[] = { ASPEED_MCTP_READY "=1", NULL };
+
+	kobject_uevent_env(kobj, KOBJ_CHANGE,
+			   ready ? pcie_ready_event : pcie_not_ready_event);
+}
+
+static u16 aspeed_mctp_pcie_setup(struct aspeed_mctp *priv)
+{
+	u32 reg;
+	u16 bdf;
+
+	regmap_read(priv->pcie.map, ASPEED_PCIE_MISC_STS_1, &reg);
+
+	reg = reg & (PCI_BUS_NUM_MASK | PCI_DEV_NUM_MASK);
+	bdf = PCI_DEVID(GET_PCI_BUS_NUM(reg), GET_PCI_DEV_NUM(reg));
+	if (reg != 0)
+		cancel_delayed_work(&priv->pcie.rst_dwork);
+	else {
+		schedule_delayed_work(&priv->pcie.rst_dwork,
+				      msecs_to_jiffies(1000));
+		bdf = 0;
+	}
+	return bdf;
+}
+
+static void aspeed_mctp_irq_enable(struct aspeed_mctp *priv)
+{
+	u32 enable = TX_CMD_SENT_INT | TX_CMD_WRONG_INT |
+		     RX_CMD_RECEIVE_INT | RX_CMD_NO_MORE_INT;
+
+	regmap_write(priv->map, ASPEED_MCTP_INT_EN, enable);
+}
+
+static void aspeed_mctp_irq_disable(struct aspeed_mctp *priv)
+{
+	regmap_write(priv->map, ASPEED_MCTP_INT_EN, 0);
+}
+
+static void aspeed_mctp_reset_work(struct work_struct *work)
+{
+	struct aspeed_mctp *priv = container_of(work, typeof(*priv),
+						pcie.rst_dwork.work);
+	struct kobject *kobj = &priv->mctp_miscdev.this_device->kobj;
+	u16 bdf;
+
+	if (priv->pcie.need_uevent) {
+		aspeed_mctp_send_pcie_uevent(kobj, false);
+		priv->pcie.need_uevent = false;
+	}
+
+	bdf = aspeed_mctp_pcie_setup(priv);
+	if (bdf) {
+		if (priv->match_data->need_address_mapping)
+			regmap_update_bits(priv->map, ASPEED_MCTP_EID,
+					   MEMORY_SPACE_MAPPING, BIT(31));
+		/*
+		 * In some condition, tx som and eom will not match expected result.
+		 * e.g. When Maximum Transmit Unit (MTU) set to 64 byte, and then transfer
+		 * size set between 61 ~ 124 (MTU-3 ~ 2*MTU-4), the engine will set all
+		 * packet vdm header eom to 1, no matter what it setted. To fix that
+		 * issue, the driver set MTU to next level(e.g. 64 to 128).
+		 */
+		regmap_update_bits(priv->map, ASPEED_MCTP_ENGINE_CTRL,
+				   TX_MAX_PAYLOAD_SIZE_MASK,
+				   FIELD_GET(TX_MAX_PAYLOAD_SIZE_MASK, fls(ASPEED_MCTP_MTU >> 6)));
+		aspeed_mctp_flush_all_tx_queues(priv);
+		if (!priv->miss_mctp_int)
+			aspeed_mctp_irq_enable(priv);
+		aspeed_mctp_rx_trigger(&priv->rx);
+		aspeed_mctp_send_pcie_uevent(kobj, true);
+	}
+}
+
+static void aspeed_mctp_channels_init(struct aspeed_mctp *priv)
+{
+	aspeed_mctp_rx_chan_init(&priv->rx);
+	aspeed_mctp_tx_chan_init(&priv->tx);
+}
+
+static irqreturn_t aspeed_mctp_irq_handler(int irq, void *arg)
+{
+	struct aspeed_mctp *priv = arg;
+	u32 handled = 0;
+	u32 status;
+
+	regmap_read(priv->map, ASPEED_MCTP_INT_STS, &status);
+	regmap_write(priv->map, ASPEED_MCTP_INT_STS, status);
+
+	if (status & TX_CMD_SENT_INT) {
+		tasklet_hi_schedule(&priv->tx.tasklet);
+		if (!priv->match_data->fifo_auto_surround)
+			priv->tx.rd_ptr = priv->tx.rd_ptr + 1 % TX_PACKET_COUNT;
+		handled |= TX_CMD_SENT_INT;
+	}
+
+	if (status & TX_CMD_WRONG_INT) {
+		/* TODO: print the actual command */
+		dev_warn(priv->dev, "TX wrong");
+
+		handled |= TX_CMD_WRONG_INT;
+	}
+
+	if (status & RX_CMD_RECEIVE_INT) {
+		tasklet_hi_schedule(&priv->rx.tasklet);
+
+		handled |= RX_CMD_RECEIVE_INT;
+	}
+
+	if (status & RX_CMD_NO_MORE_INT) {
+		dev_dbg(priv->dev, "RX full");
+		priv->rx.stopped = true;
+		tasklet_hi_schedule(&priv->rx.tasklet);
+
+		handled |= RX_CMD_NO_MORE_INT;
+	}
+
+	if (!handled)
+		return IRQ_NONE;
+
+	return IRQ_HANDLED;
+}
+
+static irqreturn_t aspeed_mctp_pcie_rst_irq_handler(int irq, void *arg)
+{
+	struct aspeed_mctp *priv = arg;
+
+	aspeed_mctp_channels_init(priv);
+
+	priv->pcie.need_uevent = true;
+	priv->eid = 0;
+
+	schedule_delayed_work(&priv->pcie.rst_dwork, 0);
+
+	return IRQ_HANDLED;
+}
+
+static void aspeed_mctp_drv_init(struct aspeed_mctp *priv)
+{
+	INIT_LIST_HEAD(&priv->clients);
+	INIT_LIST_HEAD(&priv->mctp_type_handlers);
+	INIT_LIST_HEAD(&priv->endpoints);
+
+	spin_lock_init(&priv->clients_lock);
+	mutex_init(&priv->endpoints_lock);
+
+	INIT_DELAYED_WORK(&priv->pcie.rst_dwork, aspeed_mctp_reset_work);
+
+	tasklet_init(&priv->tx.tasklet, aspeed_mctp_tx_tasklet,
+		     (unsigned long)&priv->tx);
+	tasklet_init(&priv->rx.tasklet, aspeed_mctp_rx_tasklet,
+		     (unsigned long)&priv->rx);
+}
+
+static void aspeed_mctp_drv_fini(struct aspeed_mctp *priv)
+{
+	aspeed_mctp_eid_info_list_remove(&priv->endpoints);
+	tasklet_disable(&priv->tx.tasklet);
+	tasklet_kill(&priv->tx.tasklet);
+	tasklet_disable(&priv->rx.tasklet);
+	tasklet_kill(&priv->rx.tasklet);
+
+	cancel_delayed_work_sync(&priv->pcie.rst_dwork);
+}
+
+static int aspeed_mctp_resources_init(struct aspeed_mctp *priv)
+{
+	struct platform_device *pdev = to_platform_device(priv->dev);
+	void __iomem *regs;
+
+	regs = devm_platform_ioremap_resource(pdev, 0);
+	if (IS_ERR(regs)) {
+		dev_err(priv->dev, "Failed to get regmap!\n");
+		return PTR_ERR(regs);
+	}
+
+	priv->map = devm_regmap_init_mmio(priv->dev, regs,
+					  &aspeed_mctp_regmap_cfg);
+	if (IS_ERR(priv->map))
+		return PTR_ERR(priv->map);
+
+	priv->reset =
+		priv->rc_f ?
+			      devm_reset_control_get_by_index(priv->dev, 0) :
+			      devm_reset_control_get_shared_by_index(priv->dev, 0);
+	if (IS_ERR(priv->reset)) {
+		dev_err(priv->dev, "Failed to get reset!\n");
+		return PTR_ERR(priv->reset);
+	}
+
+	if (priv->rc_f) {
+		priv->reset_dma = devm_reset_control_get_shared_by_index(priv->dev, 1);
+		if (IS_ERR(priv->reset_dma)) {
+			dev_err(priv->dev, "Failed to get ep reset!\n");
+			return PTR_ERR(priv->reset_dma);
+		}
+	}
+	priv->pcie.map =
+		syscon_regmap_lookup_by_phandle(priv->dev->of_node,
+						"aspeed,pcieh");
+	if (IS_ERR(priv->pcie.map)) {
+		dev_err(priv->dev, "Failed to find PCIe Host regmap!\n");
+		return PTR_ERR(priv->pcie.map);
+	}
+
+	platform_set_drvdata(pdev, priv);
+
+	return 0;
+}
+
+static int aspeed_mctp_dma_init(struct aspeed_mctp *priv)
+{
+	struct mctp_channel *tx = &priv->tx;
+	struct mctp_channel *rx = &priv->rx;
+	int ret = -ENOMEM;
+
+	BUILD_BUG_ON(TX_PACKET_COUNT >= TX_MAX_PACKET_COUNT);
+	BUILD_BUG_ON(RX_PACKET_COUNT >= RX_MAX_PACKET_COUNT);
+
+	tx->cmd.vaddr = dma_alloc_coherent(priv->dev, TX_CMD_BUF_SIZE,
+					   &tx->cmd.dma_handle, GFP_KERNEL);
+
+	if (!tx->cmd.vaddr)
+		return ret;
+
+	tx->data.vaddr = dma_alloc_coherent(
+		priv->dev,
+		PAGE_ALIGN(TX_PACKET_COUNT *
+			   priv->match_data->packet_unit_size),
+		&tx->data.dma_handle, GFP_KERNEL);
+
+	if (!tx->data.vaddr)
+		goto out_tx_data;
+
+	rx->cmd.vaddr = dma_alloc_coherent(
+		priv->dev,
+		PAGE_ALIGN(priv->rx_packet_count * priv->match_data->rx_cmd_size),
+		&rx->cmd.dma_handle, GFP_KERNEL);
+
+	if (!rx->cmd.vaddr)
+		goto out_tx_cmd;
+
+	rx->data.vaddr = dma_alloc_coherent(
+		priv->dev,
+		PAGE_ALIGN(priv->rx_packet_count * priv->match_data->packet_unit_size),
+		&rx->data.dma_handle, GFP_KERNEL);
+
+	if (!rx->data.vaddr)
+		goto out_rx_data;
+
+	return 0;
+out_rx_data:
+	dma_free_coherent(
+		priv->dev,
+		PAGE_ALIGN(priv->rx_packet_count * priv->match_data->rx_cmd_size),
+		rx->cmd.vaddr, rx->cmd.dma_handle);
+
+out_tx_cmd:
+	dma_free_coherent(priv->dev,
+			  PAGE_ALIGN(TX_PACKET_COUNT *
+				     priv->match_data->packet_unit_size),
+			  tx->data.vaddr, tx->data.dma_handle);
+
+out_tx_data:
+	dma_free_coherent(priv->dev, TX_CMD_BUF_SIZE, tx->cmd.vaddr,
+			  tx->cmd.dma_handle);
+	return ret;
+}
+
+static void aspeed_mctp_dma_fini(struct aspeed_mctp *priv)
+{
+	struct mctp_channel *tx = &priv->tx;
+	struct mctp_channel *rx = &priv->rx;
+
+	dma_free_coherent(priv->dev, TX_CMD_BUF_SIZE, tx->cmd.vaddr,
+			  tx->cmd.dma_handle);
+
+	dma_free_coherent(
+		priv->dev,
+		PAGE_ALIGN(priv->rx_packet_count * priv->match_data->rx_cmd_size),
+		rx->cmd.vaddr, rx->cmd.dma_handle);
+
+	dma_free_coherent(priv->dev,
+			  PAGE_ALIGN(TX_PACKET_COUNT *
+				     priv->match_data->packet_unit_size),
+			  tx->data.vaddr, tx->data.dma_handle);
+
+	dma_free_coherent(priv->dev,
+			  PAGE_ALIGN(priv->rx_packet_count *
+				     priv->match_data->packet_unit_size),
+			  rx->data.vaddr, rx->data.dma_handle);
+}
+
+static int aspeed_mctp_irq_init(struct aspeed_mctp *priv)
+{
+	struct platform_device *pdev = to_platform_device(priv->dev);
+	int irq, ret;
+
+	irq = platform_get_irq_byname_optional(pdev, "mctp");
+	if (irq < 0) {
+		/* mctp irq is option */
+		priv->miss_mctp_int = 1;
+	} else {
+		ret = devm_request_irq(priv->dev, irq, aspeed_mctp_irq_handler,
+				       IRQF_SHARED, dev_name(&pdev->dev), priv);
+		if (ret)
+			return ret;
+		aspeed_mctp_irq_enable(priv);
+	}
+	irq = platform_get_irq_byname(pdev, "pcie");
+	if (!irq)
+		return -ENODEV;
+
+	ret = devm_request_irq(priv->dev, irq, aspeed_mctp_pcie_rst_irq_handler,
+			       IRQF_SHARED, dev_name(&pdev->dev), priv);
+	if (ret)
+		return ret;
+
+	return 0;
+}
+
+static void aspeed_mctp_hw_reset(struct aspeed_mctp *priv)
+{
+	u32 reg;
+
+	/*
+	 * XXX: We need to skip the reset when we probe multiple times.
+	 * Currently calling reset more than once seems to make the HW upset,
+	 * however, we do need to reset once after the first boot before we're
+	 * able to use the HW.
+	 */
+	if (!priv->rc_f) {
+		regmap_read(priv->map, ASPEED_MCTP_TX_BUF_ADDR, &reg);
+
+		if (reg) {
+			dev_info(priv->dev,
+				"Already initialized - skipping hardware reset\n");
+			return;
+		}
+	}
+
+	if (reset_control_deassert(priv->reset) != 0)
+		dev_warn(priv->dev, "Failed to deassert reset\n");
+
+	if (priv->rc_f) {
+		if (reset_control_deassert(priv->reset_dma) != 0)
+			dev_warn(priv->dev, "Failed to deassert ep reset\n");
+	}
+}
+
+static int aspeed_mctp_probe(struct platform_device *pdev)
+{
+	struct aspeed_mctp *priv;
+	int ret;
+	u16 bdf;
+
+	priv = devm_kzalloc(&pdev->dev, sizeof(*priv), GFP_KERNEL);
+	if (!priv) {
+		ret = -ENOMEM;
+		goto out;
+	}
+	priv->dev = &pdev->dev;
+	priv->rc_f =
+		of_find_property(priv->dev->of_node, "pcie_rc", NULL) ? 1 : 0;
+	priv->match_data = of_device_get_match_data(priv->dev);
+
+	ret = device_property_read_u32(priv->dev, "aspeed,rx-packet-count",
+				       &priv->rx_packet_count);
+	if (ret) {
+		priv->rx_packet_count = RX_PACKET_COUNT;
+	} else if (priv->rx_packet_count % 4 ||
+		   priv->rx_packet_count >= RX_MAX_PACKET_COUNT) {
+		dev_err(priv->dev,
+			"The aspeed,rx-packet-count:%d should be 4-aligned and less than %ld",
+			priv->rx_packet_count, RX_MAX_PACKET_COUNT);
+		ret = -EINVAL;
+		goto out;
+	}
+
+	ret = device_property_read_u32(priv->dev, "aspeed,rx-ring-count",
+				       &priv->rx_ring_count);
+	if (ret)
+		priv->rx_ring_count = RX_RING_COUNT;
+
+	ret = device_property_read_u32(priv->dev, "aspeed,tx-ring-count",
+				       &priv->tx_ring_count);
+	if (ret)
+		priv->tx_ring_count = TX_RING_COUNT;
+
+	aspeed_mctp_drv_init(priv);
+
+	ret = aspeed_mctp_resources_init(priv);
+	if (ret) {
+		dev_err(priv->dev, "Failed to init resources\n");
+		goto out_drv;
+	}
+
+	ret = aspeed_mctp_dma_init(priv);
+	if (ret) {
+		dev_err(priv->dev, "Failed to init DMA\n");
+		goto out_drv;
+	}
+
+	aspeed_mctp_hw_reset(priv);
+
+	aspeed_mctp_channels_init(priv);
+
+	priv->mctp_miscdev.parent = priv->dev;
+	priv->mctp_miscdev.minor = MISC_DYNAMIC_MINOR;
+	priv->mctp_miscdev.name =
+		priv->rc_f ? "aspeed-mctp1" : "aspeed-mctp";
+	priv->mctp_miscdev.fops = &aspeed_mctp_fops;
+	ret = misc_register(&priv->mctp_miscdev);
+	if (ret) {
+		dev_err(priv->dev, "Failed to register miscdev\n");
+		goto out_dma;
+	}
+	priv->mctp_miscdev.this_device->type = &aspeed_mctp_type;
+
+	ret = aspeed_mctp_irq_init(priv);
+	if (ret) {
+		dev_err(priv->dev, "Failed to init IRQ!\n");
+		goto out_dma;
+	}
+	bdf = aspeed_mctp_pcie_setup(priv);
+	if (bdf != 0) {
+		if (priv->match_data->need_address_mapping)
+			regmap_update_bits(priv->map, ASPEED_MCTP_EID,
+					   MEMORY_SPACE_MAPPING, BIT(31));
+		/*
+		 * In some condition, tx som and eom will not match expected result.
+		 * e.g. When Maximum Transmit Unit (MTU) set to 64 byte, and then transfer
+		 * size set between 61 ~ 124 (MTU-3 ~ 2*MTU-4), the engine will set all
+		 * packet vdm header eom to 1, no matter what it setted. To fix that
+		 * issue, the driver set MTU to next level(e.g. 64 to 128).
+		 */
+		regmap_update_bits(priv->map, ASPEED_MCTP_ENGINE_CTRL,
+				   TX_MAX_PAYLOAD_SIZE_MASK,
+				   FIELD_GET(TX_MAX_PAYLOAD_SIZE_MASK, fls(ASPEED_MCTP_MTU >> 6)));
+	}
+
+	priv->peci_mctp = platform_device_register_data(
+		priv->dev, priv->rc_f ? "peci1-mctp" : "peci0-mctp",
+		PLATFORM_DEVID_NONE, NULL, 0);
+	if (IS_ERR(priv->peci_mctp))
+		dev_err(priv->dev, "Failed to register peci-mctp device\n");
+
+	aspeed_mctp_rx_trigger(&priv->rx);
+
+	return 0;
+
+out_dma:
+	aspeed_mctp_dma_fini(priv);
+out_drv:
+	aspeed_mctp_drv_fini(priv);
+out:
+	dev_err(&pdev->dev, "Failed to probe Aspeed MCTP: %d\n", ret);
+	return ret;
+}
+
+static int aspeed_mctp_remove(struct platform_device *pdev)
+{
+	struct aspeed_mctp *priv = platform_get_drvdata(pdev);
+
+	platform_device_unregister(priv->peci_mctp);
+
+	misc_deregister(&priv->mctp_miscdev);
+
+	aspeed_mctp_irq_disable(priv);
+
+	aspeed_mctp_dma_fini(priv);
+
+	aspeed_mctp_drv_fini(priv);
+
+	if (priv->rc_f)
+		reset_control_assert(priv->reset_dma);
+
+	reset_control_assert(priv->reset);
+
+	return 0;
+}
+
+static const struct aspeed_mctp_match_data ast2500_mctp_match_data = {
+	.rx_cmd_size = sizeof(struct aspeed_mctp_rx_cmd),
+	.packet_unit_size = 128,
+	.need_address_mapping = true,
+	.vdm_hdr_direct_xfer = false,
+	.fifo_auto_surround = false,
+};
+
+static const struct aspeed_mctp_match_data ast2600_mctp_match_data = {
+	.rx_cmd_size = sizeof(u32),
+	.packet_unit_size = sizeof(struct mctp_pcie_packet_data),
+	.need_address_mapping = false,
+	.vdm_hdr_direct_xfer = true,
+	.fifo_auto_surround = true,
+};
+
+static const struct of_device_id aspeed_mctp_match_table[] = {
+	{ .compatible = "aspeed,ast2500-mctp", .data = &ast2500_mctp_match_data},
+	{ .compatible = "aspeed,ast2600-mctp", .data = &ast2600_mctp_match_data},
+	{ }
+};
+
+static struct platform_driver aspeed_mctp_driver = {
+	.driver	= {
+		.name		= "aspeed-mctp",
+		.of_match_table	= of_match_ptr(aspeed_mctp_match_table),
+	},
+	.probe	= aspeed_mctp_probe,
+	.remove	= aspeed_mctp_remove,
+};
+
+static int __init aspeed_mctp_init(void)
+{
+	packet_cache =
+		kmem_cache_create_usercopy("mctp-packet",
+					   sizeof(struct mctp_pcie_packet),
+					   0, 0, 0,
+					   sizeof(struct mctp_pcie_packet),
+					   NULL);
+	if (!packet_cache)
+		return -ENOMEM;
+
+	return platform_driver_register(&aspeed_mctp_driver);
+}
+
+static void __exit aspeed_mctp_exit(void)
+{
+	platform_driver_unregister(&aspeed_mctp_driver);
+	kmem_cache_destroy(packet_cache);
+}
+
+module_init(aspeed_mctp_init)
+module_exit(aspeed_mctp_exit)
+
+MODULE_DEVICE_TABLE(of, aspeed_mctp_match_table);
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Iwona Winiarska <iwona.winiarska@intel.com>");
+MODULE_DESCRIPTION("Aspeed MCTP driver");
diff --git a/drivers/soc/aspeed/aspeed-otp.c b/drivers/soc/aspeed/aspeed-otp.c
new file mode 100644
index 000000000000..afbdba435a8c
--- /dev/null
+++ b/drivers/soc/aspeed/aspeed-otp.c
@@ -0,0 +1,639 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Copyright (C) ASPEED Technology Inc.
+ */
+
+#include <linux/errno.h>
+#include <linux/fs.h>
+#include <linux/miscdevice.h>
+#include <linux/slab.h>
+#include <linux/platform_device.h>
+#include <linux/regmap.h>
+#include <linux/spinlock.h>
+#include <linux/uaccess.h>
+#include <linux/mfd/syscon.h>
+#include <linux/of.h>
+#include <linux/module.h>
+#include <asm/io.h>
+#include <uapi/linux/aspeed-otp.h>
+
+#define ASPEED_REVISION_ID0	0x04
+#define ASPEED_REVISION_ID1	0x14
+#define ID0_AST2600A0	0x05000303
+#define ID1_AST2600A0	0x05000303
+#define ID0_AST2600A1	0x05010303
+#define ID1_AST2600A1	0x05010303
+#define ID0_AST2600A2	0x05010303
+#define ID1_AST2600A2	0x05020303
+#define ID0_AST2600A3	0x05030303
+#define ID1_AST2600A3	0x05030303
+#define ID0_AST2620A1	0x05010203
+#define ID1_AST2620A1	0x05010203
+#define ID0_AST2620A2	0x05010203
+#define ID1_AST2620A2	0x05020203
+#define ID0_AST2620A3	0x05030203
+#define ID1_AST2620A3	0x05030203
+#define ID0_AST2605A2	0x05010103
+#define ID1_AST2605A2	0x05020103
+#define ID0_AST2605A3	0x05030103
+#define ID1_AST2605A3	0x05030103
+#define ID0_AST2625A3	0x05030403
+#define ID1_AST2625A3	0x05030403
+
+#define OTP_PROTECT_KEY	0x0
+#define  OTP_PASSWD	0x349fe38a
+#define OTP_COMMAND	0x4
+#define OTP_TIMING	0x8
+#define OTP_ADDR	0x10
+#define OTP_STATUS	0x14
+#define OTP_COMPARE_1	0x20
+#define OTP_COMPARE_2	0x24
+#define OTP_COMPARE_3	0x28
+#define OTP_COMPARE_4	0x2c
+#define SW_REV_ID0	0x68
+#define SW_REV_ID1	0x6c
+#define SEC_KEY_NUM	0x78
+#define RETRY		20
+
+struct aspeed_otp {
+	struct miscdevice miscdev;
+	void __iomem *reg_base;
+	bool is_open;
+	u32 otp_ver;
+	u32 *data;
+};
+
+static DEFINE_SPINLOCK(otp_state_lock);
+
+static inline u32 aspeed_otp_read(struct aspeed_otp *ctx, u32 reg)
+{
+	int val;
+
+	val = readl(ctx->reg_base + reg);
+	// printk("read:reg = 0x%08x, val = 0x%08x\n", reg, val);
+	return val;
+}
+
+static inline void aspeed_otp_write(struct aspeed_otp *ctx, u32 val, u32 reg)
+{
+	// printk("write:reg = 0x%08x, val = 0x%08x\n", reg, val);
+	writel(val, ctx->reg_base + reg);
+}
+
+static uint32_t chip_version(u32 revid0, u32 revid1)
+{
+	if (revid0 == ID0_AST2600A0 && revid1 == ID1_AST2600A0) {
+		/* AST2600-A0 */
+		return OTP_A0;
+	} else if (revid0 == ID0_AST2600A1 && revid1 == ID1_AST2600A1) {
+		/* AST2600-A1 */
+		return OTP_A1;
+	} else if (revid0 == ID0_AST2600A2 && revid1 == ID1_AST2600A2) {
+		/* AST2600-A2 */
+		return OTP_A2;
+	} else if (revid0 == ID0_AST2600A3 && revid1 == ID1_AST2600A3) {
+		/* AST2600-A3 */
+		return OTP_A3;
+	} else if (revid0 == ID0_AST2620A1 && revid1 == ID1_AST2620A1) {
+		/* AST2620-A1 */
+		return OTP_A1;
+	} else if (revid0 == ID0_AST2620A2 && revid1 == ID1_AST2620A2) {
+		/* AST2620-A2 */
+		return OTP_A2;
+	} else if (revid0 == ID0_AST2620A3 && revid1 == ID1_AST2620A3) {
+		/* AST2620-A3 */
+		return OTP_A3;
+	} else if (revid0 == ID0_AST2605A2 && revid1 == ID1_AST2605A2) {
+		/* AST2605-A2 */
+		return OTP_A2;
+	} else if (revid0 == ID0_AST2605A3 && revid1 == ID1_AST2605A3) {
+		/* AST2605-A3 */
+		return OTP_A3;
+	} else if (revid0 == ID0_AST2625A3 && revid1 == ID1_AST2625A3) {
+		/* AST2605-A3 */
+		return OTP_A3;
+	}
+	return -1;
+}
+
+static void wait_complete(struct aspeed_otp *ctx)
+{
+	int reg;
+	int i = 0;
+
+	do {
+		reg = aspeed_otp_read(ctx, OTP_STATUS);
+		if ((reg & 0x6) == 0x6)
+			i++;
+	} while (i != 2);
+}
+
+static void otp_write(struct aspeed_otp *ctx, u32 otp_addr, u32 val)
+{
+	aspeed_otp_write(ctx, otp_addr, OTP_ADDR); //write address
+	aspeed_otp_write(ctx, val, OTP_COMPARE_1); //write val
+	aspeed_otp_write(ctx, 0x23b1e362, OTP_COMMAND); //write command
+	wait_complete(ctx);
+}
+
+static void otp_soak(struct aspeed_otp *ctx, int soak)
+{
+	if (ctx->otp_ver == OTP_A2 || ctx->otp_ver == OTP_A3) {
+		switch (soak) {
+		case 0: //default
+			otp_write(ctx, 0x3000, 0x0); // Write MRA
+			otp_write(ctx, 0x5000, 0x0); // Write MRB
+			otp_write(ctx, 0x1000, 0x0); // Write MR
+			break;
+		case 1: //normal program
+			otp_write(ctx, 0x3000, 0x1320); // Write MRA
+			otp_write(ctx, 0x5000, 0x1008); // Write MRB
+			otp_write(ctx, 0x1000, 0x0024); // Write MR
+			aspeed_otp_write(ctx, 0x04191388, OTP_TIMING); // 200us
+			break;
+		case 2: //soak program
+			otp_write(ctx, 0x3000, 0x1320); // Write MRA
+			otp_write(ctx, 0x5000, 0x0007); // Write MRB
+			otp_write(ctx, 0x1000, 0x0100); // Write MR
+			aspeed_otp_write(ctx, 0x04193a98, OTP_TIMING); // 600us
+			break;
+		}
+	} else {
+		switch (soak) {
+		case 0: //default
+			otp_write(ctx, 0x3000, 0x0); // Write MRA
+			otp_write(ctx, 0x5000, 0x0); // Write MRB
+			otp_write(ctx, 0x1000, 0x0); // Write MR
+			break;
+		case 1: //normal program
+			otp_write(ctx, 0x3000, 0x4021); // Write MRA
+			otp_write(ctx, 0x5000, 0x302f); // Write MRB
+			otp_write(ctx, 0x1000, 0x4020); // Write MR
+			aspeed_otp_write(ctx, 0x04190760, OTP_TIMING); // 75us
+			break;
+		case 2: //soak program
+			otp_write(ctx, 0x3000, 0x4021); // Write MRA
+			otp_write(ctx, 0x5000, 0x1027); // Write MRB
+			otp_write(ctx, 0x1000, 0x4820); // Write MR
+			aspeed_otp_write(ctx, 0x041930d4, OTP_TIMING); // 500us
+			break;
+		}
+	}
+
+	wait_complete(ctx);
+}
+
+static int verify_bit(struct aspeed_otp *ctx, u32 otp_addr, int bit_offset, int value)
+{
+	u32 ret[2];
+
+	if (otp_addr % 2 == 0)
+		aspeed_otp_write(ctx, otp_addr, OTP_ADDR); //Read address
+	else
+		aspeed_otp_write(ctx, otp_addr - 1, OTP_ADDR); //Read address
+
+	aspeed_otp_write(ctx, 0x23b1e361, OTP_COMMAND); //trigger read
+	wait_complete(ctx);
+	ret[0] = aspeed_otp_read(ctx, OTP_COMPARE_1);
+	ret[1] = aspeed_otp_read(ctx, OTP_COMPARE_2);
+
+	if (otp_addr % 2 == 0) {
+		if (((ret[0] >> bit_offset) & 1) == value)
+			return 0;
+		else
+			return -1;
+	} else {
+		if (((ret[1] >> bit_offset) & 1) == value)
+			return 0;
+		else
+			return -1;
+	}
+}
+
+static void otp_prog(struct aspeed_otp *ctx, u32 otp_addr, u32 prog_bit)
+{
+	otp_write(ctx, 0x0, prog_bit);
+	aspeed_otp_write(ctx, otp_addr, OTP_ADDR); //write address
+	aspeed_otp_write(ctx, prog_bit, OTP_COMPARE_1); //write data
+	aspeed_otp_write(ctx, 0x23b1e364, OTP_COMMAND); //write command
+	wait_complete(ctx);
+}
+
+static void _otp_prog_bit(struct aspeed_otp *ctx, u32 value, u32 prog_address, u32 bit_offset)
+{
+	int prog_bit;
+
+	if (prog_address % 2 == 0) {
+		if (value)
+			prog_bit = ~(0x1 << bit_offset);
+		else
+			return;
+	} else {
+		if (ctx->otp_ver != OTP_A3)
+			prog_address |= 1 << 15;
+		if (!value)
+			prog_bit = 0x1 << bit_offset;
+		else
+			return;
+	}
+	otp_prog(ctx, prog_address, prog_bit);
+}
+
+static int otp_prog_bit(struct aspeed_otp *ctx, u32 value, u32 prog_address, u32 bit_offset)
+{
+	int pass;
+	int i;
+
+	otp_soak(ctx, 1);
+	_otp_prog_bit(ctx, value, prog_address, bit_offset);
+	pass = 0;
+
+	for (i = 0; i < RETRY; i++) {
+		if (verify_bit(ctx, prog_address, bit_offset, value) != 0) {
+			otp_soak(ctx, 2);
+			_otp_prog_bit(ctx, value, prog_address, bit_offset);
+			if (verify_bit(ctx, prog_address, bit_offset, value) != 0) {
+				otp_soak(ctx, 1);
+			} else {
+				pass = 1;
+				break;
+			}
+		} else {
+			pass = 1;
+			break;
+		}
+	}
+	otp_soak(ctx, 0);
+	return pass;
+}
+
+static void otp_read_conf_dw(struct aspeed_otp *ctx, u32 offset, u32 *buf)
+{
+	u32 config_offset;
+
+	config_offset = 0x800;
+	config_offset |= (offset / 8) * 0x200;
+	config_offset |= (offset % 8) * 0x2;
+
+	aspeed_otp_write(ctx, config_offset, OTP_ADDR); //Read address
+	aspeed_otp_write(ctx, 0x23b1e361, OTP_COMMAND); //trigger read
+	wait_complete(ctx);
+	buf[0] = aspeed_otp_read(ctx, OTP_COMPARE_1);
+}
+
+static void otp_read_conf(struct aspeed_otp *ctx, u32 offset, u32 len)
+{
+	int i, j;
+
+	otp_soak(ctx, 0);
+	for (i = offset, j = 0; j < len; i++, j++)
+		otp_read_conf_dw(ctx, i, &ctx->data[j]);
+}
+
+static void otp_read_data_2dw(struct aspeed_otp *ctx, u32 offset, u32 *buf)
+{
+	aspeed_otp_write(ctx, offset, OTP_ADDR); //Read address
+	aspeed_otp_write(ctx, 0x23b1e361, OTP_COMMAND); //trigger read
+	wait_complete(ctx);
+	buf[0] = aspeed_otp_read(ctx, OTP_COMPARE_1);
+	buf[1] = aspeed_otp_read(ctx, OTP_COMPARE_2);
+}
+
+static void otp_read_data(struct aspeed_otp *ctx, u32 offset, u32 len)
+{
+	int i, j;
+	u32 ret[2];
+
+	otp_soak(ctx, 0);
+
+	i = offset;
+	j = 0;
+	if (offset % 2) {
+		otp_read_data_2dw(ctx, i - 1, ret);
+		ctx->data[0] = ret[1];
+		i++;
+		j++;
+	}
+	for (; j < len; i += 2, j += 2)
+		otp_read_data_2dw(ctx, i, &ctx->data[j]);
+}
+
+static int otp_prog_data(struct aspeed_otp *ctx, u32 value, u32 dw_offset, u32 bit_offset)
+{
+	u32 read[2];
+	int otp_bit;
+
+	if (dw_offset % 2 == 0) {
+		otp_read_data_2dw(ctx, dw_offset, read);
+		otp_bit = (read[0] >> bit_offset) & 0x1;
+
+		if (otp_bit == 1 && value == 0) {
+			pr_err("OTPDATA%X[%X] = 1\n", dw_offset, bit_offset);
+			pr_err("OTP is programed, which can't be cleaned\n");
+			return -EINVAL;
+		}
+	} else {
+		otp_read_data_2dw(ctx, dw_offset - 1, read);
+		otp_bit = (read[1] >> bit_offset) & 0x1;
+
+		if (otp_bit == 0 && value == 1) {
+			pr_err("OTPDATA%X[%X] = 1\n", dw_offset, bit_offset);
+			pr_err("OTP is programed, which can't be writen\n");
+			return -EINVAL;
+		}
+	}
+	if (otp_bit == value) {
+		pr_err("OTPDATA%X[%X] = %d\n", dw_offset, bit_offset, value);
+		pr_err("No need to program\n");
+		return 0;
+	}
+
+	return otp_prog_bit(ctx, value, dw_offset, bit_offset);
+}
+
+static int otp_prog_conf(struct aspeed_otp *ctx, u32 value, u32 dw_offset, u32 bit_offset)
+{
+	u32 read;
+	u32 prog_address = 0;
+	int otp_bit;
+
+	otp_read_conf_dw(ctx, dw_offset, &read);
+
+	prog_address = 0x800;
+	prog_address |= (dw_offset / 8) * 0x200;
+	prog_address |= (dw_offset % 8) * 0x2;
+	otp_bit = (read >> bit_offset) & 0x1;
+	if (otp_bit == value) {
+		pr_err("OTPCFG%X[%X] = %d\n", dw_offset, bit_offset, value);
+		pr_err("No need to program\n");
+		return 0;
+	}
+	if (otp_bit == 1 && value == 0) {
+		pr_err("OTPCFG%X[%X] = 1\n", dw_offset, bit_offset);
+		pr_err("OTP is programed, which can't be clean\n");
+		return -EINVAL;
+	}
+
+	return otp_prog_bit(ctx, value, prog_address, bit_offset);
+}
+
+struct aspeed_otp *glob_ctx;
+
+void otp_read_data_buf(u32 offset, u32 *buf, u32 len)
+{
+	int i, j;
+	u32 ret[2];
+
+	aspeed_otp_write(glob_ctx, OTP_PASSWD, OTP_PROTECT_KEY);
+
+	otp_soak(glob_ctx, 0);
+
+	i = offset;
+	j = 0;
+	if (offset % 2) {
+		otp_read_data_2dw(glob_ctx, i - 1, ret);
+		buf[0] = ret[1];
+		i++;
+		j++;
+	}
+	for (; j < len; i += 2, j += 2)
+		otp_read_data_2dw(glob_ctx, i, &buf[j]);
+	aspeed_otp_write(glob_ctx, 0, OTP_PROTECT_KEY);
+}
+EXPORT_SYMBOL(otp_read_data_buf);
+
+void otp_read_conf_buf(u32 offset, u32 *buf, u32 len)
+{
+	int i, j;
+
+	aspeed_otp_write(glob_ctx, OTP_PASSWD, OTP_PROTECT_KEY);
+	otp_soak(glob_ctx, 0);
+	for (i = offset, j = 0; j < len; i++, j++)
+		otp_read_conf_dw(glob_ctx, i, &buf[j]);
+	aspeed_otp_write(glob_ctx, 0, OTP_PROTECT_KEY);
+}
+EXPORT_SYMBOL(otp_read_conf_buf);
+
+static long otp_ioctl(struct file *file, unsigned int cmd, unsigned long arg)
+{
+	struct miscdevice *c = file->private_data;
+	struct aspeed_otp *ctx = container_of(c, struct aspeed_otp, miscdev);
+	void __user *argp = (void __user *)arg;
+	struct otp_read xfer;
+	struct otp_prog prog;
+	u32 reg_read[2];
+	int ret = 0;
+
+	switch (cmd) {
+	case ASPEED_OTP_READ_DATA:
+		if (copy_from_user(&xfer, argp, sizeof(struct otp_read)))
+			return -EFAULT;
+		if ((xfer.offset + xfer.len) > 0x800) {
+			pr_err("out of range");
+			return -EINVAL;
+		}
+
+		aspeed_otp_write(ctx, OTP_PASSWD, OTP_PROTECT_KEY);
+		otp_read_data(ctx, xfer.offset, xfer.len);
+		aspeed_otp_write(ctx, 0, OTP_PROTECT_KEY);
+
+		if (copy_to_user(xfer.data, ctx->data, xfer.len * 4))
+			return -EFAULT;
+		if (copy_to_user(argp, &xfer, sizeof(struct otp_read)))
+			return -EFAULT;
+		break;
+	case ASPEED_OTP_READ_CONF:
+		if (copy_from_user(&xfer, argp, sizeof(struct otp_read)))
+			return -EFAULT;
+		if ((xfer.offset + xfer.len) > 0x800) {
+			pr_err("out of range");
+			return -EINVAL;
+		}
+
+		aspeed_otp_write(ctx, OTP_PASSWD, OTP_PROTECT_KEY);
+		otp_read_conf(ctx, xfer.offset, xfer.len);
+		aspeed_otp_write(ctx, 0, OTP_PROTECT_KEY);
+
+		if (copy_to_user(xfer.data, ctx->data, xfer.len * 4))
+			return -EFAULT;
+		if (copy_to_user(argp, &xfer, sizeof(struct otp_read)))
+			return -EFAULT;
+		break;
+	case ASPEED_OTP_PROG_DATA:
+		if (copy_from_user(&prog, argp, sizeof(struct otp_prog)))
+			return -EFAULT;
+		if (prog.bit_offset >= 32 || (prog.value != 0 && prog.value != 1)) {
+			pr_err("out of range");
+			return -EINVAL;
+		}
+		if (prog.dw_offset >= 0x800) {
+			pr_err("out of range");
+			return -EINVAL;
+		}
+		aspeed_otp_write(ctx, OTP_PASSWD, OTP_PROTECT_KEY);
+		ret = otp_prog_data(ctx, prog.value, prog.dw_offset, prog.bit_offset);
+		break;
+	case ASPEED_OTP_PROG_CONF:
+		if (copy_from_user(&prog, argp, sizeof(struct otp_prog)))
+			return -EFAULT;
+		if (prog.bit_offset >= 32 || (prog.value != 0 && prog.value != 1)) {
+			pr_err("out of range");
+			return -EINVAL;
+		}
+		if (prog.dw_offset >= 0x20) {
+			pr_err("out of range");
+			return -EINVAL;
+		}
+		aspeed_otp_write(ctx, OTP_PASSWD, OTP_PROTECT_KEY);
+		ret = otp_prog_conf(ctx, prog.value, prog.dw_offset, prog.bit_offset);
+		break;
+	case ASPEED_OTP_VER:
+		if (copy_to_user(argp, &ctx->otp_ver, sizeof(u32)))
+			return -EFAULT;
+		break;
+	case ASPEED_OTP_SW_RID:
+		reg_read[0] = aspeed_otp_read(ctx, SW_REV_ID0);
+		reg_read[1] = aspeed_otp_read(ctx, SW_REV_ID1);
+		if (copy_to_user(argp, reg_read, sizeof(u32) * 2))
+			return -EFAULT;
+		break;
+	case ASPEED_SEC_KEY_NUM:
+		reg_read[0] = aspeed_otp_read(ctx, SEC_KEY_NUM) & 3;
+		if (copy_to_user(argp, reg_read, sizeof(u32)))
+			return -EFAULT;
+		break;
+	}
+	return ret;
+}
+
+static int otp_open(struct inode *inode, struct file *file)
+{
+	struct miscdevice *c = file->private_data;
+	struct aspeed_otp *ctx = container_of(c, struct aspeed_otp, miscdev);
+
+	spin_lock(&otp_state_lock);
+
+	if (ctx->is_open) {
+		spin_unlock(&otp_state_lock);
+		return -EBUSY;
+	}
+
+	ctx->is_open = true;
+
+	spin_unlock(&otp_state_lock);
+
+	return 0;
+}
+
+static int otp_release(struct inode *inode, struct file *file)
+{
+	struct miscdevice *c = file->private_data;
+	struct aspeed_otp *ctx = container_of(c, struct aspeed_otp, miscdev);
+
+	spin_lock(&otp_state_lock);
+
+	ctx->is_open = false;
+
+	spin_unlock(&otp_state_lock);
+
+	return 0;
+}
+
+static const struct file_operations otp_fops = {
+	.owner =		THIS_MODULE,
+	.unlocked_ioctl =	otp_ioctl,
+	.open =			otp_open,
+	.release =		otp_release,
+};
+
+static const struct of_device_id aspeed_otp_of_matches[] = {
+	{ .compatible = "aspeed,ast2600-otp" },
+	{ }
+};
+MODULE_DEVICE_TABLE(of, aspeed_otp_of_matches);
+
+static int aspeed_otp_probe(struct platform_device *pdev)
+{
+	struct device *dev = &pdev->dev;
+	struct regmap *scu;
+	struct aspeed_otp *priv;
+	struct resource *res;
+	u32 revid0, revid1;
+	int rc;
+
+	priv = devm_kzalloc(dev, sizeof(*priv), GFP_KERNEL);
+	glob_ctx = priv;
+	if (!priv)
+		return -ENOMEM;
+
+	res = platform_get_resource(pdev, IORESOURCE_MEM, 0);
+	if (!res) {
+		dev_err(&pdev->dev, "cannot get IORESOURCE_MEM\n");
+		return -ENOENT;
+	}
+
+	priv->reg_base = devm_ioremap_resource(&pdev->dev, res);
+	if (!priv->reg_base)
+		return -EIO;
+
+	scu = syscon_regmap_lookup_by_phandle(dev->of_node, "aspeed,scu");
+	if (IS_ERR(scu)) {
+		dev_err(dev, "failed to find 2600 SCU regmap\n");
+		return PTR_ERR(scu);
+	}
+
+	regmap_read(scu, ASPEED_REVISION_ID0, &revid0);
+	regmap_read(scu, ASPEED_REVISION_ID1, &revid1);
+
+	priv->otp_ver = chip_version(revid0, revid1);
+
+	if (priv->otp_ver == -1) {
+		dev_err(dev, "invalid SCU\n");
+		return -EINVAL;
+	}
+
+	priv->data = kmalloc(8192, GFP_KERNEL);
+	if (!priv->data)
+		return -ENOMEM;
+
+	dev_set_drvdata(dev, priv);
+
+	/* Set up the miscdevice */
+	priv->miscdev.minor = MISC_DYNAMIC_MINOR;
+	priv->miscdev.name = "aspeed-otp";
+	priv->miscdev.fops = &otp_fops;
+
+	/* Register the device */
+	rc = misc_register(&priv->miscdev);
+	if (rc) {
+		dev_err(dev, "Unable to register device\n");
+		return rc;
+	}
+
+	return 0;
+}
+
+static int aspeed_otp_remove(struct platform_device *pdev)
+{
+	struct aspeed_otp *ctx = dev_get_drvdata(&pdev->dev);
+
+	kfree(ctx->data);
+	misc_deregister(&ctx->miscdev);
+
+	return 0;
+}
+
+static struct platform_driver aspeed_otp_driver = {
+	.probe = aspeed_otp_probe,
+	.remove = aspeed_otp_remove,
+	.driver = {
+		.name = KBUILD_MODNAME,
+		.of_match_table = aspeed_otp_of_matches,
+	},
+};
+
+module_platform_driver(aspeed_otp_driver);
+
+MODULE_AUTHOR("Johnny Huang <johnny_huang@aspeedtech.com>");
+MODULE_LICENSE("GPL");
+MODULE_DESCRIPTION("ASPEED OTP Driver");
diff --git a/drivers/soc/aspeed/aspeed-ssp.c b/drivers/soc/aspeed/aspeed-ssp.c
new file mode 100644
index 000000000000..1e84c2907267
--- /dev/null
+++ b/drivers/soc/aspeed/aspeed-ssp.c
@@ -0,0 +1,208 @@
+// SPDX-License-Identifier: GPL-2.0-or-later
+// Copyright (C) ASPEED Technology Inc.
+
+#include <linux/io.h>
+#include <linux/fs.h>
+#include <linux/mod_devicetable.h>
+#include <linux/module.h>
+#include <linux/miscdevice.h>
+#include <linux/delay.h>
+#include <linux/init.h>
+#include <linux/firmware.h>
+#include <linux/platform_device.h>
+#include <linux/interrupt.h>
+#include <linux/of.h>
+#include <linux/of_reserved_mem.h>
+#include <linux/of_address.h>
+#include <linux/of_irq.h>
+#include <linux/mfd/syscon.h>
+#include <linux/regmap.h>
+#include <linux/dma-mapping.h>
+
+#define SSP_FILE_NAME			"ast2600_ssp.bin"
+#define AST2600_CVIC_TRIGGER		0x28
+#define AST2600_CVIC_PENDING_STATUS	0x18
+#define AST2600_CVIC_PENDING_CLEAR	0x1C
+
+#define SSP_CTRL_REG			0xa00
+#define SSP_CTRL_RESET_ASSERT		BIT(1)
+#define SSP_CTRL_EN			BIT(0)
+
+#define SSP_MEM_BASE_REG		0xa04
+#define SSP_IMEM_LIMIT_REG		0xa08
+#define SSP_DMEM_LIMIT_REG		0xa0c
+#define SSP_CACHE_RANGE_REG		0xa40
+#define SSP_CACHE_INVALID_REG		0xa44
+#define SSP_CACHE_CTRL_REG		0xa48
+#define SSP_CACHE_CLEAR_ICACHE		BIT(2)
+#define SSP_CACHE_CLEAR_DCACHE		BIT(1)
+#define SSP_CACHE_EN			BIT(0)
+
+#define SSP_TOTAL_MEM_SZ		(32 * 1024 * 1024)
+#define SSP_CACHED_MEM_SZ		(16 * 1024 * 1024)
+#define SSP_UNCACHED_MEM_SZ		(SSP_TOTAL_MEM_SZ - SSP_CACHED_MEM_SZ)
+#define SSP_CACHE_1ST_16MB_ENABLE	BIT(0)
+
+struct ast2600_ssp {
+	struct device	*dev;
+	struct regmap	*scu;
+	dma_addr_t		ssp_mem_phy_addr;
+	void __iomem	*ssp_mem_vir_addr;
+	void __iomem	*cvic;
+	int			irq[16];
+	int			n_irq;
+};
+
+static int ast_ssp_open(struct inode *inode, struct file *file)
+{
+	return 0;
+}
+
+static int ast_ssp_release(struct inode *inode, struct file *file)
+{
+	return 0;
+}
+
+static const struct file_operations ast_ssp_fops = {
+	.owner		= THIS_MODULE,
+	.open		= ast_ssp_open,
+	.release	= ast_ssp_release,
+	.llseek		= no_llseek,
+};
+
+struct miscdevice ast_ssp_misc = {
+	.minor = MISC_DYNAMIC_MINOR,
+	.name = "ast-ssp",
+	.fops = &ast_ssp_fops,
+};
+
+static irqreturn_t ast2600_ssp_interrupt(int irq, void *dev_id)
+{
+	struct ast2600_ssp *priv = dev_id;
+	u32 isr = readl(priv->cvic + AST2600_CVIC_PENDING_STATUS);
+
+	dev_info(priv->dev, "isr %x\n", isr);
+	writel(isr, priv->cvic + AST2600_CVIC_PENDING_CLEAR);
+
+	return IRQ_HANDLED;
+}
+static int ast_ssp_probe(struct platform_device *pdev)
+{
+	struct device_node *np, *mnode = dev_of_node(&pdev->dev);
+	const struct firmware *firmware;
+	struct ast2600_ssp *priv;
+	int i, ret;
+
+	priv = kzalloc(sizeof(*priv), GFP_KERNEL);
+	if (!priv)
+		return -ENOMEM;
+
+	priv->dev = &pdev->dev;
+	priv->scu = syscon_regmap_lookup_by_phandle(priv->dev->of_node, "aspeed,scu");
+	if (IS_ERR(priv->scu)) {
+		dev_err(priv->dev, "failed to find SCU regmap\n");
+		return -EINVAL;
+	}
+	platform_set_drvdata(pdev, priv);
+
+	ret = misc_register(&ast_ssp_misc);
+	if (ret) {
+		pr_err("can't misc_register :(\n");
+		return -EIO;
+	}
+	dev_set_drvdata(ast_ssp_misc.this_device, pdev);
+
+	ret = of_reserved_mem_device_init(&pdev->dev);
+	if (ret) {
+		dev_err(priv->dev,
+			"failed to initialize reserved mem: %d\n", ret);
+	}
+	ret = dma_set_mask_and_coherent(&pdev->dev, DMA_BIT_MASK(32));
+	priv->ssp_mem_vir_addr = dma_alloc_coherent(priv->dev, SSP_TOTAL_MEM_SZ,
+					   &priv->ssp_mem_phy_addr, GFP_KERNEL);
+
+	dev_info(priv->dev, "Virtual addr = 0x%08x, PHY addr = 0x%08x\n",
+	       (uint32_t)priv->ssp_mem_vir_addr, priv->ssp_mem_phy_addr);
+	if (request_firmware(&firmware, SSP_FILE_NAME, &pdev->dev) < 0) {
+		dev_err(&pdev->dev, "don't have %s\n", SSP_FILE_NAME);
+		release_firmware(firmware);
+		return 0;
+	}
+
+	memcpy(priv->ssp_mem_vir_addr, (void *)firmware->data, firmware->size);
+	release_firmware(firmware);
+
+	np = of_parse_phandle(mnode, "aspeed,cvic", 0);
+	if (!np) {
+		dev_err(&pdev->dev, "can't find CVIC\n");
+		return -EINVAL;
+	}
+
+	priv->cvic = devm_of_iomap(&pdev->dev, np, 0, NULL);
+	if (IS_ERR(priv->cvic)) {
+		dev_err(&pdev->dev, "can't map CVIC\n");
+		return -EINVAL;
+	}
+
+	i = 0;
+	while (0 != (priv->irq[i] = irq_of_parse_and_map(mnode, i))) {
+		ret = request_irq(priv->irq[i], ast2600_ssp_interrupt, 0,
+				  "ssp-sw-irq", priv);
+		i++;
+	}
+	priv->n_irq = i;
+	dev_info(priv->dev, "%d ISRs registered\n", priv->n_irq);
+
+	regmap_write(priv->scu, SSP_CTRL_REG, 0);
+	mdelay(1);
+	regmap_write(priv->scu, SSP_MEM_BASE_REG, priv->ssp_mem_phy_addr);
+	regmap_write(priv->scu, SSP_IMEM_LIMIT_REG, priv->ssp_mem_phy_addr + SSP_CACHED_MEM_SZ);
+	regmap_write(priv->scu, SSP_DMEM_LIMIT_REG, priv->ssp_mem_phy_addr + SSP_TOTAL_MEM_SZ);
+
+	regmap_write(priv->scu, SSP_CACHE_RANGE_REG, SSP_CACHE_1ST_16MB_ENABLE);
+
+	regmap_write(priv->scu, SSP_CTRL_REG, SSP_CTRL_RESET_ASSERT);
+	mdelay(1);
+	regmap_write(priv->scu, SSP_CTRL_REG, 0);
+	mdelay(1);
+	regmap_write(priv->scu, SSP_CTRL_REG, SSP_CTRL_EN);
+	dev_info(priv->dev, "Init successful\n");
+	return 0;
+}
+
+static int ast_ssp_remove(struct platform_device *pdev)
+{
+	struct ast2600_ssp *priv = platform_get_drvdata(pdev);
+	int i;
+
+	dev_info(priv->dev, "SSP module removed\n");
+	regmap_write(priv->scu, SSP_CTRL_REG, 0);
+	for (i = 0; i < priv->n_irq; i++)
+		free_irq(priv->irq[i], priv);
+
+	dma_free_coherent(priv->dev, SSP_TOTAL_MEM_SZ, priv->ssp_mem_vir_addr, priv->ssp_mem_phy_addr);
+	kfree(priv);
+
+	misc_deregister((struct miscdevice *)&ast_ssp_misc);
+
+	return 0;
+}
+
+static const struct of_device_id of_ast_ssp_match_table[] = {
+	{ .compatible = "aspeed,ast2600-ssp", },
+	{},
+};
+MODULE_DEVICE_TABLE(of, of_ast_ssp_match_table);
+
+static struct platform_driver ast_ssp_driver = {
+	.probe		= ast_ssp_probe,
+	.remove		= ast_ssp_remove,
+	.driver		= {
+		.name	= KBUILD_MODNAME,
+		.of_match_table = of_ast_ssp_match_table,
+	},
+};
+
+module_platform_driver(ast_ssp_driver);
+
+MODULE_LICENSE("Dual BSD/GPL");
diff --git a/drivers/soc/aspeed/aspeed-udma.c b/drivers/soc/aspeed/aspeed-udma.c
new file mode 100644
index 000000000000..819115e738fb
--- /dev/null
+++ b/drivers/soc/aspeed/aspeed-udma.c
@@ -0,0 +1,464 @@
+// SPDX-License-Identifier: GPL-2.0-or-later
+/*
+ * Copyright 2020 Aspeed Technology Inc.
+ */
+#include <linux/io.h>
+#include <linux/module.h>
+#include <linux/interrupt.h>
+#include <linux/module.h>
+#include <linux/of.h>
+#include <linux/of_device.h>
+#include <linux/platform_device.h>
+#include <linux/dma-mapping.h>
+#include <linux/spinlock.h>
+#include <linux/soc/aspeed/aspeed-udma.h>
+#include <linux/delay.h>
+
+#define DEVICE_NAME "aspeed-udma"
+
+/* UART DMA registers offset */
+#define UDMA_TX_DMA_EN		0x000
+#define UDMA_RX_DMA_EN		0x004
+#define UDMA_MISC		0x008
+#define   UDMA_MISC_TX_BUFSZ_MASK	GENMASK(1, 0)
+#define   UDMA_MISC_TX_BUFSZ_SHIFT	0
+#define   UDMA_MISC_RX_BUFSZ_MASK	GENMASK(3, 2)
+#define   UDMA_MISC_RX_BUFSZ_SHIFT	2
+#define UDMA_TIMEOUT_TIMER	0x00c
+#define UDMA_TX_DMA_RST		0x020
+#define UDMA_RX_DMA_RST		0x024
+#define UDMA_TX_DMA_INT_EN	0x030
+#define UDMA_TX_DMA_INT_STAT	0x034
+#define UDMA_RX_DMA_INT_EN	0x038
+#define UDMA_RX_DMA_INT_STAT	0x03c
+
+#define UDMA_CHX_OFF(x)		((x) * 0x20)
+#define UDMA_CHX_TX_RD_PTR(x)	(0x040 + UDMA_CHX_OFF(x))
+#define UDMA_CHX_TX_WR_PTR(x)	(0x044 + UDMA_CHX_OFF(x))
+#define UDMA_CHX_TX_BUF_BASE(x)	(0x048 + UDMA_CHX_OFF(x))
+#define UDMA_CHX_TX_CTRL(x)	(0x04c + UDMA_CHX_OFF(x))
+#define   UDMA_TX_CTRL_TMOUT_DISABLE	BIT(4)
+#define   UDMA_TX_CTRL_BUFSZ_MASK	GENMASK(3, 0)
+#define   UDMA_TX_CTRL_BUFSZ_SHIFT	0
+#define UDMA_CHX_RX_RD_PTR(x)	(0x050 + UDMA_CHX_OFF(x))
+#define UDMA_CHX_RX_WR_PTR(x)	(0x054 + UDMA_CHX_OFF(x))
+#define UDMA_CHX_RX_BUF_BASE(x)	(0x058 + UDMA_CHX_OFF(x))
+#define UDMA_CHX_RX_CTRL(x)	(0x05c + UDMA_CHX_OFF(x))
+#define   UDMA_RX_CTRL_TMOUT_DISABLE	BIT(4)
+#define   UDMA_RX_CTRL_BUFSZ_MASK	GENMASK(3, 0)
+#define   UDMA_RX_CTRL_BUFSZ_SHIFT	0
+
+#define UDMA_MAX_CHANNEL	14
+#define UDMA_TIMEOUT		0x200
+
+enum aspeed_udma_bufsz_code {
+	UDMA_BUFSZ_CODE_1KB,
+	UDMA_BUFSZ_CODE_4KB,
+	UDMA_BUFSZ_CODE_16KB,
+	UDMA_BUFSZ_CODE_64KB,
+
+	/*
+	 * 128KB and above are supported ONLY for
+	 * virtual UARTs. For physical UARTs, the
+	 * size code is wrapped around at the 64K
+	 * boundary.
+	 */
+	UDMA_BUFSZ_CODE_128KB,
+	UDMA_BUFSZ_CODE_256KB,
+	UDMA_BUFSZ_CODE_512KB,
+	UDMA_BUFSZ_CODE_1024KB,
+	UDMA_BUFSZ_CODE_2048KB,
+	UDMA_BUFSZ_CODE_4096KB,
+	UDMA_BUFSZ_CODE_8192KB,
+	UDMA_BUFSZ_CODE_16384KB,
+};
+
+struct aspeed_udma_chan {
+	dma_addr_t dma_addr;
+
+	struct circ_buf *rb;
+	u32 rb_sz;
+
+	aspeed_udma_cb_t cb;
+	void *cb_arg;
+
+	bool dis_tmout;
+};
+
+struct aspeed_udma {
+	struct device *dev;
+	u8 __iomem *regs;
+	int irq;
+	struct aspeed_udma_chan tx_chs[UDMA_MAX_CHANNEL];
+	struct aspeed_udma_chan rx_chs[UDMA_MAX_CHANNEL];
+	spinlock_t lock;
+};
+
+struct aspeed_udma udma[1];
+
+static int aspeed_udma_get_bufsz_code(u32 buf_sz)
+{
+	switch (buf_sz) {
+	case 0x400:
+		return UDMA_BUFSZ_CODE_1KB;
+	case 0x1000:
+		return UDMA_BUFSZ_CODE_4KB;
+	case 0x4000:
+		return UDMA_BUFSZ_CODE_16KB;
+	case 0x10000:
+		return UDMA_BUFSZ_CODE_64KB;
+	case 0x20000:
+		return UDMA_BUFSZ_CODE_128KB;
+	case 0x40000:
+		return UDMA_BUFSZ_CODE_256KB;
+	case 0x80000:
+		return UDMA_BUFSZ_CODE_512KB;
+	case 0x100000:
+		return UDMA_BUFSZ_CODE_1024KB;
+	case 0x200000:
+		return UDMA_BUFSZ_CODE_2048KB;
+	case 0x400000:
+		return UDMA_BUFSZ_CODE_4096KB;
+	case 0x800000:
+		return UDMA_BUFSZ_CODE_8192KB;
+	case 0x1000000:
+		return UDMA_BUFSZ_CODE_16384KB;
+	default:
+		return -1;
+	}
+
+	return -1;
+}
+
+static u32 aspeed_udma_get_tx_rptr(u32 ch_no)
+{
+	return readl(udma->regs + UDMA_CHX_TX_RD_PTR(ch_no));
+}
+
+static u32 aspeed_udma_get_rx_wptr(u32 ch_no)
+{
+	return readl(udma->regs + UDMA_CHX_RX_WR_PTR(ch_no));
+}
+
+static void aspeed_udma_set_ptr(u32 ch_no, u32 ptr, bool is_tx)
+{
+	writel(ptr, udma->regs +
+	       ((is_tx) ? UDMA_CHX_TX_WR_PTR(ch_no) : UDMA_CHX_RX_RD_PTR(ch_no)));
+}
+
+void aspeed_udma_set_tx_wptr(u32 ch_no, u32 wptr)
+{
+	aspeed_udma_set_ptr(ch_no, wptr, true);
+}
+EXPORT_SYMBOL(aspeed_udma_set_tx_wptr);
+
+void aspeed_udma_set_rx_rptr(u32 ch_no, u32 rptr)
+{
+	aspeed_udma_set_ptr(ch_no, rptr, false);
+}
+EXPORT_SYMBOL(aspeed_udma_set_rx_rptr);
+
+static int aspeed_udma_free_chan(u32 ch_no, bool is_tx)
+{
+	u32 reg;
+	unsigned long flags;
+
+	if (ch_no > UDMA_MAX_CHANNEL)
+		return -EINVAL;
+
+	spin_lock_irqsave(&udma->lock, flags);
+
+	reg = readl(udma->regs +
+			((is_tx) ? UDMA_TX_DMA_INT_EN : UDMA_RX_DMA_INT_EN));
+	reg &= ~(0x1 << ch_no);
+
+	writel(reg, udma->regs +
+			((is_tx) ? UDMA_TX_DMA_INT_EN : UDMA_RX_DMA_INT_EN));
+
+	spin_unlock_irqrestore(&udma->lock, flags);
+
+	return 0;
+}
+
+int aspeed_udma_free_tx_chan(u32 ch_no)
+{
+	return aspeed_udma_free_chan(ch_no, true);
+}
+EXPORT_SYMBOL(aspeed_udma_free_tx_chan);
+
+int aspeed_udma_free_rx_chan(u32 ch_no)
+{
+	return aspeed_udma_free_chan(ch_no, false);
+}
+EXPORT_SYMBOL(aspeed_udma_free_rx_chan);
+
+static int aspeed_udma_request_chan(u32 ch_no, dma_addr_t addr,
+		struct circ_buf *rb, u32 rb_sz,
+		aspeed_udma_cb_t cb, void *id, bool dis_tmout, bool is_tx)
+{
+	int retval = 0;
+	int rbsz_code;
+
+	u32 reg;
+	unsigned long flags;
+	struct aspeed_udma_chan *ch;
+
+	if (ch_no > UDMA_MAX_CHANNEL) {
+		retval = -EINVAL;
+		goto out;
+	}
+
+	if (IS_ERR_OR_NULL(rb) || IS_ERR_OR_NULL(rb->buf)) {
+		retval = -EINVAL;
+		goto out;
+	}
+
+	rbsz_code = aspeed_udma_get_bufsz_code(rb_sz);
+	if (rbsz_code < 0) {
+		retval = -EINVAL;
+		goto out;
+	}
+
+	spin_lock_irqsave(&udma->lock, flags);
+
+	if (is_tx) {
+		reg = readl(udma->regs + UDMA_TX_DMA_INT_EN);
+		if (reg & (0x1 << ch_no)) {
+			retval = -EBUSY;
+			goto unlock_n_out;
+		}
+
+		reg |= (0x1 << ch_no);
+		writel(reg, udma->regs + UDMA_TX_DMA_INT_EN);
+
+		reg = readl(udma->regs + UDMA_CHX_TX_CTRL(ch_no));
+		reg |= (dis_tmout) ? UDMA_TX_CTRL_TMOUT_DISABLE : 0;
+		reg |= (rbsz_code << UDMA_TX_CTRL_BUFSZ_SHIFT) & UDMA_TX_CTRL_BUFSZ_MASK;
+		writel(reg, udma->regs + UDMA_CHX_TX_CTRL(ch_no));
+
+		writel(addr, udma->regs + UDMA_CHX_TX_BUF_BASE(ch_no));
+	} else {
+		reg = readl(udma->regs + UDMA_RX_DMA_INT_EN);
+		if (reg & (0x1 << ch_no)) {
+			retval = -EBUSY;
+			goto unlock_n_out;
+		}
+
+		reg |= (0x1 << ch_no);
+		writel(reg, udma->regs + UDMA_RX_DMA_INT_EN);
+
+		reg = readl(udma->regs + UDMA_CHX_RX_CTRL(ch_no));
+		reg |= (dis_tmout) ? UDMA_RX_CTRL_TMOUT_DISABLE : 0;
+		reg |= (rbsz_code << UDMA_RX_CTRL_BUFSZ_SHIFT) & UDMA_RX_CTRL_BUFSZ_MASK;
+		writel(reg, udma->regs + UDMA_CHX_RX_CTRL(ch_no));
+
+		writel(addr, udma->regs + UDMA_CHX_RX_BUF_BASE(ch_no));
+	}
+
+	ch = (is_tx) ? &udma->tx_chs[ch_no] : &udma->rx_chs[ch_no];
+	ch->rb = rb;
+	ch->rb_sz = rb_sz;
+	ch->cb = cb;
+	ch->cb_arg = id;
+	ch->dma_addr = addr;
+	ch->dis_tmout = dis_tmout;
+
+unlock_n_out:
+	spin_unlock_irqrestore(&udma->lock, flags);
+out:
+	return 0;
+}
+
+int aspeed_udma_request_tx_chan(u32 ch_no, dma_addr_t addr,
+		struct circ_buf *rb, u32 rb_sz,
+		aspeed_udma_cb_t cb, void *id, bool dis_tmout)
+{
+	return aspeed_udma_request_chan(ch_no, addr, rb, rb_sz, cb, id,
+									dis_tmout, true);
+}
+EXPORT_SYMBOL(aspeed_udma_request_tx_chan);
+
+int aspeed_udma_request_rx_chan(u32 ch_no, dma_addr_t addr,
+		struct circ_buf *rb, u32 rb_sz,
+		aspeed_udma_cb_t cb, void *id, bool dis_tmout)
+{
+	return aspeed_udma_request_chan(ch_no, addr, rb, rb_sz, cb, id,
+									dis_tmout, false);
+}
+EXPORT_SYMBOL(aspeed_udma_request_rx_chan);
+
+static void aspeed_udma_chan_ctrl(u32 ch_no, u32 op, bool is_tx)
+{
+	unsigned long flags;
+	u32 reg_en, reg_rst;
+	u32 reg_en_off = (is_tx) ? UDMA_TX_DMA_EN : UDMA_RX_DMA_EN;
+	u32 reg_rst_off = (is_tx) ? UDMA_TX_DMA_RST : UDMA_TX_DMA_RST;
+
+	if (ch_no > UDMA_MAX_CHANNEL)
+		return;
+
+	spin_lock_irqsave(&udma->lock, flags);
+
+	reg_en = readl(udma->regs + reg_en_off);
+	reg_rst = readl(udma->regs + reg_rst_off);
+
+	switch (op) {
+	case ASPEED_UDMA_OP_ENABLE:
+		reg_en |= (0x1 << ch_no);
+		writel(reg_en, udma->regs + reg_en_off);
+		break;
+	case ASPEED_UDMA_OP_DISABLE:
+		reg_en &= ~(0x1 << ch_no);
+		writel(reg_en, udma->regs + reg_en_off);
+		break;
+	case ASPEED_UDMA_OP_RESET:
+		reg_en &= ~(0x1 << ch_no);
+		writel(reg_en, udma->regs + reg_en_off);
+
+		reg_rst |= (0x1 << ch_no);
+		writel(reg_rst, udma->regs + reg_rst_off);
+
+		udelay(100);
+
+		reg_rst &= ~(0x1 << ch_no);
+		writel(reg_rst, udma->regs + reg_rst_off);
+		break;
+	default:
+		break;
+	}
+
+	spin_unlock_irqrestore(&udma->lock, flags);
+}
+
+void aspeed_udma_tx_chan_ctrl(u32 ch_no, enum aspeed_udma_ops op)
+{
+	aspeed_udma_chan_ctrl(ch_no, op, true);
+}
+EXPORT_SYMBOL(aspeed_udma_tx_chan_ctrl);
+
+void aspeed_udma_rx_chan_ctrl(u32 ch_no, enum aspeed_udma_ops op)
+{
+	aspeed_udma_chan_ctrl(ch_no, op, false);
+}
+EXPORT_SYMBOL(aspeed_udma_rx_chan_ctrl);
+
+static irqreturn_t aspeed_udma_isr(int irq, void *arg)
+{
+	u32 bit;
+	unsigned long tx_stat = readl(udma->regs + UDMA_TX_DMA_INT_STAT);
+	unsigned long rx_stat = readl(udma->regs + UDMA_RX_DMA_INT_STAT);
+
+	if (udma != (struct aspeed_udma *)arg)
+		return IRQ_NONE;
+
+	if (tx_stat == 0 && rx_stat == 0)
+		return IRQ_NONE;
+
+	for_each_set_bit(bit, &tx_stat, UDMA_MAX_CHANNEL) {
+		writel((0x1 << bit), udma->regs + UDMA_TX_DMA_INT_STAT);
+		if (udma->tx_chs[bit].cb)
+			udma->tx_chs[bit].cb(aspeed_udma_get_tx_rptr(bit),
+					udma->tx_chs[bit].cb_arg);
+	}
+
+	for_each_set_bit(bit, &rx_stat, UDMA_MAX_CHANNEL) {
+		writel((0x1 << bit), udma->regs + UDMA_RX_DMA_INT_STAT);
+		if (udma->rx_chs[bit].cb)
+			udma->rx_chs[bit].cb(aspeed_udma_get_rx_wptr(bit),
+					udma->rx_chs[bit].cb_arg);
+	}
+
+	return IRQ_HANDLED;
+}
+
+static int aspeed_udma_probe(struct platform_device *pdev)
+{
+	int i, rc;
+	uint32_t reg;
+	struct resource *res;
+	struct device *dev = &pdev->dev;
+
+	res = platform_get_resource(pdev, IORESOURCE_MEM, 0);
+	if (IS_ERR_OR_NULL(res)) {
+		dev_err(dev, "failed to get register base\n");
+		return -ENODEV;
+	}
+
+	udma->regs = devm_ioremap_resource(dev, res);
+	if (IS_ERR_OR_NULL(udma->regs)) {
+		dev_err(dev, "failed to map registers\n");
+		return PTR_ERR(udma->regs);
+	}
+
+	/* disable for safety */
+	writel(0x0, udma->regs + UDMA_TX_DMA_EN);
+	writel(0x0, udma->regs + UDMA_RX_DMA_EN);
+
+	udma->irq = platform_get_irq(pdev, 0);
+	if (udma->irq < 0) {
+		dev_err(dev, "failed to get IRQ number\n");
+		return -ENODEV;
+	}
+
+	rc = devm_request_irq(dev, udma->irq, aspeed_udma_isr,
+			IRQF_SHARED, DEVICE_NAME, udma);
+	if (rc) {
+		dev_err(dev, "failed to request IRQ handler\n");
+		return rc;
+	}
+
+	/*
+	 * For AST2600 A1 legacy design.
+	 *  - TX ringbuffer size: 4KB
+	 *  - RX ringbuffer size: 64KB
+	 *  - Timeout timer disabled
+	 */
+	reg = ((UDMA_BUFSZ_CODE_4KB << UDMA_MISC_TX_BUFSZ_SHIFT) & UDMA_MISC_TX_BUFSZ_MASK) |
+	      ((UDMA_BUFSZ_CODE_64KB << UDMA_MISC_RX_BUFSZ_SHIFT) & UDMA_MISC_RX_BUFSZ_MASK);
+	writel(reg, udma->regs + UDMA_MISC);
+
+	for (i = 0; i < UDMA_MAX_CHANNEL; ++i) {
+		writel(0, udma->regs + UDMA_CHX_TX_WR_PTR(i));
+		writel(0, udma->regs + UDMA_CHX_RX_RD_PTR(i));
+	}
+
+	writel(0xffffffff, udma->regs + UDMA_TX_DMA_RST);
+	writel(0x0, udma->regs + UDMA_TX_DMA_RST);
+
+	writel(0xffffffff, udma->regs + UDMA_RX_DMA_RST);
+	writel(0x0, udma->regs + UDMA_RX_DMA_RST);
+
+	writel(0x0, udma->regs + UDMA_TX_DMA_INT_EN);
+	writel(0xffffffff, udma->regs + UDMA_TX_DMA_INT_STAT);
+	writel(0x0, udma->regs + UDMA_RX_DMA_INT_EN);
+	writel(0xffffffff, udma->regs + UDMA_RX_DMA_INT_STAT);
+
+	writel(UDMA_TIMEOUT, udma->regs + UDMA_TIMEOUT_TIMER);
+
+	spin_lock_init(&udma->lock);
+
+	dev_set_drvdata(dev, udma);
+
+	return 0;
+}
+
+static const struct of_device_id aspeed_udma_match[] = {
+	{ .compatible = "aspeed,ast2500-udma" },
+	{ .compatible = "aspeed,ast2600-udma" },
+	{ },
+};
+
+static struct platform_driver aspeed_udma_driver = {
+	.driver = {
+		.name = DEVICE_NAME,
+		.of_match_table = aspeed_udma_match,
+
+	},
+	.probe = aspeed_udma_probe,
+};
+
+module_platform_driver(aspeed_udma_driver);
+
+MODULE_AUTHOR("Chia-Wei Wang <chiawei_wang@aspeedtech.com>");
+MODULE_LICENSE("GPL");
+MODULE_DESCRIPTION("Aspeed UDMA Engine Driver");
diff --git a/drivers/soc/aspeed/aspeed-usb-ahp.c b/drivers/soc/aspeed/aspeed-usb-ahp.c
new file mode 100644
index 000000000000..c07703229989
--- /dev/null
+++ b/drivers/soc/aspeed/aspeed-usb-ahp.c
@@ -0,0 +1,47 @@
+// SPDX-License-Identifier: GPL-2.0+
+/*
+ * Copyright 2021 Aspeed Technology Inc.
+ */
+
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/platform_device.h>
+#include <linux/of.h>
+#include <linux/of_address.h>
+#include <linux/regmap.h>
+#include <asm/io.h>
+
+static const struct of_device_id aspeed_usb_ahp_dt_ids[] = {
+	{
+		.compatible = "aspeed,ast2600-usb2ahp",
+	},
+	{ }
+};
+MODULE_DEVICE_TABLE(of, aspeed_usb_ahp_dt_ids);
+
+static int aspeed_usb_ahp_probe(struct platform_device *pdev)
+{
+	dev_info(&pdev->dev, "Initialized USB2AHP\n");
+
+	return 0;
+}
+
+static int aspeed_usb_ahp_remove(struct platform_device *pdev)
+{
+	dev_info(&pdev->dev, "Remove USB2AHP\n");
+
+	return 0;
+}
+
+static struct platform_driver aspeed_usb_ahp_driver = {
+	.probe		= aspeed_usb_ahp_probe,
+	.remove		= aspeed_usb_ahp_remove,
+	.driver		= {
+		.name	= KBUILD_MODNAME,
+		.of_match_table	= aspeed_usb_ahp_dt_ids,
+	},
+};
+module_platform_driver(aspeed_usb_ahp_driver);
+
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Neal Liu <neal_liu@aspeedtech.com>");
diff --git a/drivers/soc/aspeed/aspeed-usb-phy.c b/drivers/soc/aspeed/aspeed-usb-phy.c
new file mode 100644
index 000000000000..f92e3683ffdd
--- /dev/null
+++ b/drivers/soc/aspeed/aspeed-usb-phy.c
@@ -0,0 +1,70 @@
+// SPDX-License-Identifier: GPL-2.0+
+/*
+ * Copyright 2021 Aspeed Technology Inc.
+ */
+
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/platform_device.h>
+#include <linux/of.h>
+#include <linux/of_address.h>
+#include <linux/regmap.h>
+#include <asm/io.h>
+
+struct usb_phy_ctrl {
+	u32 offset;
+	u32 set_bit;
+};
+
+static const struct of_device_id aspeed_usb_phy_dt_ids[] = {
+	{
+		.compatible = "aspeed,ast2600-uphyb",
+	},
+	{ }
+};
+MODULE_DEVICE_TABLE(of, aspeed_usb_phy_dt_ids);
+
+static int aspeed_usb_phy_probe(struct platform_device *pdev)
+{
+	struct device_node *node = pdev->dev.of_node;
+	struct usb_phy_ctrl *ctrl_data;
+	void __iomem *base;
+	int ret;
+
+	ctrl_data = devm_kzalloc(&pdev->dev, sizeof(struct usb_phy_ctrl), GFP_KERNEL);
+	if (!ctrl_data)
+		return -ENOMEM;
+
+	base = of_iomap(node, 0);
+
+	ret = of_property_read_u32_array(node, "ctrl", (u32 *)ctrl_data, 2);
+	if (ret < 0) {
+		dev_err(&pdev->dev, "Could not read ctrl property\n");
+		return -EINVAL;
+	}
+
+	writel(readl(base + ctrl_data->offset) | BIT(ctrl_data->set_bit),
+		base + ctrl_data->offset);
+
+	dev_info(&pdev->dev, "Initialized USB PHY\n");
+
+	return 0;
+}
+
+static int aspeed_usb_phy_remove(struct platform_device *pdev)
+{
+	return 0;
+}
+
+static struct platform_driver aspeed_usb_phy_driver = {
+	.probe		= aspeed_usb_phy_probe,
+	.remove		= aspeed_usb_phy_remove,
+	.driver		= {
+		.name	= KBUILD_MODNAME,
+		.of_match_table	= aspeed_usb_phy_dt_ids,
+	},
+};
+module_platform_driver(aspeed_usb_phy_driver);
+
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Neal Liu <neal_liu@aspeedtech.com>");
diff --git a/drivers/soc/aspeed/ast_video.c b/drivers/soc/aspeed/ast_video.c
new file mode 100644
index 000000000000..d5f5998263bd
--- /dev/null
+++ b/drivers/soc/aspeed/ast_video.c
@@ -0,0 +1,3411 @@
+// SPDX-License-Identifier: GPL-2.0-or-later
+// Copyright (C) 2021 Aspeed Technology Inc.
+
+#include <linux/poll.h>
+#include <linux/slab.h>
+#include <linux/sched.h>
+#include <linux/clk.h>
+#include <linux/reset.h>
+
+#include <linux/module.h>
+#include <linux/fs.h>
+#include <linux/init.h>
+#include <linux/platform_device.h>
+#include <linux/types.h>
+#include <linux/interrupt.h>
+#include <linux/mm.h>
+#include <linux/delay.h>
+#include <linux/miscdevice.h>
+#include <linux/hwmon-sysfs.h>
+#include <linux/regmap.h>
+#include <linux/mfd/syscon.h>
+#include <linux/dma-mapping.h>
+#include <asm/io.h>
+#include <linux/of.h>
+#include <linux/of_reserved_mem.h>
+#include <asm/uaccess.h>
+//#include <linux/aspeed-sdmc.h>
+//#include <linux/ast_lcd.h>
+#include <linux/fb.h>
+
+/***********************************************************************/
+/* Register for VIDEO */
+#define AST_VIDEO_PROTECT		0x000		/*	protection key register	*/
+#define AST_VIDEO_SEQ_CTRL		0x004		/*	Video Sequence Control register	*/
+#define AST_VIDEO_PASS_CTRL		0x008		/*	Video Pass 1 Control register	*/
+
+//VR008[5]=1
+#define AST_VIDEO_DIRECT_BASE	0x00C		/*	Video Direct Frame buffer mode control Register VR008[5]=1 */
+#define AST_VIDEO_DIRECT_CTRL	0x010		/*	Video Direct Frame buffer mode control Register VR008[5]=1 */
+
+//VR008[5]=0
+#define AST_VIDEO_TIMING_H		0x00C		/*	Video Timing Generation Setting Register */
+#define AST_VIDEO_TIMING_V		0x010		/*	Video Timing Generation Setting Register */
+#define AST_VIDEO_SCAL_FACTOR	0x014		/*	Video Scaling Factor Register */
+
+#define AST_VIDEO_SCALING0		0x018		/*	Video Scaling Filter Parameter Register #0 */
+#define AST_VIDEO_SCALING1		0x01C		/*	Video Scaling Filter Parameter Register #1 */
+#define AST_VIDEO_SCALING2		0x020		/*	Video Scaling Filter Parameter Register #2 */
+#define AST_VIDEO_SCALING3		0x024		/*	Video Scaling Filter Parameter Register #3 */
+
+#define AST_VIDEO_BCD_CTRL		0x02C		/*	Video BCD Control Register */
+#define AST_VIDEO_CAPTURE_WIN	0x030		/*	 Video Capturing Window Setting Register */
+#define AST_VIDEO_COMPRESS_WIN	0x034		/*	 Video Compression Window Setting Register */
+
+
+#define AST_VIDEO_COMPRESS_PRO	0x038		/* Video Compression Stream Buffer Processing Offset Register */
+#define AST_VIDEO_COMPRESS_READ	0x03C		/* Video Compression Stream Buffer Read Offset Register */
+
+#define AST_VIDEO_JPEG_HEADER_BUFF		0x040		/*	Video Based Address of JPEG Header Buffer Register */
+#define AST_VIDEO_SOURCE_BUFF0	0x044		/*	Video Based Address of Video Source Buffer #1 Register */
+#define AST_VIDEO_SOURCE_SCAN_LINE	0x048		/*	Video Scan Line Offset of Video Source Buffer Register */
+#define AST_VIDEO_SOURCE_BUFF1	0x04C		/*	Video Based Address of Video Source Buffer #2 Register */
+#define AST_VIDEO_BCD_BUFF		0x050		/*	Video Base Address of BCD Flag Buffer Register */
+#define AST_VIDEO_STREAM_BUFF	0x054		/*	Video Base Address of Compressed Video Stream Buffer Register */
+#define AST_VIDEO_STREAM_SIZE	0x058		/*	Video Stream Buffer Size Register */
+
+#define AST_VIDEO_COMPRESS_CTRL	0x060		/* Video Compression Control Register */
+
+
+#define AST_VIDEO_COMPRESS_DATA_COUNT		0x070		/* Video Total Size of Compressed Video Stream Read Back Register */
+#define AST_VIDEO_COMPRESS_BLOCK_COUNT		0x074		/* Video Total Number of Compressed Video Block Read Back Register */
+#define AST_VIDEO_COMPRESS_FRAME_END		0x078		/* Video Frame-end offset of compressed video stream buffer read back Register */
+
+
+
+#define AST_VIDEO_DEF_HEADER	0x080		/* Video User Defined Header Parameter Setting with Compression */
+#define AST_VIDEO_JPEG_COUNT	0x084		/* true jpeg size */
+
+#define AST_VIDEO_H_DETECT_STS  0x090		/* Video Source Left/Right Edge Detection Read Back Register */
+#define AST_VIDEO_V_DETECT_STS  0x094		/* Video Source Top/Bottom Edge Detection Read Back Register */
+
+
+#define AST_VIDEO_MODE_DET_STS	0x098		/* Video Mode Detection Status Read Back Register */
+
+#define AST_VIDEO_MODE_DET1		0x0A4		/* Video Mode Detection Control Register 1*/
+#define AST_VIDEO_MODE_DET2		0x0A8		/* Video Mode Detection Control Register 2*/
+
+
+#define AST_VIDEO_BONDING_X		0x0D4
+#define AST_VIDEO_BONDING_Y		0x0D8
+
+#define AST_VM_SEQ_CTRL			0x204		/* Video Management Control Sequence Register */
+#define AST_VM_PASS_CTRL			0x208		/* Video Management Pass 1 Control register	*/
+#define AST_VM_SCAL_FACTOR		0x214		/* Video Management Scaling Factor Register */
+#define AST_VM_BCD_CTRL			0x22C		/* Video Management BCD Control Register */
+#define AST_VM_CAPTURE_WIN		0x230		/* Video Management Capturing Window Setting Register */
+#define AST_VM_COMPRESS_WIN		0x234		/* Video Management Compression Window Setting Register */
+#define AST_VM_JPEG_HEADER_BUFF	0x240		/* Video Management Based Address of JPEG Header Buffer Register */
+#define AST_VM_SOURCE_BUFF0		0x244		/* Video Management Based Address of Video Source Buffer Register */
+#define AST_VM_SOURCE_SCAN_LINE	0x248		/* Video Management Scan Line Offset of Video Source Buffer Register */
+
+#define AST_VM_COMPRESS_BUFF		0x254		/* Video Management Based Address of Compressed Video Buffer Register */
+#define AST_VM_STREAM_SIZE			0x258		/* Video Management Buffer Size Register */
+#define AST_VM_COMPRESS_CTRL			0x260		/* Video Management Compression or Video Profile 2-5 Decompression Control Register */
+#define AST_VM_COMPRESS_VR264			0x264		/* VR264 REserved */
+#define AST_VM_COMPRESS_BLOCK_COUNT		0x274		/* Video Total Number of Compressed Video Block Read Back Register */
+#define AST_VM_COMPRESS_FRAME_END	0x278	/*16 bytes align */	/* Video Management Frame-end offset of compressed video stream buffer read back Register */
+
+
+#define AST_VIDEO_CTRL			0x300		/* Video Control Register */
+#define AST_VIDEO_INT_EN		0x304		/* Video interrupt Enable */
+#define AST_VIDEO_INT_STS		0x308		/* Video interrupt status */
+#define AST_VIDEO_MODE_DETECT	0x30C		/* Video Mode Detection Parameter Register */
+
+#define AST_VIDEO_CRC1			0x320		/* Primary CRC Parameter Register */
+#define AST_VIDEO_CRC2			0x324		/* Second CRC Parameter Register */
+#define AST_VIDEO_DATA_TRUNCA	0x328		/* Video Data Truncation Register */
+
+
+#define AST_VIDEO_SCRATCH_340	0x340		/* Video Scratch Remap Read Back */
+#define AST_VIDEO_SCRATCH_344	0x344		/* Video Scratch Remap Read Back */
+#define AST_VIDEO_SCRATCH_348	0x348		/* Video Scratch Remap Read Back */
+#define AST_VIDEO_SCRATCH_34C	0x34C		/* Video Scratch Remap Read Back */
+#define AST_VIDEO_SCRATCH_350	0x350		/* Video Scratch Remap Read Back */
+#define AST_VIDEO_SCRATCH_354	0x354		/* Video Scratch Remap Read Back */
+#define AST_VIDEO_SCRATCH_358	0x358		/* Video Scratch Remap Read Back */
+#define AST_VIDEO_SCRATCH_35C	0x35C		/* Video Scratch Remap Read Back */
+#define AST_VIDEO_SCRATCH_360	0x360		/* Video Scratch Remap Read Back */
+#define AST_VIDEO_SCRATCH_364	0x364		/* Video Scratch Remap Read Back */
+
+
+#define AST_VIDEO_ENCRYPT_SRAM	0x400		/* Video RC4/AES128 Encryption Key Register #0 ~ #63 */
+#define AST_VIDEO_MULTI_JPEG_SRAM	(AST_VIDEO_ENCRYPT_SRAM)		/* Multi JPEG registers */
+
+#define REG_32_BIT_SZ_IN_BYTES (sizeof(u32))
+
+#define SET_FRAME_W_H(w, h) ((((u32) (h)) & 0x1fff) | ((((u32) (w)) & 0x1fff) << 13))
+#define SET_FRAME_START_ADDR(addr) ((addr) & 0x7fffff80)
+
+/////////////////////////////////////////////////////////////////////////////
+
+/*	AST_VIDEO_PROTECT: 0x000  - protection key register */
+#define VIDEO_PROTECT_UNLOCK			0x1A038AA8
+
+/*	AST_VIDEO_SEQ_CTRL		0x004		Video Sequence Control register	*/
+#define VIDEO_HALT_ENG_STS				(1 << 21)
+#define VIDEO_COMPRESS_BUSY				(1 << 18)
+#define VIDEO_CAPTURE_BUSY				(1 << 16)
+#define VIDEO_HALT_ENG_TRIGGER			(1 << 12)
+#define VIDEO_COMPRESS_FORMAT_MASK		(3 << 10)
+#define VIDEO_GET_COMPRESS_FORMAT(x)		((x >> 10) & 0x3)   // 0 YUV444
+#define VIDEO_COMPRESS_FORMAT(x)		(x << 10)	// 0 YUV444
+#define YUV420		1
+
+#define G5_VIDEO_COMPRESS_JPEG_MODE			(1 << 13)
+#define VIDEO_YUV2RGB_DITHER_EN			(1 << 8)
+
+#define VIDEO_COMPRESS_JPEG_MODE			(1 << 8)
+
+//if bit 0 : 1
+#define VIDEO_INPUT_MODE_CHG_WDT		(1 << 7)
+#define VIDEO_INSERT_FULL_COMPRESS		(1 << 6)
+#define VIDEO_AUTO_COMPRESS				(1 << 5)
+#define VIDEO_COMPRESS_TRIGGER			(1 << 4)
+#define VIDEO_CAPTURE_MULTI_FRAME		(1 << 3)
+#define VIDEO_COMPRESS_FORCE_IDLE		(1 << 2)
+#define VIDEO_CAPTURE_TRIGGER			(1 << 1)
+#define VIDEO_DETECT_TRIGGER			(1 << 0)
+
+/*	AST_VIDEO_PASS_CTRL			0x008		Video Pass1 Control register	*/
+#define G6_VIDEO_MULTI_JPEG_FLAG_MODE	(1 << 31)
+#define G6_VIDEO_MULTI_JPEG_MODE		(1 << 30)
+#define G6_VIDEO_JPEG__COUNT(x)			((x) << 24)
+#define G6_VIDEO_FRAME_CT_MASK			(0x3f << 24)
+//x * source frame rate / 60
+#define VIDEO_FRAME_RATE_CTRL(x)		(x << 16)
+#define VIDEO_HSYNC_POLARITY_CTRL		(1 << 15)
+#define VIDEO_INTERLANCE_MODE			(1 << 14)
+#define VIDEO_DUAL_EDGE_MODE			(1 << 13)	//0 : Single edage
+#define VIDEO_18BIT_SINGLE_EDGE			(1 << 12)	//0: 24bits
+#define VIDEO_DVO_INPUT_DELAY_MASK		(7 << 9)
+#define VIDEO_DVO_INPUT_DELAY(x)		(x << 9) //0 : no delay , 1: 1ns, 2: 2ns, 3:3ns, 4: inversed clock but no delay
+// if biit 5 : 0
+#define VIDEO_HW_CURSOR_DIS				(1 << 8)
+// if biit 5 : 1
+#define VIDEO_AUTO_FATCH					(1 << 8)	//
+#define VIDEO_CAPTURE_FORMATE_MASK		(3 << 6)
+
+#define VIDEO_SET_CAPTURE_FORMAT(x)			(x << 6)
+#define JPEG_MODE		1
+#define RGB_MODE		2
+#define GRAY_MODE		3
+#define VIDEO_DIRT_FATCH				(1 << 5)
+// if biit 5 : 0
+#define VIDEO_INTERNAL_DE				(1 << 4)
+#define VIDEO_EXT_ADC_ATTRIBUTE			(1 << 3)
+
+// if biit 5 : 1
+#define VIDEO_16BPP_MODE				(1 << 4)
+#define VIDEO_16BPP_MODE_555			(1 << 3)	//0:565
+
+#define VIDEO_FROM_EXT_SOURCE			(1 << 2)
+#define VIDEO_SO_VSYNC_POLARITY			(1 << 1)
+#define VIDEO_SO_HSYNC_POLARITY			(1 << 0)
+
+/*	AST_VIDEO_TIMING_H		0x00C		Video Timing Generation Setting Register */
+#define VIDEO_HSYNC_PIXEL_FIRST_SET(x)	(((x) & 0xfff) << 16)
+#define VIDEO_HSYNC_PIXEL_FIRST_MASK	0xFFFF0000
+#define VIDEO_HSYNC_PIXEL_LAST_SET(x)	((x) & 0xfff)
+#define VIDEO_HSYNC_PIXEL_LAST_MASK	0x0000FFFF
+
+/*	AST_VIDEO_DIRECT_CTRL	0x010		Video Direct Frame buffer mode control Register VR008[5]=1 */
+#define VIDEO_FETCH_TIMING(x)			((x) << 16)
+#define VIDEO_FETCH_LINE_OFFSET(x)		(x & 0xffff)
+
+/*	AST_VIDEO_TIMING_V		0x010		Video Timing Generation Setting Register */
+#define VIDEO_VSYNC_PIXEL_FIRST_SET(x)	((x) << 16)
+#define VIDEO_VSYNC_PIXEL_LAST_SET(x)	(x)
+
+/*	AST_VIDEO_SCAL_FACTOR	0x014		Video Scaling Factor Register */
+#define VIDEO_V_SCAL_FACTOR(x)			(((x) & 0xffff) << 16)
+#define VIDEO_V_SCAL_FACTOR_MASK		(0x0000ffff)
+#define VIDEO_H_SCAL_FACTOR(x)			((x) & 0xffff)
+#define VIDEO_H_SCAL_FACTOR_MASK		(0xffff0000)
+
+/*	AST_VIDEO_SCALING0		0x018		Video Scaling Filter Parameter Register #0 */
+/*	AST_VIDEO_SCALING1		0x01C		Video Scaling Filter Parameter Register #1 */
+/*	AST_VIDEO_SCALING2		0x020		Video Scaling Filter Parameter Register #2 */
+/*	AST_VIDEO_SCALING3		0x024		Video Scaling Filter Parameter Register #3 */
+
+
+/*	AST_VIDEO_BCD_CTRL		0x02C		Video BCD Control Register */
+#define VIDEO_SET_ABCD_TOL(x)			((x & 0xff) << 24)
+#define VIDEO_GET_ABCD_TOL(x)			((x >> 24) & 0xff)
+#define VIDEO_SET_BCD_TOL(x)			((x & 0xff) << 16)
+#define VIDEO_GET_BCD_TOL(x)			((x >> 16) & 0xff)
+
+#define VIDEO_ABCD_CHG_EN				(1 << 1)
+#define VIDEO_BCD_CHG_EN				(1)
+
+/*	 AST_VIDEO_CAPTURE_WIN	0x030		Video Capturing Window Setting Register */
+#define VIDEO_CAPTURE_V(x)				(x & 0x7ff)
+#define VIDEO_CAPTURE_H(x)				((x & 0x7ff) << 16)
+
+/*	 AST_VIDEO_COMPRESS_WIN	0x034		Video Compression Window Setting Register */
+#define VIDEO_COMPRESS_V(x)				(x & 0x7ff)
+#define VIDEO_GET_COMPRESS_V(x)			(x & 0x7ff)
+#define VIDEO_COMPRESS_H(x)				((x & 0x7ff) << 16)
+#define VIDEO_GET_COMPRESS_H(x)			((x >> 16) & 0x7ff)
+
+/*	AST_VIDEO_RESET :0x03c	 - system reset control register */
+
+/*	AST_VIDEO_STREAM_SIZE	0x058		Video Stream Buffer Size Register */
+#define VIDEO_STREAM_PKT_N(x)			(x << 3)
+#define STREAM_4_PKTS		0
+#define STREAM_8_PKTS		1
+#define STREAM_16_PKTS		2
+#define STREAM_32_PKTS		3
+#define STREAM_64_PKTS		4
+#define STREAM_128_PKTS		5
+
+#define VIDEO_STREAM_PKT_SIZE(x)		(x)
+#define STREAM_1KB		0
+#define STREAM_2KB		1
+#define STREAM_4KB		2
+#define STREAM_8KB		3
+#define STREAM_16KB		4
+#define STREAM_32KB		5
+#define STREAM_64KB		6
+#define STREAM_128KB	7
+
+/* AST_VIDEO_COMPRESS_CTRL	0x060		Video Compression Control Register */
+#define VIDEO_HQ_DCT_LUM(x)				((x) << 27)
+#define VIDEO_GET_HQ_DCT_LUM(x)			((x >> 27) & 0x1f)
+#define VIDEO_HQ_DCT_CHROM(x)				((x) << 22)
+#define VIDEO_GET_HQ_DCT_CHROM(x)			((x >> 22) & 0x1f)
+#define VIDEO_HQ_DCT_MASK					(0x3ff << 22)
+#define VIDEO_DCT_HUFFMAN_ENCODE(x)		((x) << 20)
+#define VIDEO_DCT_RESET						(1 << 17)
+#define VIDEO_HQ_ENABLE					(1 << 16)
+#define VIDEO_GET_HQ_ENABLE(x)				((x >> 16) & 0x1)
+#define VIDEO_DCT_LUM(x)					((x) << 11)
+#define VIDEO_GET_DCT_LUM(x)				((x >> 11) & 0x1f)
+#define VIDEO_DCT_CHROM(x)					((x) << 6)
+#define VIDEO_GET_DCT_CHROM(x)			((x >> 6) & 0x1f)
+#define VIDEO_DCT_MASK						(0x3ff << 6)
+#define VIDEO_ENCRYP_ENABLE				(1 << 5)
+#define VIDEO_COMPRESS_QUANTIZ_MODE		(1 << 2)
+#define VIDEO_4COLOR_VQ_ENCODE			(1 << 1)
+#define VIDEO_DCT_ONLY_ENCODE				(1)
+#define VIDEO_DCT_VQ_MASK					(0x3)
+
+/* AST_VIDEO_COMPRESS_BLOCK_COUNT - 0x074		Video Total Number of Compressed Video Block Read Back Register */
+#define GET_BLOCK_CHG(x)				((x >> 16) & 0xffff)
+
+/* AST_VIDEO_H_DETECT_STS  0x090		Video Source Left/Right Edge Detection Read Back Register */
+#define VIDEO_DET_INTERLANCE_MODE		(1 << 31)
+#define VIDEO_GET_HSYNC_RIGHT(x)		((x & 0x0FFF0000) >> 16)
+#define VIDEO_GET_HSYNC_LEFT(x)			(x & 0xFFF)
+#define VIDEO_NO_DISPLAY_CLOCK_DET		(1 << 15)
+#define VIDEO_NO_ACT_DISPLAY_DET		(1 << 14)
+#define VIDEO_NO_HSYNC_DET				(1 << 13)
+#define VIDEO_NO_VSYNC_DET				(1 << 12)
+
+/* AST_VIDEO_V_DETECT_STS  0x094		Video Source Top/Bottom Edge Detection Read Back Register */
+#define VIDEO_GET_VSYNC_BOTTOM(x)		((x & 0x0FFF0000) >> 16)
+#define VIDEO_GET_VSYNC_TOP(x)			(x & 0xFFF)
+
+
+/* AST_VIDEO_MODE_DET_STS	0x098		Video Mode Detection Status Read Back Register */
+#define VIDEO_DET_HSYNC_RDY				(1 << 31)
+#define VIDEO_DET_VSYNC_RDY				(1 << 30)
+#define VIDEO_DET_HSYNC_POLAR			(1 << 29)
+#define VIDEO_DET_VSYNC_POLAR			(1 << 28)
+#define VIDEO_GET_VER_SCAN_LINE(x)		((x >> 16) & 0xfff)
+#define VIDEO_OUT_SYNC					(1 << 15)
+#define VIDEO_DET_VER_STABLE			(1 << 14)
+#define VIDEO_DET_HOR_STABLE			(1 << 13)
+#define VIDEO_DET_FROM_ADC				(1 << 12)
+#define VIDEO_DET_HOR_PERIOD(x)			(x & 0xfff)
+
+
+/* AST_VIDEO_MODE_DET1		0x0A4		Video Mode Detection Control Register 1*/
+#define VIDEO_DET_HSYNC_DELAY_MASK		(0xff << 16)
+#define VIDEO_DET_LONG_H_STABLE_EN		(1 << 29)
+
+/* AST_VM_SEQ_CTRL	0x204		Video Management Control Sequence Register */
+#define VIDEO_VM_SET_YUV420				(1 << 10)
+#define VIDEO_VM_JPEG_COMPRESS_MODE		(1 << 8)
+#define VIDEO_VM_AUTO_COMPRESS			(1 << 5)
+#define VIDEO_VM_COMPRESS_TRIGGER		(1 << 4)
+#define VIDEO_VM_CAPTURE_TRIGGER			(1 << 1)
+
+/* AST_VM_BUFF_SIZE			0x258		Video Management Buffer Size Register */
+#define VM_STREAM_PKT_SIZE(x)		(x)
+#define STREAM_1MB		0
+#define STREAM_2MB		1
+#define STREAM_3MB		2
+#define STREAM_4MB		3
+
+/* AST_VIDEO_CTRL			0x300		Video Control Register */
+#define VIDEO_CTRL_CRYPTO(x)			(x << 17)
+#define VIDEO_CTRL_CRYPTO_AES			(1 << 17)
+#define VIDEO_CTRL_CRYPTO_FAST			(1 << 16)
+#define VIDEO_CTRL_ADDRESS_MAP_MULTI_JPEG	(0x3 << 30)
+//15 reserved
+#define VIDEO_CTRL_RC4_VC				(1 << 14)
+#define VIDEO_CTRL_CAPTURE_MASK			(3 << 12)
+#define VIDEO_CTRL_CAPTURE_MODE(x)		(x << 12)
+#define VIDEO_CTRL_COMPRESS_MASK		(3 << 10)
+#define VIDEO_CTRL_COMPRESS_MODE(x)		(x << 10)
+#define MODE_32BPP_YUV444		0
+#define MODE_24BPP_YUV444		1
+#define MODE_16BPP_YUV422		3
+
+#define VIDEO_CTRL_RC4_TEST_MODE		(1 << 9)
+#define VIDEO_CTRL_RC4_RST				(1 << 8)
+#define VIDEO_CTRL_RC4_VIDEO_M_SEL		(1 << 7)		//video management
+#define VIDEO_CTRL_RC4_VIDEO_2_SEL		(1 << 6)		// Video 2
+
+#define VIDEO_CTRL_DWN_SCALING_MASK		(0x3 << 4)
+#define VIDEO_CTRL_DWN_SCALING_ENABLE_LINE_BUFFER		(0x1 << 4)
+
+#define VIDEO_CTRL_VSYNC_DELAY_MASK		(3 << 2)
+#define VIDEO_CTRL_VSYNC_DELAY(x)		(x << 2)
+#define NO_DELAY			0
+#define DELAY_DIV12_HSYNC	1
+#define AUTO_DELAY			2
+
+/* AST_VIDEO_INT_EN			0x304		Video interrupt Enable */
+/* AST_VIDEO_INT_STS		0x308		Video interrupt status */
+#define VM_COMPRESS_COMPLETE			(1 << 17)
+#define VM_CAPTURE_COMPLETE			(1 << 16)
+
+#define VIDEO_FRAME_COMPLETE			(1 << 5)
+#define VIDEO_MODE_DETECT_RDY			(1 << 4)
+#define VIDEO_COMPRESS_COMPLETE			(1 << 3)
+#define VIDEO_COMPRESS_PKT_COMPLETE		(1 << 2)
+#define VIDEO_CAPTURE_COMPLETE			(1 << 1)
+#define VIDEO_MODE_DETECT_WDT			(1 << 0)
+
+/* AST_VIDEO_MODE_DETECT	0x30C		Video Mode Detection Parameter Register */
+#define VIDEO_MODE_HOR_TOLER(x)			(x << 28)
+#define VIDEO_MODE_VER_TOLER(x)			(x << 24)
+#define VIDEO_MODE_HOR_STABLE(x)		(x << 20)
+#define VIDEO_MODE_VER_STABLE(x)		(x << 16)
+#define VIDEO_MODE_EDG_THROD(x)			(x << 8)
+
+#define MODEDETECTION_VERTICAL_STABLE_MAXIMUM       0x6
+#define MODEDETECTION_HORIZONTAL_STABLE_MAXIMUM     0x6
+#define MODEDETECTION_VERTICAL_STABLE_THRESHOLD     0x2
+#define MODEDETECTION_HORIZONTAL_STABLE_THRESHOLD   0x2
+
+/* AST_VIDEO_SCRATCH_34C	0x34C		Video Scratch Remap Read Back */
+#define SCRATCH_VGA_GET_REFLASH_RATE(x)			((x >> 8) & 0xf)
+#define SCRATCH_VGA_GET_COLOR_MODE(x)			((x >> 4) & 0xf)
+
+/* AST_VIDEO_SCRATCH_350	0x350		Video Scratch Remap Read Back */
+#define SCRATCH_VGA_GET_MODE_HEADER(x)			((x >> 8) & 0xff)
+#define SCRATCH_VGA_GET_NEW_COLOR_MODE(x)		((x >> 16) & 0xff)
+#define SCRATCH_VGA_GET_NEW_PIXEL_CLK(x)		((x >> 24) & 0xff)
+
+
+/* AST_VIDEO_SCRATCH_35C	0x35C		Video Scratch Remap Read Back */
+#define SCRATCH_VGA_PWR_STS_HSYNC				(1 << 31)
+#define SCRATCH_VGA_PWR_STS_VSYNC				(1 << 30)
+#define SCRATCH_VGA_ATTRIBTE_INDEX_BIT5			(1 << 29)
+#define SCRATCH_VGA_MASK_REG					(1 << 28)
+#define SCRATCH_VGA_CRT_RST						(1 << 27)
+#define SCRATCH_VGA_SCREEN_OFF					(1 << 26)
+#define SCRATCH_VGA_RESET						(1 << 25)
+#define SCRATCH_VGA_ENABLE						(1 << 24)
+/***********************************************************************/
+//#define CONFIG_AST_VIDEO_LOCK
+#define CONFIG_AUTO_MODE
+
+#define CONFIG_AST_VIDEO_DEBUG
+
+#ifdef CONFIG_AST_VIDEO_DEBUG
+#define VIDEO_DBG(fmt, args...) pr_debug("%s() " fmt, __func__, ## args)
+#else
+#define VIDEO_DBG(fmt, args...)
+#endif
+
+//VR08[2]
+enum ast_video_source {
+	VIDEO_SOURCE_INT_VGA = 0,
+	VIDEO_SOURCE_INT_CRT,
+	VIDEO_SOURCE_EXT_ADC,
+	VIDEO_SOURCE_EXT_DIGITAL,
+};
+
+//VR08[5]
+enum ast_vga_mode {
+	VIDEO_VGA_DIRECT_MODE = 0,
+	VIDEO_VGA_CAPTURE_MODE,
+};
+
+//VR08[4]
+enum ast_video_dis_en {
+	VIDEO_EXT_DE_SIGNAL = 0,
+	VIDEO_INT_DE_SIGNAL,
+};
+
+enum video_color_format {
+	VIDEO_COLOR_RGB565 = 0,
+	VIDEO_COLOR_RGB888,
+	VIDEO_COLOR_YUV444,
+	VIDEO_COLOR_YUV420,
+};
+
+enum vga_color_mode {
+	VGA_NO_SIGNAL = 0,
+	EGA_MODE,
+	VGA_MODE,
+	VGA_15BPP_MODE,
+	VGA_16BPP_MODE,
+	VGA_32BPP_MODE,
+	VGA_CGA_MODE,
+	VGA_TEXT_MODE,
+};
+
+enum video_stage {
+	NONE,
+	POLARITY,
+	RESOLUTION,
+	INIT,
+	RUN,
+};
+
+struct aspeed_multi_jpeg_frame {
+	u32 dwSizeInBytes;			// Image size in bytes
+	u32 dwOffsetInBytes;			// Offset in bytes
+	u16	wXPixels;				// In: X coordinate
+	u16	wYPixels;				// In: Y coordinate
+	u16	wWidthPixels;			// In: Width for Fetch
+	u16	wHeightPixels;			// In: Height for Fetch
+};
+
+#define MAX_MULTI_FRAME_CT (32)
+
+struct aspeed_multi_jpeg_config {
+	u8 multi_jpeg_frames;	// frame count
+	struct aspeed_multi_jpeg_frame frame[MAX_MULTI_FRAME_CT];	// The Multi Frames
+};
+/***********************************************************************/
+struct ast_video_config {
+	u8	engine;					//0: engine 0, engine 1
+	u8	compression_mode;		//0:DCT, 1:DCT_VQ mix VQ-2 color, 2:DCT_VQ mix VQ-4 color		9:
+	u8	compression_format;		//0:ASPEED 1:JPEG 2:Multi-JPEG
+	u8	capture_format;			//0:CCIR601-2 YUV, 1:JPEG YUV, 2:RGB for ASPEED mode only, 3:Gray
+	u8	rc4_enable;				//0:disable 1:enable
+	u8	EncodeKeys[256];
+
+	u8	YUV420_mode;			//0:YUV444, 1:YUV420
+	u8	Visual_Lossless;
+	u8	Y_JPEGTableSelector;
+	u8	AdvanceTableSelector;
+	u8	AutoMode;
+};
+
+struct ast_auto_mode {
+	u8	engine_idx;					//set 0: engine 0, engine 1
+	u8	differential;					//set 0: full, 1:diff frame
+	u8	mode_change;				//get 0: no, 1:change
+	u32	total_size;					//get
+	u32	block_count;					//get
+};
+
+struct ast_capture_mode {
+	u8	engine_idx;					//set 0: engine 0, engine 1
+	u8	differential;					//set 0: full, 1:diff frame
+	u8	mode_change;				//get 0: no, 1:change
+};
+
+struct ast_compression_mode {
+	u8	engine_idx;					//set 0: engine 0, engine 1
+	u8	mode_change;				//get 0: no, 1:change
+	u32	total_size;					//get
+	u32	block_count;					//get
+};
+
+struct ast_scaling {
+	u8	engine;					//0: engine 0, engine 1
+	u8	enable;
+	u16	x;
+	u16	y;
+};
+
+struct ast_mode_detection {
+	unsigned char		result;		//0: pass, 1: fail
+	unsigned short	src_x;
+	unsigned short	src_y;
+};
+
+//IOCTL ..
+#define VIDEOIOC_BASE       'V'
+
+#define AST_VIDEO_RESET						_IO(VIDEOIOC_BASE, 0x0)
+#define AST_VIDEO_IOC_GET_VGA_SIGNAL		_IOR(VIDEOIOC_BASE, 0x1, unsigned char)
+#define AST_VIDEO_GET_MEM_SIZE_IOCRX		_IOR(VIDEOIOC_BASE, 0x2, unsigned long)
+#define AST_VIDEO_GET_JPEG_OFFSET_IOCRX		_IOR(VIDEOIOC_BASE, 0x3, unsigned long)
+#define AST_VIDEO_VGA_MODE_DETECTION		_IOWR(VIDEOIOC_BASE, 0x4, struct ast_mode_detection*)
+
+#define AST_VIDEO_ENG_CONFIG				_IOW(VIDEOIOC_BASE, 0x5, struct ast_video_config*)
+#define AST_VIDEO_SET_SCALING				_IOW(VIDEOIOC_BASE, 0x6, struct ast_scaling*)
+
+#define AST_VIDEO_AUTOMODE_TRIGGER			_IOWR(VIDEOIOC_BASE, 0x7, struct ast_auto_mode*)
+#define AST_VIDEO_CAPTURE_TRIGGER			_IOWR(VIDEOIOC_BASE, 0x8, struct ast_capture_mode*)
+#define AST_VIDEO_COMPRESSION_TRIGGER		_IOWR(VIDEOIOC_BASE, 0x9, struct ast_compression_mode*)
+
+#define AST_VIDEO_SET_VGA_DISPLAY			_IOW(VIDEOIOC_BASE, 0xa, int)
+#define AST_VIDEO_SET_ENCRYPTION			_IOW(VIDEOIOC_BASE, 0xb, int)
+#define AST_VIDEO_SET_ENCRYPTION_KEY		_IOW(VIDEOIOC_BASE, 0xc, unsigned char*)
+#define AST_VIDEO_SET_CRT_COMPRESSION		_IOW(VIDEOIOC_BASE, 0xd, struct fb_var_screeninfo*)
+
+#define AST_VIDEO_MULTIJPEG_AUTOMODE_TRIGGER	_IOWR(VIDEOIOC_BASE, 0xe, struct aspeed_multi_jpeg_config*)
+#define AST_VIDEO_MULTIJPEG_TRIGGER			_IOWR(VIDEOIOC_BASE, 0xf, struct aspeed_multi_jpeg_config*)
+/***********************************************************************/
+struct fbinfo {
+	u16		x;
+	u16		y;
+	u8	color_mode;	//0:NON, 1:EGA, 2:VGA, 3:15bpp, 4:16bpp, 5:32bpp
+	u32	PixelClock;
+};
+
+//For Socket Transfer head formate ..
+
+struct aspeed_video_config {
+	u8		version;
+	u32		dram_base;
+};
+
+struct aspeed_video_mem {
+	dma_addr_t	dma;
+	void *virt;
+};
+
+struct ast_video_data {
+	struct device		*misc_dev;
+	void __iomem		*reg_base;			/* virtual */
+	struct regmap		*scu;
+	struct regmap		*gfx;
+	int	irq;				//Video IRQ number
+	struct aspeed_video_config	*config;
+	struct reset_control *reset;
+	struct clk			*vclk;
+	struct clk			*eclk;
+//	compress_header
+
+	struct aspeed_video_mem		video_mem;
+
+	dma_addr_t             stream_phy;            /* phy */
+	void                   *stream_virt;           /* virt */
+	dma_addr_t             buff0_phy;             /* phy */
+	void                   *buff0_virt;            /* virt */
+	dma_addr_t             buff1_phy;             /* phy */
+	void                   *buff1_virt;            /* virt */
+	dma_addr_t             bcd_phy;               /* phy */
+	void                   *bcd_virt;              /* virt */
+	dma_addr_t             jpeg_phy;              /* phy */
+	void                   *jpeg_virt;             /* virt */
+	dma_addr_t             jpeg_buf0_phy;              /* phy */
+	void                   *jpeg_buf0_virt;             /* virt */
+	dma_addr_t             jpeg_tbl_phy;          /* phy */
+	void                   *jpeg_tbl_virt;         /* virt */
+
+	//config
+	enum ast_video_source  input_source;
+	u8	rc4_enable;
+	u8 EncodeKeys[256];
+	u8	scaling;
+
+//JPEG
+	u32		video_mem_size;			/* phy size*/
+	u32		video_jpeg_offset;			/* assigned jpeg memory size*/
+	u8 mode_change;
+	struct completion	mode_detect_complete;
+	struct completion	automode_complete;
+	struct completion	capture_complete;
+	struct completion	compression_complete;
+
+	wait_queue_head_t	queue;
+
+	u32 flag;
+	wait_queue_head_t	video_wq;
+
+	u32 thread_flag;
+	struct task_struct		*thread_task;
+
+	struct fbinfo					src_fbinfo;
+	struct fbinfo					dest_fbinfo;
+	struct completion				complete;
+	u32		sts;
+	u8		direct_mode;
+	u8		stage;
+	u32	bandwidth;
+	struct mutex lock;
+
+	bool is_open;
+	int	multi_jpeg;
+};
+
+//  RC4 structure
+struct rc4_state {
+	int x;
+	int y;
+	int m[256];
+};
+
+
+/***********************************************************************/
+#define AST_SCU_MISC1_CTRL			0x2C		/*	Misc. Control register */
+#define SCU_MISC_VGA_CRT_DIS		BIT(6)
+
+static void ast_scu_set_vga_display(struct ast_video_data *ast_video, u8 enable)
+{
+	if (enable)
+		regmap_update_bits(ast_video->scu, AST_SCU_MISC1_CTRL, SCU_MISC_VGA_CRT_DIS, 0);
+	else
+		regmap_update_bits(ast_video->scu, AST_SCU_MISC1_CTRL, SCU_MISC_VGA_CRT_DIS, SCU_MISC_VGA_CRT_DIS);
+}
+
+static int ast_scu_get_vga_display(struct ast_video_data *ast_video)
+{
+	u32 val;
+
+	regmap_read(ast_video->scu, AST_SCU_MISC1_CTRL, &val);
+
+	if (val & SCU_MISC_VGA_CRT_DIS)
+		return 0;
+	else
+		return 1;
+}
+
+/***********************************************************************/
+
+
+static inline void
+ast_video_write(struct ast_video_data *ast_video, u32 val, u32 reg)
+{
+//	VIDEO_DBG("write offset: %x, val: %x\n",reg,val);
+#ifdef CONFIG_AST_VIDEO_LOCK
+	//unlock
+	writel(VIDEO_PROTECT_UNLOCK, ast_video->reg_base);
+	writel(val, ast_video->reg_base + reg);
+	//lock
+	writel(0xaa, ast_video->reg_base);
+#else
+	//Video is lock after reset, need always unlock
+	//unlock
+	writel(VIDEO_PROTECT_UNLOCK, ast_video->reg_base);
+	writel(val, ast_video->reg_base + reg);
+#endif
+}
+
+static inline u32
+ast_video_read(struct ast_video_data *ast_video, u32 reg)
+{
+	u32 val = readl(ast_video->reg_base + reg);
+//	VIDEO_DBG("read offset: %x, val: %x\n",reg,val);
+	return val;
+}
+
+/************************************************ JPEG ***************************************************************************************/
+void ast_init_jpeg_table(struct ast_video_data *ast_video)
+{
+	int i = 0;
+	int base = 0;
+	u32 *tlb_table = ast_video->jpeg_tbl_virt;
+	//JPEG header default value:
+	for (i = 0; i < 12; i++) {
+		base = (256 * i);
+		tlb_table[base + 0] = 0xE0FFD8FF;
+		tlb_table[base + 1] = 0x464A1000;
+		tlb_table[base + 2] = 0x01004649;
+		tlb_table[base + 3] = 0x60000101;
+		tlb_table[base + 4] = 0x00006000;
+		tlb_table[base + 5] = 0x0F00FEFF;
+		tlb_table[base + 6] = 0x00002D05;
+		tlb_table[base + 7] = 0x00000000;
+		tlb_table[base + 8] = 0x00000000;
+		tlb_table[base + 9] = 0x00DBFF00;
+		tlb_table[base + 44] = 0x081100C0;
+		tlb_table[base + 45] = 0x00000000;
+		tlb_table[base + 47] = 0x03011102;
+		tlb_table[base + 48] = 0xC4FF0111;
+		tlb_table[base + 49] = 0x00001F00;
+		tlb_table[base + 50] = 0x01010501;
+		tlb_table[base + 51] = 0x01010101;
+		tlb_table[base + 52] = 0x00000000;
+		tlb_table[base + 53] = 0x00000000;
+		tlb_table[base + 54] = 0x04030201;
+		tlb_table[base + 55] = 0x08070605;
+		tlb_table[base + 56] = 0xFF0B0A09;
+		tlb_table[base + 57] = 0x10B500C4;
+		tlb_table[base + 58] = 0x03010200;
+		tlb_table[base + 59] = 0x03040203;
+		tlb_table[base + 60] = 0x04040505;
+		tlb_table[base + 61] = 0x7D010000;
+		tlb_table[base + 62] = 0x00030201;
+		tlb_table[base + 63] = 0x12051104;
+		tlb_table[base + 64] = 0x06413121;
+		tlb_table[base + 65] = 0x07615113;
+		tlb_table[base + 66] = 0x32147122;
+		tlb_table[base + 67] = 0x08A19181;
+		tlb_table[base + 68] = 0xC1B14223;
+		tlb_table[base + 69] = 0xF0D15215;
+		tlb_table[base + 70] = 0x72623324;
+		tlb_table[base + 71] = 0x160A0982;
+		tlb_table[base + 72] = 0x1A191817;
+		tlb_table[base + 73] = 0x28272625;
+		tlb_table[base + 74] = 0x35342A29;
+		tlb_table[base + 75] = 0x39383736;
+		tlb_table[base + 76] = 0x4544433A;
+		tlb_table[base + 77] = 0x49484746;
+		tlb_table[base + 78] = 0x5554534A;
+		tlb_table[base + 79] = 0x59585756;
+		tlb_table[base + 80] = 0x6564635A;
+		tlb_table[base + 81] = 0x69686766;
+		tlb_table[base + 82] = 0x7574736A;
+		tlb_table[base + 83] = 0x79787776;
+		tlb_table[base + 84] = 0x8584837A;
+		tlb_table[base + 85] = 0x89888786;
+		tlb_table[base + 86] = 0x9493928A;
+		tlb_table[base + 87] = 0x98979695;
+		tlb_table[base + 88] = 0xA3A29A99;
+		tlb_table[base + 89] = 0xA7A6A5A4;
+		tlb_table[base + 90] = 0xB2AAA9A8;
+		tlb_table[base + 91] = 0xB6B5B4B3;
+		tlb_table[base + 92] = 0xBAB9B8B7;
+		tlb_table[base + 93] = 0xC5C4C3C2;
+		tlb_table[base + 94] = 0xC9C8C7C6;
+		tlb_table[base + 95] = 0xD4D3D2CA;
+		tlb_table[base + 96] = 0xD8D7D6D5;
+		tlb_table[base + 97] = 0xE2E1DAD9;
+		tlb_table[base + 98] = 0xE6E5E4E3;
+		tlb_table[base + 99] = 0xEAE9E8E7;
+		tlb_table[base + 100] = 0xF4F3F2F1;
+		tlb_table[base + 101] = 0xF8F7F6F5;
+		tlb_table[base + 102] = 0xC4FFFAF9;
+		tlb_table[base + 103] = 0x00011F00;
+		tlb_table[base + 104] = 0x01010103;
+		tlb_table[base + 105] = 0x01010101;
+		tlb_table[base + 106] = 0x00000101;
+		tlb_table[base + 107] = 0x00000000;
+		tlb_table[base + 108] = 0x04030201;
+		tlb_table[base + 109] = 0x08070605;
+		tlb_table[base + 110] = 0xFF0B0A09;
+		tlb_table[base + 111] = 0x11B500C4;
+		tlb_table[base + 112] = 0x02010200;
+		tlb_table[base + 113] = 0x04030404;
+		tlb_table[base + 114] = 0x04040507;
+		tlb_table[base + 115] = 0x77020100;
+		tlb_table[base + 116] = 0x03020100;
+		tlb_table[base + 117] = 0x21050411;
+		tlb_table[base + 118] = 0x41120631;
+		tlb_table[base + 119] = 0x71610751;
+		tlb_table[base + 120] = 0x81322213;
+		tlb_table[base + 121] = 0x91421408;
+		tlb_table[base + 122] = 0x09C1B1A1;
+		tlb_table[base + 123] = 0xF0523323;
+		tlb_table[base + 124] = 0xD1726215;
+		tlb_table[base + 125] = 0x3424160A;
+		tlb_table[base + 126] = 0x17F125E1;
+		tlb_table[base + 127] = 0x261A1918;
+		tlb_table[base + 128] = 0x2A292827;
+		tlb_table[base + 129] = 0x38373635;
+		tlb_table[base + 130] = 0x44433A39;
+		tlb_table[base + 131] = 0x48474645;
+		tlb_table[base + 132] = 0x54534A49;
+		tlb_table[base + 133] = 0x58575655;
+		tlb_table[base + 134] = 0x64635A59;
+		tlb_table[base + 135] = 0x68676665;
+		tlb_table[base + 136] = 0x74736A69;
+		tlb_table[base + 137] = 0x78777675;
+		tlb_table[base + 138] = 0x83827A79;
+		tlb_table[base + 139] = 0x87868584;
+		tlb_table[base + 140] = 0x928A8988;
+		tlb_table[base + 141] = 0x96959493;
+		tlb_table[base + 142] = 0x9A999897;
+		tlb_table[base + 143] = 0xA5A4A3A2;
+		tlb_table[base + 144] = 0xA9A8A7A6;
+		tlb_table[base + 145] = 0xB4B3B2AA;
+		tlb_table[base + 146] = 0xB8B7B6B5;
+		tlb_table[base + 147] = 0xC3C2BAB9;
+		tlb_table[base + 148] = 0xC7C6C5C4;
+		tlb_table[base + 149] = 0xD2CAC9C8;
+		tlb_table[base + 150] = 0xD6D5D4D3;
+		tlb_table[base + 151] = 0xDAD9D8D7;
+		tlb_table[base + 152] = 0xE5E4E3E2;
+		tlb_table[base + 153] = 0xE9E8E7E6;
+		tlb_table[base + 154] = 0xF4F3F2EA;
+		tlb_table[base + 155] = 0xF8F7F6F5;
+		tlb_table[base + 156] = 0xDAFFFAF9;
+		tlb_table[base + 157] = 0x01030C00;
+		tlb_table[base + 158] = 0x03110200;
+		tlb_table[base + 159] = 0x003F0011;
+
+		//Table 0
+		if (i == 0) {
+			tlb_table[base + 10] = 0x0D140043;
+			tlb_table[base + 11] = 0x0C0F110F;
+			tlb_table[base + 12] = 0x11101114;
+			tlb_table[base + 13] = 0x17141516;
+			tlb_table[base + 14] = 0x1E20321E;
+			tlb_table[base + 15] = 0x3D1E1B1B;
+			tlb_table[base + 16] = 0x32242E2B;
+			tlb_table[base + 17] = 0x4B4C3F48;
+			tlb_table[base + 18] = 0x44463F47;
+			tlb_table[base + 19] = 0x61735A50;
+			tlb_table[base + 20] = 0x566C5550;
+			tlb_table[base + 21] = 0x88644644;
+			tlb_table[base + 22] = 0x7A766C65;
+			tlb_table[base + 23] = 0x4D808280;
+			tlb_table[base + 24] = 0x8C978D60;
+			tlb_table[base + 25] = 0x7E73967D;
+			tlb_table[base + 26] = 0xDBFF7B80;
+			tlb_table[base + 27] = 0x1F014300;
+			tlb_table[base + 28] = 0x272D2121;
+			tlb_table[base + 29] = 0x3030582D;
+			tlb_table[base + 30] = 0x697BB958;
+			tlb_table[base + 31] = 0xB8B9B97B;
+			tlb_table[base + 32] = 0xB9B8A6A6;
+			tlb_table[base + 33] = 0xB9B9B9B9;
+			tlb_table[base + 34] = 0xB9B9B9B9;
+			tlb_table[base + 35] = 0xB9B9B9B9;
+			tlb_table[base + 36] = 0xB9B9B9B9;
+			tlb_table[base + 37] = 0xB9B9B9B9;
+			tlb_table[base + 38] = 0xB9B9B9B9;
+			tlb_table[base + 39] = 0xB9B9B9B9;
+			tlb_table[base + 40] = 0xB9B9B9B9;
+			tlb_table[base + 41] = 0xB9B9B9B9;
+			tlb_table[base + 42] = 0xB9B9B9B9;
+			tlb_table[base + 43] = 0xFFB9B9B9;
+		}
+		//Table 1
+		if (i == 1) {
+			tlb_table[base + 10] = 0x0C110043;
+			tlb_table[base + 11] = 0x0A0D0F0D;
+			tlb_table[base + 12] = 0x0F0E0F11;
+			tlb_table[base + 13] = 0x14111213;
+			tlb_table[base + 14] = 0x1A1C2B1A;
+			tlb_table[base + 15] = 0x351A1818;
+			tlb_table[base + 16] = 0x2B1F2826;
+			tlb_table[base + 17] = 0x4142373F;
+			tlb_table[base + 18] = 0x3C3D373E;
+			tlb_table[base + 19] = 0x55644E46;
+			tlb_table[base + 20] = 0x4B5F4A46;
+			tlb_table[base + 21] = 0x77573D3C;
+			tlb_table[base + 22] = 0x6B675F58;
+			tlb_table[base + 23] = 0x43707170;
+			tlb_table[base + 24] = 0x7A847B54;
+			tlb_table[base + 25] = 0x6E64836D;
+			tlb_table[base + 26] = 0xDBFF6C70;
+			tlb_table[base + 27] = 0x1B014300;
+			tlb_table[base + 28] = 0x22271D1D;
+			tlb_table[base + 29] = 0x2A2A4C27;
+			tlb_table[base + 30] = 0x5B6BA04C;
+			tlb_table[base + 31] = 0xA0A0A06B;
+			tlb_table[base + 32] = 0xA0A0A0A0;
+			tlb_table[base + 33] = 0xA0A0A0A0;
+			tlb_table[base + 34] = 0xA0A0A0A0;
+			tlb_table[base + 35] = 0xA0A0A0A0;
+			tlb_table[base + 36] = 0xA0A0A0A0;
+			tlb_table[base + 37] = 0xA0A0A0A0;
+			tlb_table[base + 38] = 0xA0A0A0A0;
+			tlb_table[base + 39] = 0xA0A0A0A0;
+			tlb_table[base + 40] = 0xA0A0A0A0;
+			tlb_table[base + 41] = 0xA0A0A0A0;
+			tlb_table[base + 42] = 0xA0A0A0A0;
+			tlb_table[base + 43] = 0xFFA0A0A0;
+		}
+		//Table 2
+		if (i == 2) {
+			tlb_table[base + 10] = 0x090E0043;
+			tlb_table[base + 11] = 0x090A0C0A;
+			tlb_table[base + 12] = 0x0C0B0C0E;
+			tlb_table[base + 13] = 0x110E0F10;
+			tlb_table[base + 14] = 0x15172415;
+			tlb_table[base + 15] = 0x2C151313;
+			tlb_table[base + 16] = 0x241A211F;
+			tlb_table[base + 17] = 0x36372E34;
+			tlb_table[base + 18] = 0x31322E33;
+			tlb_table[base + 19] = 0x4653413A;
+			tlb_table[base + 20] = 0x3E4E3D3A;
+			tlb_table[base + 21] = 0x62483231;
+			tlb_table[base + 22] = 0x58564E49;
+			tlb_table[base + 23] = 0x385D5E5D;
+			tlb_table[base + 24] = 0x656D6645;
+			tlb_table[base + 25] = 0x5B536C5A;
+			tlb_table[base + 26] = 0xDBFF595D;
+			tlb_table[base + 27] = 0x16014300;
+			tlb_table[base + 28] = 0x1C201818;
+			tlb_table[base + 29] = 0x22223F20;
+			tlb_table[base + 30] = 0x4B58853F;
+			tlb_table[base + 31] = 0x85858558;
+			tlb_table[base + 32] = 0x85858585;
+			tlb_table[base + 33] = 0x85858585;
+			tlb_table[base + 34] = 0x85858585;
+			tlb_table[base + 35] = 0x85858585;
+			tlb_table[base + 36] = 0x85858585;
+			tlb_table[base + 37] = 0x85858585;
+			tlb_table[base + 38] = 0x85858585;
+			tlb_table[base + 39] = 0x85858585;
+			tlb_table[base + 40] = 0x85858585;
+			tlb_table[base + 41] = 0x85858585;
+			tlb_table[base + 42] = 0x85858585;
+			tlb_table[base + 43] = 0xFF858585;
+		}
+		//Table 3
+		if (i == 3) {
+			tlb_table[base + 10] = 0x070B0043;
+			tlb_table[base + 11] = 0x07080A08;
+			tlb_table[base + 12] = 0x0A090A0B;
+			tlb_table[base + 13] = 0x0D0B0C0C;
+			tlb_table[base + 14] = 0x11121C11;
+			tlb_table[base + 15] = 0x23110F0F;
+			tlb_table[base + 16] = 0x1C141A19;
+			tlb_table[base + 17] = 0x2B2B2429;
+			tlb_table[base + 18] = 0x27282428;
+			tlb_table[base + 19] = 0x3842332E;
+			tlb_table[base + 20] = 0x313E302E;
+			tlb_table[base + 21] = 0x4E392827;
+			tlb_table[base + 22] = 0x46443E3A;
+			tlb_table[base + 23] = 0x2C4A4A4A;
+			tlb_table[base + 24] = 0x50565137;
+			tlb_table[base + 25] = 0x48425647;
+			tlb_table[base + 26] = 0xDBFF474A;
+			tlb_table[base + 27] = 0x12014300;
+			tlb_table[base + 28] = 0x161A1313;
+			tlb_table[base + 29] = 0x1C1C331A;
+			tlb_table[base + 30] = 0x3D486C33;
+			tlb_table[base + 31] = 0x6C6C6C48;
+			tlb_table[base + 32] = 0x6C6C6C6C;
+			tlb_table[base + 33] = 0x6C6C6C6C;
+			tlb_table[base + 34] = 0x6C6C6C6C;
+			tlb_table[base + 35] = 0x6C6C6C6C;
+			tlb_table[base + 36] = 0x6C6C6C6C;
+			tlb_table[base + 37] = 0x6C6C6C6C;
+			tlb_table[base + 38] = 0x6C6C6C6C;
+			tlb_table[base + 39] = 0x6C6C6C6C;
+			tlb_table[base + 40] = 0x6C6C6C6C;
+			tlb_table[base + 41] = 0x6C6C6C6C;
+			tlb_table[base + 42] = 0x6C6C6C6C;
+			tlb_table[base + 43] = 0xFF6C6C6C;
+		}
+		//Table 4
+		if (i == 4) {
+			tlb_table[base + 10] = 0x06090043;
+			tlb_table[base + 11] = 0x05060706;
+			tlb_table[base + 12] = 0x07070709;
+			tlb_table[base + 13] = 0x0A09090A;
+			tlb_table[base + 14] = 0x0D0E160D;
+			tlb_table[base + 15] = 0x1B0D0C0C;
+			tlb_table[base + 16] = 0x16101413;
+			tlb_table[base + 17] = 0x21221C20;
+			tlb_table[base + 18] = 0x1E1F1C20;
+			tlb_table[base + 19] = 0x2B332824;
+			tlb_table[base + 20] = 0x26302624;
+			tlb_table[base + 21] = 0x3D2D1F1E;
+			tlb_table[base + 22] = 0x3735302D;
+			tlb_table[base + 23] = 0x22393A39;
+			tlb_table[base + 24] = 0x3F443F2B;
+			tlb_table[base + 25] = 0x38334338;
+			tlb_table[base + 26] = 0xDBFF3739;
+			tlb_table[base + 27] = 0x0D014300;
+			tlb_table[base + 28] = 0x11130E0E;
+			tlb_table[base + 29] = 0x15152613;
+			tlb_table[base + 30] = 0x2D355026;
+			tlb_table[base + 31] = 0x50505035;
+			tlb_table[base + 32] = 0x50505050;
+			tlb_table[base + 33] = 0x50505050;
+			tlb_table[base + 34] = 0x50505050;
+			tlb_table[base + 35] = 0x50505050;
+			tlb_table[base + 36] = 0x50505050;
+			tlb_table[base + 37] = 0x50505050;
+			tlb_table[base + 38] = 0x50505050;
+			tlb_table[base + 39] = 0x50505050;
+			tlb_table[base + 40] = 0x50505050;
+			tlb_table[base + 41] = 0x50505050;
+			tlb_table[base + 42] = 0x50505050;
+			tlb_table[base + 43] = 0xFF505050;
+		}
+		//Table 5
+		if (i == 5) {
+			tlb_table[base + 10] = 0x04060043;
+			tlb_table[base + 11] = 0x03040504;
+			tlb_table[base + 12] = 0x05040506;
+			tlb_table[base + 13] = 0x07060606;
+			tlb_table[base + 14] = 0x09090F09;
+			tlb_table[base + 15] = 0x12090808;
+			tlb_table[base + 16] = 0x0F0A0D0D;
+			tlb_table[base + 17] = 0x16161315;
+			tlb_table[base + 18] = 0x14151315;
+			tlb_table[base + 19] = 0x1D221B18;
+			tlb_table[base + 20] = 0x19201918;
+			tlb_table[base + 21] = 0x281E1514;
+			tlb_table[base + 22] = 0x2423201E;
+			tlb_table[base + 23] = 0x17262726;
+			tlb_table[base + 24] = 0x2A2D2A1C;
+			tlb_table[base + 25] = 0x25222D25;
+			tlb_table[base + 26] = 0xDBFF2526;
+			tlb_table[base + 27] = 0x09014300;
+			tlb_table[base + 28] = 0x0B0D0A0A;
+			tlb_table[base + 29] = 0x0E0E1A0D;
+			tlb_table[base + 30] = 0x1F25371A;
+			tlb_table[base + 31] = 0x37373725;
+			tlb_table[base + 32] = 0x37373737;
+			tlb_table[base + 33] = 0x37373737;
+			tlb_table[base + 34] = 0x37373737;
+			tlb_table[base + 35] = 0x37373737;
+			tlb_table[base + 36] = 0x37373737;
+			tlb_table[base + 37] = 0x37373737;
+			tlb_table[base + 38] = 0x37373737;
+			tlb_table[base + 39] = 0x37373737;
+			tlb_table[base + 40] = 0x37373737;
+			tlb_table[base + 41] = 0x37373737;
+			tlb_table[base + 42] = 0x37373737;
+			tlb_table[base + 43] = 0xFF373737;
+		}
+		//Table 6
+		if (i == 6) {
+			tlb_table[base + 10] = 0x02030043;
+			tlb_table[base + 11] = 0x01020202;
+			tlb_table[base + 12] = 0x02020203;
+			tlb_table[base + 13] = 0x03030303;
+			tlb_table[base + 14] = 0x04040704;
+			tlb_table[base + 15] = 0x09040404;
+			tlb_table[base + 16] = 0x07050606;
+			tlb_table[base + 17] = 0x0B0B090A;
+			tlb_table[base + 18] = 0x0A0A090A;
+			tlb_table[base + 19] = 0x0E110D0C;
+			tlb_table[base + 20] = 0x0C100C0C;
+			tlb_table[base + 21] = 0x140F0A0A;
+			tlb_table[base + 22] = 0x1211100F;
+			tlb_table[base + 23] = 0x0B131313;
+			tlb_table[base + 24] = 0x1516150E;
+			tlb_table[base + 25] = 0x12111612;
+			tlb_table[base + 26] = 0xDBFF1213;
+			tlb_table[base + 27] = 0x04014300;
+			tlb_table[base + 28] = 0x05060505;
+			tlb_table[base + 29] = 0x07070D06;
+			tlb_table[base + 30] = 0x0F121B0D;
+			tlb_table[base + 31] = 0x1B1B1B12;
+			tlb_table[base + 32] = 0x1B1B1B1B;
+			tlb_table[base + 33] = 0x1B1B1B1B;
+			tlb_table[base + 34] = 0x1B1B1B1B;
+			tlb_table[base + 35] = 0x1B1B1B1B;
+			tlb_table[base + 36] = 0x1B1B1B1B;
+			tlb_table[base + 37] = 0x1B1B1B1B;
+			tlb_table[base + 38] = 0x1B1B1B1B;
+			tlb_table[base + 39] = 0x1B1B1B1B;
+			tlb_table[base + 40] = 0x1B1B1B1B;
+			tlb_table[base + 41] = 0x1B1B1B1B;
+			tlb_table[base + 42] = 0x1B1B1B1B;
+			tlb_table[base + 43] = 0xFF1B1B1B;
+		}
+		//Table 7
+		if (i == 7) {
+			tlb_table[base + 10] = 0x01020043;
+			tlb_table[base + 11] = 0x01010101;
+			tlb_table[base + 12] = 0x01010102;
+			tlb_table[base + 13] = 0x02020202;
+			tlb_table[base + 14] = 0x03030503;
+			tlb_table[base + 15] = 0x06030202;
+			tlb_table[base + 16] = 0x05030404;
+			tlb_table[base + 17] = 0x07070607;
+			tlb_table[base + 18] = 0x06070607;
+			tlb_table[base + 19] = 0x090B0908;
+			tlb_table[base + 20] = 0x080A0808;
+			tlb_table[base + 21] = 0x0D0A0706;
+			tlb_table[base + 22] = 0x0C0B0A0A;
+			tlb_table[base + 23] = 0x070C0D0C;
+			tlb_table[base + 24] = 0x0E0F0E09;
+			tlb_table[base + 25] = 0x0C0B0F0C;
+			tlb_table[base + 26] = 0xDBFF0C0C;
+			tlb_table[base + 27] = 0x03014300;
+			tlb_table[base + 28] = 0x03040303;
+			tlb_table[base + 29] = 0x04040804;
+			tlb_table[base + 30] = 0x0A0C1208;
+			tlb_table[base + 31] = 0x1212120C;
+			tlb_table[base + 32] = 0x12121212;
+			tlb_table[base + 33] = 0x12121212;
+			tlb_table[base + 34] = 0x12121212;
+			tlb_table[base + 35] = 0x12121212;
+			tlb_table[base + 36] = 0x12121212;
+			tlb_table[base + 37] = 0x12121212;
+			tlb_table[base + 38] = 0x12121212;
+			tlb_table[base + 39] = 0x12121212;
+			tlb_table[base + 40] = 0x12121212;
+			tlb_table[base + 41] = 0x12121212;
+			tlb_table[base + 42] = 0x12121212;
+			tlb_table[base + 43] = 0xFF121212;
+		}
+		//Table 8
+		if (i == 8) {
+			tlb_table[base + 10] = 0x01020043;
+			tlb_table[base + 11] = 0x01010101;
+			tlb_table[base + 12] = 0x01010102;
+			tlb_table[base + 13] = 0x02020202;
+			tlb_table[base + 14] = 0x03030503;
+			tlb_table[base + 15] = 0x06030202;
+			tlb_table[base + 16] = 0x05030404;
+			tlb_table[base + 17] = 0x07070607;
+			tlb_table[base + 18] = 0x06070607;
+			tlb_table[base + 19] = 0x090B0908;
+			tlb_table[base + 20] = 0x080A0808;
+			tlb_table[base + 21] = 0x0D0A0706;
+			tlb_table[base + 22] = 0x0C0B0A0A;
+			tlb_table[base + 23] = 0x070C0D0C;
+			tlb_table[base + 24] = 0x0E0F0E09;
+			tlb_table[base + 25] = 0x0C0B0F0C;
+			tlb_table[base + 26] = 0xDBFF0C0C;
+			tlb_table[base + 27] = 0x02014300;
+			tlb_table[base + 28] = 0x03030202;
+			tlb_table[base + 29] = 0x04040703;
+			tlb_table[base + 30] = 0x080A0F07;
+			tlb_table[base + 31] = 0x0F0F0F0A;
+			tlb_table[base + 32] = 0x0F0F0F0F;
+			tlb_table[base + 33] = 0x0F0F0F0F;
+			tlb_table[base + 34] = 0x0F0F0F0F;
+			tlb_table[base + 35] = 0x0F0F0F0F;
+			tlb_table[base + 36] = 0x0F0F0F0F;
+			tlb_table[base + 37] = 0x0F0F0F0F;
+			tlb_table[base + 38] = 0x0F0F0F0F;
+			tlb_table[base + 39] = 0x0F0F0F0F;
+			tlb_table[base + 40] = 0x0F0F0F0F;
+			tlb_table[base + 41] = 0x0F0F0F0F;
+			tlb_table[base + 42] = 0x0F0F0F0F;
+			tlb_table[base + 43] = 0xFF0F0F0F;
+		}
+		//Table 9
+		if (i == 9) {
+			tlb_table[base + 10] = 0x01010043;
+			tlb_table[base + 11] = 0x01010101;
+			tlb_table[base + 12] = 0x01010101;
+			tlb_table[base + 13] = 0x01010101;
+			tlb_table[base + 14] = 0x02020302;
+			tlb_table[base + 15] = 0x04020202;
+			tlb_table[base + 16] = 0x03020303;
+			tlb_table[base + 17] = 0x05050405;
+			tlb_table[base + 18] = 0x05050405;
+			tlb_table[base + 19] = 0x07080606;
+			tlb_table[base + 20] = 0x06080606;
+			tlb_table[base + 21] = 0x0A070505;
+			tlb_table[base + 22] = 0x09080807;
+			tlb_table[base + 23] = 0x05090909;
+			tlb_table[base + 24] = 0x0A0B0A07;
+			tlb_table[base + 25] = 0x09080B09;
+			tlb_table[base + 26] = 0xDBFF0909;
+			tlb_table[base + 27] = 0x02014300;
+			tlb_table[base + 28] = 0x02030202;
+			tlb_table[base + 29] = 0x03030503;
+			tlb_table[base + 30] = 0x07080C05;
+			tlb_table[base + 31] = 0x0C0C0C08;
+			tlb_table[base + 32] = 0x0C0C0C0C;
+			tlb_table[base + 33] = 0x0C0C0C0C;
+			tlb_table[base + 34] = 0x0C0C0C0C;
+			tlb_table[base + 35] = 0x0C0C0C0C;
+			tlb_table[base + 36] = 0x0C0C0C0C;
+			tlb_table[base + 37] = 0x0C0C0C0C;
+			tlb_table[base + 38] = 0x0C0C0C0C;
+			tlb_table[base + 39] = 0x0C0C0C0C;
+			tlb_table[base + 40] = 0x0C0C0C0C;
+			tlb_table[base + 41] = 0x0C0C0C0C;
+			tlb_table[base + 42] = 0x0C0C0C0C;
+			tlb_table[base + 43] = 0xFF0C0C0C;
+		}
+		//Table 10
+		if (i == 10) {
+			tlb_table[base + 10] = 0x01010043;
+			tlb_table[base + 11] = 0x01010101;
+			tlb_table[base + 12] = 0x01010101;
+			tlb_table[base + 13] = 0x01010101;
+			tlb_table[base + 14] = 0x01010201;
+			tlb_table[base + 15] = 0x03010101;
+			tlb_table[base + 16] = 0x02010202;
+			tlb_table[base + 17] = 0x03030303;
+			tlb_table[base + 18] = 0x03030303;
+			tlb_table[base + 19] = 0x04050404;
+			tlb_table[base + 20] = 0x04050404;
+			tlb_table[base + 21] = 0x06050303;
+			tlb_table[base + 22] = 0x06050505;
+			tlb_table[base + 23] = 0x03060606;
+			tlb_table[base + 24] = 0x07070704;
+			tlb_table[base + 25] = 0x06050706;
+			tlb_table[base + 26] = 0xDBFF0606;
+			tlb_table[base + 27] = 0x01014300;
+			tlb_table[base + 28] = 0x01020101;
+			tlb_table[base + 29] = 0x02020402;
+			tlb_table[base + 30] = 0x05060904;
+			tlb_table[base + 31] = 0x09090906;
+			tlb_table[base + 32] = 0x09090909;
+			tlb_table[base + 33] = 0x09090909;
+			tlb_table[base + 34] = 0x09090909;
+			tlb_table[base + 35] = 0x09090909;
+			tlb_table[base + 36] = 0x09090909;
+			tlb_table[base + 37] = 0x09090909;
+			tlb_table[base + 38] = 0x09090909;
+			tlb_table[base + 39] = 0x09090909;
+			tlb_table[base + 40] = 0x09090909;
+			tlb_table[base + 41] = 0x09090909;
+			tlb_table[base + 42] = 0x09090909;
+			tlb_table[base + 43] = 0xFF090909;
+		}
+		//Table 11
+		if (i == 11) {
+			tlb_table[base + 10] = 0x01010043;
+			tlb_table[base + 11] = 0x01010101;
+			tlb_table[base + 12] = 0x01010101;
+			tlb_table[base + 13] = 0x01010101;
+			tlb_table[base + 14] = 0x01010101;
+			tlb_table[base + 15] = 0x01010101;
+			tlb_table[base + 16] = 0x01010101;
+			tlb_table[base + 17] = 0x01010101;
+			tlb_table[base + 18] = 0x01010101;
+			tlb_table[base + 19] = 0x02020202;
+			tlb_table[base + 20] = 0x02020202;
+			tlb_table[base + 21] = 0x03020101;
+			tlb_table[base + 22] = 0x03020202;
+			tlb_table[base + 23] = 0x01030303;
+			tlb_table[base + 24] = 0x03030302;
+			tlb_table[base + 25] = 0x03020303;
+			tlb_table[base + 26] = 0xDBFF0403;
+			tlb_table[base + 27] = 0x01014300;
+			tlb_table[base + 28] = 0x01010101;
+			tlb_table[base + 29] = 0x01010201;
+			tlb_table[base + 30] = 0x03040602;
+			tlb_table[base + 31] = 0x06060604;
+			tlb_table[base + 32] = 0x06060606;
+			tlb_table[base + 33] = 0x06060606;
+			tlb_table[base + 34] = 0x06060606;
+			tlb_table[base + 35] = 0x06060606;
+			tlb_table[base + 36] = 0x06060606;
+			tlb_table[base + 37] = 0x06060606;
+			tlb_table[base + 38] = 0x06060606;
+			tlb_table[base + 39] = 0x06060606;
+			tlb_table[base + 40] = 0x06060606;
+			tlb_table[base + 41] = 0x06060606;
+			tlb_table[base + 42] = 0x06060606;
+			tlb_table[base + 43] = 0xFF060606;
+		}
+	}
+
+
+}
+
+static void ast_video_encryption_key_setup(struct ast_video_data *ast_video)
+{
+	int i, j, k, a, StringLength;
+	struct rc4_state *s = kmalloc(sizeof(struct rc4_state), GFP_KERNEL);
+	u8 *expkey = kmalloc(256, GFP_KERNEL);
+	u32     temp;
+
+	if (!s || !expkey)
+		goto out_free;
+	//key expansion
+	StringLength = strlen(ast_video->EncodeKeys);
+//	pr_info("key %s , len = %d\n",ast_video->EncodeKeys, StringLength);
+	for (i = 0; i < 256; i++) {
+		expkey[i] = ast_video->EncodeKeys[i % StringLength];
+//		pr_info(" %x ", expkey[i]);
+	}
+//	pr_info("\n");
+	//rc4 setup
+	s->x = 0;
+	s->y = 0;
+
+	for (i = 0; i < 256; i++)
+		s->m[i] = i;
+
+	j = k = 0;
+	for (i = 0; i < 256; i++) {
+		a = s->m[i];
+		j = (unsigned char)(j + a + expkey[k]);
+		s->m[i] = s->m[j];
+		s->m[j] = a;
+		k++;
+	}
+	for (i = 0; i < 64; i++) {
+		temp = s->m[i * 4] + ((s->m[i * 4 + 1]) << 8) + ((s->m[i * 4 + 2]) << 16) + ((s->m[i * 4 + 3]) << 24);
+		ast_video_write(ast_video, temp, AST_VIDEO_ENCRYPT_SRAM + i * 4);
+	}
+out_free:
+	kfree(s);
+	kfree(expkey);
+}
+
+static u8 ast_get_vga_signal(struct ast_video_data *ast_video)
+{
+	u32 VR34C, VR350, VR35C;
+	u8	color_mode;
+
+	VR35C = ast_video_read(ast_video, AST_VIDEO_SCRATCH_35C);
+	VR35C &= 0xff000000;
+
+	if (VR35C & (SCRATCH_VGA_PWR_STS_HSYNC | SCRATCH_VGA_PWR_STS_VSYNC)) {
+		VIDEO_DBG("No VGA Signal : PWR STS %x\n", VR35C);
+		return VGA_NO_SIGNAL;
+	}
+	if (VR35C == SCRATCH_VGA_MASK_REG) {
+		VIDEO_DBG("No VGA Signal : MASK %x\n", VR35C);
+		return VGA_NO_SIGNAL;
+	}
+	if (VR35C & SCRATCH_VGA_SCREEN_OFF) {
+		VIDEO_DBG("No VGA Signal : Screen off %x\n", VR35C);
+		return VGA_NO_SIGNAL;
+	}
+	if (!(VR35C & (SCRATCH_VGA_ATTRIBTE_INDEX_BIT5 | SCRATCH_VGA_MASK_REG | SCRATCH_VGA_CRT_RST | SCRATCH_VGA_RESET | SCRATCH_VGA_ENABLE))) {
+		VIDEO_DBG("NO VGA Signal : unknown %x\n", VR35C);
+		return VGA_NO_SIGNAL;
+	}
+
+	VIDEO_DBG("VGA Signal VR35C %x\n", VR35C);
+	VR350 = ast_video_read(ast_video, AST_VIDEO_SCRATCH_350);
+	if (SCRATCH_VGA_GET_MODE_HEADER(VR350) == 0xA8) {
+		color_mode = SCRATCH_VGA_GET_NEW_COLOR_MODE(VR350);
+	} else {
+		VR34C = ast_video_read(ast_video, AST_VIDEO_SCRATCH_34C);
+		if (SCRATCH_VGA_GET_COLOR_MODE(VR34C) >= VGA_15BPP_MODE)
+			color_mode = SCRATCH_VGA_GET_COLOR_MODE(VR34C);
+		else
+			color_mode = SCRATCH_VGA_GET_COLOR_MODE(VR34C);
+	}
+
+	if (color_mode == 0) {
+		VIDEO_DBG("EGA Mode\n");
+		ast_video->src_fbinfo.color_mode = EGA_MODE;
+		return EGA_MODE;
+	}
+	if (color_mode == 1) {
+		VIDEO_DBG("VGA Mode\n");
+		ast_video->src_fbinfo.color_mode = VGA_MODE;
+		return VGA_MODE;
+	}
+	if (color_mode == 2) {
+		VIDEO_DBG("15BPP Mode\n");
+		ast_video->src_fbinfo.color_mode = VGA_15BPP_MODE;
+		return VGA_15BPP_MODE;
+	}
+	if (color_mode == 3) {
+		VIDEO_DBG("16BPP Mode\n");
+		ast_video->src_fbinfo.color_mode = VGA_16BPP_MODE;
+		return VGA_16BPP_MODE;
+	}
+	if (color_mode == 4) {
+		VIDEO_DBG("32BPP Mode\n");
+		ast_video->src_fbinfo.color_mode = VGA_32BPP_MODE;
+		return VGA_32BPP_MODE;
+	}
+	pr_info("TODO ... unknown ..\n");
+	ast_video->src_fbinfo.color_mode = VGA_MODE;
+	return VGA_MODE;
+}
+
+static void ast_video_set_eng_config(struct ast_video_data *ast_video, struct ast_video_config *video_config)
+{
+	int i, base = 0;
+	u32 ctrl = 0;	//for VR004, VR204
+	u32 compress_ctrl = 0x00080000;
+	u32 *tlb_table = ast_video->jpeg_tbl_virt;
+
+	VIDEO_DBG("\n");
+
+	switch (video_config->engine) {
+	case 0:
+		ctrl = ast_video_read(ast_video, AST_VIDEO_SEQ_CTRL);
+		break;
+	case 1:
+		ctrl = ast_video_read(ast_video, AST_VM_SEQ_CTRL);
+		break;
+	}
+
+	if (video_config->AutoMode)
+		ctrl |= VIDEO_AUTO_COMPRESS;
+	else
+		ctrl &= ~VIDEO_AUTO_COMPRESS;
+
+	ast_video_write(ast_video, VIDEO_COMPRESS_COMPLETE | VIDEO_CAPTURE_COMPLETE | VIDEO_MODE_DETECT_WDT, AST_VIDEO_INT_EN);
+
+	if (ast_video->config->version >= 6) {
+		switch (video_config->compression_format) {
+		case 2:
+			ast_video->multi_jpeg = 1;
+			ctrl &= ~G5_VIDEO_COMPRESS_JPEG_MODE;
+			ast_video_write(ast_video, (ast_video_read(ast_video, AST_VIDEO_PASS_CTRL) | G6_VIDEO_MULTI_JPEG_FLAG_MODE) &
+					~(G6_VIDEO_FRAME_CT_MASK | G6_VIDEO_MULTI_JPEG_MODE), AST_VIDEO_PASS_CTRL);
+			break;
+		case 0:
+			ctrl &= ~G5_VIDEO_COMPRESS_JPEG_MODE;
+			ast_video_write(ast_video, ast_video_read(ast_video, AST_VIDEO_PASS_CTRL) &
+					~(G6_VIDEO_FRAME_CT_MASK | G6_VIDEO_MULTI_JPEG_MODE | G6_VIDEO_MULTI_JPEG_FLAG_MODE), AST_VIDEO_PASS_CTRL);
+			break;
+		case 1:
+			ctrl |= G5_VIDEO_COMPRESS_JPEG_MODE;
+			ast_video_write(ast_video, ast_video_read(ast_video, AST_VIDEO_PASS_CTRL) &
+					~(G6_VIDEO_FRAME_CT_MASK | G6_VIDEO_MULTI_JPEG_MODE | G6_VIDEO_MULTI_JPEG_FLAG_MODE), AST_VIDEO_PASS_CTRL);
+			break;
+		}
+	} else if (ast_video->config->version == 5) {
+		if (video_config->compression_format)
+			ctrl |= G5_VIDEO_COMPRESS_JPEG_MODE;
+		else
+			ctrl &= ~G5_VIDEO_COMPRESS_JPEG_MODE;
+	} else {
+		if (video_config->compression_format)
+			ctrl |= VIDEO_COMPRESS_JPEG_MODE;
+		else
+			ctrl &= ~VIDEO_COMPRESS_JPEG_MODE;
+	}
+	ctrl &= ~VIDEO_COMPRESS_FORMAT_MASK;
+
+	if (video_config->YUV420_mode)
+		ctrl |= VIDEO_COMPRESS_FORMAT(YUV420);
+
+	if (video_config->rc4_enable)
+		compress_ctrl |= VIDEO_ENCRYP_ENABLE;
+
+	switch (video_config->compression_mode) {
+	case 0:	//DCT only
+		compress_ctrl |= VIDEO_DCT_ONLY_ENCODE;
+		break;
+	case 1:	//DCT VQ mix 2-color
+		compress_ctrl &= ~(VIDEO_4COLOR_VQ_ENCODE | VIDEO_DCT_ONLY_ENCODE);
+		break;
+	case 2:	//DCT VQ mix 4-color
+		compress_ctrl |= VIDEO_4COLOR_VQ_ENCODE;
+		break;
+	default:
+		pr_info("error for compression mode~~~~\n");
+		break;
+	}
+
+	if (video_config->Visual_Lossless) {
+		compress_ctrl |= VIDEO_HQ_ENABLE;
+		compress_ctrl |= VIDEO_HQ_DCT_LUM(video_config->AdvanceTableSelector);
+		compress_ctrl |= VIDEO_HQ_DCT_CHROM((video_config->AdvanceTableSelector + 16));
+	} else
+		compress_ctrl &= ~VIDEO_HQ_ENABLE;
+
+	switch (video_config->engine) {
+	case 0:
+		ast_video_write(ast_video, ctrl, AST_VIDEO_SEQ_CTRL);
+		ast_video_write(ast_video, compress_ctrl | VIDEO_DCT_LUM(video_config->Y_JPEGTableSelector) | VIDEO_DCT_CHROM(video_config->Y_JPEGTableSelector + 16), AST_VIDEO_COMPRESS_CTRL);
+		break;
+	case 1:
+		ast_video_write(ast_video, ctrl, AST_VM_SEQ_CTRL);
+		ast_video_write(ast_video, compress_ctrl | VIDEO_DCT_LUM(video_config->Y_JPEGTableSelector) | VIDEO_DCT_CHROM(video_config->Y_JPEGTableSelector + 16), AST_VM_COMPRESS_CTRL);
+		break;
+	}
+
+	if (video_config->compression_format >= 1) {
+		for (i = 0; i < 12; i++) {
+			base = (1024 * i);
+			if (video_config->YUV420_mode)	//yuv420
+				tlb_table[base + 46] = 0x00220103; //for YUV420 mode
+			else
+				tlb_table[base + 46] = 0x00110103; //for YUV444 mode)
+		}
+	}
+}
+
+static void ast_video_set_0_scaling(struct ast_video_data *ast_video, struct ast_scaling *scaling)
+{
+	u32 scan_line, v_factor, h_factor;
+	u32 ctrl = ast_video_read(ast_video, AST_VIDEO_CTRL);
+	//no scaling
+	ctrl &= ~VIDEO_CTRL_DWN_SCALING_MASK;
+
+	if (scaling->enable) {
+		if ((ast_video->src_fbinfo.x == scaling->x) && (ast_video->src_fbinfo.y == scaling->y)) {
+			ast_video_write(ast_video, 0x00200000, AST_VIDEO_SCALING0);
+			ast_video_write(ast_video, 0x00200000, AST_VIDEO_SCALING1);
+			ast_video_write(ast_video, 0x00200000, AST_VIDEO_SCALING2);
+			ast_video_write(ast_video, 0x00200000, AST_VIDEO_SCALING3);
+			//compression x,y
+			ast_video_write(ast_video, VIDEO_COMPRESS_H(ast_video->src_fbinfo.x) | VIDEO_COMPRESS_V(ast_video->src_fbinfo.y), AST_VIDEO_COMPRESS_WIN);
+			ast_video_write(ast_video, 0x10001000, AST_VIDEO_SCAL_FACTOR);
+		} else {
+			//Down-Scaling
+			VIDEO_DBG("Scaling Enable\n");
+			//Calculate scaling factor D / S = 4096 / Factor  ======> Factor = (S / D) * 4096
+			h_factor = ((ast_video->src_fbinfo.x - 1) * 4096) / (scaling->x - 1);
+			if (h_factor < 4096)
+				h_factor = 4096;
+			if ((h_factor * (scaling->x - 1)) != (ast_video->src_fbinfo.x - 1) * 4096)
+				h_factor += 1;
+
+			//Calculate scaling factor D / S = 4096 / Factor	======> Factor = (S / D) * 4096
+			v_factor = ((ast_video->src_fbinfo.y - 1) * 4096) / (scaling->y - 1);
+			if (v_factor < 4096)
+				v_factor = 4096;
+			if ((v_factor * (scaling->y - 1)) != (ast_video->src_fbinfo.y - 1) * 4096)
+				v_factor += 1;
+
+			if ((ast_video->config->version != 5) && (ast_video->config->version != 6))
+				ctrl |= VIDEO_CTRL_DWN_SCALING_ENABLE_LINE_BUFFER;
+
+			if (ast_video->src_fbinfo.x <= scaling->x * 2) {
+				ast_video_write(ast_video, 0x00101000, AST_VIDEO_SCALING0);
+				ast_video_write(ast_video, 0x00101000, AST_VIDEO_SCALING1);
+				ast_video_write(ast_video, 0x00101000, AST_VIDEO_SCALING2);
+				ast_video_write(ast_video, 0x00101000, AST_VIDEO_SCALING3);
+			} else {
+				ast_video_write(ast_video, 0x08080808, AST_VIDEO_SCALING0);
+				ast_video_write(ast_video, 0x08080808, AST_VIDEO_SCALING1);
+				ast_video_write(ast_video, 0x08080808, AST_VIDEO_SCALING2);
+				ast_video_write(ast_video, 0x08080808, AST_VIDEO_SCALING3);
+			}
+			//compression x,y
+			ast_video_write(ast_video, VIDEO_COMPRESS_H(scaling->x) | VIDEO_COMPRESS_V(scaling->y), AST_VIDEO_COMPRESS_WIN);
+
+			VIDEO_DBG("Scaling factor : v : %d , h : %d\n", v_factor, h_factor);
+			ast_video_write(ast_video, VIDEO_V_SCAL_FACTOR(v_factor) | VIDEO_H_SCAL_FACTOR(h_factor), AST_VIDEO_SCAL_FACTOR);
+		}
+	} else {// 1:1
+		VIDEO_DBG("Scaling Disable\n");
+		v_factor = 4096;
+		h_factor = 4096;
+		ast_video_write(ast_video, 0x00200000, AST_VIDEO_SCALING0);
+		ast_video_write(ast_video, 0x00200000, AST_VIDEO_SCALING1);
+		ast_video_write(ast_video, 0x00200000, AST_VIDEO_SCALING2);
+		ast_video_write(ast_video, 0x00200000, AST_VIDEO_SCALING3);
+		//compression x,y
+		ast_video_write(ast_video, VIDEO_COMPRESS_H(ast_video->src_fbinfo.x) | VIDEO_COMPRESS_V(ast_video->src_fbinfo.y), AST_VIDEO_COMPRESS_WIN);
+
+		ast_video_write(ast_video, 0x10001000, AST_VIDEO_SCAL_FACTOR);
+	}
+	ast_video_write(ast_video, ctrl, AST_VIDEO_CTRL);
+
+	//capture x y
+	if ((ast_video->config->version == 5) || (ast_video->config->version == 6)) {
+		//A1 issue fix
+		if (ast_video->src_fbinfo.x == 1680)
+			ast_video_write(ast_video, VIDEO_CAPTURE_H(1728) | VIDEO_CAPTURE_V(ast_video->src_fbinfo.y), AST_VIDEO_CAPTURE_WIN);
+		else
+			ast_video_write(ast_video, VIDEO_CAPTURE_H(ast_video->src_fbinfo.x) |	VIDEO_CAPTURE_V(ast_video->src_fbinfo.y), AST_VIDEO_CAPTURE_WIN);
+	} else {
+		ast_video_write(ast_video, VIDEO_CAPTURE_H(ast_video->src_fbinfo.x) |	VIDEO_CAPTURE_V(ast_video->src_fbinfo.y), AST_VIDEO_CAPTURE_WIN);
+	}
+
+
+	if ((ast_video->src_fbinfo.x % 8) == 0)
+		ast_video_write(ast_video, ast_video->src_fbinfo.x * 4, AST_VIDEO_SOURCE_SCAN_LINE);
+	else {
+		scan_line = ast_video->src_fbinfo.x;
+		scan_line = scan_line + 16 - (scan_line % 16);
+		scan_line = scan_line * 4;
+		ast_video_write(ast_video, scan_line, AST_VIDEO_SOURCE_SCAN_LINE);
+	}
+
+}
+
+static void ast_video_set_1_scaling(struct ast_video_data *ast_video, struct ast_scaling *scaling)
+{
+	u32 v_factor, h_factor;
+
+	if (scaling->enable) {
+		if ((ast_video->src_fbinfo.x == scaling->x) && (ast_video->src_fbinfo.y == scaling->y)) {
+			ast_video_write(ast_video, VIDEO_COMPRESS_H(ast_video->src_fbinfo.x) | VIDEO_COMPRESS_V(ast_video->src_fbinfo.y), AST_VM_COMPRESS_WIN);
+			ast_video_write(ast_video, 0x10001000, AST_VM_SCAL_FACTOR);
+		} else {
+			//Down-Scaling
+			VIDEO_DBG("Scaling Enable\n");
+			//Calculate scaling factor D / S = 4096 / Factor  ======> Factor = (S / D) * 4096
+			h_factor = ((ast_video->src_fbinfo.x - 1) * 4096) / (scaling->x - 1);
+			if (h_factor < 4096)
+				h_factor = 4096;
+			if ((h_factor * (scaling->x - 1)) != (ast_video->src_fbinfo.x - 1) * 4096)
+				h_factor += 1;
+
+			//Calculate scaling factor D / S = 4096 / Factor	======> Factor = (S / D) * 4096
+			v_factor = ((ast_video->src_fbinfo.y - 1) * 4096) / (scaling->y - 1);
+			if (v_factor < 4096)
+				v_factor = 4096;
+			if ((v_factor * (scaling->y - 1)) != (ast_video->src_fbinfo.y - 1) * 4096)
+				v_factor += 1;
+
+			//compression x,y
+			ast_video_write(ast_video, VIDEO_COMPRESS_H(scaling->x) | VIDEO_COMPRESS_V(scaling->y), AST_VM_COMPRESS_WIN);
+			ast_video_write(ast_video, VIDEO_V_SCAL_FACTOR(v_factor) | VIDEO_H_SCAL_FACTOR(h_factor), AST_VM_SCAL_FACTOR);
+		}
+	} else {// 1:1
+		VIDEO_DBG("Scaling Disable\n");
+		ast_video_write(ast_video, VIDEO_COMPRESS_H(ast_video->src_fbinfo.x) | VIDEO_COMPRESS_V(ast_video->src_fbinfo.y), AST_VM_COMPRESS_WIN);
+		ast_video_write(ast_video, 0x10001000, AST_VM_SCAL_FACTOR);
+	}
+
+	//capture x y
+	if (ast_video->config->version >= 5) {
+		if (ast_video->src_fbinfo.x == 1680)
+			ast_video_write(ast_video, VIDEO_CAPTURE_H(1728) | VIDEO_CAPTURE_V(ast_video->src_fbinfo.y), AST_VM_CAPTURE_WIN);
+		else
+			ast_video_write(ast_video, VIDEO_CAPTURE_H(ast_video->src_fbinfo.x) | VIDEO_CAPTURE_V(ast_video->src_fbinfo.y), AST_VM_CAPTURE_WIN);
+	} else {
+		ast_video_write(ast_video, VIDEO_CAPTURE_H(ast_video->src_fbinfo.x) | VIDEO_CAPTURE_V(ast_video->src_fbinfo.y), AST_VM_CAPTURE_WIN);
+	}
+
+
+}
+
+static void ast_video_mode_detect_trigger(struct ast_video_data *ast_video)
+{
+	VIDEO_DBG("\n");
+
+	if (!(ast_video_read(ast_video, AST_VIDEO_SEQ_CTRL) & VIDEO_CAPTURE_BUSY))
+		pr_info("ERROR ~~ Capture Eng busy !! 0x04 : %x\n", ast_video_read(ast_video, AST_VIDEO_SEQ_CTRL));
+
+	init_completion(&ast_video->mode_detect_complete);
+
+	ast_video_write(ast_video, VIDEO_MODE_DETECT_RDY, AST_VIDEO_INT_EN);
+
+	ast_video_write(ast_video, ast_video_read(ast_video, AST_VIDEO_SEQ_CTRL) & ~(VIDEO_DETECT_TRIGGER | VIDEO_INPUT_MODE_CHG_WDT), AST_VIDEO_SEQ_CTRL);
+
+	ast_video_write(ast_video, ast_video_read(ast_video, AST_VIDEO_SEQ_CTRL) | VIDEO_DETECT_TRIGGER, AST_VIDEO_SEQ_CTRL);
+
+	wait_for_completion_interruptible(&ast_video->mode_detect_complete);
+
+	ast_video_write(ast_video, 0, AST_VIDEO_INT_EN);
+
+}
+
+static void ast_video_vga_mode_detect(struct ast_video_data *ast_video, struct ast_mode_detection *mode_detect)
+{
+	u32 H_Start, H_End, V_Start, V_End;
+	u32 H_Temp = 0, V_Temp = 0, RefreshRateIndex, ColorDepthIndex;
+	u32 VGA_Scratch_Register_350, VGA_Scratch_Register_354, VGA_Scratch_Register_34C, Color_Depth, Mode_Clock;
+	u8 Direct_Mode;
+
+	VIDEO_DBG("\n");
+
+	//set input signal  and Check polarity (video engine prefers negative signal)
+	ast_video_write(ast_video, (ast_video_read(ast_video, AST_VIDEO_PASS_CTRL) &
+								~(VIDEO_DIRT_FATCH | VIDEO_EXT_ADC_ATTRIBUTE)) |
+					VIDEO_INTERNAL_DE |
+					VIDEO_SO_VSYNC_POLARITY | VIDEO_SO_HSYNC_POLARITY,
+					AST_VIDEO_PASS_CTRL);
+
+	ast_video_mode_detect_trigger(ast_video);
+
+	//Enable Watchdog detection
+	ast_video_write(ast_video, ast_video_read(ast_video, AST_VIDEO_SEQ_CTRL) | VIDEO_INPUT_MODE_CHG_WDT, AST_VIDEO_SEQ_CTRL);
+
+Redo:
+	//for store lock
+	ast_video_mode_detect_trigger(ast_video);
+
+	H_Start = VIDEO_GET_HSYNC_LEFT(ast_video_read(ast_video, AST_VIDEO_H_DETECT_STS));
+	H_End = VIDEO_GET_HSYNC_RIGHT(ast_video_read(ast_video, AST_VIDEO_H_DETECT_STS));
+
+	V_Start = VIDEO_GET_VSYNC_TOP(ast_video_read(ast_video, AST_VIDEO_V_DETECT_STS));
+	V_End = VIDEO_GET_VSYNC_BOTTOM(ast_video_read(ast_video, AST_VIDEO_V_DETECT_STS));
+
+
+	//Check if cable quality is too bad. If it is bad then we use 0x65 as threshold
+	//Because RGB data is arrived slower than H-sync, V-sync. We have to read more times to confirm RGB data is arrived
+	if ((abs(H_Temp - H_Start) > 1) || ((H_Start <= 1) || (V_Start <= 1) || (H_Start == 0xFFF) || (V_Start == 0xFFF))) {
+		H_Temp = VIDEO_GET_HSYNC_LEFT(ast_video_read(ast_video, AST_VIDEO_H_DETECT_STS));
+		V_Temp = VIDEO_GET_VSYNC_TOP(ast_video_read(ast_video, AST_VIDEO_V_DETECT_STS));
+		goto Redo;
+	}
+
+//	VIDEO_DBG("H S: %d, E: %d, V S: %d, E: %d\n", H_Start, H_End, V_Start, V_End);
+
+	ast_video_write(ast_video, VIDEO_HSYNC_PIXEL_FIRST_SET(H_Start - 1) | VIDEO_HSYNC_PIXEL_LAST_SET(H_End), AST_VIDEO_TIMING_H);
+	ast_video_write(ast_video, VIDEO_VSYNC_PIXEL_FIRST_SET(V_Start) | VIDEO_VSYNC_PIXEL_LAST_SET(V_End + 1), AST_VIDEO_TIMING_V);
+
+	ast_video->src_fbinfo.x = (H_End - H_Start) + 1;
+	ast_video->src_fbinfo.y = (V_End - V_Start) + 1;
+
+	VIDEO_DBG("screen mode x:%d, y:%d\n", ast_video->src_fbinfo.x, ast_video->src_fbinfo.y);
+
+	mode_detect->src_x = ast_video->src_fbinfo.x;
+	mode_detect->src_y = ast_video->src_fbinfo.y;
+
+	VGA_Scratch_Register_350 = ast_video_read(ast_video, AST_VIDEO_SCRATCH_350);
+	VGA_Scratch_Register_34C = ast_video_read(ast_video, AST_VIDEO_SCRATCH_34C);
+	VGA_Scratch_Register_354 = ast_video_read(ast_video, AST_VIDEO_SCRATCH_354);
+
+	ast_video_write(ast_video, ast_video_read(ast_video, AST_VIDEO_PASS_CTRL) &
+					~(VIDEO_SO_VSYNC_POLARITY | VIDEO_SO_HSYNC_POLARITY),
+					AST_VIDEO_PASS_CTRL);
+
+
+	if (((VGA_Scratch_Register_350 & 0xff00) >> 8) == 0xA8) {
+		//Driver supports to write display information in scratch register
+//		pr_info("Wide Screen Information\n");
+		/*
+		 * Index 0x94: (VIDEO:1E70:0354)
+		 * D[7:0]: HDE D[7:0]
+		 * Index 0x95: (VIDEO:1E70:0355)
+		 * D[7:0]: HDE D[15:8]
+		 * Index 0x96: (VIDEO:1E70:0356)
+		 * D[7:0]: VDE D[7:0]
+		 * Index 0x97: (VIDEO:1E70:0357)
+		 * D[7:0]: VDE D[15:8]
+		 */
+
+		Color_Depth = ((VGA_Scratch_Register_350 & 0xff0000) >> 16); //VGA's Color Depth is 0 when real color depth is less than 8
+		Mode_Clock = ((VGA_Scratch_Register_350 & 0xff000000) >> 24);
+		if (Color_Depth < 15) {
+//			pr_info("Color Depth is not 16bpp or higher\n");
+			Direct_Mode = 0;
+		} else {
+//			pr_info("Color Depth is 16bpp or higher\n");
+			Direct_Mode = 1;
+		}
+	} else { //Original mode information
+		//Judge if bandwidth is not enough then enable direct mode in internal VGA
+		/* Index 0x8E: (VIDEO:1E70:034E)
+		 * Mode ID Resolution Notes
+		 * 0x2E 640x480
+		 * 0x30 800x600
+		 * 0x31 1024x768
+		 * 0x32 1280x1024
+		 * 0x33 1600x1200
+		 * 0x34 1920x1200
+		 * 0x35 1280x800
+		 * 0x36 1440x900
+		 * 0x37 1680x1050
+		 * 0x38 1920x1080
+		 * 0x39 1366x768
+		 * 0x3A 1600x900
+		 * 0x3B 1152x864
+		 * 0x50 320x240
+		 * 0x51 400x300
+		 * 0x52 512x384
+		 * 0x6A 800x600
+		 */
+
+		RefreshRateIndex = (VGA_Scratch_Register_34C >> 8) & 0x0F;
+		ColorDepthIndex = (VGA_Scratch_Register_34C >> 4) & 0x0F;
+//		pr_info("Original mode information\n");
+		if ((ColorDepthIndex == 0xe) || (ColorDepthIndex == 0xf)) {
+			Direct_Mode = 0;
+		} else {
+			if (ColorDepthIndex > 2) {
+				if ((ast_video->src_fbinfo.x * ast_video->src_fbinfo.y) < (1024 * 768))
+					Direct_Mode = 0;
+				else
+					Direct_Mode = 1;
+			} else {
+				Direct_Mode = 0;
+			}
+		}
+	}
+
+	if (Direct_Mode) {
+		VIDEO_DBG("Direct Mode\n");
+		ast_video_write(ast_video, ast_video_read(ast_video, AST_VIDEO_PASS_CTRL) | VIDEO_DIRT_FATCH | VIDEO_AUTO_FATCH, AST_VIDEO_PASS_CTRL);
+
+//		ast_video_write(ast_video, get_vga_mem_base(), AST_VIDEO_DIRECT_BASE);
+
+		ast_video_write(ast_video, VIDEO_FETCH_TIMING(0) | VIDEO_FETCH_LINE_OFFSET(ast_video->src_fbinfo.x * 4), AST_VIDEO_DIRECT_CTRL);
+
+	} else {
+		VIDEO_DBG("Sync Mode\n");
+		ast_video_write(ast_video, ast_video_read(ast_video, AST_VIDEO_PASS_CTRL) & ~VIDEO_DIRT_FATCH, AST_VIDEO_PASS_CTRL);
+	}
+
+	//should enable WDT detection every after mode detection
+	ast_video_write(ast_video, ast_video_read(ast_video, AST_VIDEO_SEQ_CTRL) | VIDEO_INPUT_MODE_CHG_WDT, AST_VIDEO_SEQ_CTRL);
+
+}
+
+static void ast_video_capture_trigger(struct ast_video_data *ast_video, struct ast_capture_mode *capture_mode)
+{
+	int timeout = 0;
+
+	VIDEO_DBG("\n");
+
+	if (ast_video->mode_change) {
+		capture_mode->mode_change = ast_video->mode_change;
+		ast_video->mode_change = 0;
+		return;
+	}
+
+	switch (capture_mode->engine_idx) {
+	case 0:
+		init_completion(&ast_video->capture_complete);
+
+		if (capture_mode->differential)
+			ast_video_write(ast_video, ast_video_read(ast_video, AST_VIDEO_BCD_CTRL) | VIDEO_BCD_CHG_EN, AST_VIDEO_BCD_CTRL);
+		else
+			ast_video_write(ast_video, ast_video_read(ast_video, AST_VIDEO_BCD_CTRL) & ~VIDEO_BCD_CHG_EN, AST_VIDEO_BCD_CTRL);
+
+		ast_video_write(ast_video, ast_video_read(ast_video, AST_VIDEO_SEQ_CTRL) & ~(VIDEO_CAPTURE_TRIGGER | VIDEO_COMPRESS_FORCE_IDLE | VIDEO_COMPRESS_TRIGGER | VIDEO_AUTO_COMPRESS), AST_VIDEO_SEQ_CTRL);
+		//If CPU is too fast, pleas read back and trigger
+		ast_video_write(ast_video, ast_video_read(ast_video, AST_VIDEO_SEQ_CTRL) | VIDEO_CAPTURE_TRIGGER, AST_VIDEO_SEQ_CTRL);
+
+		timeout = wait_for_completion_interruptible_timeout(&ast_video->capture_complete, HZ / 2);
+
+		if (timeout == 0)
+			pr_info("Capture timeout sts %x\n", ast_video_read(ast_video, AST_VIDEO_INT_STS));
+		break;
+	case 1:
+//		init_completion(&ast_video->automode_vm_complete);
+		if (capture_mode->differential)
+			ast_video_write(ast_video, ast_video_read(ast_video, AST_VM_BCD_CTRL) | VIDEO_BCD_CHG_EN, AST_VM_BCD_CTRL);
+		else
+			ast_video_write(ast_video, ast_video_read(ast_video, AST_VM_BCD_CTRL) & ~VIDEO_BCD_CHG_EN, AST_VM_BCD_CTRL);
+		ast_video_write(ast_video, ast_video_read(ast_video, AST_VM_SEQ_CTRL) & ~(VIDEO_CAPTURE_TRIGGER | VIDEO_COMPRESS_TRIGGER | VIDEO_AUTO_COMPRESS), AST_VM_SEQ_CTRL);
+
+		ast_video_write(ast_video, ast_video_read(ast_video, AST_VM_SEQ_CTRL) | VIDEO_CAPTURE_TRIGGER, AST_VM_SEQ_CTRL);
+		udelay(10);
+//AST_G5 Issue in isr bit 19, so use polling mode for wait engine idle
+#if 1
+		timeout = 0;
+		while (1) {
+			timeout++;
+			if ((ast_video_read(ast_video, AST_VM_SEQ_CTRL) & 0x50000) == 0x50000)
+				break;
+
+			mdelay(1);
+			if (timeout > 100)
+				break;
+		}
+
+		if (timeout >= 100)
+			pr_info("Engine hang time out\n");
+
+//			pr_info("0 isr %x\n", ast_video_read(ast_video, AST_VIDEO_INT_STS));
+		//must clear it
+		ast_video_write(ast_video, (ast_video_read(ast_video, AST_VM_SEQ_CTRL) & ~(VIDEO_CAPTURE_TRIGGER | VIDEO_COMPRESS_TRIGGER)), AST_VM_SEQ_CTRL);
+//			pr_info("1 isr %x\n", ast_video_read(ast_video, AST_VIDEO_INT_STS));
+#else
+		timeout = wait_for_completion_interruptible_timeout(&ast_video->automode_vm_complete, 10 * HZ);
+
+		if (timeout == 0) {
+			pr_info("compression timeout sts %x\n", ast_video_read(ast_video, AST_VIDEO_INT_STS));
+//			return 0;
+		} else {
+			pr_info("%x size = %x\n", ast_video_read(ast_video, 0x270), ast_video_read(ast_video, AST_VM_COMPRESS_FRAME_END));
+//			return ast_video_read(ast_video, AST_VM_COMPRESS_FRAME_END);
+		}
+#endif
+		break;
+	}
+
+	if (ast_video->mode_change) {
+		capture_mode->mode_change = ast_video->mode_change;
+		ast_video->mode_change = 0;
+	}
+
+}
+
+static void ast_video_compression_trigger(struct ast_video_data *ast_video, struct ast_compression_mode *compression_mode)
+{
+	int timeout = 0;
+	int total_frames = 0;
+
+	VIDEO_DBG("\n");
+
+	if (ast_video->mode_change) {
+		compression_mode->mode_change = ast_video->mode_change;
+		ast_video->mode_change = 0;
+		return;
+	}
+
+	switch (compression_mode->engine_idx) {
+	case 0:
+		init_completion(&ast_video->compression_complete);
+		ast_video_write(ast_video, ast_video_read(ast_video, AST_VIDEO_SEQ_CTRL) & ~(VIDEO_CAPTURE_TRIGGER | VIDEO_COMPRESS_FORCE_IDLE | VIDEO_COMPRESS_TRIGGER | VIDEO_AUTO_COMPRESS), AST_VIDEO_SEQ_CTRL);
+		//If CPU is too fast, pleas read back and trigger
+		ast_video_write(ast_video, ast_video_read(ast_video, AST_VIDEO_SEQ_CTRL) | VIDEO_COMPRESS_TRIGGER, AST_VIDEO_SEQ_CTRL);
+
+		timeout = wait_for_completion_interruptible_timeout(&ast_video->compression_complete, HZ / 2);
+
+		if (timeout == 0) {
+			pr_info("compression timeout sts %x\n", ast_video_read(ast_video, AST_VIDEO_INT_STS));
+			compression_mode->total_size = 0;
+			compression_mode->block_count = 0;
+		} else {
+			compression_mode->total_size = ast_video_read(ast_video, AST_VIDEO_COMPRESS_DATA_COUNT);
+			compression_mode->block_count = ast_video_read(ast_video, AST_VIDEO_COMPRESS_BLOCK_COUNT) >> 16;
+
+			if (ast_video->config->version == 6) {
+				if (ast_video_read(ast_video, AST_VIDEO_PASS_CTRL) & G6_VIDEO_MULTI_JPEG_MODE) {
+//					ast_video_write(ast_video, (ast_video_read(ast_video, (AST_VIDEO_SEQ_CTRL) & ~(G5_VIDEO_COMPRESS_JPEG_MODE | VIDEO_CAPTURE_MULTI_FRAME))
+//								| VIDEO_AUTO_COMPRESS, AST_VIDEO_SEQ_CTRL);
+					VIDEO_DBG("done VR[400]=0x%x, VR[404]=0x%x\n",
+							ast_video_read(ast_video, AST_VIDEO_MULTI_JPEG_SRAM),
+							ast_video_read(ast_video, AST_VIDEO_MULTI_JPEG_SRAM+4));
+
+					total_frames = ((ast_video_read(ast_video, AST_VIDEO_PASS_CTRL)>>24)&0x3f)+1;
+					pr_info("total frames=%d\n", total_frames);
+					if (total_frames > 1) {
+						pr_info("TOOD ~~~~\n");
+					} else {
+//						compression_mode->frame[0].dwOffsetInBytes = 0;
+//						compression_mode->frame[0].dwSizeInBytes = ast_video_read(ast_video, AST_VIDEO_JPEG_COUNT);
+					}
+				} else if (ast_video_read(ast_video, AST_VIDEO_SEQ_CTRL) & G5_VIDEO_COMPRESS_JPEG_MODE) {
+					compression_mode->total_size = ast_video_read(ast_video, AST_VIDEO_JPEG_COUNT);
+				} else {
+//					pr_info("%d	compression_mode->total_size %d , block count %d\n",compression_mode->differential, compression_mode->total_size, compression_mode->block_count);
+				}
+			} else if (ast_video->config->version == 5) {
+				if (ast_video_read(ast_video, AST_VIDEO_SEQ_CTRL) & G5_VIDEO_COMPRESS_JPEG_MODE) {
+					compression_mode->total_size = ast_video_read(ast_video, AST_VIDEO_JPEG_COUNT);
+//					if ((buff[compression_mode->total_size - 2] != 0xff) && (buff[compression_mode->total_size - 1] != 0xd9))
+//						pr_info("Error --- %x %x\n", buff[compression_mode->total_size - 2], buff[compression_mode->total_size - 1]);
+//					pr_info("jpeg %d compression_mode->total_size %d , block count %d\n",compression_mode->differential, compression_mode->total_size, compression_mode->block_count);
+				} else {
+//					pr_info("%d	compression_mode->total_size %d , block count %d\n",compression_mode->differential, compression_mode->total_size, compression_mode->block_count);
+				}
+			} else {
+				if (ast_video_read(ast_video, AST_VIDEO_SEQ_CTRL) & VIDEO_COMPRESS_JPEG_MODE) {
+					compression_mode->total_size = ast_video_read(ast_video, AST_VIDEO_JPEG_COUNT);
+//					if ((buff[compression_mode->total_size - 2] != 0xff) && (buff[compression_mode->total_size - 1] != 0xd9)) {
+//						pr_info("Error --- %x %x\n", buff[compression_mode->total_size - 2], buff[compression_mode->total_size - 1]);
+//					}
+//					pr_info("jpeg %d compression_mode->total_size %d , block count %d\n",compression_mode->differential, compression_mode->total_size, compression_mode->block_count);
+				} else {
+//					pr_info("%d	compression_mode->total_size %d , block count %d\n",compression_mode->differential, compression_mode->total_size, compression_mode->block_count);
+				}
+			}
+		}
+
+		break;
+	case 1:
+//		init_completion(&ast_video->automode_vm_complete);
+		ast_video_write(ast_video, ast_video_read(ast_video, AST_VM_SEQ_CTRL) & ~(VIDEO_CAPTURE_TRIGGER | VIDEO_COMPRESS_TRIGGER | VIDEO_AUTO_COMPRESS), AST_VM_SEQ_CTRL);
+
+		ast_video_write(ast_video, ast_video_read(ast_video, AST_VM_SEQ_CTRL) | VIDEO_COMPRESS_TRIGGER, AST_VM_SEQ_CTRL);
+		udelay(10);
+//AST_G5 Issue in isr bit 19, so use polling mode for wait engine idle
+#if 1
+		timeout = 0;
+		while (1) {
+			timeout++;
+			if ((ast_video_read(ast_video, AST_VM_SEQ_CTRL) & 0x50000) == 0x50000)
+				break;
+
+			mdelay(1);
+			if (timeout > 100)
+				break;
+		}
+
+		if (timeout >= 100) {
+			pr_info("Engine hang time out\n");
+			compression_mode->total_size = 0;
+			compression_mode->block_count = 0;
+		} else {
+			compression_mode->total_size = ast_video_read(ast_video, AST_VM_COMPRESS_FRAME_END);
+			compression_mode->block_count = ast_video_read(ast_video, AST_VM_COMPRESS_BLOCK_COUNT);
+		}
+
+//			pr_info("0 isr %x\n", ast_video_read(ast_video, AST_VIDEO_INT_STS));
+		//must clear it
+		ast_video_write(ast_video, ast_video_read(ast_video, AST_VM_SEQ_CTRL) & ~VIDEO_COMPRESS_TRIGGER, AST_VM_SEQ_CTRL);
+//			pr_info("1 isr %x\n", ast_video_read(ast_video, AST_VIDEO_INT_STS));
+#else
+		timeout = wait_for_completion_interruptible_timeout(&ast_video->automode_vm_complete, 10 * HZ);
+
+		if (timeout == 0) {
+			pr_info("compression timeout sts %x\n", ast_video_read(ast_video, AST_VIDEO_INT_STS));
+//			return 0;
+		} else {
+			pr_info("%x size = %x\n", ast_video_read(ast_video, 0x270), ast_video_read(ast_video, AST_VM_COMPRESS_FRAME_END));
+//			return ast_video_read(ast_video, AST_VM_COMPRESS_FRAME_END);
+		}
+#endif
+		break;
+	}
+
+	if (ast_video->mode_change) {
+		compression_mode->mode_change = ast_video->mode_change;
+		ast_video->mode_change = 0;
+	} else
+		compression_mode->mode_change = 0;
+
+}
+
+/*return compression size */
+static void ast_video_auto_mode_trigger(struct ast_video_data *ast_video, struct ast_auto_mode *auto_mode)
+{
+	int timeout = 0;
+
+	VIDEO_DBG("\n");
+
+	if (ast_video->mode_change) {
+		auto_mode->mode_change = ast_video->mode_change;
+		ast_video->mode_change = 0;
+		return;
+	}
+
+	switch (auto_mode->engine_idx) {
+	case 0:
+		init_completion(&ast_video->automode_complete);
+
+		if (auto_mode->differential)
+			ast_video_write(ast_video, ast_video_read(ast_video, AST_VIDEO_BCD_CTRL) | VIDEO_BCD_CHG_EN, AST_VIDEO_BCD_CTRL);
+		else
+			ast_video_write(ast_video, ast_video_read(ast_video, AST_VIDEO_BCD_CTRL) & ~VIDEO_BCD_CHG_EN, AST_VIDEO_BCD_CTRL);
+
+		ast_video_write(ast_video, (ast_video_read(ast_video, AST_VIDEO_SEQ_CTRL) & ~(VIDEO_CAPTURE_TRIGGER | VIDEO_COMPRESS_FORCE_IDLE | VIDEO_COMPRESS_TRIGGER)) | VIDEO_AUTO_COMPRESS, AST_VIDEO_SEQ_CTRL);
+		//If CPU is too fast, pleas read back and trigger
+		ast_video_write(ast_video, ast_video_read(ast_video, AST_VIDEO_SEQ_CTRL) | VIDEO_COMPRESS_TRIGGER | VIDEO_CAPTURE_TRIGGER, AST_VIDEO_SEQ_CTRL);
+
+		timeout = wait_for_completion_interruptible_timeout(&ast_video->automode_complete, HZ / 2);
+
+		if (timeout == 0) {
+			pr_info("auto compression timeout sts %x\n", ast_video_read(ast_video, AST_VIDEO_INT_STS));
+			auto_mode->total_size = 0;
+			auto_mode->block_count = 0;
+		} else {
+			auto_mode->total_size = ast_video_read(ast_video, AST_VIDEO_COMPRESS_DATA_COUNT);
+			auto_mode->block_count = ast_video_read(ast_video, AST_VIDEO_COMPRESS_BLOCK_COUNT) >> 16;
+			if (ast_video->config->version >= 5) {
+				if (ast_video_read(ast_video, AST_VIDEO_SEQ_CTRL) & G5_VIDEO_COMPRESS_JPEG_MODE) {
+					auto_mode->total_size = ast_video_read(ast_video, AST_VIDEO_JPEG_COUNT);
+//					if ((buff[auto_mode->total_size - 2] != 0xff) && (buff[auto_mode->total_size - 1] != 0xd9))
+//						pr_info("Error --- %x %x\n", buff[auto_mode->total_size - 2], buff[auto_mode->total_size - 1]);
+//					pr_info("jpeg %d auto_mode->total_size %d , block count %d\n",auto_mode->differential, auto_mode->total_size, auto_mode->block_count);
+				} else {
+//					pr_info("%d	auto_mode->total_size %d , block count %d\n",auto_mode->differential, auto_mode->total_size, auto_mode->block_count);
+				}
+			} else {
+				if (ast_video_read(ast_video, AST_VIDEO_SEQ_CTRL) & VIDEO_COMPRESS_JPEG_MODE) {
+					auto_mode->total_size = ast_video_read(ast_video, AST_VIDEO_JPEG_COUNT);
+//					if ((buff[auto_mode->total_size - 2] != 0xff) && (buff[auto_mode->total_size - 1] != 0xd9)) {
+//						pr_info("Error --- %x %x\n", buff[auto_mode->total_size - 2], buff[auto_mode->total_size - 1]);
+//					}
+//					pr_info("jpeg %d auto_mode->total_size %d , block count %d\n",auto_mode->differential, auto_mode->total_size, auto_mode->block_count);
+				} else {
+//					pr_info("%d	auto_mode->total_size %d , block count %d\n",auto_mode->differential, auto_mode->total_size, auto_mode->block_count);
+				}
+			}
+		}
+
+		break;
+	case 1:
+//			init_completion(&ast_video->automode_vm_complete);
+		if (auto_mode->differential)
+			ast_video_write(ast_video, ast_video_read(ast_video, AST_VM_BCD_CTRL) | VIDEO_BCD_CHG_EN, AST_VM_BCD_CTRL);
+		else
+			ast_video_write(ast_video, ast_video_read(ast_video, AST_VM_BCD_CTRL) & ~VIDEO_BCD_CHG_EN, AST_VM_BCD_CTRL);
+		ast_video_write(ast_video, (ast_video_read(ast_video, AST_VM_SEQ_CTRL) & ~(VIDEO_CAPTURE_TRIGGER | VIDEO_COMPRESS_TRIGGER)) | VIDEO_AUTO_COMPRESS, AST_VM_SEQ_CTRL);
+
+		ast_video_write(ast_video, ast_video_read(ast_video, AST_VM_SEQ_CTRL) | VIDEO_CAPTURE_TRIGGER | VIDEO_COMPRESS_TRIGGER, AST_VM_SEQ_CTRL);
+		udelay(10);
+//AST_G5 Issue in isr bit 19, so use polling mode for wait engine idle
+#if 1
+		timeout = 0;
+		while (1) {
+			timeout++;
+			if ((ast_video_read(ast_video, AST_VM_SEQ_CTRL) & 0x50000) == 0x50000)
+				break;
+
+			mdelay(1);
+			if (timeout > 100)
+				break;
+		}
+
+		if (timeout >= 100) {
+			pr_info("Engine hang time out\n");
+			auto_mode->total_size = 0;
+			auto_mode->block_count = 0;
+		} else {
+			auto_mode->total_size = ast_video_read(ast_video, AST_VM_COMPRESS_FRAME_END);
+			auto_mode->block_count = ast_video_read(ast_video, AST_VM_COMPRESS_BLOCK_COUNT);
+		}
+
+//			pr_info("0 isr %x\n", ast_video_read(ast_video, AST_VIDEO_INT_STS));
+		//must clear it
+		ast_video_write(ast_video, (ast_video_read(ast_video, AST_VM_SEQ_CTRL) & ~(VIDEO_CAPTURE_TRIGGER | VIDEO_COMPRESS_TRIGGER)), AST_VM_SEQ_CTRL);
+//			pr_info("1 isr %x\n", ast_video_read(ast_video, AST_VIDEO_INT_STS));
+#else
+		timeout = wait_for_completion_interruptible_timeout(&ast_video->automode_vm_complete, 10 * HZ);
+
+		if (timeout == 0) {
+			pr_info("compression timeout sts %x\n", ast_video_read(ast_video, AST_VIDEO_INT_STS));
+//			return 0;
+		} else {
+			pr_info("%x size = %x\n", ast_video_read(ast_video, 0x270), ast_video_read(ast_video, AST_VM_COMPRESS_FRAME_END));
+//			return ast_video_read(ast_video, AST_VM_COMPRESS_FRAME_END);
+		}
+#endif
+		break;
+	}
+
+	if (ast_video->mode_change) {
+		auto_mode->mode_change = ast_video->mode_change;
+		ast_video->mode_change = 0;
+	}
+
+}
+
+static void ast_video_multi_jpeg_trigger(struct ast_video_data *ast_video, struct aspeed_multi_jpeg_config *multi_jpeg)
+{
+	u32 yuv_shift;
+	u32 yuv_msk;
+	u32 scan_lines;
+	int timeout = 0;
+	u32 x0;
+	u32 y0;
+	int i = 0;
+	u32 dw_w_h;
+	u32 start_addr;
+	u32 multi_jpeg_data = 0;
+	u32 VR044 = ast_video_read(ast_video, AST_VIDEO_SOURCE_BUFF0);
+
+	init_completion(&ast_video->compression_complete);
+
+	scan_lines = ast_video_read(ast_video, AST_VIDEO_SOURCE_SCAN_LINE);
+
+	if (ast_video_read(ast_video, AST_VIDEO_SEQ_CTRL) & VIDEO_COMPRESS_FORMAT(YUV420)) {
+		// YUV 420
+		VIDEO_DBG("Debug: YUV420\n");
+		yuv_shift = 4;
+		yuv_msk = 0xf;
+	} else {
+		// YUV 444
+		VIDEO_DBG("Debug: YUV444\n");
+		yuv_shift = 3;
+		yuv_msk = 0x7;
+	}
+
+	ast_video_write(ast_video, ast_video_read(ast_video, AST_VIDEO_PASS_CTRL) | G6_VIDEO_MULTI_JPEG_FLAG_MODE |
+			(G6_VIDEO_JPEG__COUNT(multi_jpeg->multi_jpeg_frames - 1) | G6_VIDEO_MULTI_JPEG_MODE), AST_VIDEO_PASS_CTRL);
+
+	ast_video_write(ast_video, ast_video_read(ast_video, AST_VIDEO_BCD_CTRL) & ~VIDEO_BCD_CHG_EN, AST_VIDEO_BCD_CTRL);
+
+	ast_video_write(ast_video, ast_video_read(ast_video, AST_VIDEO_CTRL) | VIDEO_CTRL_ADDRESS_MAP_MULTI_JPEG, AST_VIDEO_CTRL);
+
+	for (i = 0; i < multi_jpeg->multi_jpeg_frames; i++) {
+		VIDEO_DBG("Debug: Before: [%d]: x: %#x y: %#x w: %#x h: %#x\n", i,
+			multi_jpeg->frame[i].wXPixels, multi_jpeg->frame[i].wYPixels,
+			multi_jpeg->frame[i].wWidthPixels, multi_jpeg->frame[i].wHeightPixels);
+		x0 = multi_jpeg->frame[i].wXPixels;
+		y0 = multi_jpeg->frame[i].wYPixels;
+		dw_w_h = SET_FRAME_W_H(multi_jpeg->frame[i].wWidthPixels, multi_jpeg->frame[i].wHeightPixels);
+		start_addr = VR044 + (scan_lines * y0) + ((256 * x0) / (1 << yuv_shift));
+		VIDEO_DBG("VR%x dw_w_h: %#x, VR%x : addr : %#x, x0 %d, y0 %d\n",
+				AST_VIDEO_MULTI_JPEG_SRAM + (8 * i), dw_w_h,
+				AST_VIDEO_MULTI_JPEG_SRAM + (8 * i) + 4, start_addr, x0, y0);
+		ast_video_write(ast_video, dw_w_h, AST_VIDEO_MULTI_JPEG_SRAM + (8 * i));
+		ast_video_write(ast_video, start_addr, AST_VIDEO_MULTI_JPEG_SRAM + (8 * i) + 4);
+	}
+
+	ast_video_write(ast_video, ast_video_read(ast_video, AST_VIDEO_SEQ_CTRL) & ~(VIDEO_CAPTURE_TRIGGER | VIDEO_COMPRESS_FORCE_IDLE | VIDEO_COMPRESS_TRIGGER), AST_VIDEO_SEQ_CTRL);
+	//set mode for multi-jpeg mode VR004[5:3]
+	ast_video_write(ast_video, (ast_video_read(ast_video, AST_VIDEO_SEQ_CTRL) & ~VIDEO_AUTO_COMPRESS)
+				| VIDEO_CAPTURE_MULTI_FRAME | G5_VIDEO_COMPRESS_JPEG_MODE, AST_VIDEO_SEQ_CTRL);
+
+	//If CPU is too fast, pleas read back and trigger
+	ast_video_write(ast_video, ast_video_read(ast_video, AST_VIDEO_SEQ_CTRL) | VIDEO_COMPRESS_TRIGGER, AST_VIDEO_SEQ_CTRL);
+
+	timeout = wait_for_completion_interruptible_timeout(&ast_video->compression_complete, HZ / 2);
+
+	if (timeout == 0) {
+		pr_info("multi compression timeout sts %x\n", ast_video_read(ast_video, AST_VIDEO_INT_STS));
+		multi_jpeg->multi_jpeg_frames = 0;
+	} else {
+		VIDEO_DBG("400 %x , 404 %x\n", ast_video_read(ast_video, AST_VIDEO_MULTI_JPEG_SRAM), ast_video_read(ast_video, AST_VIDEO_MULTI_JPEG_SRAM + 4));
+		VIDEO_DBG("408 %x , 40c %x\n", ast_video_read(ast_video, AST_VIDEO_MULTI_JPEG_SRAM + 8), ast_video_read(ast_video, AST_VIDEO_MULTI_JPEG_SRAM + 0xC));
+		for (i = 0; i < multi_jpeg->multi_jpeg_frames; i++) {
+			multi_jpeg_data = ast_video_read(ast_video, AST_VIDEO_MULTI_JPEG_SRAM + (8 * i) + 4);
+			if (multi_jpeg_data & BIT(7)) {
+				multi_jpeg->frame[i].dwSizeInBytes = ast_video_read(ast_video, AST_VIDEO_MULTI_JPEG_SRAM + (8 * i)) & 0xffffff;
+				multi_jpeg->frame[i].dwOffsetInBytes = (multi_jpeg_data & ~BIT(7)) >> 1;
+			} else {
+				multi_jpeg->frame[i].dwSizeInBytes = 0;
+			}
+			VIDEO_DBG("[%d] size %d , dwOffsetInBytes %x\n", i, multi_jpeg->frame[i].dwSizeInBytes, multi_jpeg->frame[i].dwOffsetInBytes);
+		}
+	}
+
+	ast_video_write(ast_video, (ast_video_read(ast_video, AST_VIDEO_SEQ_CTRL) & ~(G5_VIDEO_COMPRESS_JPEG_MODE | VIDEO_CAPTURE_MULTI_FRAME))
+			| VIDEO_AUTO_COMPRESS, AST_VIDEO_SEQ_CTRL);
+	ast_video_write(ast_video, ast_video_read(ast_video, AST_VIDEO_PASS_CTRL) &
+			~(G6_VIDEO_FRAME_CT_MASK | G6_VIDEO_MULTI_JPEG_MODE), AST_VIDEO_PASS_CTRL);
+
+}
+
+static void ast_video_multi_jpeg_automode_trigger(struct ast_video_data *ast_video, struct aspeed_multi_jpeg_config *multi_jpeg)
+{
+	struct ast_auto_mode auto_mode;
+	u32 yuv_shift = 0;
+	u32 bonding_x, bonding_y;
+	u32 x, y;
+	int i, j = 0;
+	u8 *bcd_buf = (u8 *)ast_video->bcd_virt;
+	u32 max_x, min_x, max_y, min_y;
+
+	auto_mode.engine_idx = 0;
+	auto_mode.mode_change = 0;
+
+	//bcd : 0 first fram.
+	if (multi_jpeg->multi_jpeg_frames)
+		auto_mode.differential = 1;
+	else	//first frame
+		auto_mode.differential = 0;
+
+	VIDEO_DBG("multi_jpeg_frames %d\n", multi_jpeg->multi_jpeg_frames);
+	//do aspeed mode first
+	ast_video_auto_mode_trigger(ast_video, &auto_mode);
+	if (ast_video->mode_change)
+		return;
+
+	if (ast_video_read(ast_video, AST_VIDEO_SEQ_CTRL) & VIDEO_COMPRESS_FORMAT(YUV420)) {
+		// YUV 420
+		VIDEO_DBG("Debug: YUV420\n");
+		yuv_shift = 4;
+	} else {
+		// YUV 444
+		VIDEO_DBG("Debug: YUV444\n");
+		yuv_shift = 3;
+	}
+
+	VIDEO_DBG("w %d, h %d bcd phy [%x]\n", ast_video->src_fbinfo.x, ast_video->src_fbinfo.y, (u32)ast_video->bcd_phy);
+
+	if (auto_mode.differential) {
+		//find bonding box
+		multi_jpeg->multi_jpeg_frames = 1;
+		bonding_x = ast_video_read(ast_video, AST_VIDEO_BONDING_X);
+		bonding_y = ast_video_read(ast_video, AST_VIDEO_BONDING_Y);
+		VIDEO_DBG("bonding box %x , %x\n", bonding_x, bonding_y);
+#if 1
+		x = ast_video->src_fbinfo.x / (1 << yuv_shift);
+		y = ast_video->src_fbinfo.y / (1 << yuv_shift);
+
+		min_x = 0x3ff;
+		min_y = 0x3ff;
+		max_x = 0;
+		max_y = 0;
+		VIDEO_DBG("block x %d ,y %d\n", x, y);
+
+		for (j = 0; j < y; j++) {
+			for (i = 0; i < x; i++) {
+				if ((*(bcd_buf + (x*j) + i) & 0xf) != 0xf) {
+//					VIDEO_DBG("[%x]: x: %d ,y: %d : data : %x\n",(x*j) + i, i, j, *(bcd_buf + (x*j) + i));
+					if (i < min_x)
+						min_x = i;
+					if (i > max_x)
+						max_x = i;
+					if (j < min_y)
+						min_y = j;
+					if (j > max_y)
+						max_y = j;
+				}
+			}
+		}
+		bonding_x = (max_x << 16) | min_x;
+		bonding_y = (max_y << 16) | min_y;
+		VIDEO_DBG("bonding box %x , %x\n", bonding_x, bonding_y);
+#endif
+		if ((bonding_y == 0x3ff) && (bonding_x == 0x3ff)) {
+			multi_jpeg->frame[0].dwSizeInBytes = 0;
+			return;
+		}
+		multi_jpeg->frame[0].wXPixels = (bonding_x & 0xffff) * (1 << yuv_shift);
+		VIDEO_DBG("x : %d, %d, yuv block size %d\n", multi_jpeg->frame[0].wXPixels, (bonding_x & 0xffff), (1 << yuv_shift));
+		multi_jpeg->frame[0].wYPixels = (bonding_y & 0xffff) * (1 << yuv_shift);
+		VIDEO_DBG("y : %d, %d, yuv block size %d\n", multi_jpeg->frame[0].wYPixels, (bonding_y & 0xffff), (1 << yuv_shift));
+		multi_jpeg->frame[0].wWidthPixels = ((bonding_x >> 16) + 1 - (bonding_x & 0xffff)) * (1 << yuv_shift);
+		multi_jpeg->frame[0].wHeightPixels = ((bonding_y >> 16) + 1 - (bonding_y & 0xffff)) * (1 << yuv_shift);
+		VIDEO_DBG("w %d , h : %d\n", multi_jpeg->frame[0].wWidthPixels, multi_jpeg->frame[0].wHeightPixels);
+	} else {
+		//first frame
+		multi_jpeg->multi_jpeg_frames = 1;
+		multi_jpeg->frame[0].wXPixels = 0;
+		multi_jpeg->frame[0].wYPixels = 0;
+		multi_jpeg->frame[0].wWidthPixels = ast_video->src_fbinfo.x;
+		multi_jpeg->frame[0].wHeightPixels = ast_video->src_fbinfo.y;
+	}
+	ast_video_multi_jpeg_trigger(ast_video, multi_jpeg);
+}
+
+static void ast_video_mode_detect_info(struct ast_video_data *ast_video)
+
+{
+	u32 H_Start, H_End, V_Start, V_End;
+
+	H_Start = VIDEO_GET_HSYNC_LEFT(ast_video_read(ast_video, AST_VIDEO_H_DETECT_STS));
+	H_End = VIDEO_GET_HSYNC_RIGHT(ast_video_read(ast_video, AST_VIDEO_H_DETECT_STS));
+
+	V_Start = VIDEO_GET_VSYNC_TOP(ast_video_read(ast_video, AST_VIDEO_V_DETECT_STS));
+	V_End = VIDEO_GET_VSYNC_BOTTOM(ast_video_read(ast_video, AST_VIDEO_V_DETECT_STS));
+
+	VIDEO_DBG("Get H_Start = %d, H_End = %d, V_Start = %d, V_End = %d\n", H_Start, H_End, V_Start, V_End);
+
+	ast_video->src_fbinfo.x = (H_End - H_Start) + 1;
+	ast_video->src_fbinfo.y = (V_End - V_Start) + 1;
+	VIDEO_DBG("source : x = %d, y = %d , color mode = %x\n", ast_video->src_fbinfo.x, ast_video->src_fbinfo.y, ast_video->src_fbinfo.color_mode);
+}
+
+
+static irqreturn_t ast_video_isr(int this_irq, void *dev_id)
+{
+	u32 status;
+	u32 swap0, swap1;
+	struct ast_video_data *ast_video = dev_id;
+
+	status = ast_video_read(ast_video, AST_VIDEO_INT_STS);
+
+	VIDEO_DBG("%x\n", status);
+
+	if (status & VIDEO_MODE_DETECT_RDY) {
+		ast_video_write(ast_video, VIDEO_MODE_DETECT_RDY, AST_VIDEO_INT_STS);
+		complete(&ast_video->mode_detect_complete);
+	}
+
+	if (status & VIDEO_MODE_DETECT_WDT) {
+		ast_video->mode_change = 1;
+		VIDEO_DBG("mode change\n");
+		ast_video_write(ast_video, VIDEO_MODE_DETECT_WDT, AST_VIDEO_INT_STS);
+	}
+
+	if (ast_video_read(ast_video, AST_VIDEO_SEQ_CTRL) & VIDEO_AUTO_COMPRESS) {
+		if ((status & (VIDEO_COMPRESS_COMPLETE | VIDEO_CAPTURE_COMPLETE)) == (VIDEO_COMPRESS_COMPLETE | VIDEO_CAPTURE_COMPLETE)) {
+			ast_video_write(ast_video, VIDEO_COMPRESS_COMPLETE | VIDEO_CAPTURE_COMPLETE, AST_VIDEO_INT_STS);
+			if (!ast_video->multi_jpeg) {
+				swap0 = ast_video_read(ast_video, AST_VIDEO_SOURCE_BUFF0);
+				swap1 = ast_video_read(ast_video, AST_VIDEO_SOURCE_BUFF1);
+				ast_video_write(ast_video, swap1, AST_VIDEO_SOURCE_BUFF0);
+				ast_video_write(ast_video, swap0, AST_VIDEO_SOURCE_BUFF1);
+			}
+			VIDEO_DBG("auto mode complete\n");
+			complete(&ast_video->automode_complete);
+		}
+	} else {
+		if (status & VIDEO_COMPRESS_COMPLETE) {
+			ast_video_write(ast_video, VIDEO_COMPRESS_COMPLETE, AST_VIDEO_INT_STS);
+			VIDEO_DBG("compress complete swap\n");
+			swap0 = ast_video_read(ast_video, AST_VIDEO_SOURCE_BUFF0);
+			swap1 = ast_video_read(ast_video, AST_VIDEO_SOURCE_BUFF1);
+			ast_video_write(ast_video, swap1, AST_VIDEO_SOURCE_BUFF0);
+			ast_video_write(ast_video, swap0, AST_VIDEO_SOURCE_BUFF1);
+			complete(&ast_video->compression_complete);
+		}
+		if (status & VIDEO_CAPTURE_COMPLETE) {
+			ast_video_write(ast_video, VIDEO_CAPTURE_COMPLETE, AST_VIDEO_INT_STS);
+			VIDEO_DBG("capture complete\n");
+			complete(&ast_video->capture_complete);
+		}
+	}
+
+	return IRQ_HANDLED;
+}
+
+#define AST_CRT_ADDR				0x80
+
+static void ast_set_crt_compression(struct ast_video_data *ast_video, struct fb_var_screeninfo *fb_info)
+{
+	u32 val;
+
+	//if use crt compression, need give capture engine clk and also can't less then 1/4 dram controller clk
+	//now set d-pll for 66mhz
+	regmap_write(ast_video->scu, 0x028, 0x5c822029);
+	regmap_write(ast_video->scu, 0x130, 0x00000580);
+	regmap_update_bits(ast_video->scu, AST_SCU_MISC1_CTRL, BIT(20), BIT(20));
+
+	ast_video->src_fbinfo.x = fb_info->xres;
+	ast_video->src_fbinfo.y = fb_info->yres;
+
+	//VR008[5] = 1
+	//VR008[8]<=0
+	ast_video_write(ast_video, (ast_video_read(ast_video, AST_VIDEO_PASS_CTRL) | VIDEO_DIRT_FATCH) & ~VIDEO_AUTO_FATCH, AST_VIDEO_PASS_CTRL);
+
+	//VR008[4]<=0 when CRT60[8:7]=10. VR008[4]<=1 when CRT60[8:7]=00.
+	//regmap_read(ast_video->gfx, AST_CRT_CTRL1, &val);
+//	pr_info("AST_CRT_CTRL1 %x\n", val);
+	if (fb_info->bits_per_pixel == 32)
+		ast_video_write(ast_video, ast_video_read(ast_video, AST_VIDEO_PASS_CTRL) & ~VIDEO_16BPP_MODE, AST_VIDEO_PASS_CTRL);
+	else if (fb_info->bits_per_pixel == 16)
+		ast_video_write(ast_video, ast_video_read(ast_video, AST_VIDEO_PASS_CTRL) | VIDEO_16BPP_MODE, AST_VIDEO_PASS_CTRL);
+	else
+		pr_info("error\n");
+
+	//VR00C <= CRT80
+	regmap_read(ast_video->gfx, AST_CRT_ADDR, &val);
+//	pr_info("AST_CRT_ADDR %x\n", val);
+	ast_video_write(ast_video, val, AST_VIDEO_DIRECT_BASE);
+
+	//VR010[14:0] <= CRT84[14:0]
+	//var->xres * var->bits_per_pixel /8;
+//	regmap_read(ast_video->gfx, AST_CRT_OFFSET, &val);
+//	pr_info("AST_CRT_OFFSET %x\n", val);
+	val = fb_info->xres * fb_info->bits_per_pixel / 8;
+	ast_video_write(ast_video, val, AST_VIDEO_DIRECT_CTRL);
+
+	//VR010[15]<=0 //force VGA blank, don;t have to do
+}
+
+static void ast_video_ctrl_init(struct ast_video_data *ast_video)
+{
+	VIDEO_DBG("\n");
+
+	ast_video_write(ast_video, (u32)ast_video->buff0_phy, AST_VIDEO_SOURCE_BUFF0);
+	ast_video_write(ast_video, (u32)ast_video->buff1_phy, AST_VIDEO_SOURCE_BUFF1);
+	ast_video_write(ast_video, (u32)ast_video->bcd_phy, AST_VIDEO_BCD_BUFF);
+	ast_video_write(ast_video, (u32)ast_video->stream_phy, AST_VIDEO_STREAM_BUFF);
+	ast_video_write(ast_video, (u32)ast_video->jpeg_tbl_phy, AST_VIDEO_JPEG_HEADER_BUFF);
+	ast_video_write(ast_video, (u32)ast_video->jpeg_tbl_phy, AST_VM_JPEG_HEADER_BUFF);
+	ast_video_write(ast_video, (u32)ast_video->jpeg_buf0_phy, AST_VM_SOURCE_BUFF0);
+	ast_video_write(ast_video, (u32)ast_video->jpeg_phy, AST_VM_COMPRESS_BUFF);
+	ast_video_write(ast_video, 0, AST_VIDEO_COMPRESS_READ);
+
+	//clr int sts
+	ast_video_write(ast_video, 0xffffffff, AST_VIDEO_INT_STS);
+	ast_video_write(ast_video, 0, AST_VIDEO_BCD_CTRL);
+
+	// =============================  JPEG init ===========================================
+	ast_init_jpeg_table(ast_video);
+	ast_video_write(ast_video,  VM_STREAM_PKT_SIZE(STREAM_3MB), AST_VM_STREAM_SIZE);
+	ast_video_write(ast_video,  0x00080000 | VIDEO_DCT_LUM(4) | VIDEO_DCT_CHROM(4 + 16) | VIDEO_DCT_ONLY_ENCODE, AST_VM_COMPRESS_CTRL);
+
+	//WriteMMIOLong(0x1e700238, 0x00000000);
+	//WriteMMIOLong(0x1e70023c, 0x00000000);
+
+	ast_video_write(ast_video, 0x00001E00, AST_VM_SOURCE_SCAN_LINE); //buffer pitch
+	ast_video_write(ast_video, 0x00000000, 0x268);
+	ast_video_write(ast_video, 0x00001234, 0x280);
+
+	ast_video_write(ast_video, 0x00000000, AST_VM_PASS_CTRL);
+	ast_video_write(ast_video, 0x00000000, AST_VM_BCD_CTRL);
+
+	// ===============================================================================
+
+
+	//Specification define bit 12:13 must always 0;
+	ast_video_write(ast_video, (ast_video_read(ast_video, AST_VIDEO_PASS_CTRL) &
+								~(VIDEO_DUAL_EDGE_MODE | VIDEO_18BIT_SINGLE_EDGE)) |
+					VIDEO_DVO_INPUT_DELAY(0x4),
+					AST_VIDEO_PASS_CTRL);
+
+	ast_video_write(ast_video, VIDEO_STREAM_PKT_N(STREAM_32_PKTS) |
+					VIDEO_STREAM_PKT_SIZE(STREAM_128KB), AST_VIDEO_STREAM_SIZE);
+
+
+	//rc4 init reset ..
+	ast_video_write(ast_video, ast_video_read(ast_video, AST_VIDEO_CTRL) | VIDEO_CTRL_RC4_RST, AST_VIDEO_CTRL);
+	ast_video_write(ast_video, ast_video_read(ast_video, AST_VIDEO_CTRL) & ~VIDEO_CTRL_RC4_RST, AST_VIDEO_CTRL);
+
+	//CRC/REDUCE_BIT register clear
+	ast_video_write(ast_video, 0, AST_VIDEO_CRC1);
+	ast_video_write(ast_video, 0, AST_VIDEO_CRC2);
+	ast_video_write(ast_video, 0, AST_VIDEO_DATA_TRUNCA);
+	ast_video_write(ast_video, 0, AST_VIDEO_COMPRESS_READ);
+
+	ast_video_write(ast_video, (ast_video_read(ast_video, AST_VIDEO_MODE_DETECT) & 0xff) |
+					VIDEO_MODE_HOR_TOLER(6) |
+					VIDEO_MODE_VER_TOLER(6) |
+					VIDEO_MODE_HOR_STABLE(2) |
+					VIDEO_MODE_VER_STABLE(2) |
+					VIDEO_MODE_EDG_THROD(0x65)
+					, AST_VIDEO_MODE_DETECT);
+
+	if (ast_video->config->version == 6)
+		ast_video_write(ast_video, (ast_video_read(ast_video, AST_VIDEO_MODE_DET2) | BIT(13)), AST_VIDEO_MODE_DET2);
+
+}
+
+static void ast_scu_reset_video(struct ast_video_data *ast_video)
+{
+	reset_control_assert(ast_video->reset);
+	udelay(100);
+	reset_control_deassert(ast_video->reset);
+}
+
+static long ast_video_ioctl(struct file *fp, unsigned int cmd, unsigned long arg)
+{
+	int ret = 1;
+	struct miscdevice *c = fp->private_data;
+	struct ast_video_data *ast_video = dev_get_drvdata(c->this_device);
+	struct ast_scaling set_scaling;
+	struct ast_video_config video_config;
+	struct ast_capture_mode capture_mode;
+	struct ast_compression_mode compression_mode;
+	struct aspeed_multi_jpeg_config multi_jpeg;
+	struct fb_var_screeninfo fb_info;
+	int vga_enable = 0;
+	int encrypt_en = 0;
+	struct ast_mode_detection mode_detection;
+	struct ast_auto_mode auto_mode;
+	void __user *argp = (void __user *)arg;
+
+
+	switch (cmd) {
+	case AST_VIDEO_RESET:
+		ast_scu_reset_video(ast_video);
+		//rc4 init reset ..
+		ast_video_write(ast_video, ast_video_read(ast_video, AST_VIDEO_CTRL) | VIDEO_CTRL_RC4_RST, AST_VIDEO_CTRL);
+		ast_video_write(ast_video, ast_video_read(ast_video, AST_VIDEO_CTRL) & ~VIDEO_CTRL_RC4_RST, AST_VIDEO_CTRL);
+		ast_video_ctrl_init(ast_video);
+		ret = 0;
+		break;
+	case AST_VIDEO_IOC_GET_VGA_SIGNAL:
+		ret = put_user(ast_get_vga_signal(ast_video), (unsigned char __user *)arg);
+		break;
+	case AST_VIDEO_GET_MEM_SIZE_IOCRX:
+		ret = __put_user(ast_video->video_mem_size, (unsigned long __user *)arg);
+		break;
+	case AST_VIDEO_GET_JPEG_OFFSET_IOCRX:
+		ret = __put_user(ast_video->video_jpeg_offset, (unsigned long __user *)arg);
+		break;
+	case AST_VIDEO_VGA_MODE_DETECTION:
+		ret = copy_from_user(&mode_detection, argp, sizeof(struct ast_mode_detection));
+		ast_video_vga_mode_detect(ast_video, &mode_detection);
+		ret = copy_to_user(argp, &mode_detection, sizeof(struct ast_mode_detection));
+		break;
+	case AST_VIDEO_ENG_CONFIG:
+		ret = copy_from_user(&video_config, argp, sizeof(struct ast_video_config));
+		ast_video_set_eng_config(ast_video, &video_config);
+		break;
+	case AST_VIDEO_SET_SCALING:
+		ret = copy_from_user(&set_scaling, argp, sizeof(struct ast_scaling));
+		switch (set_scaling.engine) {
+		case 0:
+			ast_video_set_0_scaling(ast_video, &set_scaling);
+			break;
+		case 1:
+			ast_video_set_1_scaling(ast_video, &set_scaling);
+			break;
+		}
+		break;
+	case AST_VIDEO_AUTOMODE_TRIGGER:
+		ret = copy_from_user(&auto_mode, argp, sizeof(struct ast_auto_mode));
+		ast_video_auto_mode_trigger(ast_video, &auto_mode);
+		ret = copy_to_user(argp, &auto_mode, sizeof(struct ast_auto_mode));
+		break;
+	case AST_VIDEO_CAPTURE_TRIGGER:
+		ret = copy_from_user(&capture_mode, argp, sizeof(capture_mode));
+		ast_video_capture_trigger(ast_video, &capture_mode);
+		ret = copy_to_user(argp, &capture_mode, sizeof(capture_mode));
+		break;
+	case AST_VIDEO_COMPRESSION_TRIGGER:
+		ret = copy_from_user(&compression_mode, argp, sizeof(compression_mode));
+		ast_video_compression_trigger(ast_video, &compression_mode);
+		ret = copy_to_user(argp, &compression_mode, sizeof(compression_mode));
+		break;
+	case AST_VIDEO_SET_VGA_DISPLAY:
+		ret = __get_user(vga_enable, (int __user *)arg);
+		ast_scu_set_vga_display(ast_video, vga_enable);
+		break;
+	case AST_VIDEO_SET_ENCRYPTION:
+		ret = __get_user(encrypt_en, (int __user *)arg);
+		if (encrypt_en)
+			ast_video_write(ast_video, ast_video_read(ast_video, AST_VIDEO_COMPRESS_CTRL) | VIDEO_ENCRYP_ENABLE, AST_VIDEO_COMPRESS_CTRL);
+		else
+			ast_video_write(ast_video, ast_video_read(ast_video, AST_VIDEO_COMPRESS_CTRL) & ~VIDEO_ENCRYP_ENABLE, AST_VIDEO_COMPRESS_CTRL);
+		break;
+	case AST_VIDEO_SET_ENCRYPTION_KEY:
+		memset(ast_video->EncodeKeys, 0, 256);
+		//due to system have enter key must be remove
+		ret = copy_from_user(ast_video->EncodeKeys, argp, 256 - 1);
+		pr_info("encryption key '%s'\n", ast_video->EncodeKeys);
+//			memcpy(ast_video->EncodeKeys, key, strlen(key) - 1);
+		ast_video_encryption_key_setup(ast_video);
+		ret = 0;
+		break;
+	case AST_VIDEO_SET_CRT_COMPRESSION:
+		ret = copy_from_user(&fb_info, argp, sizeof(struct fb_var_screeninfo));
+		ast_set_crt_compression(ast_video, &fb_info);
+		ret = 0;
+		break;
+	case AST_VIDEO_MULTIJPEG_AUTOMODE_TRIGGER:
+		ret = copy_from_user(&multi_jpeg, argp, sizeof(multi_jpeg));
+		ast_video_multi_jpeg_automode_trigger(ast_video, &multi_jpeg);
+		ret = copy_to_user(argp, &multi_jpeg, sizeof(multi_jpeg));
+		break;
+	case AST_VIDEO_MULTIJPEG_TRIGGER:
+		ret = copy_from_user(&multi_jpeg, argp, sizeof(multi_jpeg));
+		ast_video_multi_jpeg_trigger(ast_video, &multi_jpeg);
+		ret = copy_to_user(argp, &multi_jpeg, sizeof(multi_jpeg));
+		break;
+	default:
+		ret = 3;
+		break;
+	}
+	return ret;
+
+}
+
+/** @note munmap handler is done by vma close handler */
+static int ast_video_mmap(struct file *file, struct vm_area_struct *vma)
+{
+	struct miscdevice *c = file->private_data;
+	struct ast_video_data *ast_video = dev_get_drvdata(c->this_device);
+	size_t size = vma->vm_end - vma->vm_start;
+
+	vma->vm_private_data = ast_video;
+
+	if (PAGE_ALIGN(size) > ast_video->video_mem_size) {
+		pr_err("required length exceed the size of physical sram (%x)\n", ast_video->video_mem_size);
+		return -EAGAIN;
+	}
+
+	if ((ast_video->stream_phy + (vma->vm_pgoff << PAGE_SHIFT) + size)
+		> (ast_video->stream_phy + ast_video->video_mem_size)) {
+		pr_err("required sram range exceed the size of phisical sram\n");
+		return -EAGAIN;
+	}
+
+	vma->vm_flags |= VM_IO;
+	vma->vm_page_prot = pgprot_noncached(vma->vm_page_prot);
+
+	if (io_remap_pfn_range(vma, vma->vm_start,
+						   ((u32)ast_video->stream_phy >> PAGE_SHIFT),
+						   size,
+						   vma->vm_page_prot)) {
+		pr_err("remap_pfn_range faile at %s()\n", __func__);
+		return -EAGAIN;
+	}
+
+	return 0;
+}
+
+static int ast_video_open(struct inode *inode, struct file *file)
+{
+	struct miscdevice *c = file->private_data;
+	struct ast_video_data *ast_video = dev_get_drvdata(c->this_device);
+
+	VIDEO_DBG("\n");
+
+	ast_video->is_open = true;
+
+	return 0;
+
+}
+
+static int ast_video_release(struct inode *inode, struct file *file)
+{
+	struct miscdevice *c = file->private_data;
+	struct ast_video_data *ast_video = dev_get_drvdata(c->this_device);
+
+	VIDEO_DBG("\n");
+
+	ast_video->is_open = false;
+	return 0;
+}
+
+static const struct file_operations ast_video_fops = {
+	.owner			= THIS_MODULE,
+	.llseek			= no_llseek,
+	.unlocked_ioctl		= ast_video_ioctl,
+	.open			= ast_video_open,
+	.release		= ast_video_release,
+	.mmap			= ast_video_mmap,
+};
+
+struct miscdevice ast_video_misc = {
+	.minor = MISC_DYNAMIC_MINOR,
+	.name = "ast-video",
+	.fops = &ast_video_fops,
+};
+
+/************************************************** SYS FS **************************************************************/
+static ssize_t vga_display_show(struct device *dev, struct device_attribute *attr, char *buf)
+{
+	struct ast_video_data *ast_video = dev_get_drvdata(dev);
+
+	return sprintf(buf, "%d: %s\n", ast_scu_get_vga_display(ast_video), ast_scu_get_vga_display(ast_video) ? "Enable" : "Disable");
+}
+
+static ssize_t vga_display_store(struct device *dev, struct device_attribute *attr, const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val;
+	struct ast_video_data *ast_video = dev_get_drvdata(dev);
+
+	ret = kstrtoul(buf, 10, &val);
+	if (ret)
+		pr_err("%s: input invalid", __func__);
+
+	if (val)
+		ast_scu_set_vga_display(ast_video, 1);
+	else
+		ast_scu_set_vga_display(ast_video, 0);
+
+	return count;
+}
+
+static DEVICE_ATTR_RW(vga_display);
+
+static ssize_t video_reset_store(struct device *dev, struct device_attribute *attr, const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val;
+	struct ast_video_data *ast_video = dev_get_drvdata(dev);
+
+	ret = kstrtoul(buf, 10, &val);
+	if (ret)
+		pr_err("%s: input invalid", __func__);
+
+	if (val) {
+		ast_scu_reset_video(ast_video);
+		//rc4 init reset ..
+		ast_video_write(ast_video, ast_video_read(ast_video, AST_VIDEO_CTRL) | VIDEO_CTRL_RC4_RST, AST_VIDEO_CTRL);
+		ast_video_write(ast_video, ast_video_read(ast_video, AST_VIDEO_CTRL) & ~VIDEO_CTRL_RC4_RST, AST_VIDEO_CTRL);
+		ast_video_ctrl_init(ast_video);
+	}
+
+	return count;
+}
+
+static DEVICE_ATTR_WO(video_reset);
+
+static ssize_t video_mode_detect_show(struct device *dev, struct device_attribute *attr, char *buf)
+{
+	int ret = 0;
+	struct ast_video_data *ast_video = dev_get_drvdata(dev);
+
+	if (ret < 0)
+		return ret;
+
+	ast_video_mode_detect_info(ast_video);
+
+	return sprintf(buf, "%i\n", ret);
+}
+
+static ssize_t video_mode_detect_store(struct device *dev, struct device_attribute *attr, const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val;
+	struct ast_video_data *ast_video = dev_get_drvdata(dev);
+
+	ret = kstrtoul(buf, 10, &val);
+	if (ret)
+		pr_err("%s: input invalid", __func__);
+
+	if (val)
+		ast_video_mode_detect_trigger(ast_video);
+
+	return count;
+}
+
+static DEVICE_ATTR_RW(video_mode_detect);
+
+static struct attribute *ast_video_attributes[] = {
+	&dev_attr_vga_display.attr,
+	&dev_attr_video_reset.attr,
+	&dev_attr_video_mode_detect.attr,
+	NULL
+};
+
+static const struct attribute_group video_attribute_group = {
+	.attrs = ast_video_attributes
+};
+
+/**************************   Vudeo SYSFS  **********************************************************/
+enum ast_video_trigger_mode {
+	VIDEO_CAPTURE_MODE = 0,
+	VIDEO_COMPRESSION_MODE,
+	VIDEO_BUFFER_MODE,
+};
+
+static u8 ast_get_trigger_mode(struct ast_video_data *ast_video, u8 eng_idx)
+{
+	//VR0004[3:5] 00:capture/compression/buffer
+	u32 mode = 0;
+
+	switch (eng_idx) {
+	case 0:
+		mode = ast_video_read(ast_video, AST_VIDEO_SEQ_CTRL) & (VIDEO_CAPTURE_MULTI_FRAME | VIDEO_AUTO_COMPRESS);
+		if (mode == 0)
+			return VIDEO_CAPTURE_MODE;
+		if (mode == VIDEO_AUTO_COMPRESS)
+			return VIDEO_COMPRESSION_MODE;
+		if (mode == (VIDEO_CAPTURE_MULTI_FRAME | VIDEO_AUTO_COMPRESS))
+			return VIDEO_BUFFER_MODE;
+		pr_info("ERROR Mode\n");
+		break;
+	case 1:
+		mode = ast_video_read(ast_video, AST_VM_SEQ_CTRL) & (VIDEO_CAPTURE_MULTI_FRAME | VIDEO_AUTO_COMPRESS);
+		if (mode == 0)
+			return VIDEO_CAPTURE_MODE;
+		if (mode == VIDEO_AUTO_COMPRESS)
+			return VIDEO_COMPRESSION_MODE;
+		if (mode == (VIDEO_CAPTURE_MULTI_FRAME | VIDEO_AUTO_COMPRESS))
+			return VIDEO_BUFFER_MODE;
+		pr_info("ERROR Mode\n");
+		break;
+	}
+
+	return mode;
+}
+
+static void ast_set_trigger_mode(struct ast_video_data *ast_video, u8 eng_idx, u8 mode)
+{
+	//VR0004[3:5] 00/01/11:capture/frame/stream
+	switch (eng_idx) {
+	case 0:	//video 1
+		if (mode == VIDEO_CAPTURE_MODE)
+			ast_video_write(ast_video, ast_video_read(ast_video, AST_VIDEO_SEQ_CTRL) & ~(VIDEO_CAPTURE_MULTI_FRAME | VIDEO_AUTO_COMPRESS), AST_VIDEO_SEQ_CTRL);
+		else if (mode == VIDEO_COMPRESSION_MODE)
+			ast_video_write(ast_video, (ast_video_read(ast_video, AST_VIDEO_SEQ_CTRL) | VIDEO_AUTO_COMPRESS) & ~(VIDEO_CAPTURE_MULTI_FRAME), AST_VIDEO_SEQ_CTRL);
+		else if (mode == VIDEO_BUFFER_MODE)
+			ast_video_write(ast_video, ast_video_read(ast_video, AST_VIDEO_SEQ_CTRL) | VIDEO_CAPTURE_MULTI_FRAME | VIDEO_AUTO_COMPRESS, AST_VIDEO_SEQ_CTRL);
+		else
+			pr_info("ERROR Mode\n");
+		break;
+	case 1:	//video M
+		if (mode == VIDEO_CAPTURE_MODE)
+			ast_video_write(ast_video, ast_video_read(ast_video, AST_VM_SEQ_CTRL) & ~(VIDEO_CAPTURE_MULTI_FRAME | VIDEO_AUTO_COMPRESS), AST_VM_SEQ_CTRL);
+		else if (mode == VIDEO_COMPRESSION_MODE)
+			ast_video_write(ast_video, (ast_video_read(ast_video, AST_VM_SEQ_CTRL) | VIDEO_AUTO_COMPRESS) & ~(VIDEO_CAPTURE_MULTI_FRAME), AST_VM_SEQ_CTRL);
+		else if (mode == VIDEO_BUFFER_MODE)
+			ast_video_write(ast_video, ast_video_read(ast_video, AST_VM_SEQ_CTRL) | VIDEO_CAPTURE_MULTI_FRAME | VIDEO_AUTO_COMPRESS, AST_VM_SEQ_CTRL);
+		else
+			pr_info("ERROR Mode\n");
+		break;
+	}
+}
+
+static u8 ast_get_compress_yuv_mode(struct ast_video_data *ast_video, u8 eng_idx)
+{
+	switch (eng_idx) {
+	case 0:
+		return VIDEO_GET_COMPRESS_FORMAT(ast_video_read(ast_video, AST_VIDEO_SEQ_CTRL));
+	case 1:
+		return VIDEO_GET_COMPRESS_FORMAT(ast_video_read(ast_video, AST_VM_SEQ_CTRL));
+	}
+	return 0;
+}
+
+static void ast_set_compress_yuv_mode(struct ast_video_data *ast_video, u8 eng_idx, u8 yuv_mode)
+{
+	int i, base = 0;
+	u32 *tlb_table = ast_video->jpeg_tbl_virt;
+
+	switch (eng_idx) {
+	case 0:	//video 1
+		if (yuv_mode)	//YUV420
+			ast_video_write(ast_video, (ast_video_read(ast_video, AST_VIDEO_SEQ_CTRL) & ~VIDEO_COMPRESS_FORMAT_MASK) | VIDEO_COMPRESS_FORMAT(YUV420), AST_VIDEO_SEQ_CTRL);
+		else
+			ast_video_write(ast_video, ast_video_read(ast_video, AST_VIDEO_SEQ_CTRL) & ~VIDEO_COMPRESS_FORMAT_MASK, AST_VIDEO_SEQ_CTRL);
+		break;
+	case 1:	//video M
+		if (yuv_mode)	//YUV420
+			ast_video_write(ast_video, (ast_video_read(ast_video, AST_VM_SEQ_CTRL) & ~VIDEO_COMPRESS_FORMAT_MASK) | VIDEO_COMPRESS_FORMAT(YUV420), AST_VM_SEQ_CTRL);
+		else
+			ast_video_write(ast_video, ast_video_read(ast_video, AST_VM_SEQ_CTRL) & ~VIDEO_COMPRESS_FORMAT_MASK, AST_VM_SEQ_CTRL);
+
+		for (i = 0; i < 12; i++) {
+			base = (256 * i);
+			if (yuv_mode)	//yuv420
+				tlb_table[base + 46] = 0x00220103; //for YUV420 mode
+			else
+				tlb_table[base + 46] = 0x00110103; //for YUV444 mode)
+		}
+
+		break;
+	}
+}
+
+static u8 ast_get_compress_jpeg_mode(struct ast_video_data *ast_video, u8 eng_idx)
+{
+	switch (eng_idx) {
+	case 0:
+		if ((ast_video->config->version == 5) || (ast_video->config->version == 6)) {
+			if (ast_video_read(ast_video, AST_VIDEO_SEQ_CTRL) & G5_VIDEO_COMPRESS_JPEG_MODE)
+				return 1;
+			else
+				return 0;
+		} else {
+			if (ast_video_read(ast_video, AST_VIDEO_SEQ_CTRL) & VIDEO_COMPRESS_JPEG_MODE)
+				return 1;
+			else
+				return 0;
+		}
+		break;
+	case 1:
+		if ((ast_video->config->version == 5) || (ast_video->config->version == 6)) {
+			if (ast_video_read(ast_video, AST_VM_SEQ_CTRL) & G5_VIDEO_COMPRESS_JPEG_MODE)
+				return 1;
+			else
+				return 0;
+		} else {
+			if (ast_video_read(ast_video, AST_VM_SEQ_CTRL) & VIDEO_COMPRESS_JPEG_MODE)
+				return 1;
+			else
+				return 0;
+		}
+		break;
+	}
+	return 0;
+}
+
+static void ast_set_compress_jpeg_mode(struct ast_video_data *ast_video, u8 eng_idx, u8 jpeg_mode)
+{
+	switch (eng_idx) {
+	case 0:	//video 1
+		if (jpeg_mode) {
+			if ((ast_video->config->version == 5) || (ast_video->config->version == 6))
+				ast_video_write(ast_video, ast_video_read(ast_video, AST_VIDEO_SEQ_CTRL) | G5_VIDEO_COMPRESS_JPEG_MODE, AST_VIDEO_SEQ_CTRL);
+			else
+				ast_video_write(ast_video, ast_video_read(ast_video, AST_VIDEO_SEQ_CTRL) | VIDEO_COMPRESS_JPEG_MODE, AST_VIDEO_SEQ_CTRL);
+		} else {
+			if ((ast_video->config->version == 5) || (ast_video->config->version == 6))
+				ast_video_write(ast_video, ast_video_read(ast_video, AST_VIDEO_SEQ_CTRL) & ~G5_VIDEO_COMPRESS_JPEG_MODE, AST_VIDEO_SEQ_CTRL);
+			else
+				ast_video_write(ast_video, ast_video_read(ast_video, AST_VIDEO_SEQ_CTRL) & ~VIDEO_COMPRESS_JPEG_MODE, AST_VIDEO_SEQ_CTRL);
+
+		}
+		break;
+	case 1:	//video M
+		if (jpeg_mode) {
+			if ((ast_video->config->version == 5) || (ast_video->config->version == 6))
+				ast_video_write(ast_video, ast_video_read(ast_video, AST_VM_SEQ_CTRL) | G5_VIDEO_COMPRESS_JPEG_MODE, AST_VM_SEQ_CTRL);
+			else
+				ast_video_write(ast_video, ast_video_read(ast_video, AST_VM_SEQ_CTRL) | VIDEO_COMPRESS_JPEG_MODE, AST_VM_SEQ_CTRL);
+		} else {
+			if ((ast_video->config->version == 5) || (ast_video->config->version == 6))
+				ast_video_write(ast_video, ast_video_read(ast_video, AST_VM_SEQ_CTRL) & ~G5_VIDEO_COMPRESS_JPEG_MODE, AST_VM_SEQ_CTRL);
+			else
+				ast_video_write(ast_video, ast_video_read(ast_video, AST_VM_SEQ_CTRL) & ~VIDEO_COMPRESS_JPEG_MODE, AST_VM_SEQ_CTRL);
+		}
+		break;
+	}
+}
+
+static u8 ast_get_compress_encrypt_en(struct ast_video_data *ast_video, u8 eng_idx)
+{
+	switch (eng_idx) {
+	case 0:
+		if (ast_video_read(ast_video, AST_VIDEO_COMPRESS_CTRL) & VIDEO_ENCRYP_ENABLE)
+			return 1;
+		else
+			return 0;
+		break;
+	case 1:
+		if (ast_video_read(ast_video, AST_VM_COMPRESS_CTRL) & VIDEO_ENCRYP_ENABLE)
+			return 1;
+		else
+			return 0;
+		break;
+	}
+
+	return 0;
+}
+
+static void ast_set_compress_encrypt_en(struct ast_video_data *ast_video, u8 eng_idx, u8 enable)
+{
+	switch (eng_idx) {
+	case 0:	//video 1
+		if (enable)
+			ast_video_write(ast_video, ast_video_read(ast_video, AST_VIDEO_COMPRESS_CTRL) | VIDEO_ENCRYP_ENABLE, AST_VIDEO_COMPRESS_CTRL);
+		else
+			ast_video_write(ast_video, ast_video_read(ast_video, AST_VIDEO_COMPRESS_CTRL) & ~VIDEO_ENCRYP_ENABLE, AST_VIDEO_COMPRESS_CTRL);
+		break;
+	case 1:	//video M
+		if (enable)
+			ast_video_write(ast_video, ast_video_read(ast_video, AST_VM_COMPRESS_CTRL) | VIDEO_ENCRYP_ENABLE, AST_VIDEO_COMPRESS_CTRL);
+		else
+			ast_video_write(ast_video, ast_video_read(ast_video, AST_VM_COMPRESS_CTRL) & ~VIDEO_ENCRYP_ENABLE, AST_VIDEO_COMPRESS_CTRL);
+		break;
+	}
+}
+
+static u8 *ast_get_compress_encrypt_key(struct ast_video_data *ast_video, u8 eng_idx)
+{
+	switch (eng_idx) {
+	case 0:
+		return ast_video->EncodeKeys;
+	case 1:
+		return ast_video->EncodeKeys;
+	}
+	return 0;
+}
+
+static void ast_set_compress_encrypt_key(struct ast_video_data *ast_video, u8 eng_idx, u8 *key)
+{
+	switch (eng_idx) {
+	case 0:	//video 1
+		memset(ast_video->EncodeKeys, 0, 256);
+		//due to system have enter key must be remove
+		memcpy(ast_video->EncodeKeys, key, strlen(key) - 1);
+		ast_video_encryption_key_setup(ast_video);
+		break;
+	case 1:	//video M
+		break;
+	}
+}
+
+static u8 ast_get_compress_encrypt_mode(struct ast_video_data *ast_video)
+{
+	if (ast_video_read(ast_video, AST_VIDEO_CTRL) & VIDEO_CTRL_CRYPTO_AES)
+		return 1;
+	else
+		return 0;
+}
+
+static void ast_set_compress_encrypt_mode(struct ast_video_data *ast_video, u8 mode)
+{
+	if (mode)
+		ast_video_write(ast_video, ast_video_read(ast_video, AST_VIDEO_CTRL) | VIDEO_CTRL_CRYPTO_AES, AST_VIDEO_CTRL);
+	else
+		ast_video_write(ast_video, ast_video_read(ast_video, AST_VIDEO_CTRL) & ~VIDEO_CTRL_CRYPTO_AES, AST_VIDEO_CTRL);
+}
+
+static ssize_t
+ast_store_compress(struct device *dev, struct device_attribute *attr, const char *sysfsbuf, size_t count)
+{
+	int ret;
+	unsigned long input_val;
+	struct ast_video_data *ast_video = dev_get_drvdata(dev);
+	struct sensor_device_attribute_2 *sensor_attr = to_sensor_dev_attr_2(attr);
+
+	ret = kstrtoul(sysfsbuf, 10, &input_val);
+	if (ret)
+		pr_err("%s: input invalid", __func__);
+	//sensor_attr->index : ch#
+	//sensor_attr->nr : attr#
+	switch (sensor_attr->nr) {
+	case 0:	//compress mode
+		ast_set_trigger_mode(ast_video, sensor_attr->index, input_val);
+		break;
+	case 1: //yuv mode
+		ast_set_compress_yuv_mode(ast_video, sensor_attr->index, input_val);
+		break;
+	case 2: //jpeg/aspeed mode
+		ast_set_compress_jpeg_mode(ast_video, sensor_attr->index, input_val);
+		break;
+	case 3: //
+		ast_set_compress_encrypt_en(ast_video, sensor_attr->index, input_val);
+		break;
+	case 4: //
+		ast_set_compress_encrypt_key(ast_video, sensor_attr->index, (u8 *)sysfsbuf);
+		break;
+	case 5: //
+		ast_set_compress_encrypt_mode(ast_video, sensor_attr->index);
+		break;
+
+	default:
+		return -EINVAL;
+	}
+
+	return count;
+}
+
+static ssize_t
+ast_show_compress(struct device *dev, struct device_attribute *attr, char *sysfsbuf)
+{
+	struct ast_video_data *ast_video = dev_get_drvdata(dev);
+	struct sensor_device_attribute_2 *sensor_attr = to_sensor_dev_attr_2(attr);
+
+	//sensor_attr->index : ch#
+	//sensor_attr->nr : attr#
+	switch (sensor_attr->nr) {
+	case 0:
+		return sprintf(sysfsbuf, "%d [0:Single, 1:Frame, 2:Stream]\n", ast_get_trigger_mode(ast_video, sensor_attr->index));
+	case 1:
+		return sprintf(sysfsbuf, "%d:%s\n", ast_get_compress_yuv_mode(ast_video, sensor_attr->index), ast_get_compress_yuv_mode(ast_video, sensor_attr->index) ? "YUV420" : "YUV444");
+	case 2:
+		return sprintf(sysfsbuf, "%d:%s\n", ast_get_compress_jpeg_mode(ast_video, sensor_attr->index), ast_get_compress_jpeg_mode(ast_video, sensor_attr->index) ? "JPEG" : "ASPEED");
+	case 3:
+		return sprintf(sysfsbuf, "%d:%s\n", ast_get_compress_encrypt_en(ast_video, sensor_attr->index), ast_get_compress_encrypt_en(ast_video, sensor_attr->index) ? "Enable" : "Disable");
+	case 4:
+		return sprintf(sysfsbuf, "%s\n", ast_get_compress_encrypt_key(ast_video, sensor_attr->index));
+	case 5:
+		return sprintf(sysfsbuf, "%d:%s\n", ast_get_compress_encrypt_mode(ast_video), ast_get_compress_encrypt_mode(ast_video) ? "AES" : "RC4");
+	default:
+		return -EINVAL;
+	}
+	return -EINVAL;
+}
+
+#define sysfs_compress(index) \
+static SENSOR_DEVICE_ATTR_2(compress##index##_trigger_mode, 0644, \
+	ast_show_compress, ast_store_compress, 0, index); \
+static SENSOR_DEVICE_ATTR_2(compress##index##_yuv, 0644, \
+	ast_show_compress, ast_store_compress, 1, index); \
+static SENSOR_DEVICE_ATTR_2(compress##index##_jpeg, 0644, \
+	ast_show_compress, ast_store_compress, 2, index); \
+static SENSOR_DEVICE_ATTR_2(compress##index##_encrypt_en, 0644, \
+	ast_show_compress, ast_store_compress, 3, index); \
+static SENSOR_DEVICE_ATTR_2(compress##index##_encrypt_key, 0644, \
+	ast_show_compress, ast_store_compress, 4, index); \
+static SENSOR_DEVICE_ATTR_2(compress##index##_encrypt_mode, 0644, \
+	ast_show_compress, ast_store_compress, 5, index); \
+\
+static struct attribute *compress##index##_attributes[] = { \
+	&sensor_dev_attr_compress##index##_trigger_mode.dev_attr.attr, \
+	&sensor_dev_attr_compress##index##_yuv.dev_attr.attr, \
+	&sensor_dev_attr_compress##index##_jpeg.dev_attr.attr, \
+	&sensor_dev_attr_compress##index##_encrypt_en.dev_attr.attr, \
+	&sensor_dev_attr_compress##index##_encrypt_key.dev_attr.attr, \
+	&sensor_dev_attr_compress##index##_encrypt_mode.dev_attr.attr, \
+	NULL \
+}
+
+sysfs_compress(0);
+sysfs_compress(1);
+/************************************************** SYS FS Capture ***********************************************************/
+static const struct attribute_group compress_attribute_groups[] = {
+	{ .attrs = compress0_attributes },
+	{ .attrs = compress1_attributes },
+};
+
+/************************************************** SYS FS End ***********************************************************/
+static const struct aspeed_video_config ast2600_config = {
+	.version = 6,
+	.dram_base = 0x80000000,
+};
+
+static const struct aspeed_video_config ast2500_config = {
+	.version = 5,
+	.dram_base = 0x80000000,
+};
+
+static const struct aspeed_video_config ast2400_config = {
+	.version = 4,
+	.dram_base = 0x40000000,
+};
+
+static const struct of_device_id aspeed_video_matches[] = {
+	{ .compatible = "aspeed,ast2400-video",	.data = &ast2400_config, },
+	{ .compatible = "aspeed,ast2500-video",	.data = &ast2500_config, },
+	{ .compatible = "aspeed,ast2600-video",	.data = &ast2600_config, },
+	{},
+};
+
+MODULE_DEVICE_TABLE(of, aspeed_video_matches);
+
+#define CONFIG_AST_VIDEO_MEM_SIZE	0x2800000
+
+static int ast_video_probe(struct platform_device *pdev)
+{
+	struct resource *res0;
+	int ret = 0;
+	int i;
+	struct ast_video_data *ast_video;
+	const struct of_device_id *video_dev_id;
+
+	ast_video = devm_kzalloc(&pdev->dev, sizeof(struct ast_video_data), GFP_KERNEL);
+	if (!ast_video)
+		return -ENOMEM;
+
+	video_dev_id = of_match_node(aspeed_video_matches, pdev->dev.of_node);
+	if (!video_dev_id)
+		return -EINVAL;
+
+	ast_video->config = (struct aspeed_video_config *) video_dev_id->data;
+
+	if (ast_video->config->version == 6) {
+		ast_video->gfx = syscon_regmap_lookup_by_compatible("aspeed,ast2600-gfx");
+		if (IS_ERR(ast_video->gfx))
+			dev_err(&pdev->dev, " 2600 GFX regmap not enable\n");
+
+		ast_video->scu = syscon_regmap_lookup_by_compatible("aspeed,ast2600-scu");
+		if (IS_ERR(ast_video->scu)) {
+			dev_err(&pdev->dev, "failed to find 2600 SCU regmap\n");
+			return PTR_ERR(ast_video->scu);
+		}
+	} else if (ast_video->config->version == 5) {
+		ast_video->gfx = syscon_regmap_lookup_by_compatible("aspeed,ast2500-gfx");
+		if (IS_ERR(ast_video->gfx)) {
+			dev_err(&pdev->dev, "failed to find 2500 GFX regmap\n");
+			return PTR_ERR(ast_video->gfx);
+		}
+		ast_video->scu = syscon_regmap_lookup_by_compatible("aspeed,ast2500-scu");
+		if (IS_ERR(ast_video->scu)) {
+			dev_err(&pdev->dev, "failed to find 2500 SCU regmap\n");
+			return PTR_ERR(ast_video->scu);
+		}
+	} else {
+		ast_video->gfx = syscon_regmap_lookup_by_compatible("aspeed,ast-g4-gfx");
+		if (IS_ERR(ast_video->gfx)) {
+			dev_err(&pdev->dev, "failed to find 2400 GFX regmap\n");
+			return PTR_ERR(ast_video->gfx);
+		}
+		ast_video->scu = syscon_regmap_lookup_by_compatible("aspeed,ast2400-scu");
+		if (IS_ERR(ast_video->scu)) {
+			dev_err(&pdev->dev, "failed to find 2400 SCU regmap\n");
+			return PTR_ERR(ast_video->scu);
+		}
+	}
+
+	res0 = platform_get_resource(pdev, IORESOURCE_MEM, 0);
+	if (res0 == NULL) {
+		dev_err(&pdev->dev, "cannot get IORESOURCE_MEM\n");
+		ret = -ENOENT;
+		goto out;
+	}
+	ast_video->reg_base = devm_ioremap_resource(&pdev->dev, res0);
+	if (!ast_video->reg_base) {
+		ret = -EIO;
+		goto out;
+	}
+
+	//Phy assign
+	ast_video->video_mem_size = CONFIG_AST_VIDEO_MEM_SIZE;
+	VIDEO_DBG("video_mem_size %d MB\n", ast_video->video_mem_size / 1024 / 1024);
+
+	of_reserved_mem_device_init(&pdev->dev);
+
+	ret = dma_set_mask_and_coherent(&pdev->dev, DMA_BIT_MASK(32));
+	if (ret) {
+		dev_err(&pdev->dev, "Failed to set DMA mask\n");
+		of_reserved_mem_device_release(&pdev->dev);
+		goto out;
+	}
+
+	ast_video->video_mem.virt = dma_alloc_coherent(&pdev->dev, CONFIG_AST_VIDEO_MEM_SIZE, &ast_video->video_mem.dma, GFP_KERNEL);
+	if (!ast_video->video_mem.virt)
+		return -ENOMEM;
+
+	ast_video->stream_phy = ast_video->video_mem.dma;
+	ast_video->buff0_phy = (ast_video->video_mem.dma + 0x400000);   //4M : size 10MB
+	ast_video->buff1_phy = (ast_video->video_mem.dma + 0xe00000);   //14M : size 10MB
+	ast_video->bcd_phy = (ast_video->video_mem.dma + 0x1800000);    //24M : size 1MB
+	ast_video->jpeg_buf0_phy = (ast_video->video_mem.dma + 0x1900000);   //25MB: size 10 MB
+	ast_video->video_jpeg_offset = 0x2300000;						//TODO define
+	ast_video->jpeg_phy = (ast_video->video_mem.dma + 0x2300000);   //35MB: size 4 MB
+	ast_video->jpeg_tbl_phy = (ast_video->video_mem.dma + 0x2700000);       //39MB: size 1 MB
+
+	VIDEO_DBG("\nstream_phy: %x, buff0_phy: %x, buff1_phy:%x, bcd_phy:%x\njpeg_phy:%x, jpeg_tbl_phy:%x\n",
+			  (u32)ast_video->stream_phy, (u32)ast_video->buff0_phy, (u32)ast_video->buff1_phy, (u32)ast_video->bcd_phy, (u32)ast_video->jpeg_phy, (u32)ast_video->jpeg_tbl_phy);
+
+	//virt assign
+	ast_video->stream_virt = ast_video->video_mem.virt;
+	ast_video->buff0_virt = ast_video->stream_virt + 0x400000; //4M : size 10MB
+	ast_video->buff1_virt = ast_video->stream_virt + 0xe00000; //14M : size 10MB
+	ast_video->bcd_virt = ast_video->stream_virt + 0x1800000;  //24M : size 4MB
+	ast_video->jpeg_buf0_virt = ast_video->stream_virt + 0x1900000;  //25MB: size x MB
+	ast_video->jpeg_virt = ast_video->stream_virt + 0x2300000; //35MB: size 4 MB
+	ast_video->jpeg_tbl_virt = ast_video->stream_virt + 0x2700000;     //39MB: size 1 MB
+
+	VIDEO_DBG("\nstream_virt: %x, buff0_virt: %x, buff1_virt:%x, bcd_virt:%x\njpeg_virt:%x, jpeg_tbl_virt:%x\n",
+			  (u32)ast_video->stream_virt, (u32)ast_video->buff0_virt, (u32)ast_video->buff1_virt, (u32)ast_video->bcd_virt, (u32)ast_video->jpeg_virt, (u32)ast_video->jpeg_tbl_virt);
+
+	memset(ast_video->stream_virt, 0, ast_video->video_mem_size);
+
+	ast_video->irq = platform_get_irq(pdev, 0);
+	if (ast_video->irq < 0) {
+		dev_err(&pdev->dev, "no irq specified\n");
+		ret = -ENOENT;
+		goto out_region1;
+	}
+
+	ast_video->reset = devm_reset_control_get(&pdev->dev, NULL);
+	if (IS_ERR(ast_video->reset)) {
+		dev_err(&pdev->dev, "can't get video reset\n");
+		return PTR_ERR(ast_video->reset);
+	}
+
+	ast_video->eclk = devm_clk_get(&pdev->dev, "eclk");
+	if (IS_ERR(ast_video->eclk)) {
+		dev_err(&pdev->dev, "no eclk clock defined\n");
+		return PTR_ERR(ast_video->eclk);
+	}
+
+	clk_prepare_enable(ast_video->eclk);
+
+	ast_video->vclk = devm_clk_get(&pdev->dev, "vclk");
+	if (IS_ERR(ast_video->vclk)) {
+		dev_err(&pdev->dev, "no vclk clock defined\n");
+		return PTR_ERR(ast_video->vclk);
+	}
+
+	clk_prepare_enable(ast_video->vclk);
+
+//	ast_scu_init_video(0);
+
+	// default config
+	ast_video->input_source = VIDEO_SOURCE_INT_VGA;
+	ast_video->rc4_enable = 0;
+	ast_video->multi_jpeg = 0;
+	strcpy(ast_video->EncodeKeys, "fedcba9876543210");
+	ast_video->scaling = 0;
+
+	ret = misc_register(&ast_video_misc);
+	if (ret) {
+		pr_err("VIDEO : failed to request interrupt\n");
+		goto out_region1;
+	}
+
+	ret = sysfs_create_group(&pdev->dev.kobj, &video_attribute_group);
+	if (ret)
+		goto out_misc;
+
+
+	for (i = 0; i < 2; i++) {
+		ret = sysfs_create_group(&pdev->dev.kobj, &compress_attribute_groups[i]);
+		if (ret)
+			goto out_create_groups;
+	}
+
+	platform_set_drvdata(pdev, ast_video);
+	dev_set_drvdata(ast_video_misc.this_device, ast_video);
+
+	ast_video_ctrl_init(ast_video);
+
+
+	ret = devm_request_irq(&pdev->dev, ast_video->irq, ast_video_isr,
+						   0, dev_name(&pdev->dev), ast_video);
+
+	if (ret) {
+		pr_info("VIDEO: Failed request irq %d\n", ast_video->irq);
+		goto out_create_groups;
+	}
+
+	pr_info("ast_video: driver successfully loaded.\n");
+
+	return 0;
+
+out_create_groups:
+	sysfs_remove_group(&pdev->dev.kobj, &compress_attribute_groups[0]);
+	sysfs_remove_group(&pdev->dev.kobj, &video_attribute_group);
+
+out_misc:
+	misc_deregister(&ast_video_misc);
+
+out_region1:
+	if (ast_video->stream_virt)
+		dma_free_coherent(&pdev->dev, CONFIG_AST_VIDEO_MEM_SIZE, ast_video->stream_virt, ast_video->video_mem.dma);
+out:
+	pr_warn("applesmc: driver init failed (ret=%d)!\n", ret);
+	return ret;
+
+}
+
+static int ast_video_remove(struct platform_device *pdev)
+{
+	struct ast_video_data *ast_video = platform_get_drvdata(pdev);
+	int i;
+
+	VIDEO_DBG("%s\n", __func__);
+
+	misc_deregister(&ast_video_misc);
+	sysfs_remove_group(&pdev->dev.kobj, &video_attribute_group);
+	for (i = 0; i < 2; i++)
+		sysfs_remove_group(&pdev->dev.kobj, &compress_attribute_groups[i]);
+
+	if (ast_video->stream_virt)
+		dma_free_coherent(&pdev->dev, CONFIG_AST_VIDEO_MEM_SIZE, ast_video->stream_virt, ast_video->video_mem.dma);
+
+	return 0;
+}
+
+#ifdef CONFIG_PM
+static int
+ast_video_suspend(struct platform_device *pdev, pm_message_t state)
+{
+	pr_info("%s: TODO\n", __func__);
+	return 0;
+}
+
+static int
+ast_video_resume(struct platform_device *pdev)
+{
+	return 0;
+}
+
+#else
+#define ast_video_suspend        NULL
+#define ast_video_resume         NULL
+#endif
+
+static struct platform_driver ast_video_driver = {
+	.probe		= ast_video_probe,
+	.remove		= ast_video_remove,
+#ifdef CONFIG_PM
+	.suspend        = ast_video_suspend,
+	.resume         = ast_video_resume,
+#endif
+	.driver		= {
+		.name   = KBUILD_MODNAME,
+		.of_match_table = aspeed_video_matches,
+	},
+};
+
+module_platform_driver(ast_video_driver);
+
+MODULE_AUTHOR("Ryan Chen <ryan_chen@aspeedtech.com>");
+MODULE_DESCRIPTION("AST Video Engine driver");
+MODULE_LICENSE("GPL");
diff --git a/drivers/soc/aspeed/rvas/Kconfig b/drivers/soc/aspeed/rvas/Kconfig
new file mode 100644
index 000000000000..ef02e1b769d7
--- /dev/null
+++ b/drivers/soc/aspeed/rvas/Kconfig
@@ -0,0 +1,9 @@
+menu "ASPEED RVAS drivers"
+
+config ASPEED_RVAS
+	tristate "ASPEED RVAS driver"
+	default n
+	help
+	  Driver for ASPEED RVAS Engine
+
+endmenu
diff --git a/drivers/soc/aspeed/rvas/Makefile b/drivers/soc/aspeed/rvas/Makefile
new file mode 100644
index 000000000000..1cccd7e37f68
--- /dev/null
+++ b/drivers/soc/aspeed/rvas/Makefile
@@ -0,0 +1,3 @@
+obj-$(CONFIG_ASPEED_RVAS) += rvas.o
+rvas-y := video_main.o hardware_engines.o video_engine.o
+
diff --git a/drivers/soc/aspeed/rvas/hardware_engines.c b/drivers/soc/aspeed/rvas/hardware_engines.c
new file mode 100644
index 000000000000..da8d5b7fc21c
--- /dev/null
+++ b/drivers/soc/aspeed/rvas/hardware_engines.c
@@ -0,0 +1,2225 @@
+// SPDX-License-Identifier: GPL-2.0+
+/*
+ * File Name     : hardware_engines.c
+ * Description   : AST2600 frame grabber hardware engines
+ *
+ * Copyright (C) 2019-2021 ASPEED Technology Inc. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ */
+
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/dma-mapping.h>
+#include <linux/mm.h>
+#include <asm/cacheflush.h>
+#include <stdbool.h>
+#include <linux/time.h>
+
+#include "hardware_engines.h"
+#include "video.h"
+#include "video_debug.h"
+
+static u32 dwBucketSizeRegOffset[BSE_MAX_BUCKET_SIZE_REGS] = { 0x20, 0x24, 0x28,
+	0x2c, 0x30, 0x34, 0x38, 0x3c, 0x40, 0x44, 0x48, 0x4c, 0x50, 0x54, 0x58,
+	0x5c };
+static u32 arrBuckSizeRegIndex[16] = { 3, 5, 8, 6, 1, 7, 11, 10, 14, 13, 2, 4,
+	9, 12, 0, 15 };
+
+static struct Resolution resTable1[0x3B - 0x30 + 1] = { { 800, 600 }, { 1024, 768 }, {
+	1280, 1024 }, { 1600, 1200 }, { 1920, 1200 }, { 1280, 800 },
+	{ 1440, 900 }, { 1680, 1050 }, { 1920, 1080 }, { 1366, 768 }, { 1600,
+		900 }, { 1152, 864 }, };
+
+static struct Resolution resTable2[0x52 - 0x50 + 1] = { { 320, 240 }, { 400, 300 }, {
+	512, 384 }, };
+
+static void prepare_bse_descriptor_2(struct Descriptor *pDAddress, u32 dwSourceAddress,
+	u32 dwDestAddress, bool bNotLastEntry, u16 wStride, u8 bytesPerPixel,
+	u32 dwFetchWidthPixels, u32 dwFetchHeight,
+	bool bInterrupt, u8 byBuckSizeRegIndex);
+
+static struct BSEAggregateRegister set_up_bse_bucket_2(struct AstRVAS *pAstRVAS,
+	u8 *abyBitIndexes, u8 byTotalBucketCount, u8 byBSBytesPerPixel,
+	u32 dwFetchWidthPixels, u32 dwFetchHeight, u32 dwBucketSizeIndex);
+
+
+static inline u32 ast_video_read(u32 video_reg_base, u32 reg)
+{
+	u32 val = readl((void *)(video_reg_base + reg));
+
+	return val;
+}
+
+// Get color depth
+static void ast_video_get_color_mode(u8 byNewColorMode, struct VideoGeometry *pvg)
+{
+	switch (byNewColorMode) {
+	case MODE_EGA:
+		pvg->gmt = VGAGraphicsMode; //4pp mode12/mode6A
+		pvg->byBitsPerPixel = 4;
+		break;
+
+	case MODE_VGA:
+		pvg->gmt = VGAGraphicsMode; //mode 13
+		pvg->byBitsPerPixel = 8;
+		break;
+
+	case MODE_BPP16:
+		pvg->gmt = AGAGraphicsMode;
+		pvg->byBitsPerPixel = 16;
+		break;
+
+	case MODE_BPP32:
+		pvg->gmt = AGAGraphicsMode;
+		pvg->byBitsPerPixel = 32;
+		break;
+
+	case MODE_TEXT:
+		pvg->gmt = TextMode;
+		pvg->byBitsPerPixel = 0;
+		break;
+
+	case MODE_CGA:
+		break;
+
+	default:
+		pvg->byBitsPerPixel = 8;
+		break;
+	}
+}
+
+//Mode ID mapping - use ID as index to the resolution table
+static void ast_video_get_indexed_mode(struct ModeInfo *pModeInfo, struct VideoGeometry *pvg)
+{
+	u8 byModeIndex = (pModeInfo->byModeID & 0xf0);
+
+	VIDEO_DBG("Mode ID %#x\n", pModeInfo->byModeID);
+	pvg->byModeID = pModeInfo->byModeID;
+
+	if (pModeInfo->byModeID == 0x12) {
+		pvg->wScreenWidth = 640;
+		pvg->wScreenHeight = 480;
+	} else if (byModeIndex == 0x20) {
+		pvg->wScreenWidth = 640;
+		pvg->wScreenHeight = 480;
+	} else if (byModeIndex == 0x30) {
+		pvg->wScreenWidth =
+			resTable1[pModeInfo->byModeID & 0x0f].wWidth;
+		pvg->wScreenHeight =
+			resTable1[pModeInfo->byModeID & 0x0f].wHeight;
+	} else if (byModeIndex == 0x50) {
+		pvg->wScreenWidth =
+			resTable2[pModeInfo->byModeID & 0x03].wWidth;
+		pvg->wScreenHeight =
+			resTable2[pModeInfo->byModeID & 0x03].wHeight;
+	} else if (byModeIndex == 0x60) {
+		pvg->wScreenWidth = 800;
+		pvg->wScreenHeight = 600;
+	} else {
+		VIDEO_DBG("Mode ID %#x\n", pModeInfo->byModeID);
+		pvg->wScreenWidth = 0;
+		pvg->wScreenHeight = 0;
+	}
+}
+
+//check special modes
+static void ast_video_set_special_modes(struct ModeInfo *pModeInfo, struct AstRVAS *pAstRVAS)
+{
+	u8 byVGACR1 = readb((void *)(pAstRVAS->grce_reg_base + GRCE_CRTC_OFFSET + 0x1)); //number of chars per line
+	u8 byVGACR7 = readb((void *)(pAstRVAS->grce_reg_base + GRCE_CRTC_OFFSET + 0x7));
+	u8 byVGACR12 = readb((void *)(pAstRVAS->grce_reg_base + GRCE_CRTC_OFFSET + 0x12));
+	u8 byVGASR1 = readb((void *)(pAstRVAS->grce_reg_base + GRCE_SEQ_OFFSET + 0x1));
+	struct VideoGeometry *pvg = &pAstRVAS->current_vg;
+	u32 dwHorizontalDisplayEnd = 0;
+	u32 dwVerticalDisplayEnd = 0;
+
+	dwHorizontalDisplayEnd = (byVGACR1 + 1) << 3;
+	dwVerticalDisplayEnd = (((byVGACR7 & 0x40) << 3)
+		| ((byVGACR7 & 0x2) << 7) | byVGACR12) + 1;
+
+	VIDEO_DBG("byVGACR1=0x%x,byVGACR7=0x%x,byVGACR12=0x%x\n", byVGACR1,
+		byVGACR7, byVGACR12);
+	VIDEO_DBG(
+		"Mode ID %#x, dwHorizontalDisplayEnd 0x%x, dwVerticalDisplayEnd 0x%x\n",
+		pModeInfo->byModeID, dwHorizontalDisplayEnd,
+		dwVerticalDisplayEnd);
+
+	// set up special mode
+	if (VGAGraphicsMode == pvg->gmt && (pvg->byBitsPerPixel == 8)) { // mode 13
+		pvg->wScreenHeight = 200;
+		pvg->wScreenWidth = 320;
+		pvg->wStride = 320;
+	} else if (TextMode == pvg->gmt) { // text mode
+		pvg->wScreenHeight = dwVerticalDisplayEnd;
+		pvg->wScreenWidth = dwHorizontalDisplayEnd;
+
+		if (!(byVGASR1 & 0x1))
+			pvg->wScreenWidth += (byVGACR1 + 1);
+
+		pvg->wStride = pvg->wScreenWidth;
+	} else if (pvg->byBitsPerPixel == 4)
+		pvg->wStride = pvg->wScreenWidth;
+}
+
+static u32 ast_video_get_pitch(struct AstRVAS *pAstRVAS)
+{
+	u32 dwPitch = 0;
+	u8 byVGACR13 = 0;
+	u8 byVGACR14 = 0;
+	u8 byVGACR17 = 0;
+	u16 wOffsetUpper = 0;
+	u16 wOffset = 0;
+	struct VideoGeometry *pvg = &pAstRVAS->current_vg;
+
+	//read actual register
+	byVGACR13 = readb((void *)(pAstRVAS->grce_reg_base + GRCE_CRTC_OFFSET + 0x13));
+	byVGACR14 = readb((void *)(pAstRVAS->grce_reg_base + GRCE_CRTC_OFFSET + 0x14));
+	byVGACR17 = readb((void *)(pAstRVAS->grce_reg_base + GRCE_CRTC_OFFSET + 0x17));
+	wOffsetUpper = readb((void *)(pAstRVAS->grce_reg_base + 0xb0));
+
+	wOffset = (wOffsetUpper << 8) | byVGACR13;
+	VIDEO_DBG(
+		"wOffsetUpper= %#x, byVGACR13= %#x, byVGACR14= %#x, byVGACR17= %#x, wOffset= %#x\n",
+		wOffsetUpper, byVGACR13, byVGACR14, byVGACR17, wOffset);
+
+	if (byVGACR14 & 0x40)
+		dwPitch = wOffset << 3; //DW mode
+	else if (byVGACR17 & 0x40)
+		dwPitch = wOffset << 1; //byte mode
+	else
+		dwPitch = wOffset << 2; //word mode
+
+	if (pvg->gmt != TextMode) {
+		u8 byBppPowerOfTwo = 0;
+
+		if (pvg->byBitsPerPixel == 32)
+			byBppPowerOfTwo = 2;
+		else if (pvg->byBitsPerPixel == 16)
+			byBppPowerOfTwo = 1;
+		else if (pvg->byBitsPerPixel == 8)
+			byBppPowerOfTwo = 0;
+		else
+			byBppPowerOfTwo = 3;	// 4bpp
+
+		//convert it to logic width in pixel
+		if (pvg->byBitsPerPixel > 4)
+			dwPitch >>= byBppPowerOfTwo;
+		else
+			dwPitch <<= byBppPowerOfTwo;
+	}
+
+	return dwPitch;
+}
+
+void update_video_geometry(struct AstRVAS *ast_rvas)
+{
+	struct ModeInfo *pModeInfo;
+	struct NewModeInfoHeader *pNMIH;
+	struct DisplayEnd *pDE;
+	u8 byNewColorMode = 0;
+	u32 VGA_Scratch_Register_350 = 0; //VIDEO_NEW_MODE_INFO_HEADER
+	u32 VGA_Scratch_Register_354 = 0; //VIDEO_HDE
+	u32 VGA_Scratch_Register_34C = 0; //VIDEO_HDE
+	struct VideoGeometry *cur_vg = &ast_rvas->current_vg;
+
+
+	VGA_Scratch_Register_350 = ast_video_read(ast_rvas->grce_reg_base,
+			AST_VIDEO_SCRATCH_350);
+	VGA_Scratch_Register_34C = ast_video_read(ast_rvas->grce_reg_base,
+			AST_VIDEO_SCRATCH_34C);
+	VGA_Scratch_Register_354 = ast_video_read(ast_rvas->grce_reg_base,
+			AST_VIDEO_SCRATCH_354);
+
+	pModeInfo = (struct ModeInfo *) (&VGA_Scratch_Register_34C);
+	pNMIH = (struct NewModeInfoHeader *) (&VGA_Scratch_Register_350);
+	pDE = (struct DisplayEnd *) (&VGA_Scratch_Register_354);
+	VIDEO_DBG(
+			"pModeInfo: byColorMode: %#x byModeID: %#x byRefreshRateIndex: %#x byScanLines: %#x\n",
+			pModeInfo->byColorMode, pModeInfo->byModeID,
+			pModeInfo->byRefreshRateIndex, pModeInfo->byScanLines);
+	VIDEO_DBG(
+			"pNMIH: byColorDepth: %#x byDisplayInfo: %#x byMhzPixelClock: %#x byReserved: %#x\n",
+			pNMIH->byColorDepth, pNMIH->byDisplayInfo,
+			pNMIH->byMhzPixelClock, pNMIH->byReserved);
+	VIDEO_DBG("pDE: HDE: %#x VDE: %#x\n", pDE->HDE, pDE->VDE);
+
+	byNewColorMode = ((pModeInfo->byColorMode) & 0xf0) >> 4;
+	VIDEO_DBG("byNewColorMode= %#x,byModeID=0x%x\n", byNewColorMode,
+			pModeInfo->byModeID);
+	ast_video_get_color_mode(byNewColorMode, cur_vg);
+
+	if (pNMIH->byDisplayInfo == MODE_GET_INFO_DE) {
+		cur_vg->wScreenWidth = pDE->HDE;
+		cur_vg->wScreenHeight = pDE->VDE;
+		cur_vg->byBitsPerPixel = pNMIH->byColorDepth;
+		cur_vg->byModeID = pModeInfo->byModeID;
+	} else
+		ast_video_get_indexed_mode(pModeInfo, cur_vg);
+
+	cur_vg->wStride = (u16) ast_video_get_pitch(ast_rvas);
+	VIDEO_DBG("Calculated pitch in pixels= %u\n", cur_vg->wStride);
+
+	if (cur_vg->wStride < cur_vg->wScreenWidth)
+		cur_vg->wStride = cur_vg->wScreenWidth;
+
+	VIDEO_DBG(
+			"Before current display width %u, height %u, pitch %u, color depth %u, mode %d\n",
+			cur_vg->wScreenWidth, cur_vg->wScreenHeight,
+			cur_vg->wStride, cur_vg->byBitsPerPixel, cur_vg->gmt);
+
+	if ((cur_vg->gmt == TextMode)
+			|| ((cur_vg->gmt == VGAGraphicsMode)
+					&& (pModeInfo->byModeID == 0x13))) {
+		ast_video_set_special_modes(pModeInfo, ast_rvas);
+	}
+
+	//mode transition
+	if (cur_vg->wScreenHeight < 200 || cur_vg->wScreenWidth < 320)
+		cur_vg->gmt = InvalidMode;
+
+	if (cur_vg->gmt == TextMode) {
+		u8 byVGACR9 = readb((void *)(ast_rvas->grce_reg_base + GRCE_CRTC_OFFSET + 0x9));
+		u32 dwCharacterHeight = ((byVGACR9) & 0x1f) + 1;
+
+		VIDEO_DBG("byModeID=0x%x,dwCharacterHeight=%d\n",
+				cur_vg->byModeID, dwCharacterHeight);
+
+		if ((dwCharacterHeight != 8) && (dwCharacterHeight != 14)
+				&& (dwCharacterHeight != 16))
+			cur_vg->gmt = InvalidMode;
+
+		if ((cur_vg->wScreenWidth > 720)
+				|| cur_vg->wScreenHeight > 400)
+			cur_vg->gmt = InvalidMode;
+	}
+
+	VIDEO_DBG(
+			"current display width %u, height %u, pitch %u, color depth %u, mode %d\n",
+			cur_vg->wScreenWidth, cur_vg->wScreenHeight,
+			cur_vg->wStride, cur_vg->byBitsPerPixel, cur_vg->gmt);
+
+}
+//
+//check and update current video geometry
+//
+bool video_geometry_change(struct AstRVAS *ast_rvas, u32 dwGRCEStatus)
+{
+	bool b_geometry_changed = false;
+	struct VideoGeometry *cur_vg = &ast_rvas->current_vg;
+	struct VideoGeometry pre_vg;
+
+	memcpy(&pre_vg, cur_vg, sizeof(pre_vg));
+	update_video_geometry(ast_rvas);
+	b_geometry_changed = memcmp(&pre_vg, cur_vg, sizeof(struct VideoGeometry))
+			!= 0;
+	VIDEO_DBG("b_geometry_changed: %d\n", b_geometry_changed);
+	return b_geometry_changed;
+}
+
+void ioctl_get_video_geometry(struct RvasIoctl *ri, struct AstRVAS *ast_rvas)
+{
+	memcpy(&ri->vg, &ast_rvas->current_vg, sizeof(struct VideoGeometry));
+//	VIDEO_DBG("b_geometry_changed: %d\n", b_geometry_changed);
+}
+
+void print_frame_buffer(u32 dwSizeByBytes, struct VGAMemInfo FBInfo)
+{
+	u32 iter = 0;
+	u32 *pdwFrameBufferAddrBase = NULL;
+	u32 dwNumMappedPages = 0;
+
+	dwNumMappedPages = ((dwSizeByBytes + 4095) >> 12);
+	pdwFrameBufferAddrBase = (u32 *) ioremap(FBInfo.dwFBPhysStart, dwNumMappedPages << 12);
+
+	if (pdwFrameBufferAddrBase) {
+		VIDEO_DBG("==============%s===========\n", __func__);
+
+		for (iter = 0; iter < (dwSizeByBytes >> 2); iter++) {
+			VIDEO_DBG("0x%x, ", pdwFrameBufferAddrBase[iter]);
+
+			if ((iter % 16) == 0)
+				VIDEO_DBG("\n");
+		}
+
+		VIDEO_DBG("===========END=============\n");
+		iounmap((void *) pdwFrameBufferAddrBase);
+	}
+}
+
+void ioctl_get_grc_register(struct RvasIoctl *ri, struct AstRVAS *pAstRVAS)
+{
+	u32 virt_add = 0;
+	u32 size = 0;
+
+	VIDEO_DBG("Start\n");
+	virt_add = (u32)get_virt_add_rsvd_mem((u32) ri->rmh, pAstRVAS);
+	size = ri->rmh1_mem_size;
+
+	if (virt_is_valid_rsvd_mem((u32) ri->rmh, size, pAstRVAS)) {
+		memcpy((void *) virt_add,
+			(const void *) (pAstRVAS->grce_reg_base), 0x40);
+		memset((void *) (((u8 *) virt_add) + 0x40), 0x0, 0x20);
+		memcpy((void *) (((u8 *) virt_add) + 0x60),
+			(const void *) (pAstRVAS->grce_reg_base + 0x60),
+			GRCE_SIZE - 0x60);
+		ri->rs = SuccessStatus;
+	} else
+		ri->rs = InvalidMemoryHandle;
+}
+
+void ioctl_read_snoop_map(struct RvasIoctl *ri, struct AstRVAS *pAstRVAS)
+{
+
+	struct ContextTable *pct = get_context_entry(ri->rc, pAstRVAS);
+	u32 virt_add = 0;
+	u32 size = 0;
+
+	virt_add = (u32)get_virt_add_rsvd_mem((u32) ri->rmh, pAstRVAS);
+	size = ri->rmh_mem_size;
+
+	disable_grce_tse_interrupt(pAstRVAS);
+	VIDEO_DBG("Start\n");
+
+	if (pct) {
+		if (virt_is_valid_rsvd_mem((u32) ri->rmh, size, pAstRVAS)) {
+			update_all_snoop_context(pAstRVAS);
+			memcpy((void *) virt_add, pct->aqwSnoopMap,
+				sizeof(pct->aqwSnoopMap));
+
+			if (ri->flag) {
+				///get the context snoop address
+				memset(pct->aqwSnoopMap, 0x00,
+					sizeof(pct->aqwSnoopMap));
+				memset(&(pct->sa), 0x00, sizeof(pct->sa));
+			}
+			ri->rs = SuccessStatus;
+		} else
+			ri->rs = InvalidMemoryHandle;
+	} else
+		ri->rs = InvalidContextHandle;
+
+	enable_grce_tse_interrupt(pAstRVAS);
+}
+
+void ioctl_read_snoop_aggregate(struct RvasIoctl *ri, struct AstRVAS *pAstRVAS)
+{
+	struct ContextTable *pct = get_context_entry(ri->rc, pAstRVAS);
+
+	disable_grce_tse_interrupt(pAstRVAS);
+
+	if (pct) {
+		update_all_snoop_context(pAstRVAS);
+		memcpy(&(ri->sa), &(pct->sa), sizeof(pct->sa));
+		VIDEO_DBG("ri->sa.qwCol: %#llx qwRow: %#llx flag: %u\n",
+			ri->sa.qwCol, ri->sa.qwRow, ri->flag);
+
+		if (ri->flag)
+			memset(&(pct->sa), 0x00, sizeof(pct->sa));
+
+		ri->rs = SuccessStatus;
+	} else {
+		ri->rs = InvalidContextHandle;
+		VIDEO_DBG("Invalid Context\n");
+	}
+
+	enable_grce_tse_interrupt(pAstRVAS);
+}
+
+void ioctl_set_tse_tsicr(struct RvasIoctl *ri, struct AstRVAS *pAstRVAS)
+{
+	u32 addrTSICR;
+
+	pAstRVAS->tse_tsicr = ri->tse_counter;
+	addrTSICR = pAstRVAS->fg_reg_base + TSE_TileSnoop_Interrupt_Count;
+	writel(pAstRVAS->tse_tsicr, (void *)addrTSICR);// max wait time before interrupt
+	ri->rs = SuccessStatus;
+}
+
+
+void ioctl_get_tse_tsicr(struct RvasIoctl *ri, struct AstRVAS *pAstRVAS)
+{
+	ri->tse_counter = pAstRVAS->tse_tsicr;
+	ri->rs = SuccessStatus;
+}
+
+// Get the screen offset from the GRC registers
+u32 get_screen_offset(struct AstRVAS *pAstRVAS)
+{
+	u32 dwScreenOffset = 0;
+	u32 addrVGACRC = pAstRVAS->grce_reg_base + GRCE_CRTC + 0xC; // Ch
+	u32 addrVGACRD = pAstRVAS->grce_reg_base + GRCE_CRTC + 0xD; // Dh
+	u32 addrVGACRAF = pAstRVAS->grce_reg_base + GRCE_CRTCEXT + 0x2F;
+
+	if (pAstRVAS->current_vg.gmt == AGAGraphicsMode) {
+		dwScreenOffset = ((readb((void *)addrVGACRAF)) << 16) | ((readb((void *)addrVGACRC)) << 8) |
+				(readb((void *)addrVGACRD));
+		dwScreenOffset *= pAstRVAS->current_vg.byBitsPerPixel >> 3;
+	}
+
+	VIDEO_DBG("ScreenOffset: %#8.8x\n", dwScreenOffset);
+
+	return dwScreenOffset;
+}
+
+void reset_snoop_engine(struct AstRVAS *pAstRVAS)
+{
+	u32 addr_snoop = pAstRVAS->fg_reg_base + TSE_SnoopMap_Offset;
+	u32 reg_value = 0;
+	u32 iter;
+
+	writel(0x0, (void *)(pAstRVAS->fg_reg_base + TSE_SnoopCommand_Register_Offset));
+	writel(0x3, (void *)(pAstRVAS->fg_reg_base + TSE_Status_Register_Offset));
+	reg_value = readl((void *)(pAstRVAS->fg_reg_base + TSE_Status_Register_Offset));
+	reg_value = readl((void *)(pAstRVAS->fg_reg_base + TSE_CS0Reg));
+	reg_value = readl((void *)(pAstRVAS->fg_reg_base + TSE_CS1Reg));
+	reg_value = readl((void *)(pAstRVAS->fg_reg_base + TSE_RS0Reg));
+	reg_value = readl((void *)(pAstRVAS->fg_reg_base + TSE_RS1Reg));
+
+	//Clear TSRR00 to TSRR126 (TSRR01 to TSRR127), Snoop Map
+	for (iter = 0; iter < 0x80; ++iter) {
+		reg_value = readl((void *)addr_snoop)+1;
+		writel(reg_value, (void *)addr_snoop);
+	}
+
+	reg_value = readl((void *)(pAstRVAS->fg_reg_base + TSE_TileCount_Register_Offset));
+}
+
+void set_snoop_engine(bool b_geom_chg, struct AstRVAS *pAstRVAS)
+{
+	u32 tscmd_reg = pAstRVAS->fg_reg_base + TSE_SnoopCommand_Register_Offset;
+	u32 tsfbsa_reg = pAstRVAS->fg_reg_base + TSE_FrameBuffer_Offset;
+	u32 tsulr_reg = pAstRVAS->fg_reg_base + TSE_UpperLimit_Offset;
+	u32 new_tsfbsa = 0;
+	u32 tscmd = 0;
+	u8 byBytesPerPixel = 0x0;
+	u8 byTSCMDBytesPerPixel = 0x0;
+	int cContext;
+	u32 dwStride;
+	struct ContextTable **ppctContextTable = pAstRVAS->ppctContextTable;
+
+	// Calculate Start Address into the Frame Buffer
+	new_tsfbsa = get_screen_offset(pAstRVAS);
+	tscmd = readl((void *)(pAstRVAS->fg_reg_base + TSE_SnoopCommand_Register_Offset));
+
+	tscmd &= (1<<TSCMD_INT_ENBL_BIT);
+
+	VIDEO_DBG("Latest TSFBSA: %#8.8x\n", new_tsfbsa);
+//	VIDEO_DBG(
+//	        "ri->vg: bpp: %u Mode: %#x gmt: %d Width: %u Height: %u Stride: %u geom_chg: %d\n",
+//	        ri->vg.byBitsPerPixel, ri->vg.byModeID, ri->vg.gmt,
+//	        ri->vg.wScreenWidth, ri->vg.wScreenHeight, ri->vg.wStride,
+//	        b_geom_chg);
+	VIDEO_DBG(
+		"pAstRVAS->current_vg: bpp %u Mode:%#x gmt:%d Width:%u Height:%u Stride:%u\n",
+		pAstRVAS->current_vg.byBitsPerPixel,
+		pAstRVAS->current_vg.byModeID, pAstRVAS->current_vg.gmt,
+		pAstRVAS->current_vg.wScreenWidth,
+		pAstRVAS->current_vg.wScreenHeight,
+		pAstRVAS->current_vg.wStride);
+
+	if (b_geom_chg || (readl((void *)tsfbsa_reg) != new_tsfbsa)) {
+		byBytesPerPixel = pAstRVAS->current_vg.byBitsPerPixel >> 3;
+
+		if ((pAstRVAS->current_vg.gmt == VGAGraphicsMode)
+			|| (pAstRVAS->current_vg.byBitsPerPixel == 4))
+			byTSCMDBytesPerPixel = 0;
+		else {
+			switch (byBytesPerPixel) {
+			case 1:
+				byTSCMDBytesPerPixel = 0;
+				break;
+
+			case 2:
+				byTSCMDBytesPerPixel = 1;
+				break;
+
+			case 3:
+			case 4:
+				byTSCMDBytesPerPixel = 2;
+				break;
+			}
+		}
+		dwStride = pAstRVAS->current_vg.wStride;
+
+		if (byBytesPerPixel == 3)
+			dwStride = (dwStride + dwStride + dwStride) >> 2;
+		else if (pAstRVAS->current_vg.byBitsPerPixel == 4)
+			dwStride >>= 1;
+
+		// set TSE SCR
+		// start the tile snoop engine
+		// flip the 15 bit
+		if (!(readl((void *)tscmd_reg) & TSCMD_SCREEN_OWNER))
+			tscmd |= TSCMD_SCREEN_OWNER;
+
+		tscmd |= (dwStride << TSCMD_PITCH_BIT) | (1 << TSCMD_CPT_BIT)
+			| (1 << TSCMD_RPT_BIT)
+			| (byTSCMDBytesPerPixel << TSCMD_BPP_BIT)
+			| (1 << TSCMD_VGA_MODE_BIT) | (1 << TSCMD_TSE_ENBL_BIT);
+		VIDEO_DBG("tscmd: %#8.8x\n", tscmd);
+		// set the TSFBSA & TSULR
+		writel(new_tsfbsa, (void *)tsfbsa_reg);
+		writel(BSE_UPPER_LIMIT, (void *)tsulr_reg);
+		writel(tscmd, (void *)tscmd_reg);
+		//reset snoop information
+		get_snoop_map_data(pAstRVAS);
+		memset((void *) pAstRVAS->accrued_sm, 0,
+			sizeof(pAstRVAS->accrued_sm));
+		memset((void *) &pAstRVAS->accrued_sa, 0,
+			sizeof(pAstRVAS->accrued_sa));
+
+		for (cContext = 0; cContext < MAX_NUM_CONTEXT; cContext++) {
+			if (ppctContextTable[cContext]) {
+				memset(ppctContextTable[cContext]->aqwSnoopMap,
+					0,
+					sizeof(ppctContextTable[cContext]->aqwSnoopMap));
+				memset(&(ppctContextTable[cContext]->sa), 0,
+					sizeof(ppctContextTable[cContext]->sa));
+			}
+		}      // for each context
+	} // if
+}
+
+//
+// ReadSnoopMap to Clear
+//
+void get_snoop_map_data(struct AstRVAS *pAstRVAS)
+{
+	u32 dwSMDword;
+	u64 aqwSnoopMap[SNOOP_MAP_QWORD_COUNT];
+	//u32 dw_iter;
+
+	get_snoop_aggregate(pAstRVAS);
+	memcpy((void *) aqwSnoopMap,
+		(const void *) (pAstRVAS->fg_reg_base + TSE_SnoopMap_Offset),
+		sizeof(aqwSnoopMap));
+
+	//VIDEO_DBG("Snoop Map:\n");
+	//VIDEO_DBG("==========\n");
+
+	//for (dw_iter = 0; dw_iter < SNOOP_MAP_QWORD_COUNT; ++dw_iter)
+		//VIDEO_DBG("[%2u]: 0x%16.16llx\n", dw_iter, aqwSnoopMap[dw_iter]);
+
+
+	//VIDEO_DBG("==========\n\n");
+
+	// copy 512 snoop map
+	for (dwSMDword = 0; dwSMDword < SNOOP_MAP_QWORD_COUNT; ++dwSMDword)
+		pAstRVAS->accrued_sm[dwSMDword] |= aqwSnoopMap[dwSMDword];
+}
+
+void get_snoop_aggregate(struct AstRVAS *pAstRVAS)
+{
+	u64 qwRow = 0;
+	u64 qwCol = 0;
+
+	// copy the snoop aggregate,row 64 bits
+	qwRow = readl((void *)(pAstRVAS->fg_reg_base + TSE_RS1Reg));
+	qwRow = qwRow << 32;
+	qwRow |= readl((void *)(pAstRVAS->fg_reg_base + TSE_RS0Reg));
+
+	// column
+	qwCol = readl((void *)(pAstRVAS->fg_reg_base + TSE_CS1Reg));
+	qwCol = qwCol << 32;
+	qwCol |= readl((void *)(pAstRVAS->fg_reg_base + TSE_CS0Reg));
+
+	VIDEO_DBG("Snoop Aggregate Row: 0x%16.16llx\n", qwRow);
+	VIDEO_DBG("Snoop Aggregate Col: 0x%16.16llx\n", qwCol);
+	VIDEO_DBG("DRIVER:: %s\n", __func__);
+	VIDEO_DBG("DRIVER:: row [%#llx]\n", qwRow);
+	VIDEO_DBG("DRIVER:: col [%#llx]\n", qwCol);
+
+	pAstRVAS->accrued_sa.qwCol |= qwCol;
+	pAstRVAS->accrued_sa.qwRow |= qwRow;
+}
+
+//
+//
+//
+u64 reinterpret_32bpp_snoop_row_as_24bpp(u64 theSnoopRow)
+{
+	u64 qwResult = 0;
+	u64 qwSourceBit = 1;
+	u32 cSourceBit;
+	u64 qwBitResult = 0;
+
+	for (cSourceBit = 0; cSourceBit < 64; ++cSourceBit) {
+		if (theSnoopRow & qwSourceBit) {
+			qwBitResult = ((cSourceBit * 128) / 96);
+			qwResult |= (((u64) 3) << qwBitResult);
+		}
+
+		qwSourceBit <<= 1;
+	}
+
+	return qwResult;
+}
+
+//
+//one tile: 32x32,
+//
+void convert_snoop_map(struct AstRVAS *pAstRVAS)
+{
+	u32 dwAllRows = (pAstRVAS->current_vg.wScreenHeight + 31) >> 5;
+	u32 cRow;
+
+	for (cRow = 0; cRow < dwAllRows; ++cRow)
+		pAstRVAS->accrued_sm[cRow] =
+			reinterpret_32bpp_snoop_row_as_24bpp(
+				pAstRVAS->accrued_sm[cRow]);
+
+	pAstRVAS->accrued_sa.qwCol = reinterpret_32bpp_snoop_row_as_24bpp(
+		pAstRVAS->accrued_sa.qwCol);
+}
+
+//
+//
+//
+void update_all_snoop_context(struct AstRVAS *pAstRVAS)
+{
+	u32 cContext;
+	u32 iSMDword;
+	struct ContextTable **ppctContextTable = pAstRVAS->ppctContextTable;
+
+	if (pAstRVAS->current_vg.byBitsPerPixel == 24)
+		convert_snoop_map(pAstRVAS);
+
+	for (cContext = 0; cContext < MAX_NUM_CONTEXT; cContext++)
+		if (ppctContextTable[cContext]) {
+			for (iSMDword = 0; iSMDword < SNOOP_MAP_QWORD_COUNT;
+				iSMDword++)
+				ppctContextTable[cContext]->aqwSnoopMap[iSMDword] |=
+					pAstRVAS->accrued_sm[iSMDword];
+
+			ppctContextTable[cContext]->sa.qwRow |=
+				pAstRVAS->accrued_sa.qwRow;
+			ppctContextTable[cContext]->sa.qwCol |=
+				pAstRVAS->accrued_sa.qwCol;
+		}
+
+	//reset snoop map and aggregate
+	memset((void *) pAstRVAS->accrued_sm, 0, sizeof(pAstRVAS->accrued_sm));
+	memset((void *) &pAstRVAS->accrued_sa, 0x00,
+		sizeof(pAstRVAS->accrued_sa));
+}
+
+static u32 setup_tfe_cr(struct FetchOperation *pfo)
+{
+	u32 dwTFECR = 0;
+
+	if (pfo->bEnableRLE)
+		dwTFECR = (pfo->byRLETripletCode << 24)
+			| (pfo->byRLERepeatCode << 16);
+
+	dwTFECR &= TFCTL_DESCRIPTOR_IN_DDR_MASK;
+	dwTFECR |= 1;
+	dwTFECR |= 1 << 1; // enabled IRQ
+	VIDEO_DBG("dwTFECR: %#x\n", dwTFECR);
+	return dwTFECR;
+}
+
+static void start_skip_mode_skip(struct Descriptor *pDescriptorVirtualAddr,
+	u32 dwDescPhysicalAddr, u32 dwSourceAddr, u32 dwDestAddr, u16 wStride,
+	u8 bytesPerPixel, u32 dwFetchWidthPixels, u32 dwFetchHeight,
+	bool bRLEOverFLow)
+{
+
+	struct Descriptor *pVirtDesc = pDescriptorVirtualAddr;
+
+	// Fetch Skipping data to a temp buffer
+	prepare_tfe_descriptor(pVirtDesc, dwSourceAddr, dwDestAddr, true, 1,
+		false, wStride, bytesPerPixel, dwFetchWidthPixels, dwFetchHeight,
+		LowByteMode, bRLEOverFLow, 0);
+
+	dwDestAddr += dwFetchWidthPixels * dwFetchHeight;
+	pVirtDesc++;
+
+	if (bytesPerPixel == 3 || bytesPerPixel == 4) {
+		prepare_tfe_descriptor(pVirtDesc, dwSourceAddr, dwDestAddr,
+			true, 1, false, wStride, bytesPerPixel,
+			dwFetchWidthPixels, dwFetchHeight, MiddleByteMode,
+			bRLEOverFLow, 0);
+
+		dwDestAddr += dwFetchWidthPixels * dwFetchHeight;
+		pVirtDesc++;
+	}
+
+	prepare_tfe_descriptor(pVirtDesc, dwSourceAddr, dwDestAddr, false, 1,
+		false, wStride, bytesPerPixel, dwFetchWidthPixels, dwFetchHeight,
+		TopByteMode, bRLEOverFLow, 1);
+}
+
+// calculate pure fetch size
+static u32 calculate_fetch_size(enum SelectedByteMode sbm, u8 bytesPerPixel,
+	u32 dwFetchWidthPixels, u32 dwFetchHeight)
+{
+	u32 dwFetchSize = 0;
+
+	switch (sbm) {
+	case AllBytesMode:
+		dwFetchSize = dwFetchWidthPixels * dwFetchHeight
+			* bytesPerPixel;
+		break;
+
+	case SkipMode:
+		if (bytesPerPixel == 3 || bytesPerPixel == 4)
+			dwFetchSize = dwFetchWidthPixels * dwFetchHeight * 3;
+		else
+			dwFetchSize = dwFetchWidthPixels * dwFetchHeight
+				* bytesPerPixel;
+		break;
+
+	case PlanarToPackedMode:
+		dwFetchSize = (dwFetchWidthPixels * dwFetchHeight);
+		break;
+
+	case PackedToPackedMode:
+		break;
+
+	default:
+		VIDEO_DBG("Mode= %d is not supported\n", sbm);
+		break;
+	} //switch
+	return dwFetchSize;
+}
+
+static void display_fetch_info(struct FetchVideoTilesArg *pFVTDescriptor, u32 dwCD)
+{
+	struct FetchRegion *pfr = NULL;
+
+	pfr = &(pFVTDescriptor->pfo[dwCD].fr);
+	VIDEO_DBG("FETCH - 1 dwCD: %u\n", dwCD);
+	VIDEO_DBG("pfr->wLeftX :%d\n", pfr->wLeftX);
+	VIDEO_DBG("pfr->wTopY :%d\n", pfr->wTopY);
+	VIDEO_DBG("pfr->wRightX :%d\n", pfr->wRightX);
+	VIDEO_DBG("pfr->wBottomY :%d\n", pfr->wBottomY);
+	VIDEO_DBG(" bEanbleRLE %d\n", pFVTDescriptor->pfo[dwCD].bEnableRLE);
+	VIDEO_DBG("Stride : %d\n", pFVTDescriptor->vg.wStride);
+}
+
+
+void ioctl_fetch_video_tiles(struct RvasIoctl *ri, struct AstRVAS *pAstRVAS)
+{
+	struct FetchVideoTilesArg *pFVTDescriptor;
+	u32 dwCD = 0;
+	struct Descriptor *pDescriptorVirtualAddr;
+	u32 dwDescPhysicalAddr;
+	u32 dwSourcePhyAddr;
+	u32 dwDestinationPhyAddr;
+	u8 bytesPerPixel;
+	struct FetchRegion *pfr;
+	bool bNotLastEntry = false;
+	u32 dwTFECR = 0;
+	u32 dwTotalFetchSize = 0;
+	u32 dwRLESize = 0;
+	bool bRLEOverFLow = false;
+	u32 dwFetchWidthPixels = 0;
+	u32 dwFetchHeight = 0;
+	u32 arg_phys = 0;
+	u32 data_phys_out = 0;
+	u32 data_phys_temp = 0;
+	u16 stride = 0;
+	bool bSkippingMode = false;
+	void *desc_virt = NULL;
+	u32 desc_phy = 0;
+	struct ContextTable *ctx_entry = NULL;
+
+	VIDEO_DBG("DRIVER:::: TILE FETCH CHAINING\n");
+	ctx_entry = get_context_entry(ri->rc, pAstRVAS);
+
+	if (ctx_entry) {
+		desc_virt = ctx_entry->desc_virt;
+		desc_phy = ctx_entry->desc_phy;
+	} else {
+		VIDEO_DBG("Returning with invalid Context handle: 0x%p\n", ri->rc);
+		ri->rs = InvalidContextHandle;
+		return;
+	}
+
+	ri->rs = SuccessStatus;
+	//struct FetchVideoTilesArg buffer
+	arg_phys = get_phys_add_rsvd_mem((u32) ri->rmh, pAstRVAS);
+	//Fetch final dest buffer
+	data_phys_out = get_phys_add_rsvd_mem((u32) ri->rmh1, pAstRVAS);
+	//Intermediate Buffer
+	data_phys_temp = get_phys_add_rsvd_mem((u32) ri->rmh2, pAstRVAS);
+
+	dwDestinationPhyAddr = data_phys_out;
+	pFVTDescriptor = (struct FetchVideoTilesArg *) get_virt_add_rsvd_mem((u32)ri->rmh, pAstRVAS);
+	VIDEO_DBG("Destination virtual Add: 0x%p\n", get_virt_add_rsvd_mem((u32)ri->rmh, pAstRVAS));
+	VIDEO_DBG("Destination Physical Add: %#x\n", dwDestinationPhyAddr);
+	memset(desc_virt, 0x00, PAGE_SIZE);
+
+	if (arg_phys && data_phys_out && data_phys_temp) {
+		pDescriptorVirtualAddr = (struct Descriptor *) desc_virt;
+		dwDescPhysicalAddr = desc_phy;
+		VIDEO_DBG("Descriptor Virtual Addr: %#x\n",
+			(u32)pDescriptorVirtualAddr);
+		VIDEO_DBG("Descriptor Physical Addr: %#x\n", dwDescPhysicalAddr);
+		stride = pFVTDescriptor->vg.wStride;
+
+		if (pFVTDescriptor->vg.byBitsPerPixel == 4) {
+			bytesPerPixel = 1;
+			stride >>= 1;
+		} else
+			bytesPerPixel = pFVTDescriptor->vg.byBitsPerPixel >> 3;
+
+		VIDEO_DBG("u8 per pixel:%u\n", bytesPerPixel);
+		// fetch all data to Destination 1 without RLE
+		VIDEO_DBG("FETCH - 0\n");
+		VIDEO_DBG("COUNT OF Operation: %u\n", pFVTDescriptor->cfo);
+
+		for (dwCD = 0; dwCD < pFVTDescriptor->cfo; dwCD++) {
+			display_fetch_info(pFVTDescriptor, dwCD);
+			// Set up Control Register.
+			dwTFECR = setup_tfe_cr(&pFVTDescriptor->pfo[dwCD]);
+			pfr = &(pFVTDescriptor->pfo[dwCD].fr);
+			// find Source Address
+			if (pFVTDescriptor->vg.byBitsPerPixel == 4) {
+				dwSourcePhyAddr = get_phy_fb_start_address(pAstRVAS)
+								+ ((pfr->wLeftX * bytesPerPixel)>>1)
+								+ pfr->wTopY * stride
+								* bytesPerPixel;
+
+				dwFetchWidthPixels = (pfr->wRightX - pfr->wLeftX + 1)>>1;
+			} else {
+				dwSourcePhyAddr = get_phy_fb_start_address(pAstRVAS)
+						+ pfr->wLeftX * bytesPerPixel
+						+ pfr->wTopY * stride
+						* bytesPerPixel;
+
+				dwFetchWidthPixels = (pfr->wRightX - pfr->wLeftX + 1);
+			}
+			VIDEO_DBG("dwCD: %u dwSourcePhyAddr: %#x\n", dwCD,
+				dwSourcePhyAddr);
+			dwFetchHeight = pfr->wBottomY - pfr->wTopY + 1;
+
+			VIDEO_DBG("DESCRIPTOR virtual ADDRESS: 0x%p\n",
+				pDescriptorVirtualAddr);
+			if (pFVTDescriptor->vg.byBitsPerPixel == 4)
+				pFVTDescriptor->pfo[dwCD].sbm =
+					PlanarToPackedMode;
+
+			pFVTDescriptor->pfo[dwCD].dwFetchSize =
+				calculate_fetch_size(
+					pFVTDescriptor->pfo[dwCD].sbm,
+					bytesPerPixel, dwFetchWidthPixels,
+					dwFetchHeight);
+			bSkippingMode =
+				(pFVTDescriptor->pfo[dwCD].sbm == SkipMode) ?
+				true : false;
+
+			if (bSkippingMode && bytesPerPixel > 1) {
+				u32 skipSrcAddr = dwSourcePhyAddr;
+				u32 skipDestAddr = dwDestinationPhyAddr;
+				u8 byPostBytesPerPixel =
+					(bytesPerPixel == 2) ? 2 : 3;
+				VIDEO_DBG("In SkippingMode...\n");
+
+				if (pFVTDescriptor->pfo[dwCD].bEnableRLE) {
+					//skip data to intermediate buffer
+					skipDestAddr = data_phys_temp;
+				}
+
+				start_skip_mode_skip(pDescriptorVirtualAddr,
+					dwDescPhysicalAddr, skipSrcAddr,
+					skipDestAddr,
+					pFVTDescriptor->vg.wStride,
+					bytesPerPixel, dwFetchWidthPixels,
+					dwFetchHeight, bRLEOverFLow);
+
+				if (pFVTDescriptor->pfo[dwCD].bEnableRLE) {
+					u32 rleSrcAddr = skipDestAddr;
+					u32 rleDesAddr = dwDestinationPhyAddr;
+
+					///// take second look at skip mode for using map single
+					if (sleep_on_tfe_busy(pAstRVAS,
+						dwDescPhysicalAddr, // Descriptor physical Address
+						dwTFECR, // control register value
+						pFVTDescriptor->pfo[dwCD].dwFetchSize, // bandwidth limitor value
+						&dwRLESize,    // out:: rle size
+						&pFVTDescriptor->pfo[dwCD].dwCheckSum
+						) == false) { // out:: cs size
+						ri->rs = GenericError;
+						return;
+					}
+
+					// perform RLE from Temp buffer to dwDestinationPhyAddr
+					//VIDEO_DBG("skip rle\n");
+					prepare_tfe_descriptor(
+						pDescriptorVirtualAddr,
+						rleSrcAddr, rleDesAddr,
+						bNotLastEntry, 1,
+						pFVTDescriptor->pfo[dwCD].bEnableRLE,
+						dwFetchWidthPixels,
+						byPostBytesPerPixel,
+						dwFetchWidthPixels,
+						dwFetchHeight, AllBytesMode,
+						bRLEOverFLow, 1);
+				}
+			} else {
+				VIDEO_DBG(
+					"Preparing TFE Descriptor with no skipping...\n");
+				prepare_tfe_descriptor(pDescriptorVirtualAddr,
+					dwSourcePhyAddr, dwDestinationPhyAddr,
+					bNotLastEntry, 1,
+					pFVTDescriptor->pfo[dwCD].bEnableRLE,
+					stride, bytesPerPixel,
+					dwFetchWidthPixels, dwFetchHeight,
+					pFVTDescriptor->pfo[dwCD].sbm,
+					bRLEOverFLow, 1);
+				VIDEO_DBG(
+					"Successfully prepared TFE Descriptor with no skipping\n");
+			}
+			VIDEO_DBG("Sleeping while TFE is busy...\n");
+
+			if (sleep_on_tfe_busy(pAstRVAS, dwDescPhysicalAddr, // Descriptor physical Address
+				dwTFECR,               // control register value
+				pFVTDescriptor->pfo[dwCD].dwFetchSize, // bandwidth limitor value
+				&dwRLESize,                    // out:: rle size
+				&pFVTDescriptor->pfo[dwCD].dwCheckSum
+				) == false) {  // out:: cs size
+				ri->rs = GenericError;
+				return;
+			}
+
+			VIDEO_DBG("After sleep where TFE was busy\n");
+
+			//VIDEO_DBG("skip rle end\n");
+			if (!pFVTDescriptor->pfo[dwCD].bEnableRLE) { // RLE not enabled
+				VIDEO_DBG("RLE is off\n");
+				pFVTDescriptor->pfo[dwCD].bRLEFailed = false;
+				dwRLESize =
+					pFVTDescriptor->pfo[dwCD].dwFetchSize;
+				dwTotalFetchSize +=
+					pFVTDescriptor->pfo[dwCD].dwFetchSize;
+			} else { // RLE enabled
+				VIDEO_DBG("RLE Enabled\n");
+				if (dwRLESize
+					>= pFVTDescriptor->pfo[dwCD].dwFetchSize) { // FAILED
+					VIDEO_DBG(
+						"DRVIER:: RLE failed RLE: %u > %u\n",
+						dwRLESize,
+						pFVTDescriptor->pfo[dwCD].dwFetchSize);
+					pFVTDescriptor->pfo[dwCD].bRLEFailed =
+						true;
+
+					if (bSkippingMode) {
+						u32 skipSrcAddr =
+							dwSourcePhyAddr;
+						u32 skipDestAddr =
+							dwDestinationPhyAddr;
+
+						start_skip_mode_skip(
+							pDescriptorVirtualAddr,
+							dwDescPhysicalAddr,
+							skipSrcAddr,
+							skipDestAddr,
+							pFVTDescriptor->vg.wStride,
+							bytesPerPixel,
+							dwFetchWidthPixels,
+							dwFetchHeight,
+							bRLEOverFLow);
+					} else {
+						VIDEO_DBG(" FETCH - 4\n");
+						prepare_tfe_descriptor(
+							pDescriptorVirtualAddr,
+							dwSourcePhyAddr,
+							dwDestinationPhyAddr,
+							bNotLastEntry, 1, false,
+							pFVTDescriptor->vg.wStride,
+							bytesPerPixel,
+							dwFetchWidthPixels,
+							dwFetchHeight,
+							pFVTDescriptor->pfo[dwCD].sbm,
+							bRLEOverFLow, 1);
+					}
+
+					if (sleep_on_tfe_busy(pAstRVAS,
+						dwDescPhysicalAddr, // Descriptor physical Address
+						dwTFECR, // control register value
+						pFVTDescriptor->pfo[dwCD].dwFetchSize, // bandwidth limitor value
+						&dwRLESize,    // out:: rle size
+						&pFVTDescriptor->pfo[dwCD].dwCheckSum
+						) == false) {  // out:: cs size
+						ri->rs = GenericError;
+						return;
+					}
+
+					dwTotalFetchSize +=
+						pFVTDescriptor->pfo[dwCD].dwFetchSize;
+					dwRLESize =
+						pFVTDescriptor->pfo[dwCD].dwFetchSize;
+				}  // RLE Failed
+				else { //RLE successful
+					pFVTDescriptor->pfo[dwCD].bRLEFailed =
+						false;
+					dwTotalFetchSize += dwRLESize;
+					dwTotalFetchSize = (dwTotalFetchSize
+						+ 0x3) & 0xfffffffc;
+				}
+			} //RLE Enabled
+
+			pFVTDescriptor->pfo[dwCD].dwFetchRLESize = dwRLESize;
+			VIDEO_DBG("DRIVER:: RLE: %u, nonRLE: %u\n", dwRLESize,
+				pFVTDescriptor->pfo[dwCD].dwFetchSize);
+			VIDEO_DBG("FETCH:: loop FETCH size: %u\n", dwTotalFetchSize);
+			dwDestinationPhyAddr = data_phys_out + dwTotalFetchSize;
+		} //for TFE
+
+		pFVTDescriptor->dwTotalOutputSize = dwTotalFetchSize;
+		VIDEO_DBG("Fetch Size: %#x\n", dwTotalFetchSize);
+	} else {
+		dev_err(pAstRVAS->pdev, "Memory allocation failure\n");
+		ri->rs = InvalidMemoryHandle;
+	}
+} // End - ioctl_fetch_video_tiles
+
+void prepare_ldma_descriptor(struct Descriptor *pDAddress, u32 dwSourceAddress,
+	u32 dwDestAddress, u32 dwLDMASize, u8 byNotLastEntry)
+{
+	u8 byInterrupt = 0;
+
+	VIDEO_DBG("pDAddress: 0x%p\n", pDAddress);
+
+	// initialize to 0
+	pDAddress->dw0General = 0x00;
+	pDAddress->dw1FetchWidthLine = 0x00;
+	pDAddress->dw2SourceAddr = 0x00;
+	pDAddress->dw3DestinationAddr = 0x00;
+
+	// initialize to 0
+	if (!byNotLastEntry)
+		byInterrupt = 0x1;
+
+	pDAddress->dw0General = ((dwLDMASize - 1) << 8) | (byNotLastEntry << 1)
+		| byInterrupt;
+	pDAddress->dw2SourceAddr = dwSourceAddress;
+	pDAddress->dw3DestinationAddr = dwDestAddress;
+
+	VIDEO_DBG("u32 0: 0x%x\n", pDAddress->dw0General);
+	VIDEO_DBG("u32 1: 0x%x\n", pDAddress->dw1FetchWidthLine);
+	VIDEO_DBG("u32 2: 0x%x\n", pDAddress->dw2SourceAddr);
+	VIDEO_DBG("u32 3: 0x%x\n", pDAddress->dw3DestinationAddr);
+}
+
+//
+// ioctl_run_length_encode_data - encode buffer data
+//
+void ioctl_run_length_encode_data(struct RvasIoctl *ri, struct AstRVAS *pAstRVAS)
+{
+	struct Descriptor *pDescriptorAdd = NULL;
+	struct Descriptor *pDescriptorAddPhys = NULL;
+	u8 bytesPerPixel;
+	bool bNotLastEntry = true;
+	u32 dwTFECR = 0;
+	bool bRLEOverFLow = false;
+	u32 dwFetchWidthPixels = 0;
+	u32 dwFetchHeight = 0;
+	u32 dwPhysAddIn;
+	u32 dwPhysAddOut;
+	u32 data_size = 0;
+	void *desc_virt = NULL;
+	u32 desc_phy = 0;
+	struct ContextTable *ctx_entry = NULL;
+
+	ctx_entry = get_context_entry(ri->rc, pAstRVAS);
+	if (ctx_entry) {
+		desc_virt = ctx_entry->desc_virt;
+		desc_phy = ctx_entry->desc_phy;
+	} else {
+		ri->rs = InvalidContextHandle;
+		return;
+	}
+
+	ri->rs = SuccessStatus;
+
+	dwPhysAddIn = get_phys_add_rsvd_mem((u32) ri->rmh, pAstRVAS);
+	dwPhysAddOut = get_phys_add_rsvd_mem((u32) ri->rmh1, pAstRVAS);
+
+	data_size = ri->rmh_mem_size;
+	pDescriptorAdd = (struct Descriptor *) ctx_entry->desc_virt;
+	pDescriptorAddPhys = (struct Descriptor *) ctx_entry->desc_phy;
+
+	VIDEO_DBG("pDescriptorAdd=%#x, phy=%#x\n", (u32)pDescriptorAdd,
+		(u32)pDescriptorAddPhys);
+
+	if (dwPhysAddIn && dwPhysAddOut) {
+		// Enable TFE
+		dwTFECR = (ri->encode & 0xffff0000) << 16;
+		dwTFECR |= 1;
+		dwTFECR &= TFCTL_DESCRIPTOR_IN_DDR_MASK;
+
+		// triplet code and repeat code
+		bNotLastEntry = false;
+		bRLEOverFLow = true;
+		dwFetchWidthPixels = TILE_SIZE;
+		dwFetchHeight = data_size / TILE_SIZE;
+		bytesPerPixel = 1;
+
+		prepare_tfe_descriptor(pDescriptorAdd, dwPhysAddIn,
+			dwPhysAddOut, bNotLastEntry, 1, 1, dwFetchWidthPixels,
+			bytesPerPixel, dwFetchWidthPixels, dwFetchHeight,
+			AllBytesMode, bRLEOverFLow, 1);
+
+		if (sleep_on_tfe_busy(pAstRVAS, (u32) pDescriptorAddPhys,
+			dwTFECR, data_size, &ri->rle_len,
+			&ri->rle_checksum) == false) {
+			ri->rs = GenericError;
+			dev_err(pAstRVAS->pdev, "%s sleep_on_tfe_busy ERROR\n", __func__);
+			return;
+		}
+	} else
+		ri->rs = InvalidMemoryHandle;
+}
+
+static u32 get_video_slice_fetch_width(u8 cBuckets)
+{
+	u32 dwFetchWidthPixels = 0;
+
+	switch (cBuckets) {
+	case 3:
+		dwFetchWidthPixels = ((TILE_SIZE << 5) * 3) >> 3;
+		break;
+
+	case 8:
+		dwFetchWidthPixels = TILE_SIZE << 5;
+		break;
+
+	case 16:
+		dwFetchWidthPixels = (TILE_SIZE << 5) * 2;
+		break;
+
+	case 24:
+		dwFetchWidthPixels = (TILE_SIZE << 5) * 3;
+		break;
+
+	default:
+		dwFetchWidthPixels = TILE_SIZE << 2;
+		break;
+	}
+
+	return dwFetchWidthPixels;
+}
+
+void ioctl_fetch_video_slices(struct RvasIoctl *ri, struct AstRVAS *pAstRVAS)
+{
+	struct FetchVideoSlicesArg *pFVSA;
+	u32 dwCD;
+	struct Descriptor *pDescriptorVirtualAddr;
+	u32 dwDescPhysicalAddr;
+	u32 dwSourceAddress;
+	u32 dwDestinationAddress1Index;
+	u8 bytesPerPixel;
+	bool bNotLastEntry = true;
+	bool bInterrupt = false;
+	u32 dwTFECR = 0;
+	u32 dwFetchSize = 0;
+	bool bRLEOverFLow = false;
+	u32 dwFetchWidthPixels = 0;
+	u32 dwFetchHeight = 0;
+	u32 arg_phys = 0;
+	u32 data_phys_out = 0;
+	u32 data_phys_rle = 0;
+	struct BSEAggregateRegister aBSEAR;
+	struct Descriptor *pNextDescriptor = 0;
+	u32 dwNexDestAddr = 0;
+	u32 dwBucketSizeIter = 0;
+	bool bBucketSizeEnable = 0;
+	u32 addrBSCR = pAstRVAS->fg_reg_base + BSE_Command_Register;
+	void *desc_virt = NULL;
+	u32 desc_phy = 0;
+	struct ContextTable *ctx_entry = get_context_entry(ri->rc, pAstRVAS);
+
+	VIDEO_DBG("Start\n");
+
+	if (ctx_entry) {
+		desc_virt = ctx_entry->desc_virt;
+		desc_phy = ctx_entry->desc_phy;
+	} else {
+		pr_err("BSE: Cannot get valid context\n");
+		ri->rs = InvalidContextHandle;
+		return;
+	}
+
+	arg_phys = get_phys_add_rsvd_mem((u32) ri->rmh, pAstRVAS);
+	data_phys_out = get_phys_add_rsvd_mem((u32) ri->rmh1, pAstRVAS);
+	data_phys_rle = get_phys_add_rsvd_mem((u32) ri->rmh2, pAstRVAS);
+
+	if (!arg_phys || !data_phys_out || !data_phys_rle) {
+		pr_err("BSE: Invalid memory handle\n");
+		ri->rs = InvalidMemoryHandle;
+		return;
+	}
+	ri->rs = SuccessStatus;
+	dwDestinationAddress1Index = data_phys_out;
+	pFVSA = (struct FetchVideoSlicesArg *) get_virt_add_rsvd_mem((u32)ri->rmh, pAstRVAS);
+
+	VIDEO_DBG("bEnableRLE: %d cBuckets: %u cfr: %u\n", pFVSA->bEnableRLE,
+		pFVSA->cBuckets, pFVSA->cfr);
+
+	if (pFVSA->cfr > 1) {
+		writel(readl((void *)addrBSCR)|BSE_ENABLE_MULT_BUCKET_SZS, (void *)addrBSCR);
+		bBucketSizeEnable = 1;
+	} else {
+		writel(readl((void *)addrBSCR)&(~BSE_ENABLE_MULT_BUCKET_SZS), (void *)addrBSCR);
+		bBucketSizeEnable = 0;
+	}
+
+	VIDEO_DBG("*pdwBSCR: %#x bBucketSizeEnable: %d\n", readl((void *)addrBSCR),
+		bBucketSizeEnable);
+
+	pDescriptorVirtualAddr = ctx_entry->desc_virt;
+	dwDescPhysicalAddr = ctx_entry->desc_phy;
+	bytesPerPixel = pFVSA->vg.byBitsPerPixel >> 3;
+
+	VIDEO_DBG("BSE:: u8 per pixel: %d\n", bytesPerPixel);
+	VIDEO_DBG("BSE:: cfr: %u bucket size: %d\n", pFVSA->cfr, pFVSA->cBuckets);
+
+	pNextDescriptor = pDescriptorVirtualAddr;
+	dwNexDestAddr = dwDestinationAddress1Index;
+	// Prepare BSE Descriptors for all Regions
+	VIDEO_DBG("pNextDescriptor 0x%p dwNexDestAddr: %#x\n", pNextDescriptor,
+		dwNexDestAddr);
+
+	for (dwCD = 0; dwCD < pFVSA->cfr; dwCD++) {
+		VIDEO_DBG("dwCD: %u\n", dwCD);
+		VIDEO_DBG("pfr->wLeftX :%d\n", pFVSA->pfr[dwCD].wLeftX);
+		VIDEO_DBG("pfr->wTopY :%d\n", pFVSA->pfr[dwCD].wTopY);
+		VIDEO_DBG("pfr->wRightX :%d\n", pFVSA->pfr[dwCD].wRightX);
+		VIDEO_DBG("pfr->wBottomY :%d\n", pFVSA->pfr[dwCD].wBottomY);
+
+		dwSourceAddress = get_phy_fb_start_address(pAstRVAS)
+			+ pFVSA->pfr[dwCD].wLeftX * bytesPerPixel
+			+ pFVSA->pfr[dwCD].wTopY * pFVSA->vg.wStride
+			* bytesPerPixel;
+		dwFetchWidthPixels = (pFVSA->pfr[dwCD].wRightX
+			- pFVSA->pfr[dwCD].wLeftX + 1);
+		dwFetchHeight = pFVSA->pfr[dwCD].wBottomY
+			- pFVSA->pfr[dwCD].wTopY + 1;
+
+		VIDEO_DBG("BSE Width in Pixel: %d\n", dwFetchWidthPixels);
+		VIDEO_DBG("BSE Height: %d bBucketSizeEnable: %d\n", dwFetchHeight,
+			bBucketSizeEnable);
+
+		if (!bBucketSizeEnable) {
+			bNotLastEntry = false;
+			bInterrupt = true;
+			prepare_bse_descriptor(pDescriptorVirtualAddr,
+				dwSourceAddress, dwDestinationAddress1Index,
+				bNotLastEntry, pFVSA->vg.wStride, bytesPerPixel,
+				dwFetchWidthPixels, dwFetchHeight, bInterrupt);
+			dwFetchSize += (pFVSA->cBuckets
+				* (dwFetchWidthPixels * dwFetchHeight) >> 3);
+			aBSEAR = setUp_bse_bucket(pFVSA->abyBitIndexes,
+				pFVSA->cBuckets, bytesPerPixel,
+				dwFetchWidthPixels, dwFetchHeight);
+
+		} else {
+			if (dwCD == pFVSA->cfr - 1) {
+				bNotLastEntry = false;
+				bInterrupt = true;
+			} else {
+				bNotLastEntry = true;
+				bInterrupt = false;
+			}
+
+			prepare_bse_descriptor_2(pNextDescriptor,
+				dwSourceAddress, dwNexDestAddr, bNotLastEntry,
+				pFVSA->vg.wStride, bytesPerPixel,
+				dwFetchWidthPixels, dwFetchHeight, bInterrupt,
+				arrBuckSizeRegIndex[dwBucketSizeIter]);
+
+			aBSEAR = set_up_bse_bucket_2(pAstRVAS,
+				pFVSA->abyBitIndexes, pFVSA->cBuckets,
+				bytesPerPixel, dwFetchWidthPixels,
+				dwFetchHeight,
+				arrBuckSizeRegIndex[dwBucketSizeIter]);
+
+			dwBucketSizeIter++;
+			pNextDescriptor++;
+			dwFetchSize += pFVSA->cBuckets
+				* ((dwFetchWidthPixels * dwFetchHeight) >> 3); //each bucket size
+			dwNexDestAddr = dwDestinationAddress1Index
+				+ dwFetchSize;
+		}
+	}
+
+	//bse now
+	if (pFVSA->cBuckets <= FULL_BUCKETS_COUNT) {
+		if (bBucketSizeEnable)
+			aBSEAR.dwBSDBS = 0x80000000;
+
+		VIDEO_DBG("Sleeping on BSE to complete\n");
+
+		if (sleep_on_bse_busy(pAstRVAS, dwDescPhysicalAddr, aBSEAR,
+			dwFetchSize) == false) {
+			dev_err(pAstRVAS->pdev, ".....BSE Timeout\n");
+			ri->rs = GenericError;
+			return;
+		}
+	}
+	VIDEO_DBG("Fetched the bit slices\n");
+	//RLE
+	pFVSA->dwSlicedSize = dwFetchSize;
+	pFVSA->dwSlicedRLESize = pFVSA->dwSlicedSize;
+
+	// do RLE if RLE is on. Fetch from Destination 1 to Destination 2 with RLE on
+	bNotLastEntry = false;
+
+	if (pFVSA->bEnableRLE) {
+		VIDEO_DBG("BSE - 3 (RLE Enabled)\n");
+		// Enable TFE
+		dwTFECR = ((pFVSA->byRLETripletCode << 24)
+			| (pFVSA->byRLERepeatCode << 16));
+		dwTFECR |= ((0x1 << 1) | 1);
+		dwTFECR &= TFCTL_DESCRIPTOR_IN_DDR_MASK;
+
+		bRLEOverFLow = true;
+		bytesPerPixel = 1;
+
+		dwFetchWidthPixels = get_video_slice_fetch_width(
+			pFVSA->cBuckets);
+		dwFetchHeight = dwFetchSize / dwFetchWidthPixels;
+
+		prepare_tfe_descriptor(pDescriptorVirtualAddr, data_phys_out,
+			data_phys_rle, bNotLastEntry, 1, pFVSA->bEnableRLE,
+			dwFetchWidthPixels, bytesPerPixel, dwFetchWidthPixels,
+			dwFetchHeight, 0, bRLEOverFLow, 1);
+
+		VIDEO_DBG("TFE-RLE Control Register value: 0x%x\n", dwTFECR);
+
+		if (sleep_on_tfe_busy(pAstRVAS, dwDescPhysicalAddr, // Descriptor physical Address
+			dwTFECR,               // control register value
+			dwFetchSize,          // bandwidth limiter value
+			&pFVSA->dwSlicedRLESize,       // out:: rle size
+			&pFVSA->dwCheckSum
+			) == false) {
+			ri->rs = GenericError;
+			return;
+		}
+
+		VIDEO_DBG("Finishing RLE Fetching\n");
+
+		if (pFVSA->dwSlicedRLESize >= pFVSA->dwSlicedSize)
+			pFVSA->bRLEFailed = true;
+		else
+			pFVSA->bRLEFailed = false;
+	}        // RLE enabled
+
+	memcpy((void *) &dwFetchSize, (void *) &pFVSA->dwSlicedRLESize, 4);
+
+}
+
+void ioctl_fetch_text_data(struct RvasIoctl *ri, struct AstRVAS *pAstRVAS)
+{
+	bool bRLEOn = ri->tfm.bEnableRLE;
+
+	ri->rs = SuccessStatus;
+
+	// first time fetch
+	on_fetch_text_data(ri, bRLEOn, pAstRVAS);
+}
+
+void on_fetch_text_data(struct RvasIoctl *ri, bool bRLEOn, struct AstRVAS *pAstRVAS)
+{
+	struct Descriptor *pDescriptorAdd;
+	struct Descriptor *pDescriptorAddPhys;
+	u32 dwScreenOffset = 0x00;
+	u32 dwSourceAddress = get_phy_fb_start_address(pAstRVAS);
+	u32 dwDestinationAddress;
+	bool bRLEOverFlow = false;
+	bool bInterrupt = true;
+	u32 wFetchLines = 0;
+	u8 byCharacterPerLine = 0;
+	u16 wFetchWidthInBytes = 0;
+	u32 data_phys = 0;
+	u32 data_phys_rle = 0;
+	u32 data_phys_temp = 0;
+	u32 dwCtrlRegValue = 0;
+	u32 dwMinBufSize = 0;
+	void *desc_virt = NULL;
+	u32 desc_phy = 0;
+	struct ContextTable *ctx_entry = NULL;
+
+	VIDEO_DBG("Start\n");
+	ctx_entry = get_context_entry(ri->rc, pAstRVAS);
+	if (ctx_entry) {
+		desc_virt = ctx_entry->desc_virt;
+		desc_phy = ctx_entry->desc_phy;
+	} else {
+		ri->rs = InvalidContextHandle;
+		return;
+	}
+
+	wFetchLines = get_text_mode_fetch_lines(pAstRVAS, ri->vg.wScreenHeight);
+	byCharacterPerLine = get_text_mode_character_per_line(pAstRVAS,
+		ri->vg.wScreenWidth);
+
+	data_phys = get_phys_add_rsvd_mem((u32) ri->rmh, pAstRVAS);
+	data_phys_rle = get_phys_add_rsvd_mem((u32) ri->rmh1, pAstRVAS);
+
+	if (!data_phys || !data_phys_rle) {
+		ri->rs = InvalidMemoryHandle;
+		dev_err(pAstRVAS->pdev, "Fetch Text: Invalid Memoryhandle\n");
+		return;
+	}
+
+	dwMinBufSize = (byCharacterPerLine * wFetchLines) << 1;
+
+	if (ri->rmh_mem_size < dwMinBufSize) {
+		//either buffer is too small or invalid data in registers
+		ri->rs = GenericError;
+		dev_err(pAstRVAS->pdev, "Fetch Text: required buffer len:0x%x\n", dwMinBufSize);
+		return;
+	}
+	memset(desc_virt, 0x00, MAX_DESC_SIZE);
+	pDescriptorAdd = desc_virt;
+	pDescriptorAddPhys = (struct Descriptor *) desc_phy;
+	dwDestinationAddress = data_phys;
+
+	// Enable TFE
+	dwCtrlRegValue |= 1;
+	dwCtrlRegValue &= TFCTL_DESCRIPTOR_IN_DDR_MASK;
+	// set up the text alignment
+	dwScreenOffset = get_screen_offset(pAstRVAS);
+	dwSourceAddress += dwScreenOffset;
+	VIDEO_DBG("screen offset:%#x, Source start Addr: %#x\n", dwScreenOffset,
+		dwSourceAddress);
+	if (ri->tfm.dpm == AttrMode) { // ATTR and ASCII
+		data_phys_temp = data_phys_rle;
+		wFetchWidthInBytes = byCharacterPerLine << 3;
+		// must fetch both ascii & attr
+		VIDEO_DBG("Attribute and ASCII\n");
+		prepare_tfe_text_descriptor(desc_virt, dwSourceAddress,
+			data_phys_temp,
+			false, wFetchWidthInBytes, wFetchLines, ri->tfm.dpm,
+			bRLEOverFlow, bInterrupt);
+		ri->tfm.dwFetchSize = (byCharacterPerLine * wFetchLines) << 1;
+	} else if (ri->tfm.dpm == AsciiOnlyMode) {
+		wFetchWidthInBytes = byCharacterPerLine << 3;
+		VIDEO_DBG("ASCII Only\n");
+		prepare_tfe_text_descriptor(desc_virt, dwSourceAddress,
+			dwDestinationAddress,
+			false, wFetchWidthInBytes, wFetchLines, ri->tfm.dpm,
+			bRLEOverFlow, bInterrupt);
+		ri->tfm.dwFetchSize = byCharacterPerLine * wFetchLines;
+	} else if (ri->tfm.dpm == FontFetchMode) {
+		wFetchWidthInBytes = byCharacterPerLine << 2;
+		VIDEO_DBG("Font Only\n");
+		prepare_tfe_text_descriptor(desc_virt, dwSourceAddress,
+			dwDestinationAddress,
+			false, wFetchWidthInBytes, wFetchLines + 256,
+			ri->tfm.dpm, bRLEOverFlow, bInterrupt);
+
+		ri->tfm.dwFetchSize = MAX_TEXT_DATA_SIZE;
+	}
+	dwCtrlRegValue |= 1 << 1; // enabled IRQ
+	if (ri->tfm.dpm == AttrMode) {
+		if (sleep_on_tfe_text_busy(pAstRVAS, desc_phy, dwCtrlRegValue, // control register value
+			ri->tfm.dwFetchSize,        // bandwidth limitor value
+			&ri->tfm.dwFetchRLESize,        // out:: rle size
+			&ri->tfm.dwCheckSum) == false) {
+			dev_err(pAstRVAS->pdev, "Could not sleep_on_tfe_busy for attributes\n");
+			ri->rs = GenericError;
+			return;
+		}
+	} else {
+		if (sleep_on_tfe_text_busy(pAstRVAS, desc_phy, dwCtrlRegValue,
+			ri->tfm.dwFetchSize, &ri->tfm.dwFetchRLESize,
+			&ri->tfm.dwCheckSum) == false) {
+			ri->rs = GenericError;
+			dev_err(pAstRVAS->pdev, "Could not sleep_on_tfe_busy for others\n");
+			return;
+		}
+	}
+
+	if (ri->tfm.dpm == AttrMode) {
+		//separate ATTR from ATTR+ASCII
+		dwSourceAddress = data_phys_temp;
+		dwDestinationAddress = data_phys;
+		prepare_tfe_descriptor(desc_virt, data_phys_temp, data_phys,
+			false,        //not last entry?
+			1,        //checksum
+			false,        //RLE?
+			byCharacterPerLine,
+			2,        //byBpp,
+			byCharacterPerLine, wFetchLines, TopByteMode,
+			bRLEOverFlow, bInterrupt);
+
+		ri->tfm.dwFetchSize = byCharacterPerLine * wFetchLines;
+
+		dwCtrlRegValue |= 1 << 1;        // enabled IRQ
+		if (sleep_on_tfe_text_busy(pAstRVAS, (u32) pDescriptorAddPhys,
+			dwCtrlRegValue, ri->tfm.dwFetchSize,
+			&ri->tfm.dwFetchRLESize, &ri->tfm.dwCheckSum) == false) {
+			dev_err(pAstRVAS->pdev, "Could not sleep_on_tfe_busy for attributes # 2\n");
+			ri->rs = GenericError;
+			return;
+		}
+	}
+	// RLE enabled
+	if (bRLEOn) {
+		bRLEOverFlow = true;
+		dwCtrlRegValue = 1;
+		dwCtrlRegValue |= (ri->tfm.byRLETripletCode << 24)
+			| (ri->tfm.byRLERepeatCode << 16);
+		dwSourceAddress = dwDestinationAddress;
+		dwDestinationAddress = data_phys_rle;
+
+		// RLE only
+		prepare_tfe_descriptor(pDescriptorAdd, dwSourceAddress,
+			dwDestinationAddress,
+			false,        //not last entry?
+			1,        //checksum
+			bRLEOn,        //RLE?
+			ri->tfm.dwFetchSize / wFetchLines, 1,
+			ri->tfm.dwFetchSize / wFetchLines, wFetchLines,
+			AllBytesMode, bRLEOverFlow, bInterrupt);
+
+		dwCtrlRegValue |= 1 << 1;        // enabled IRQ
+
+		if (sleep_on_tfe_busy(pAstRVAS, (u32) pDescriptorAddPhys, // Descriptor physical Address
+			dwCtrlRegValue,        // control register value
+			ri->tfm.dwFetchSize,        // bandwidth limitor value
+			&ri->tfm.dwFetchRLESize,        // out:: rle size
+			&ri->tfm.dwCheckSum) == false) { // out:: cs size
+			dev_err(pAstRVAS->pdev, "Could not sleep_on_tfe_busy for RLE for Text Mode\n");
+			ri->rs = GenericError;
+			return;
+		}     //sleeponTFEBusy
+	}
+	if (bRLEOn) {
+		ri->tfm.bRLEFailed =
+			(ri->tfm.dwFetchRLESize < ri->tfm.dwFetchSize) ?
+			false : true;
+	}
+}
+
+u8 get_text_mode_character_per_line(struct AstRVAS *pAstRVAS, u16 wScreenWidth)
+{
+	u8 byCharPerLine = 0x00;
+	u8 byCharWidth = 0;
+	u8 byVGASR1 = readb((void *)(pAstRVAS->grce_reg_base + GRCE_SEQ + 0x1));
+
+	byCharWidth = (byVGASR1 & 0x1) ? 8 : 9;
+	byCharPerLine = wScreenWidth / byCharWidth;
+
+	return byCharPerLine;
+}
+
+u16 get_text_mode_fetch_lines(struct AstRVAS *pAstRVAS, u16 wScreenHeight)
+{
+	u8 byVGACR9 = readb((void *)(pAstRVAS->grce_reg_base + GRCE_CRTC + 0x9));
+	u8 byFontHeight = (byVGACR9 & 0x1F) + 1;
+	u16 wFetchLines;
+
+	wFetchLines = wScreenHeight / byFontHeight;
+
+	return wFetchLines;
+}
+
+//
+// HELPER Functions
+//
+
+void prepare_bse_descriptor(struct Descriptor *pDAddress, u32 dwSourceAddress,
+	u32 dwDestAddress, bool bNotLastEntry, u16 wStride, u8 bytesPerPixel,
+	u32 dwFetchWidthPixels, u32 dwFetchHeight, bool bInterrupt)
+{
+	u16 wDestinationStride;
+
+	// initialize to 0
+	pDAddress->dw0General = 0x00;
+	pDAddress->dw1FetchWidthLine = 0x00;
+	pDAddress->dw2SourceAddr = 0x00;
+	pDAddress->dw3DestinationAddr = 0x00;
+
+	wDestinationStride = dwFetchWidthPixels >> 3;
+
+	// initialize to 0
+	pDAddress->dw0General = ((wStride * bytesPerPixel) << 16)
+		| (wDestinationStride << 8) | (bNotLastEntry << 1) | bInterrupt;
+	pDAddress->dw1FetchWidthLine = ((dwFetchHeight - 1) << 16)
+		| (dwFetchWidthPixels * bytesPerPixel - 1);
+	pDAddress->dw2SourceAddr = dwSourceAddress & 0xfffffffc;
+	pDAddress->dw3DestinationAddr = dwDestAddress & 0xfffffffc;
+
+	VIDEO_DBG("After SETTING BSE Descriptor\n");
+	VIDEO_DBG("u32 0: 0x%x\n", pDAddress->dw0General);
+	VIDEO_DBG("u32 1: 0x%x\n", pDAddress->dw1FetchWidthLine);
+	VIDEO_DBG("u32 2: 0x%x\n", pDAddress->dw2SourceAddr);
+	VIDEO_DBG("u32 3: 0x%x\n", pDAddress->dw3DestinationAddr);
+}
+
+//for descriptor chaining
+void prepare_bse_descriptor_2(struct Descriptor *pDAddress, u32 dwSourceAddress,
+	u32 dwDestAddress, bool bNotLastEntry, u16 wStride, u8 bytesPerPixel,
+	u32 dwFetchWidthPixels, u32 dwFetchHeight,
+	bool bInterrupt, u8 byBuckSizeRegIndex)
+{
+	u16 wDestinationStride;
+
+	// initialize to 0
+	pDAddress->dw0General = 0x00;
+	pDAddress->dw1FetchWidthLine = 0x00;
+	pDAddress->dw2SourceAddr = 0x00;
+	pDAddress->dw3DestinationAddr = 0x00;
+
+	wDestinationStride = dwFetchWidthPixels >> 3;
+
+	// initialize to 0
+	pDAddress->dw0General = ((wStride * bytesPerPixel) << 16)
+		| (wDestinationStride << 8)
+		| (byBuckSizeRegIndex << BSE_BUCK_SZ_INDEX_POS)
+		| (bNotLastEntry << 1) | bInterrupt;
+	pDAddress->dw1FetchWidthLine = ((dwFetchHeight - 1) << 16)
+		| (dwFetchWidthPixels * bytesPerPixel - 1);
+	pDAddress->dw2SourceAddr = dwSourceAddress & 0xfffffffc;
+	pDAddress->dw3DestinationAddr = dwDestAddress & 0xfffffffc;
+
+	VIDEO_DBG("AFter SETTING BSE Descriptor\n");
+	VIDEO_DBG("u32 0: 0x%x\n", pDAddress->dw0General);
+	VIDEO_DBG("u32 1: 0x%x\n", pDAddress->dw1FetchWidthLine);
+	VIDEO_DBG("u32 2: 0x%x\n", pDAddress->dw2SourceAddr);
+	VIDEO_DBG("u32 3: 0x%x\n", pDAddress->dw3DestinationAddr);
+}
+
+struct BSEAggregateRegister set_up_bse_bucket_2(struct AstRVAS *pAstRVAS, u8 *abyBitIndexes,
+	u8 byTotalBucketCount, u8 byBSBytesPerPixel, u32 dwFetchWidthPixels,
+	u32 dwFetchHeight, u32 dwBucketSizeIndex)
+{
+	struct BSEAggregateRegister aBSEAR = { 0 };
+	u32 addrBSDBS = 0;
+	u32 addrBSCR = pAstRVAS->fg_reg_base + BSE_Command_Register;
+
+	if (dwBucketSizeIndex >= BSE_MAX_BUCKET_SIZE_REGS) {
+		dev_err(pAstRVAS->pdev, "Video::BSE bucket size index %d too big!",
+			dwBucketSizeIndex);
+		return aBSEAR;
+	}
+
+	addrBSDBS = pAstRVAS->fg_reg_base + BSE_REG_BASE + dwBucketSizeRegOffset[dwBucketSizeIndex];
+
+	// initialize
+	memset((void *) &aBSEAR, 0x00, sizeof(struct BSEAggregateRegister));
+	aBSEAR = setUp_bse_bucket(abyBitIndexes, byTotalBucketCount,
+		byBSBytesPerPixel, dwFetchWidthPixels, dwFetchHeight);
+
+	writel(aBSEAR.dwBSDBS, (void *)addrBSDBS);
+	aBSEAR.dwBSCR |= readl((void *)addrBSCR) & (BSE_ENABLE_MULT_BUCKET_SZS);
+	VIDEO_DBG("BSE Bucket size register index %d, [%#x], readback 0x%x\n",
+		dwBucketSizeIndex, aBSEAR.dwBSDBS, readl((void *)addrBSCR));
+
+	return aBSEAR;
+}
+
+struct BSEAggregateRegister setUp_bse_bucket(u8 *abyBitIndexes, u8 byTotalBucketCount,
+	u8 byBSBytesPerPixel, u32 dwFetchWidthPixels, u32 dwFetchHeight)
+{
+	struct BSEAggregateRegister aBSEAR;
+	u32 dwSrcBucketSize = MAX_LMEM_BUCKET_SIZE;
+	u32 dwDestBucketSize = dwFetchWidthPixels * dwFetchHeight >> 3; //each bucket size
+	u8 byRegisterPosition = 0;
+	u8 cBucket;
+
+	// initialize
+	memset((void *) &aBSEAR, 0x00, sizeof(struct BSEAggregateRegister));
+
+	for (cBucket = 0; cBucket < byTotalBucketCount; cBucket++) {
+		if (cBucket < 6) {
+			VIDEO_DBG("BUCKET: 0x%x, Bit Position: 0x%x\n", cBucket,
+				abyBitIndexes[cBucket]);
+			VIDEO_DBG("BSBPS0 Position: 0x%x\n", byRegisterPosition);
+			aBSEAR.adwBSBPS[0] |= abyBitIndexes[cBucket]
+				<< byRegisterPosition;
+
+			byRegisterPosition += 5;
+		} else if (cBucket >= 6 && cBucket < 12) {
+			if (cBucket == 6)
+				byRegisterPosition = 0;
+
+			VIDEO_DBG("BUCKET: 0x%x, Bit Position: 0x%x\n", cBucket,
+				abyBitIndexes[cBucket]);
+			VIDEO_DBG("BSBPS1 Position: 0x%x\n", byRegisterPosition);
+			aBSEAR.adwBSBPS[1] |= abyBitIndexes[cBucket]
+				<< byRegisterPosition;
+			byRegisterPosition += 5;
+		} else {
+			if (cBucket == 12)
+				byRegisterPosition = 0;
+
+			VIDEO_DBG("BUCKET: 0x%x, Bit Position: 0x%x\n", cBucket,
+				abyBitIndexes[cBucket]);
+			VIDEO_DBG("BSBPS2 Position: 0x%x\n", byRegisterPosition);
+			aBSEAR.adwBSBPS[2] |= abyBitIndexes[cBucket]
+				<< byRegisterPosition;
+			byRegisterPosition += 5;
+		}
+	}
+
+	aBSEAR.dwBSCR = (((byTotalBucketCount - 1) << 8)
+			| ((byBSBytesPerPixel - 1) << 4) | (0x0 << 3)
+			| (0x1 << 1) | 0x1) & BSCMD_MASK;
+	aBSEAR.dwBSDBS = ((dwSrcBucketSize << 24) | dwDestBucketSize)
+		& 0xfcfffffc;
+
+	VIDEO_DBG("dwFetchWidthPixels [%#x], dwFetchHeight [%#x]\n",
+		dwFetchWidthPixels, dwFetchHeight);
+	VIDEO_DBG("BSE Destination Bucket Size [%#x]\n", dwDestBucketSize);
+	VIDEO_DBG("BSE Control [%#x]\n", aBSEAR.dwBSCR);
+	VIDEO_DBG("BSE BSDBS [%#x]\n", aBSEAR.dwBSDBS);
+	VIDEO_DBG("BSE BSBPS0 [%#x]\n", aBSEAR.adwBSBPS[0]);
+	VIDEO_DBG("BSE BSBPS1 [%#x]\n", aBSEAR.adwBSBPS[1]);
+	VIDEO_DBG("BSE BSBPS2 [%#x]\n", aBSEAR.adwBSBPS[2]);
+
+	return aBSEAR;
+}
+
+void prepare_tfe_descriptor(struct Descriptor *pDAddress, u32 dwSourceAddress,
+	u32 dwDestAddress, bool bNotLastEntry, u8 bCheckSum,
+	bool bEnabledRLE, u16 wStride, u8 bytesPerPixel, u32 dwFetchWidthPixels,
+	u32 dwFetchHeight, enum SelectedByteMode sbm,
+	bool bRLEOverFLow, bool bInterrupt)
+{
+	enum SkipByteMode skipBM = NoByteSkip;
+	enum DataProccessMode dpm = NormalTileMode;
+	enum StartBytePosition sbp = StartFromByte0;
+
+	VIDEO_DBG("BEFORE SETTING TFE Descriptor\n");
+	// initialize to 0
+	pDAddress->dw0General = 0x00;
+	pDAddress->dw1FetchWidthLine = 0x00;
+	pDAddress->dw2SourceAddr = 0x00;
+	pDAddress->dw3DestinationAddr = 0x00;
+
+	if (dwFetchHeight & 0x3)
+		dwFetchHeight = ((dwFetchHeight + 3) >> 2) << 2;
+
+	switch (sbm) {
+	case AllBytesMode:
+		break;
+
+	case LowByteMode:
+		dpm = SplitByteMode;
+		if (bytesPerPixel == 2)
+			skipBM = SkipOneByte;
+		else if (bytesPerPixel == 3)
+			skipBM = SkipTwoByte;
+		else if (bytesPerPixel == 4)
+			skipBM = SkipThreeByte;
+		break;
+
+	case MiddleByteMode:
+		dpm = SplitByteMode;
+		if (bytesPerPixel == 2) {
+			skipBM = SkipOneByte;
+			sbp = StartFromByte1;
+		} else if (bytesPerPixel == 3) {
+			skipBM = SkipTwoByte;
+			sbp = StartFromByte1;
+		} else if (bytesPerPixel == 4) {
+			skipBM = SkipThreeByte;
+			sbp = StartFromByte1;
+		}
+		break;
+
+	case TopByteMode:
+		dpm = SplitByteMode;
+		if (bytesPerPixel == 2) {
+			skipBM = SkipOneByte;
+			sbp = StartFromByte1;
+		} else if (bytesPerPixel == 3) {
+			skipBM = SkipTwoByte;
+			sbp = StartFromByte2;
+		} else if (bytesPerPixel == 4) {
+			skipBM = SkipThreeByte;
+			sbp = StartFromByte2;
+		}
+		break;
+
+	case PlanarToPackedMode:
+		dpm = FourBitPlanarMode;
+		break;
+
+	case PackedToPackedMode:
+		dpm = FourBitPackedMode;
+		break;
+
+	default:
+		break;
+	}
+
+	if (dwFetchWidthPixels > wStride)
+		wStride = dwFetchWidthPixels;
+
+	pDAddress->dw0General = ((wStride * bytesPerPixel) << 16) | (dpm << 13)
+		| (sbp << 10) | (skipBM << 8) | (bRLEOverFLow << 7)
+		| (bCheckSum << 5) | (bEnabledRLE << 4) | (bNotLastEntry << 1)
+		| bInterrupt;
+	pDAddress->dw1FetchWidthLine = ((dwFetchHeight - 1) << 16)
+		| (dwFetchWidthPixels * bytesPerPixel - 1);
+	pDAddress->dw2SourceAddr = dwSourceAddress & 0xfffffffc;
+	pDAddress->dw3DestinationAddr = dwDestAddress & 0xfffffffc;
+
+	VIDEO_DBG("After SETTING TFE Descriptor\n");
+	VIDEO_DBG("u32 0: 0x%x\n", pDAddress->dw0General);
+	VIDEO_DBG("u32 1: 0x%x\n", pDAddress->dw1FetchWidthLine);
+	VIDEO_DBG("u32 2: 0x%x\n", pDAddress->dw2SourceAddr);
+	VIDEO_DBG("u32 3: 0x%x\n", pDAddress->dw3DestinationAddr);
+}
+
+void prepare_tfe_text_descriptor(struct Descriptor *pDAddress, u32 dwSourceAddress,
+	u32 dwDestAddress, bool bEnabledRLE, u32 dwFetchWidth,
+	u32 dwFetchHeight, enum DataProccessMode dpm, bool bRLEOverFLow,
+	bool bInterrupt)
+{
+	// initialize to 0
+	pDAddress->dw0General = 0x00;
+	pDAddress->dw1FetchWidthLine = 0x00;
+	pDAddress->dw2SourceAddr = 0x00;
+	pDAddress->dw3DestinationAddr = 0x00;
+
+	if (dwFetchHeight & 0x3)
+		dwFetchHeight = ((dwFetchHeight + 3) >> 2) << 2;
+
+	pDAddress->dw0General = (dwFetchWidth << 16) | (dpm << 13)
+		| (bRLEOverFLow << 7) | (1 << 5) | (bEnabledRLE << 4)
+		| bInterrupt;
+	pDAddress->dw1FetchWidthLine = ((dwFetchHeight - 1) << 16)
+		| (dwFetchWidth - 1);
+	pDAddress->dw2SourceAddr = dwSourceAddress & 0xfffffffc;
+	pDAddress->dw3DestinationAddr = dwDestAddress & 0xfffffffc;
+
+	VIDEO_DBG("u32 0: 0x%x\n", pDAddress->dw0General);
+	VIDEO_DBG("u32 1: 0x%x\n", pDAddress->dw1FetchWidthLine);
+	VIDEO_DBG("u32 2: 0x%x\n", pDAddress->dw2SourceAddr);
+	VIDEO_DBG("u32 3: 0x%x\n", pDAddress->dw3DestinationAddr);
+}
+
+void on_fetch_mode_13_data(struct AstRVAS *pAstRVAS, struct RvasIoctl *ri, bool bRLEOn)
+{
+	struct Descriptor *pDescriptorAdd;
+	struct Descriptor *pDescriptorAddPhys;
+	u32 dwSourceAddress = get_phy_fb_start_address(pAstRVAS);
+	u32 dwDestinationAddress;
+	bool bRLEOverFlow = false;
+	bool bNotLastEntry = false;
+	bool bInterrupt = 1;
+	u32 dwFetchHeight = MODE13_HEIGHT;
+	u32 dwFetchWidth = MODE13_WIDTH;
+	u32 data_phys = 0;
+	u32 data_phys_rle = 0;
+	u32 dwCtrlRegValue = 0x55AA0080;
+	void *desc_virt = NULL;
+	u32 desc_phy = 0;
+	struct ContextTable *ctx_entry = NULL;
+
+	VIDEO_DBG("Start, bRLEOn: %d\n", bRLEOn);
+
+	ctx_entry = get_context_entry(ri->rc, pAstRVAS);
+
+	if (ctx_entry) {
+		desc_virt = ctx_entry->desc_virt;
+		desc_phy = ctx_entry->desc_phy;
+	} else {
+		pr_err("Mode 13: Failed to get context\n");
+		ri->rs = InvalidContextHandle;
+		return;
+	}
+
+	ri->tfm.dwFetchSize = MODE13_HEIGHT * MODE13_WIDTH;
+
+	data_phys = get_phys_add_rsvd_mem((u32) ri->rmh, pAstRVAS);
+	data_phys_rle = get_phys_add_rsvd_mem((u32) ri->rmh1, pAstRVAS);
+
+	if (!data_phys || !data_phys_rle) {
+		ri->rs = InvalidMemoryHandle;
+		dev_err(pAstRVAS->pdev, "Fetch Text: Invalid Memoryhandle\n");
+		return;
+	}
+	if (!data_phys || (bRLEOn && !data_phys_rle)) {
+		pr_err("Mode 13: Invalid memory handle\n");
+		ri->rs = InvalidMemoryHandle;
+		return;
+	}
+
+	pDescriptorAdd = desc_virt;
+	pDescriptorAddPhys = (struct Descriptor *) desc_phy;
+
+	VIDEO_DBG("\n===========MODE 13 FETCHED DATA===========\n");
+
+	// Enable TFE
+	dwCtrlRegValue |= 1;
+	dwCtrlRegValue &= TFCTL_DESCRIPTOR_IN_DDR_MASK;
+	dwDestinationAddress = data_phys;
+	prepare_tfe_descriptor(pDescriptorAdd, dwSourceAddress,
+		dwDestinationAddress,
+		false, //is last entry
+		1,     //checksum
+		false, //No RLE
+		dwFetchWidth,
+		1,		//bytes per pixel
+		dwFetchWidth, dwFetchHeight, PackedToPackedMode, bRLEOverFlow,
+		1);
+
+	dwCtrlRegValue |= 1 << 1; // enabled IRQ
+
+	if (sleep_on_tfe_busy(pAstRVAS, (u32) pDescriptorAddPhys, // Descriptor physical Address
+		dwCtrlRegValue,           // control register value
+		ri->tfm.dwFetchSize,         // bandwidth limitor value
+		&ri->tfm.dwFetchRLESize,     // out:: rle size
+		&ri->tfm.dwCheckSum) == false) {     // out:: cs size
+		ri->rs = GenericError;
+		return;
+	}
+
+	// RLE enabled
+	if (bRLEOn) {
+		bRLEOverFlow = true;
+		dwCtrlRegValue = 1;
+		dwCtrlRegValue |= (ri->tfm.byRLETripletCode << 24)
+		| (ri->tfm.byRLERepeatCode << 16);
+		dwSourceAddress = data_phys;
+		dwDestinationAddress = data_phys_rle;
+		VIDEO_DBG("RLE is on\n");
+
+		prepare_tfe_descriptor(pDescriptorAdd, dwSourceAddress,
+			dwDestinationAddress,
+			bNotLastEntry,  //not last entry?
+			1,				//checksum
+			bRLEOn,		//RLE?
+			dwFetchWidth, 1, dwFetchWidth, dwFetchHeight,
+			AllBytesMode, bRLEOverFlow, bInterrupt);
+
+		dwCtrlRegValue |= 1 << 1; // enabled IRQ
+
+		if (sleep_on_tfe_busy(pAstRVAS, (u32) pDescriptorAddPhys, // Descriptor physical Address
+			dwCtrlRegValue,           // control register value
+			ri->tfm.dwFetchSize,          // bandwidth limitor value
+			&ri->tfm.dwFetchRLESize,     // out:: rle size
+			&ri->tfm.dwCheckSum) == false) {    // out:: cs size
+			ri->rs = GenericError;
+			return;
+		}    //sleeponTFEBusy
+	}
+
+	if (bRLEOn)
+		ri->tfm.bRLEFailed =
+			(ri->tfm.dwFetchRLESize < ri->tfm.dwFetchSize) ?
+			false : true;
+}
+
+void ioctl_fetch_mode_13_data(struct RvasIoctl *ri, struct AstRVAS *pAstRVAS)
+{
+	bool bRLEOn = ri->tfm.bEnableRLE;
+
+	ri->rs = SuccessStatus;
+
+	// first time fetch
+	on_fetch_mode_13_data(pAstRVAS, ri, bRLEOn);
+
+	if (ri->rs != SuccessStatus)
+		return;
+
+	//if RLE fail. need to TFE without RLE to first buffer
+	if (ri->tfm.bEnableRLE & (ri->tfm.bRLEFailed)) {
+		bRLEOn = false;
+		on_fetch_mode_13_data(pAstRVAS, ri, bRLEOn);
+	}
+}
+
+u32 get_phy_fb_start_address(struct AstRVAS *pAstRVAS)
+{
+	u32 dw_offset = get_screen_offset(pAstRVAS);
+
+	pAstRVAS->FBInfo.dwFBPhysStart = DDR_BASE + pAstRVAS->FBInfo.dwDRAMSize - pAstRVAS->FBInfo.dwVGASize + dw_offset;
+
+	VIDEO_DBG("Frame buffer start address: %#x, dram size: %#x, vga size: %#x\n",
+		pAstRVAS->FBInfo.dwFBPhysStart,
+		pAstRVAS->FBInfo.dwDRAMSize,
+		pAstRVAS->FBInfo.dwVGASize);
+
+	return pAstRVAS->FBInfo.dwFBPhysStart;
+}
+
+
+// Enable Snoop Interrupts and TSE, Disable FIQ
+static void enable_tse_interrupt(struct AstRVAS *pAstRVAS)
+{
+	u32 reg_val = 0;
+	u32 reg_addr = pAstRVAS->fg_reg_base
+		+ TSE_SnoopCommand_Register_Offset;
+
+	reg_val = readl((void *)reg_addr);
+	reg_val |= SNOOP_IRQ_MASK;
+	reg_val &= ~SNOOP_FIQ_MASK;
+
+	VIDEO_DBG("Enabled TSE Interrupts[%#X]\n", reg_val);
+	writel(reg_val, (void *)reg_addr);
+	pAstRVAS->tse_tsicr = TSE_INTR_COUNT;
+	reg_addr = pAstRVAS->fg_reg_base
+			+ TSE_TileSnoop_Interrupt_Count;
+	//set max wait time before interrupt
+	writel(pAstRVAS->tse_tsicr, (void *)reg_addr);
+}
+
+//disable tse interrupt
+static void disable_tse_interrupt(struct AstRVAS *pAstRVAS)
+{
+	u32 reg_val = 0;
+	u32 reg_addr = pAstRVAS->fg_reg_base + TSE_SnoopCommand_Register_Offset;
+
+	// Disable Snoop Interrupts and TSE, Disable FIQ
+	reg_val = readl((void *)reg_addr);
+	VIDEO_DBG("disable interrupt\n");
+	reg_val &= ~(SNOOP_IRQ_MASK | SNOOP_FIQ_MASK);
+	writel(reg_val, (void *)reg_addr);
+}
+
+static void enable_grce_interrupt(struct AstRVAS *pAstRVAS)
+{
+	u32 reg_val = 0;
+	u32 reg_addr = pAstRVAS->grce_reg_base + GRCE_CTL0;
+
+	reg_val = readl((void *)reg_addr);
+	reg_val |= GRC_IRQ_MASK;
+	writel(reg_val, (void *)reg_addr);
+	VIDEO_DBG("Enabled GRC Interrupts[%#X]\n", reg_val);
+}
+
+//enable all interrupts
+void enable_grce_tse_interrupt(struct AstRVAS *pAstRVAS)
+{
+	enable_grce_interrupt(pAstRVAS);
+	enable_tse_interrupt(pAstRVAS);
+}
+
+void disable_grce_tse_interrupt(struct AstRVAS *pAstRVAS)
+{
+	u32 reg_val = 0;
+
+	VIDEO_DBG("disable_interrupts- grce_reg_base: %#x GRCE_CTL0: %#x\n",
+		pAstRVAS->grce_reg_base, GRCE_CTL0);
+	reg_val = readl((void *)(pAstRVAS->grce_reg_base + GRCE_CTL0));
+	writel(reg_val&(~GRC_IRQ_MASK), (void *)(pAstRVAS->grce_reg_base + GRCE_CTL0));
+	disable_tse_interrupt(pAstRVAS);
+}
+
+u32 clear_tse_interrupt(struct AstRVAS *pAstRVAS)
+{
+	u32 tse_sts = 0;
+	u32 tse_tile_status = 0;
+	u32 tse_snoop_ctrl = 0;
+	u32 tse_ctrl_addr = pAstRVAS->fg_reg_base + TSE_SnoopCommand_Register_Offset;
+
+	VIDEO_DBG("clear tse inerrupt");
+	tse_sts = readl((void *)(pAstRVAS->fg_reg_base + TSE_Status_Register_Offset));
+	tse_snoop_ctrl = readl((void *)(pAstRVAS->fg_reg_base + TSE_SnoopCommand_Register_Offset));
+
+	if (tse_sts & (TSSTS_TC_SCREEN0|TSSTS_TC_SCREEN1)) {
+		if (tse_sts & TSSTS_TC_SCREEN0) {
+			VIDEO_DBG("Snoop** Update Screen 0\n");
+			 // clear interrupt and switch to screen 1
+			tse_snoop_ctrl |= TSCMD_SCREEN_OWNER;
+			writel(tse_sts, (void *)(pAstRVAS->fg_reg_base + TSE_Status_Register_Offset));
+			writel(tse_snoop_ctrl, (void *)tse_ctrl_addr);
+
+		} else if (tse_sts & TSSTS_TC_SCREEN1) {
+			VIDEO_DBG("Snoop** Update Screen 1\n");
+			tse_snoop_ctrl &= ~TSCMD_SCREEN_OWNER; // snap shutter
+			// clear status
+			writel(tse_sts, (void *)(pAstRVAS->fg_reg_base + TSE_Status_Register_Offset));
+			 // clear interrupt and switch to screen 1
+			writel(tse_snoop_ctrl, (void *)tse_ctrl_addr);
+		}
+		// read clear interrupt
+		tse_tile_status = readl((void *)(pAstRVAS->fg_reg_base
+				+ TSE_TileCount_Register_Offset));
+
+		if (tse_sts & TSSTS_FIFO_OVFL) {
+			//need to send full frame
+			dev_err(pAstRVAS->pdev, "TSE snoop fifo overflow\n");
+			writel(TSSTS_FIFO_OVFL, (void *)(pAstRVAS->fg_reg_base + TSE_Status_Register_Offset));
+			memset((void *) pAstRVAS->accrued_sm, 0xff, sizeof(pAstRVAS->accrued_sm));
+			memset((void *) &pAstRVAS->accrued_sa, 0xff,
+				sizeof(pAstRVAS->accrued_sa));
+		} else {
+			get_snoop_map_data(pAstRVAS);
+		}
+	}
+	return tse_sts;
+}
+// LDMA interrupt
+bool clear_ldma_interrupt(struct AstRVAS *pAstRVAS)
+{
+	u32 ldma_sts = 0;
+
+	ldma_sts = readl((void *)(pAstRVAS->fg_reg_base + LDMA_Status_Register));
+
+	if (ldma_sts & 0x02) {
+		//VIDEO_DBG("Got a LDMA interrupt\n");
+		// write 1 to clear the interrupt
+		writel(0x2, (void *)(pAstRVAS->fg_reg_base + LDMA_Status_Register));
+		return true;
+	}
+	return false;
+}
+
+bool clear_tfe_interrupt(struct AstRVAS *pAstRVAS)
+{
+	u32 tfe_sts = 0;
+
+	tfe_sts = readl((void *)(pAstRVAS->fg_reg_base + TFE_Status_Register));
+
+	if (tfe_sts & 0x02) {
+		// VIDEO_DBG("Debug: TFSTS Interrupt is triggered\n");
+		writel(0x2, (void *)(pAstRVAS->fg_reg_base + TFE_Status_Register));
+		return true;
+	}
+	return false;
+}
+
+bool clear_bse_interrupt(struct AstRVAS *pAstRVAS)
+{
+	u32 bse_sts = 0;
+
+	bse_sts = readl((void *)(pAstRVAS->fg_reg_base + BSE_Status_Register));
+
+	if (bse_sts & 0x02) {
+		writel(0x2, (void *)(pAstRVAS->fg_reg_base + BSE_Status_Register));
+		return true;
+	}
+	return false;
+}
+
+void setup_lmem(struct AstRVAS *pAstRVAS)
+{
+	writel(0x0, (void *)(pAstRVAS->fg_reg_base + LMEM_BASE_REG_3));
+	writel(0x2000, (void *)(pAstRVAS->fg_reg_base + LMEM_LIMIT_REG_3));
+	writel(0x9c89c8, (void *)(pAstRVAS->fg_reg_base + LMEM11_P0));
+	writel(0x9c89c8, (void *)(pAstRVAS->fg_reg_base + LMEM12_P0));
+	writel(0xf3cf3c, (void *)(pAstRVAS->fg_reg_base + LMEM11_P1));
+	writel(0x067201, (void *)(pAstRVAS->fg_reg_base + LMEM11_P2));
+	writel(0x00F3CF3C, (void *)(pAstRVAS->fg_reg_base + LMEM10_P1));
+	writel(0x00067201, (void *)(pAstRVAS->fg_reg_base + LMEM10_P2));
+}
+
+bool host_suspended(struct AstRVAS *pAstRVAS)
+{
+	u32 GRCE18 = readl((void *)(pAstRVAS->grce_reg_base + GRCE_ATTR_VGAIR0_OFFSET));
+
+	// VGAER is GRCE19
+	// VGAER bit[0]:0 - vga disabled (host suspended)
+	// 1 - vga enabled
+	VIDEO_DBG("GRCE18:%#x\n", GRCE18);
+	if (GRCE18 & 0x100)
+		return false;
+	else
+		return true;
+}
+
diff --git a/drivers/soc/aspeed/rvas/hardware_engines.h b/drivers/soc/aspeed/rvas/hardware_engines.h
new file mode 100644
index 000000000000..d1069704c736
--- /dev/null
+++ b/drivers/soc/aspeed/rvas/hardware_engines.h
@@ -0,0 +1,500 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * This file is part of the ASPEED Linux Device Driver for ASPEED Baseboard Management Controller.
+ * Refer to the README file included with this package for driver version and adapter compatibility.
+ *
+ * Copyright (C) 2019-2021 ASPEED Technology Inc. All rights reserved.
+ */
+
+#ifndef __HARDWAREENGINES_H__
+#define __HARDWAREENGINES_H__
+
+#include <linux/semaphore.h>
+#include "video_ioctl.h"
+
+#define MAX_NUM_CONTEXT				(8)
+#define MAX_NUM_MEM_TBL				(24)//each context has 3
+
+#define MAX_DESC_SIZE				(PAGE_SIZE) // (0x400)
+
+#define ENGINE_TIMEOUT_IN_SECONDS		(3)
+#define TFE_TIMEOUT_IN_MS			(750)
+#define DESCRIPTOR_SIZE				(16)
+#define TILE_SIZE				(32)
+#define MAX_LMEM_BUCKET_SIZE			(0x80)
+
+#define EIGHT_BYTE_ALIGNMENT_MASK		(0xfffffff7)
+#define SIXTEEN_BYTE_ALIGNMENT_MASK		(0x8)
+#define TFCTL_DESCRIPTOR_IN_DDR_MASK		(0xffffff7f)
+#define BSCMD_MASK				(0xffff0f37)
+
+#define TEXT_MODE_BUFFER_ALIGNMENT		(16)
+#define MODE_13_CHAR_WIDTH			(32)
+#define BSE_MEMORY_ACCESS_MASK			(0x00ffffff)
+#define MEM_TABLE_SIZE_INCR			(8)
+#define MEMORY_TABLE_GROW_INCR			(8)
+
+#define MAX_TEXT_DATA_SIZE			(8192)
+
+
+//SCU
+#define SCU000_Protection_Key_Register	(0x000)
+#define SCU040_Module_Reset_Control_Register_Set_1 (0x040)
+#define SCU044_Module_Reset_Control_Clear_Register_1 (0x044)
+#define SCU080_Clock_Stop_Control_Register_Set_1 (0x080)
+#define SCU084_Clock_Stop_Control_Clear_Register (0x084)
+#define SCU500_Hardware_Strap1_Register (0x500)
+#define SCU418_Pin_Ctrl (0x418)
+#define SCU0C0_Misc1_Ctrl (0x0C0)
+#define SCU0D0_Misc3_Ctrl (0x0D0)
+//SCU418
+#define VGAVS_ENBL			(1<<31)
+#define VGAHS_ENBL			(1<<30)
+//SCU0C0
+#define VGA_CRT_DISBL			(1<<6)
+//SCU0D0
+#define PWR_OFF_VDAC			(1<<3)
+
+#define SCU_UNLOCK_PWD			(0x1688A8A8)
+#define SCU_RVAS_ENGINE_BIT		BIT(9)
+#define SCU_RVAS_STOP_CLOCK_BIT		BIT(25)
+
+//MCR -edac
+#define MCR_CONF	0x04 /* configuration register */
+
+//DP
+#define DPTX_Configuration_Register			(0x100)
+#define DPTX_PHY_Configuration_Register		(0x104)
+//DPTX100
+#define AUX_RESETN							(24)
+//DPTX104
+#define DP_TX_I_MAIN_ON						(8)
+
+//TOP REG
+#define TOP_REG_OFFSET				(0x0)
+#define TOP_REG_CTL				(TOP_REG_OFFSET + 0x00)
+#define TOP_REG_STS				(TOP_REG_OFFSET + 0x04)
+#define LMEM_BASE_REG_3				(TOP_REG_OFFSET + 0x2c)
+#define LMEM_LIMIT_REG_3			(TOP_REG_OFFSET + 0x3c)
+#define LMEM11_P0				(TOP_REG_OFFSET + 0x4c)
+#define LMEM12_P0				(TOP_REG_OFFSET + 0x50)
+#define LMEM10_P1				(TOP_REG_OFFSET + 0x80)
+#define LMEM11_P1				(TOP_REG_OFFSET + 0x84)
+#define LMEM10_P2				(TOP_REG_OFFSET + 0xA0)
+#define LMEM11_P2				(TOP_REG_OFFSET + 0xA4)
+
+#define TSE_SnoopCommand_Register_Offset	(0x0400)
+#define TSE_TileCount_Register_Offset		(0x0418)
+#define TSE_Status_Register_Offset		(0x0404)
+#define TSE_CS0Reg				(0x0408)
+#define TSE_CS1Reg				(0x040c)
+#define TSE_RS0Reg				(0x0410)
+#define TSE_RS1Reg				(0x0414)
+#define TSE_TileSnoop_Interrupt_Count		(0x0420)
+#define TSE_FrameBuffer_Offset			(0x041c)
+#define TSE_UpperLimit_Offset		(0x0424)
+#define TSE_SnoopMap_Offset			(0x0600)
+
+
+#define TFE_Descriptor_Table_Offset		(0x0108)
+#define TFE_Descriptor_Control_Resgister	(0x0100)
+#define TFE_Status_Register			(0x0104)
+#define TFE_RLE_CheckSum			(0x010C)
+#define TFE_RLE_Byte_Count			(0x0110)
+#define TFE_RLE_LIMITOR				(0x0114)
+
+#define BSE_REG_BASE				(0x0200)
+#define BSE_Command_Register			(0x0200)
+#define BSE_Status_Register			(0x0204)
+#define BSE_Descriptor_Table_Base_Register	(0x0208)
+#define BSE_Destination_Buket_Size_Resgister	(0x020c)
+#define BSE_Bit_Position_Register_0		(0x0210)
+#define BSE_Bit_Position_Register_1		(0x0214)
+#define BSE_Bit_Position_Register_2		(0x0218)
+#define BSE_LMEM_Temp_Buffer_Offset		(0x0000)
+#define BSE_ENABLE_MULT_BUCKET_SZS		(1<<12)
+#define BSE_BUCK_SZ_INDEX_POS			(4)
+#define BSE_MAX_BUCKET_SIZE_REGS		(16)
+#define BSE_BIT_MASK_Register_Offset		(0x54)
+
+#define LDMA_Control_Register			(0x0300)
+#define LDMA_Status_Register			(0x0304)
+#define LDMA_Descriptor_Table_Base_Register	(0x0308)
+#define LDMA_CheckSum_Register			(0x030c)
+#define LDMA_LMEM_Descriptor_Offset		(0x4000)
+
+//Shadow
+#define GRCE_SIZE				(0x800)
+#define GRCE_ATTR_OFFSET			(0x0)
+#define GRCE_ATTR_VGAIR0_OFFSET	(0x18)
+#define GRCE_SEQ_OFFSET				(0x20)
+#define GRCE_GCTL_OFFSET			(0x30)
+#define GRCE_GRCCTL0_OFFSET			(0x58)
+#define GRCE_GRCSTS_OFFSET			(0x5c)
+#define GRCE_CRTC_OFFSET			(0x60)
+#define GRCE_CRTCEXT_OFFSET			(0x80)
+#define GRCE_XCURCTL_OFFSET			(0xc8)
+#define GRCE_PAL_OFFSET				(0x400)
+//size
+#define GRCELT_RAM_SIZE				(0x400)
+#define GRCE_XCURCOL_SIZE			(0x40)
+#define GRCE_XCURCTL_SIZE			(0x40)
+#define GRCE_CRTC_SIZE				(0x40)
+#define GRCE_CRTCEXT_SIZE			(0x8)
+#define GRCE_SEQ_SIZE				(0x8)
+#define GRCE_GCTL_SIZE				(0x8)
+#define GRCE_ATTR_SIZE				(0x20)
+
+#define GRCELT_RAM				(GRCE_PAL_OFFSET)
+#define GRCE_XCURCTL				(GRCE_XCURCTL_OFFSET)
+#define GRCE_CRTC				(GRCE_CRTC_OFFSET)
+#define GRCE_CRTCEXT				(GRCE_CRTCEXT_OFFSET)
+#define GRCE_SEQ				(GRCE_SEQ_OFFSET)
+#define GRCE_GCTL				(GRCE_GCTL_OFFSET)
+#define GRCE_CTL0				(GRCE_GRCCTL0_OFFSET)
+#define GRCE_STATUS_REGISTER			(GRCE_GRCSTS_OFFSET)
+#define GRCE_ATTR				(GRCE_ATTR_OFFSET)
+#define AST_VIDEO_SCRATCH_34C			(0x8c)
+#define AST_VIDEO_SCRATCH_350			(0x90)
+#define AST_VIDEO_SCRATCH_354			(0x94)
+#define MODE_GET_INFO_DE			(0xA8)
+
+//GRC interrupt
+#define GRC_FIQ_MASK				(0x000003ff)
+#define GRC_IRQ_MASK				(0x000003ff)
+#define GRC_INT_STS_MASK			(0x000003ff)
+#define GRCSTS_XCUR_POS				(1<<9)
+#define GRCSTS_XCUR_DDR				(1<<8)
+#define GRCSTS_XCUR_CTL				(1<<7)
+#define GRCSTS_PLT_RAM				(1<<6)
+#define GRCSTS_XCRTC				(1<<5)
+#define GRCSTS_CRTC				(1<<4)
+#define GRCSTS_GCTL				(1<<3)
+#define GRCSTS_SEQ				(1<<2)
+#define GRCSTS_ATTR1				(1<<1)
+#define GRCSTS_ATTR0				(1<<0)
+#define SNOOP_RESTART (GRCSTS_XCUR_CTL|GRCSTS_XCRTC|GRCSTS_CRTC|GRCSTS_GCTL)
+
+//snoop TSE
+#define SNOOP_TSE_MASK				(0x00000001)
+#define SNOOP_IRQ_MASK				(0x00000100)
+#define SNOOP_FIQ_MASK				(0x00000200)
+#define	TSCMD_SCREEN_OWNER			(1<<15)
+#define TSCMD_PITCH_BIT				(16)
+#define TSCMD_INT_ENBL_BIT			(8)
+#define TSCMD_CPT_BIT				(6)
+#define TSCMD_RPT_BIT				(4)
+#define TSCMD_BPP_BIT				(2)
+#define TSCMD_VGA_MODE_BIT			(1)
+#define TSCMD_TSE_ENBL_BIT			(0)
+#define TSSTS_FIFO_OVFL				(1<<5)
+#define TSSTS_FONT				(1<<4)
+#define TSSTS_ATTR				(1<<3)
+#define TSSTS_ASCII				(1<<2)
+#define TSSTS_TC_SCREEN1			(1<<1)
+#define TSSTS_TC_SCREEN0			(1<<0)
+#define TSSTS_ALL				(0x3f)
+
+
+
+#define TSE_INTR_COUNT				(0xCB700)	//50MHz clock ~1/60 sec
+//#define TSE_INTR_COUNT			(0x196E00)	//50MHz clock ~1/30 sec
+#define TIMER_INTR_COUNT			(0x65000)	// 25MHz clock ~1/60 sec
+
+//Timer
+/* Register byte offsets */
+// AST2600 Timer registers
+#define TIMER_STATUS_BIT(x)			(1 << ((x) - 1))
+
+#define OFFSET_TIMER1         0x00                      /* * timer 1 offset */
+#define OFFSET_TIMER2         0x10                      /* * timer 2 offset */
+#define OFFSET_TIMER3         0x20                      /* * timer 3 offset */
+#define OFFSET_TIMER4         0x40                      /* * timer 4 offset */
+#define OFFSET_TIMER5         0x50                      /* * timer 5 offset */
+#define OFFSET_TIMER6         0x60                      /* * timer 6 offset */
+#define OFFSET_TIMER7         0x70                      /* * timer 7 offset */
+#define OFFSET_TIMER8         0x80                      /* * timer 8 offset */
+
+#define OFF_TIMER_REG_CURR_CNT   0x00
+#define OFF_TIMER_REG_LOAD_CNT   0x04
+#define OFF_TIMER_REG_EO0        0x08                    /* Read to clear interrupt */
+#define OFF_TIMER_REG_EOI        0x0c                    /* Read to clear interrupt */
+#define OFF_TIMER_REG_STAT       0x10                    /* Timer Interrupt Status */
+#define OFF_TIMER_REG_CONTROL    0x30							/* Control Register */
+#define OFF_TIMER_REG_STATUS     0x34							/* Status Register */
+#define OFF_TIMER_REG_CLEAR_CONTROL    0x3C							/* Control Register */
+#define RB_OFF_TIMERS_STAT       0xA0                    /* * timers status offset */
+
+#define CTRL_TIMER1           (0)
+#define CTRL_TIMER2           (4)
+#define CTRL_TIMER3           (8)
+#define CTRL_TIMER4           (12)
+#define CTRL_TIMER5           (16)
+#define CTRL_TIMER6           (20)
+#define CTRL_TIMER7           (24)
+#define CTRL_TIMER8           (28)
+#define BIT_TIMER_ENBL           (1 << 0)
+#define BIT_TIMER_CLK_SEL        (1 << 1)
+#define BIT_INTERRUPT_ENBL       (1 << 2)
+#define BIT_TIMER_STAT           (1 << 0)
+
+#define SNOOP_MAP_QWORD_COUNT			(64)
+#define BSE_UPPER_LIMIT				(0x900000) //(0x540000)
+#define FULL_BUCKETS_COUNT			(16)
+#define MODE13_HEIGHT				(200)
+#define MODE13_WIDTH				(320)
+
+#define NUM_SNOOP_ROWS				(64)
+
+//vga memory information
+#define SCU500						(0x500)
+#define DDR_SIZE_CONFIG_BITS				(0x3)
+#define VGA_MEM_SIZE_CONFIG_BITS			(0x3)
+#define VGA_MEM_SIZE_CONFIG_BIT_POS			(13)
+#define DDR_BASE					(0x80000000)
+
+
+//grce
+#define VGACR0_REG					(0x60)
+#define VGACR9F_REG					(0x9F)
+
+
+struct ContextTable {
+	struct inode *pin;
+	struct file *pf;
+	struct SnoopAggregate sa;
+	u64 aqwSnoopMap[NUM_SNOOP_ROWS];
+	void *rc;
+	struct EventMap emEventWaiting;
+	struct EventMap emEventReceived;
+	u32 dwEventWaitInMs;
+	void *desc_virt;
+	u32 desc_phy;
+};
+
+struct MemoryMapTable {
+	struct file *pf;
+	void *pvVirtualAddr;
+	u32 dwPhysicalAddr;
+	u32 dwLength;
+	u8 byDmaAlloc;
+	u8 byReserved[3];
+};
+
+union EmDwordUnion {
+	struct EventMap em;
+	u32 dw;
+};
+
+struct Descriptor {
+	u32 dw0General;
+	u32 dw1FetchWidthLine;
+	u32 dw2SourceAddr;
+	u32 dw3DestinationAddr;
+};
+
+struct BSEAggregateRegister {
+	u32 dwBSCR;
+	u32 dwBSDBS;
+	u32 adwBSBPS[3];
+};
+
+enum SkipByteMode {
+	NoByteSkip = 0, SkipOneByte = 1, SkipTwoByte = 2, SkipThreeByte = 3
+};
+
+enum StartBytePosition {
+	StartFromByte0 = 0,
+	StartFromByte1 = 1,
+	StartFromByte2 = 2,
+	StartFromByte3 = 3
+};
+
+struct VGAMemInfo {
+	u32 dwVGASize;
+	u32 dwDRAMSize;
+	u32 dwFBPhysStart;
+};
+
+struct VideoDataBufferInfo {
+	u32 dwSize;
+	u32 dwPhys;
+	u32 dwVirt;
+};
+
+enum ColorMode {
+	MODE_EGA = 0x0, //4bpp eg. mode 12/6A
+	MODE_VGA = 0x1, //mode 13
+	MODE_BPP15 = 0x2,
+	MODE_BPP16 = 0x3,
+	MODE_BPP32 = 0x4,
+	MODE_TEXT = 0xE,
+	MODE_CGA = 0xF
+};
+
+struct ModeInfo {
+	u8 byColorMode;
+	u8 byRefreshRateIndex;
+	u8 byModeID;
+	u8 byScanLines;
+};
+
+struct NewModeInfoHeader {
+	u8 byReserved;
+	u8 byDisplayInfo;
+	u8 byColorDepth;
+	u8 byMhzPixelClock;
+};
+
+struct DisplayEnd {
+	u16 HDE;
+	u16 VDE;
+};
+
+struct Resolution {
+	u16 wWidth;
+	u16 wHeight;
+};
+
+struct Video_OsSleepStruct {
+	wait_queue_head_t queue;
+	struct timer_list tim;
+	u8 Timeout;
+};
+
+struct EngineInfo {
+	struct semaphore sem;
+	struct Video_OsSleepStruct wait;
+	u8 finished;
+};
+
+
+struct AstRVAS {
+	struct miscdevice *rvas_dev;
+	void *pdev;
+	int irq_fge;	//FrameGrabber IRQ number
+	int irq_vga; // VGA IRQ number
+	int irq_video;
+	u32 fg_reg_base;
+	u32 grce_reg_base;
+	u32 video_reg_base;
+	struct regmap *scu;
+	struct reset_control *rvas_reset;
+	struct reset_control *video_engine_reset;
+	struct VGAMemInfo FBInfo;
+	u64 accrued_sm[SNOOP_MAP_QWORD_COUNT];
+	struct SnoopAggregate accrued_sa;
+	struct VideoGeometry current_vg;
+	u32 snoop_stride;
+	u32 tse_tsicr;
+	struct EngineInfo tfe_engine;
+	struct EngineInfo bse_engine;
+	struct EngineInfo ldma_engine;
+	struct EngineInfo video_engine;
+	struct semaphore mem_sem;
+	struct semaphore context_sem;
+	struct Video_OsSleepStruct video_wait;
+	u8 video_intr_occurred;
+	u8 timer_irq_requested;
+	u8 reserved[2];
+	struct ContextTable *ppctContextTable[MAX_NUM_CONTEXT];
+	u32 dwMemoryTableSize;
+	u32 dwScreenOffset;
+	struct MemoryMapTable *ppmmtMemoryTable[MAX_NUM_MEM_TBL];
+	struct completion  video_compression_complete;
+	struct completion  video_capture_complete;
+	struct clk *vclk;
+	struct clk *eclk;
+	struct clk *rvasclk;
+};
+
+//
+// IOCTL function
+//
+void ioctl_get_video_geometry(struct RvasIoctl *ri, struct AstRVAS *ast_rvas);
+void ioctl_wait_for_video_event(struct RvasIoctl *ri, struct AstRVAS *ast_rvas);
+void ioctl_get_grc_register(struct RvasIoctl *ri, struct AstRVAS *ast_rvas);
+void ioctl_read_snoop_map(struct RvasIoctl *ri, struct AstRVAS *ast_rvas);
+void ioctl_read_snoop_aggregate(struct RvasIoctl *ri, struct AstRVAS *ast_rvas);
+void ioctl_set_tse_tsicr(struct RvasIoctl *ri, struct AstRVAS *ast_rvas);
+void ioctl_get_tse_tsicr(struct RvasIoctl *ri, struct AstRVAS *ast_rvas);
+void ioctl_reset_video_engine(struct RvasIoctl *ri, struct AstRVAS *ast_rvas);
+
+
+void ioctl_fetch_video_tiles(struct RvasIoctl *ri, struct AstRVAS *ast_rvas);
+void ioctl_fetch_video_slices(struct RvasIoctl *ri, struct AstRVAS *ast_rvas);
+void ioctl_run_length_encode_data(struct RvasIoctl *ri, struct AstRVAS *ast_rvas);
+void ioctl_fetch_text_data(struct RvasIoctl *ri, struct AstRVAS *ast_rvas);
+void ioctl_fetch_mode_13_data(struct RvasIoctl *ri, struct AstRVAS *ast_rvas);
+u32 get_phy_fb_start_address(struct AstRVAS *ast_rvas);
+bool video_geometry_change(struct AstRVAS *ast_rvas, u32 dwGRCEStatus);
+void update_video_geometry(struct AstRVAS *ast_rvas);
+//interrupts
+void enable_grce_tse_interrupt(struct AstRVAS *ast_rvas);
+void disable_grce_tse_interrupt(struct AstRVAS *ast_rvas);
+u32 clear_tse_interrupt(struct AstRVAS *ast_rvas);
+bool clear_ldma_interrupt(struct AstRVAS *ast_rvas);
+bool clear_tfe_interrupt(struct AstRVAS *ast_rvas);
+bool clear_bse_interrupt(struct AstRVAS *ast_rvas);
+u32 get_screen_offset(struct AstRVAS *ast_rvas);
+//
+void setup_lmem(struct AstRVAS *ast_rvas);
+//
+// helper functions
+//
+
+struct BSEAggregateRegister setUp_bse_bucket(u8 *abyBitIndexes, u8 byTotalBucketCount,
+	u8 byBSBytesPerPixel, u32 dwFetchWidthPixels, u32 dwFetchHeight);
+void prepare_bse_descriptor(struct Descriptor *pDAddress, u32 dwSourceAddress,
+	u32 dwDestAddress, bool bNotLastEntry, u16 wStride, u8 bytesPerPixel,
+	u32 dwFetchWidthPixels, u32 dwFetchHeight,
+	bool bInterrupt);
+
+void prepare_tfe_descriptor(struct Descriptor *pDAddress, u32 dwSourceAddress,
+	u32 dwDestAddress, bool bNotLastEntry, u8 bCheckSum,
+	bool bEnabledRLE, u16 wStride, u8 bytesPerPixel, u32 dwFetchWidthPixels,
+	u32 dwFetchHeight, enum SelectedByteMode sbm,
+	bool bRLEOverFLow, bool bInterrupt);
+void prepare_tfe_text_descriptor(struct Descriptor *pDAddress, u32 dwSourceAddress,
+	u32 dwDestAddress, bool bEnabledRLE, u32 dwFetchWidth,
+	u32 dwFetchHeight, enum DataProccessMode dpm, bool bRLEOverFLow,
+	bool bInterrupt);
+void prepare_ldma_descriptor(struct Descriptor *pDAddress, u32 dwSourceAddress,
+	u32 dwDestAddress, u32 dwLDMASize, u8 byNotLastEntry);
+
+void OnFetchVideoTileChaining(struct RvasIoctl *ri);
+void OnFetchVideoTileNoChainingWithRLE(struct RvasIoctl *ri);
+void WaitWhileEngineBusy(u32 theAddress);
+u8 get_text_mode_character_per_line(struct AstRVAS *ast_rvas, u16 wScreenWidth);
+u16 get_text_mode_fetch_lines(struct AstRVAS *ast_rvas, u16 wScreenHeight);
+void on_fetch_text_data(struct RvasIoctl *ri, bool bRLEOn, struct AstRVAS *ast_rvas);
+
+void reset_snoop_engine(struct AstRVAS *ast_rvas);
+void set_snoop_engine(bool b_geom_chg, struct AstRVAS *ast_rvas);
+u64 reinterpret_32bpp_snoop_row_as_24bpp(u64 theSnoopRow);
+
+void convert_snoop_map(struct AstRVAS *ast_rvas);
+void update_all_snoop_context(struct AstRVAS *ast_rvas);
+void get_snoop_map_data(struct AstRVAS *ast_rvas);
+void get_snoop_aggregate(struct AstRVAS *ast_rvas);
+
+void sleep_on_ldma_busy(struct AstRVAS *ast_rvas, u32 dwAddress);
+bool sleep_on_tfe_busy(struct AstRVAS *ast_rvas, u32 dwTFEDescriptorAddr,
+	u32 dwTFEControlR, u32 dwTFERleLimitor, u32 *pdwRLESize,
+	u32 *pdwCheckSum);
+
+bool sleep_on_tfe_text_busy(struct AstRVAS *ast_rvas, u32 dwTFEDescriptorAddr,
+	u32 dwTFEControlR, u32 dwTFERleLimitor, u32 *pdwRLESize,
+	u32 *pdwCheckSum);
+
+bool sleep_on_bse_busy(struct AstRVAS *ast_rvas, u32 dwBSEDescriptorAddr,
+	struct BSEAggregateRegister aBSEAR, u32 size);
+
+void enable_grce_tse_interrupt(struct AstRVAS *ast_rvas);
+void disable_grce_tse_interrupt(struct AstRVAS *ast_rvas);
+
+void disable_interrupts(struct AstRVAS *ast_rvas);
+void enable_interrupts(struct AstRVAS *ast_rvas);
+
+bool host_suspended(struct AstRVAS *pAstRVAS);
+#endif // __HARDWAREENGINES_H__
diff --git a/drivers/soc/aspeed/rvas/video.h b/drivers/soc/aspeed/rvas/video.h
new file mode 100644
index 000000000000..4f23564b7dfd
--- /dev/null
+++ b/drivers/soc/aspeed/rvas/video.h
@@ -0,0 +1,43 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/******************************************************************************
+ * video.h
+ *
+ * This file is part of the ASPEED Linux Device Driver for ASPEED Baseboard Management Controller.
+ * Refer to the README file included with this package for driver version and adapter compatibility.
+ *
+ * Copyright (C) 2019-2021 ASPEED Technology Inc. All rights reserved.
+ */
+
+#ifndef __RVAS_VIDEO_H__
+#define __RVAS_VIDEO_H__
+
+#define RVAS_DRIVER_NAME "rvas"
+#define Stringify(x) #x
+
+//
+//functions
+//
+
+
+void ioctl_new_context(struct file *file, struct RvasIoctl *pri, struct AstRVAS *pAstRVAS);
+void ioctl_delete_context(struct RvasIoctl *pri, struct AstRVAS *pAstRVAS);
+void ioctl_alloc(struct file *file, struct RvasIoctl *pri, struct AstRVAS *pAstRVAS);
+void ioctl_free(struct RvasIoctl *pri, struct AstRVAS *pAstRVAS);
+void ioctl_update_lms(u8 lms_on, struct AstRVAS *ast_rvas);
+u32 ioctl_get_lm_status(struct AstRVAS *ast_rvas);
+
+
+//void* get_from_rsvd_mem(u32 size, u32 *phys_add, struct AstRVAS *pAstRVAS);
+void *get_virt_add_rsvd_mem(u32 index, struct AstRVAS *pAstRVAS);
+u32 get_phys_add_rsvd_mem(u32 index, struct AstRVAS *pAstRVAS);
+u32 get_len_rsvd_mem(u32 index, struct AstRVAS *pAstRVAS);
+
+//int release_rsvd_mem(u32 size, u32 phys_add);
+bool virt_is_valid_rsvd_mem(u32 index, u32 size, struct AstRVAS *pAstRVAS);
+
+
+struct ContextTable *get_new_context_table_entry(struct AstRVAS *pAstRVAS);
+struct ContextTable *get_context_entry(const void *crc, struct AstRVAS *pAstRVAS);
+bool remove_context_table_entry(const void *crmh, struct AstRVAS *pAstRVAS);
+
+#endif // __RVAS_VIDEO_H__
diff --git a/drivers/soc/aspeed/rvas/video_debug.h b/drivers/soc/aspeed/rvas/video_debug.h
new file mode 100644
index 000000000000..66df480357cc
--- /dev/null
+++ b/drivers/soc/aspeed/rvas/video_debug.h
@@ -0,0 +1,32 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (C) 2019-2021  ASPEED Technology Inc.
+ */
+
+#ifndef AST_VIDEO_DEBUG_H_
+#define AST_VIDEO_DEBUG_H_
+
+#include <linux/string.h>
+#include <linux/types.h>
+#include <linux/fcntl.h>
+
+
+//#define RVAS_VIDEO_DEBUG
+//#define VIDEO_ENGINE_DEBUG
+//#define HARDWARE_ENGINE_DEBUG
+
+
+#ifdef RVAS_VIDEO_DEBUG
+#define VIDEO_DBG(fmt, args...) ({ dev_printk(KERNEL_INFO, pAstRVAS->pdev, "%s() " fmt, __func__, ## args); })
+#else
+#define VIDEO_DBG(fmt, args...) do; while (0)
+#endif // RVAS_VIDEO_DEBUG
+
+#ifdef VIDEO_ENGINE_DEBUG
+#define VIDEO_ENG_DBG(fmt, args...) ({ dev_printk(KERNEL_INFO, pAstRVAS->pdev, "%s() " fmt, __func__, ## args); })
+#else
+#define VIDEO_ENG_DBG(fmt, args...) do; while (0)
+#endif // RVAS_VIDEO_DEBUG
+
+
+#endif // AST_VIDEO_DEBUG_H_
diff --git a/drivers/soc/aspeed/rvas/video_engine.c b/drivers/soc/aspeed/rvas/video_engine.c
new file mode 100644
index 000000000000..7f319fb12565
--- /dev/null
+++ b/drivers/soc/aspeed/rvas/video_engine.c
@@ -0,0 +1,1205 @@
+// SPDX-License-Identifier: GPL-2.0+
+/*
+ * File Name     : video_engines.c
+ * Description   : AST2600 video  engines
+ *
+ * Copyright (C) 2019-2021 ASPEED Technology Inc. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ */
+#include <linux/poll.h>
+#include <linux/slab.h>
+#include <linux/sched.h>
+#include <linux/clk.h>
+#include <linux/reset.h>
+
+#include <linux/module.h>
+#include <linux/fs.h>
+#include <linux/init.h>
+#include <linux/platform_device.h>
+#include <linux/types.h>
+#include <linux/interrupt.h>
+#include <linux/mm.h>
+#include <linux/delay.h>
+#include <linux/miscdevice.h>
+#include <linux/hwmon-sysfs.h>
+#include <linux/regmap.h>
+#include <linux/mfd/syscon.h>
+#include <linux/dma-mapping.h>
+#include <asm/io.h>
+#include <linux/of.h>
+#include <linux/of_reserved_mem.h>
+#include <asm/uaccess.h>
+
+
+#include "video_ioctl.h"
+#include "video_engine.h"
+#include "video_debug.h"
+#include "hardware_engines.h"
+
+
+static struct VideoEngineMem vem;
+
+//
+//functions
+//
+
+static inline void video_write(struct AstRVAS *pAstRVAS, u32 val, u32 reg);
+static inline u32 video_read(struct AstRVAS *pAstRVAS, u32 reg);
+
+static u32 get_vga_mem_base(struct AstRVAS *pAstRVAS);
+static int reserve_video_engine_memory(struct AstRVAS *pAstRVAS);
+static void init_jpeg_table(void);
+static void video_set_scaling(struct AstRVAS *pAstRVAS);
+static int video_capture_trigger(struct AstRVAS *pAstRVAS);
+static void dump_buffer(u32 dwPhyStreamAddress, u32 size);
+
+
+
+//
+// function definitions
+//
+void ioctl_get_video_engine_config(struct VideoConfig *pVideoConfig, struct AstRVAS *pAstRVAS)
+{
+	u32 VR004_SeqCtrl = video_read(pAstRVAS, AST_VIDEO_SEQ_CTRL);
+	u32 VR060_ComCtrl = video_read(pAstRVAS, AST_VIDEO_COMPRESS_CTRL);
+
+	// status
+	pVideoConfig->rs = SuccessStatus;
+
+	pVideoConfig->engine = 0;	// engine = 1 is Video Management
+	pVideoConfig->capture_format = 0;
+	pVideoConfig->compression_mode = 0;
+
+	pVideoConfig->compression_format = (VR004_SeqCtrl >> 13) & 0x1;
+	pVideoConfig->YUV420_mode = (VR004_SeqCtrl >> 10) & 0x3;
+	pVideoConfig->AutoMode = (VR004_SeqCtrl >> 5) & 0x1;
+
+	pVideoConfig->rc4_enable = (VR060_ComCtrl >> 5) & 0x1;
+	pVideoConfig->Visual_Lossless = (VR060_ComCtrl >> 16) & 0x1;
+	pVideoConfig->Y_JPEGTableSelector = VIDEO_GET_DCT_LUM(VR060_ComCtrl);
+	pVideoConfig->AdvanceTableSelector = (VR060_ComCtrl >> 27) & 0xf;
+
+}
+
+
+void ioctl_set_video_engine_config(struct VideoConfig  *pVideoConfig, struct AstRVAS *pAstRVAS)
+{
+
+	int i, base = 0;
+	u32 ctrl = 0;	//for VR004, VR204
+	u32 compress_ctrl = 0x00080000;
+	u32 *tlb_table = vem.jpegTable.pVirt;
+
+
+	// status
+	pVideoConfig->rs = SuccessStatus;
+
+	VIDEO_ENG_DBG("\n");
+
+	ctrl = video_read(pAstRVAS, AST_VIDEO_SEQ_CTRL);
+
+	video_write(pAstRVAS, video_read(pAstRVAS, AST_VIDEO_PASS_CTRL) &
+				~(G6_VIDEO_FRAME_CT_MASK | G6_VIDEO_MULTI_JPEG_MODE | G6_VIDEO_MULTI_JPEG_FLAG_MODE), AST_VIDEO_PASS_CTRL);
+
+	ctrl &= ~VIDEO_AUTO_COMPRESS;
+	ctrl |= G5_VIDEO_COMPRESS_JPEG_MODE;
+	ctrl &= ~VIDEO_COMPRESS_FORMAT_MASK; //~(3<<10) bit 4 is set to 0
+
+	if (pVideoConfig->YUV420_mode)
+		ctrl |= VIDEO_COMPRESS_FORMAT(YUV420);
+
+	if (pVideoConfig->rc4_enable)
+		compress_ctrl |= VIDEO_ENCRYP_ENABLE;
+
+	switch (pVideoConfig->compression_mode) {
+	case 0:	//DCT only
+			compress_ctrl |= VIDEO_DCT_ONLY_ENCODE;
+			break;
+	case 1:	//DCT VQ mix 2-color
+			compress_ctrl &= ~(VIDEO_4COLOR_VQ_ENCODE | VIDEO_DCT_ONLY_ENCODE);
+			break;
+	case 2:	//DCT VQ mix 4-color
+			compress_ctrl |= VIDEO_4COLOR_VQ_ENCODE;
+			break;
+	default:
+			dev_err(pAstRVAS->pdev, "unknown compression mode:%d\n", pVideoConfig->compression_mode);
+			break;
+	}
+
+	if (pVideoConfig->Visual_Lossless) {
+		compress_ctrl |= VIDEO_HQ_ENABLE;
+		compress_ctrl |= VIDEO_HQ_DCT_LUM(pVideoConfig->AdvanceTableSelector);
+		compress_ctrl |= VIDEO_HQ_DCT_CHROM((pVideoConfig->AdvanceTableSelector + 16));
+	} else
+		compress_ctrl &= ~VIDEO_HQ_ENABLE;
+
+
+	video_write(pAstRVAS, ctrl, AST_VIDEO_SEQ_CTRL);
+	// we are using chrominance quantization table instead of luminance quantization table
+	video_write(pAstRVAS, compress_ctrl | VIDEO_DCT_LUM(pVideoConfig->Y_JPEGTableSelector) | VIDEO_DCT_CHROM(pVideoConfig->Y_JPEGTableSelector + 16), AST_VIDEO_COMPRESS_CTRL);
+	VIDEO_ENG_DBG("VR04: %#X\n", video_read(pAstRVAS, AST_VIDEO_SEQ_CTRL));
+	VIDEO_ENG_DBG("VR60: %#X\n", video_read(pAstRVAS, AST_VIDEO_COMPRESS_CTRL));
+
+	// chose a table for JPEG or multi-JPEG
+	if (pVideoConfig->compression_format >= 1) {
+		VIDEO_ENG_DBG("Choose a JPEG Table\n");
+		for (i = 0; i < 12; i++) {
+			base = (1024 * i);
+			//base = (256 * i);
+			if (pVideoConfig->YUV420_mode)	//yuv420
+				tlb_table[base + 46] = 0x00220103; //for YUV420 mode
+			else
+				tlb_table[base + 46] = 0x00110103; //for YUV444 mode)
+		}
+	}
+
+	video_set_scaling(pAstRVAS);
+}
+
+//
+void ioctl_get_video_engine_data(struct MultiJpegConfig *pArrayMJConfig, struct AstRVAS *pAstRVAS, u32 dwPhyStreamAddress)
+{
+	u32 yuv_shift;
+	u32 yuv_msk;
+	u32 scan_lines;
+	int timeout = 0;
+	u32 x0;
+	u32 y0;
+	int i = 0;
+	u32 dw_w_h;
+	u32 start_addr;
+	u32 multi_jpeg_data = 0;
+	u32 VR044;
+	u32 nextFrameOffset = 0;
+
+	pArrayMJConfig->rs = SuccessStatus;
+
+	VIDEO_ENG_DBG("\n");
+	VIDEO_ENG_DBG("before Stream buffer:\n");
+	//dump_buffer(dwPhyStreamAddress,100);
+
+	video_write(pAstRVAS, dwPhyStreamAddress, AST_VIDEO_STREAM_BUFF);
+
+	if (host_suspended(pAstRVAS)) {
+		pArrayMJConfig->rs = HostSuspended;
+		VIDEO_ENG_DBG("HostSuspended Timeout\n");
+		return;
+	}
+
+	if (video_capture_trigger(pAstRVAS) == 0) {
+		pArrayMJConfig->rs = CaptureTimedOut;
+		VIDEO_ENG_DBG("Capture Timeout\n");
+		return;
+	}
+	//dump_buffer(dwPhyStreamAddress,100);
+
+	init_completion(&pAstRVAS->video_compression_complete);
+	VIDEO_ENG_DBG("capture complete buffer:\n");
+
+	//dump_buffer(vem.captureBuf0.phy,100);
+	VR044 = video_read(pAstRVAS, AST_VIDEO_SOURCE_BUFF0);
+
+	scan_lines = video_read(pAstRVAS, AST_VIDEO_SOURCE_SCAN_LINE);
+	VIDEO_ENG_DBG("scan_lines: %#x\n", scan_lines);
+
+
+	if (video_read(pAstRVAS, AST_VIDEO_SEQ_CTRL) & VIDEO_COMPRESS_FORMAT(YUV420)) {
+		// YUV 420
+		VIDEO_ENG_DBG("Debug: YUV420\n");
+		yuv_shift = 4;
+		yuv_msk = 0xf;
+	} else {
+		// YUV 444
+		VIDEO_ENG_DBG("Debug: YUV444\n");
+		yuv_shift = 3;
+		yuv_msk = 0x7;
+	}
+
+	video_write(pAstRVAS, video_read(pAstRVAS, AST_VIDEO_PASS_CTRL) | G6_VIDEO_MULTI_JPEG_FLAG_MODE |
+			(G6_VIDEO_JPEG__COUNT(pArrayMJConfig->multi_jpeg_frames - 1) | G6_VIDEO_MULTI_JPEG_MODE), AST_VIDEO_PASS_CTRL);
+
+	video_write(pAstRVAS, video_read(pAstRVAS, AST_VIDEO_BCD_CTRL) & ~VIDEO_BCD_CHG_EN, AST_VIDEO_BCD_CTRL);
+
+	video_write(pAstRVAS, video_read(pAstRVAS, AST_VIDEO_CTRL) | VIDEO_CTRL_ADDRESS_MAP_MULTI_JPEG, AST_VIDEO_CTRL);
+
+	for (i = 0; i < pArrayMJConfig->multi_jpeg_frames; i++) {
+		VIDEO_ENG_DBG("Debug: Before: [%d]: x: %#x y: %#x w: %#x h: %#x\n", i,
+			pArrayMJConfig->frame[i].wXPixels, pArrayMJConfig->frame[i].wYPixels,
+			pArrayMJConfig->frame[i].wWidthPixels, pArrayMJConfig->frame[i].wHeightPixels);
+		x0 = pArrayMJConfig->frame[i].wXPixels;
+		y0 = pArrayMJConfig->frame[i].wYPixels;
+		dw_w_h = SET_FRAME_W_H(pArrayMJConfig->frame[i].wWidthPixels, pArrayMJConfig->frame[i].wHeightPixels);
+
+		start_addr = VR044 + (scan_lines * y0) + ((256 * x0) / (1 << yuv_shift));
+
+		VIDEO_ENG_DBG("VR%x dw_w_h: %#x, VR%x : addr : %#x, x0 %d, y0 %d\n",
+				AST_VIDEO_MULTI_JPEG_SRAM + (8 * i), dw_w_h,
+				AST_VIDEO_MULTI_JPEG_SRAM + (8 * i) + 4, start_addr, x0, y0);
+		video_write(pAstRVAS, dw_w_h, AST_VIDEO_MULTI_JPEG_SRAM + (8 * i));
+		video_write(pAstRVAS, start_addr, AST_VIDEO_MULTI_JPEG_SRAM + (8 * i) + 4);
+	}
+
+	video_write(pAstRVAS, video_read(pAstRVAS, AST_VIDEO_SEQ_CTRL) & ~(VIDEO_CAPTURE_TRIGGER | VIDEO_COMPRESS_FORCE_IDLE | VIDEO_COMPRESS_TRIGGER), AST_VIDEO_SEQ_CTRL);
+
+	//set mode for multi-jpeg mode VR004[5:3]
+	video_write(pAstRVAS, (video_read(pAstRVAS, AST_VIDEO_SEQ_CTRL) & ~VIDEO_AUTO_COMPRESS)
+				| VIDEO_CAPTURE_MULTI_FRAME | G5_VIDEO_COMPRESS_JPEG_MODE, AST_VIDEO_SEQ_CTRL);
+
+	//If CPU is too fast, pleas read back and trigger
+	video_write(pAstRVAS, video_read(pAstRVAS, AST_VIDEO_SEQ_CTRL) | VIDEO_COMPRESS_TRIGGER, AST_VIDEO_SEQ_CTRL);
+	VIDEO_ENG_DBG("wait_for_completion_interruptible_timeout...\n");
+
+	timeout = wait_for_completion_interruptible_timeout(&pAstRVAS->video_compression_complete, HZ / 2);
+
+	if (timeout == 0) {
+		dev_err(pAstRVAS->pdev, "multi compression timeout sts %x\n", video_read(pAstRVAS, AST_VIDEO_INT_STS));
+		pArrayMJConfig->multi_jpeg_frames = 0;
+		pArrayMJConfig->rs = CompressionTimedOut;
+	} else {
+		VIDEO_ENG_DBG("400 %x , 404 %x\n", video_read(pAstRVAS, AST_VIDEO_MULTI_JPEG_SRAM), video_read(pAstRVAS, AST_VIDEO_MULTI_JPEG_SRAM + 4));
+		VIDEO_ENG_DBG("408 %x , 40c %x\n", video_read(pAstRVAS, AST_VIDEO_MULTI_JPEG_SRAM + 8), video_read(pAstRVAS, AST_VIDEO_MULTI_JPEG_SRAM + 0xC));
+		VIDEO_ENG_DBG("done reading 408\n");
+
+		for (i = 0; i < pArrayMJConfig->multi_jpeg_frames; i++) {
+
+			pArrayMJConfig->frame[i].dwOffsetInBytes = nextFrameOffset;
+
+			multi_jpeg_data = video_read(pAstRVAS, AST_VIDEO_MULTI_JPEG_SRAM + (8 * i) + 4);
+			if (multi_jpeg_data & BIT(7)) {
+				pArrayMJConfig->frame[i].dwSizeInBytes = video_read(pAstRVAS, AST_VIDEO_MULTI_JPEG_SRAM + (8 * i)) & 0xffffff;
+				nextFrameOffset = (multi_jpeg_data & ~BIT(7)) >> 1;
+			} else {
+				pArrayMJConfig->frame[i].dwSizeInBytes = 0;
+				nextFrameOffset = 0;
+			}
+			VIDEO_ENG_DBG("[%d] size %d, dwOffsetInBytes %x\n", i, pArrayMJConfig->frame[i].dwSizeInBytes, pArrayMJConfig->frame[i].dwOffsetInBytes);
+		} //for
+	}
+
+	video_write(pAstRVAS, (video_read(pAstRVAS, AST_VIDEO_SEQ_CTRL) & ~(G5_VIDEO_COMPRESS_JPEG_MODE | VIDEO_CAPTURE_MULTI_FRAME))
+			| VIDEO_AUTO_COMPRESS, AST_VIDEO_SEQ_CTRL);
+	video_write(pAstRVAS, video_read(pAstRVAS, AST_VIDEO_PASS_CTRL) &
+			~(G6_VIDEO_FRAME_CT_MASK | G6_VIDEO_MULTI_JPEG_MODE), AST_VIDEO_PASS_CTRL);
+
+	//VIDEO_ENG_DBG("after Stream buffer:\n");
+	//dump_buffer(dwPhyStreamAddress,100);
+}
+
+
+irqreturn_t ast_video_isr(int this_irq, void *dev_id)
+{
+	u32 status;
+	//u32 swap0, swap1;
+	struct AstRVAS *pAstRVAS = dev_id;
+
+	status = video_read(pAstRVAS, AST_VIDEO_INT_STS);
+
+	VIDEO_ENG_DBG("%x\n", status);
+
+
+	if (status & VIDEO_COMPRESS_COMPLETE) {
+		video_write(pAstRVAS, VIDEO_COMPRESS_COMPLETE, AST_VIDEO_INT_STS);
+		VIDEO_ENG_DBG("compress complete swap\n");
+		// no need to swap for better performance
+		// swap0 = video_read(pAstRVAS, AST_VIDEO_SOURCE_BUFF0);
+		// swap1 = video_read(pAstRVAS, AST_VIDEO_SOURCE_BUFF1);
+		// video_write(pAstRVAS, swap1, AST_VIDEO_SOURCE_BUFF0);
+		// video_write(pAstRVAS, swap0, AST_VIDEO_SOURCE_BUFF1);
+		complete(&pAstRVAS->video_compression_complete);
+	}
+	if (status & VIDEO_CAPTURE_COMPLETE) {
+		video_write(pAstRVAS, VIDEO_CAPTURE_COMPLETE, AST_VIDEO_INT_STS);
+		VIDEO_ENG_DBG("capture complete\n");
+		complete(&pAstRVAS->video_capture_complete);
+	}
+
+	return IRQ_HANDLED;
+}
+
+void enable_video_interrupt(struct AstRVAS *pAstRVAS)
+{
+	u32 intCtrReg = video_read(pAstRVAS, AST_VIDEO_INT_EN);
+
+	intCtrReg = (VIDEO_COMPRESS_COMPLETE | VIDEO_CAPTURE_COMPLETE);
+	video_write(pAstRVAS, intCtrReg, AST_VIDEO_INT_EN);
+}
+
+void disable_video_interrupt(struct AstRVAS *pAstRVAS)
+{
+	video_write(pAstRVAS, 0, AST_VIDEO_INT_EN);
+}
+
+void video_engine_rc4Reset(struct AstRVAS *pAstRVAS)
+{
+	//rc4 init reset ..
+	video_write(pAstRVAS, video_read(pAstRVAS, AST_VIDEO_CTRL) | VIDEO_CTRL_RC4_RST, AST_VIDEO_CTRL);
+	video_write(pAstRVAS, video_read(pAstRVAS, AST_VIDEO_CTRL) & ~VIDEO_CTRL_RC4_RST, AST_VIDEO_CTRL);
+}
+
+// setup functions
+int video_engine_reserveMem(struct AstRVAS *pAstRVAS)
+{
+	int result = 0;
+
+	// reserve mem
+	result = reserve_video_engine_memory(pAstRVAS);
+	if (result < 0) {
+		dev_err(pAstRVAS->pdev, "Error Reserving Video Engine Memory\n");
+		return result;
+	}
+	return 0;
+}
+
+
+
+int free_video_engine_memory(struct AstRVAS *pAstRVAS)
+{
+	int size = vem.captureBuf0.size + vem.captureBuf1.size + vem.jpegTable.size;
+
+	if (size && vem.captureBuf0.pVirt) {
+		dma_free_coherent(pAstRVAS->pdev, size,
+				vem.captureBuf0.pVirt,
+				vem.captureBuf0.phy);
+	} else {
+		return -1;
+	}
+	VIDEO_ENG_DBG("After dma_free_coherent\n");
+
+	return 0;
+}
+
+// this function needs to be called when graphic mode change
+void video_set_Window(struct AstRVAS *pAstRVAS)
+{
+	u32 scan_line;
+
+	VIDEO_ENG_DBG("\n");
+
+	//compression x,y
+	video_write(pAstRVAS, VIDEO_COMPRESS_H(pAstRVAS->current_vg.wStride) | VIDEO_COMPRESS_V(pAstRVAS->current_vg.wScreenHeight), AST_VIDEO_COMPRESS_WIN);
+	VIDEO_ENG_DBG("reg offset[%#x]: %#x\n", AST_VIDEO_COMPRESS_WIN, video_read(pAstRVAS, AST_VIDEO_COMPRESS_WIN));
+
+	if (pAstRVAS->current_vg.wStride == 1680)
+		video_write(pAstRVAS, VIDEO_CAPTURE_H(1728) | VIDEO_CAPTURE_V(pAstRVAS->current_vg.wScreenHeight), AST_VIDEO_CAPTURE_WIN);
+	else
+		video_write(pAstRVAS, VIDEO_CAPTURE_H(pAstRVAS->current_vg.wStride) | VIDEO_CAPTURE_V(pAstRVAS->current_vg.wScreenHeight), AST_VIDEO_CAPTURE_WIN);
+
+	VIDEO_ENG_DBG("reg offset[%#x]: %#x\n", AST_VIDEO_CAPTURE_WIN, video_read(pAstRVAS, AST_VIDEO_CAPTURE_WIN));
+
+	// set scan_line VR048
+	if ((pAstRVAS->current_vg.wStride % 8) == 0)
+		video_write(pAstRVAS, pAstRVAS->current_vg.wStride * 4, AST_VIDEO_SOURCE_SCAN_LINE);
+	else {
+		scan_line = pAstRVAS->current_vg.wStride;
+		scan_line = scan_line + 16 - (scan_line % 16);
+		scan_line = scan_line * 4;
+		video_write(pAstRVAS, scan_line, AST_VIDEO_SOURCE_SCAN_LINE);
+	}
+	VIDEO_ENG_DBG("reg offset[%#x]: %#x\n", AST_VIDEO_SOURCE_SCAN_LINE, video_read(pAstRVAS, AST_VIDEO_SOURCE_SCAN_LINE));
+}
+
+void set_direct_mode(struct AstRVAS *pAstRVAS)
+{
+	int Direct_Mode = 0;
+	u32 ColorDepthIndex;
+	u32 VGA_Scratch_Register_350, VGA_Scratch_Register_354, VGA_Scratch_Register_34C, Color_Depth;
+
+	VIDEO_ENG_DBG("\n");
+	video_write(pAstRVAS, video_read(pAstRVAS, AST_VIDEO_PASS_CTRL) & ~(VIDEO_AUTO_FETCH | VIDEO_DIRECT_FETCH), AST_VIDEO_PASS_CTRL);
+
+	VGA_Scratch_Register_350 = video_read(pAstRVAS, AST_VIDEO_E_SCRATCH_350);
+	VGA_Scratch_Register_34C = video_read(pAstRVAS, AST_VIDEO_E_SCRATCH_34C);
+	VGA_Scratch_Register_354 = video_read(pAstRVAS, AST_VIDEO_E_SCRATCH_354);
+
+	if (((VGA_Scratch_Register_350 & 0xff00) >> 8) == 0xA8) {
+
+		Color_Depth = ((VGA_Scratch_Register_350 & 0xff0000) >> 16);
+
+		if (Color_Depth < 15)
+			Direct_Mode = 0;
+		else
+			Direct_Mode = 1;
+
+	} else { //Original mode information
+		ColorDepthIndex = (VGA_Scratch_Register_34C >> 4) & 0x0F;
+
+		if ((ColorDepthIndex == 0xe) || (ColorDepthIndex == 0xf)) {
+			Direct_Mode = 0;
+		} else {
+			if (ColorDepthIndex > 2) {
+				Direct_Mode = 1;
+			} else {
+				Direct_Mode = 0;
+			}
+		}
+	}
+
+	if (Direct_Mode) {
+		VIDEO_ENG_DBG("Direct Mode\n");
+		video_write(pAstRVAS, video_read(pAstRVAS, AST_VIDEO_PASS_CTRL) | VIDEO_AUTO_FETCH | VIDEO_DIRECT_FETCH, AST_VIDEO_PASS_CTRL);
+		video_write(pAstRVAS, get_vga_mem_base(pAstRVAS), AST_VIDEO_DIRECT_BASE);
+		video_write(pAstRVAS, VIDEO_FETCH_TIMING(0) | VIDEO_FETCH_LINE_OFFSET(pAstRVAS->current_vg.wStride * 4), AST_VIDEO_DIRECT_CTRL);
+	} else {
+		VIDEO_ENG_DBG("Sync None Direct Mode\n");
+		video_write(pAstRVAS, video_read(pAstRVAS, AST_VIDEO_PASS_CTRL) & ~(VIDEO_AUTO_FETCH | VIDEO_DIRECT_FETCH), AST_VIDEO_PASS_CTRL);
+	}
+}
+
+// return timeout 0 - timeout; non 0 is successful
+static int video_capture_trigger(struct AstRVAS *pAstRVAS)
+{
+	int timeout = 0;
+
+	VIDEO_ENG_DBG("\n");
+
+	init_completion(&pAstRVAS->video_capture_complete);
+
+	video_write(pAstRVAS, video_read(pAstRVAS, AST_VIDEO_BCD_CTRL) & ~VIDEO_BCD_CHG_EN, AST_VIDEO_BCD_CTRL);
+	video_write(pAstRVAS, video_read(pAstRVAS, AST_VIDEO_SEQ_CTRL) & ~(VIDEO_CAPTURE_TRIGGER | VIDEO_COMPRESS_FORCE_IDLE | VIDEO_COMPRESS_TRIGGER | VIDEO_AUTO_COMPRESS), AST_VIDEO_SEQ_CTRL);
+	//If CPU is too fast, pleas read back and trigger
+	video_write(pAstRVAS, video_read(pAstRVAS, AST_VIDEO_SEQ_CTRL) | VIDEO_CAPTURE_TRIGGER, AST_VIDEO_SEQ_CTRL);
+
+	timeout = wait_for_completion_interruptible_timeout(&pAstRVAS->video_capture_complete, HZ / 2);
+
+	if (timeout == 0)
+		dev_err(pAstRVAS->pdev, "Capture timeout sts %x\n", video_read(pAstRVAS, AST_VIDEO_INT_STS));
+
+	//dump_buffer(vem.captureBuf0.phy, 1024);
+	return timeout;
+}
+
+//
+// static functions
+//
+static u32 get_vga_mem_base(struct AstRVAS *pAstRVAS)
+{
+	u32 vga_mem_size, mem_size;
+
+	mem_size = pAstRVAS->FBInfo.dwDRAMSize;
+	vga_mem_size = pAstRVAS->FBInfo.dwVGASize;
+	VIDEO_ENG_DBG("VGA Info : MEM Size %dMB, VGA Mem Size %dMB\n", mem_size/1024/1024, vga_mem_size/1024/1024);
+	return (mem_size - vga_mem_size);
+}
+
+static void dump_buffer(u32 dwPhyStreamAddress, u32 size)
+{
+	u32 iC;
+	u32 val = 0;
+
+	for (iC = 0; iC < size; iC += 4) {
+		val = readl((void *)(dwPhyStreamAddress + iC));
+		VIDEO_ENG_DBG("%#x, ", val);
+	}
+
+}
+
+
+static void video_set_scaling(struct AstRVAS *pAstRVAS)
+{
+	u32 ctrl = video_read(pAstRVAS, AST_VIDEO_CTRL);
+	//no scaling
+	ctrl &= ~VIDEO_CTRL_DWN_SCALING_MASK;
+
+	VIDEO_ENG_DBG("Scaling Disable\n");
+	video_write(pAstRVAS, 0x00200000, AST_VIDEO_SCALING0);
+	video_write(pAstRVAS, 0x00200000, AST_VIDEO_SCALING1);
+	video_write(pAstRVAS, 0x00200000, AST_VIDEO_SCALING2);
+	video_write(pAstRVAS, 0x00200000, AST_VIDEO_SCALING3);
+
+	video_write(pAstRVAS, 0x10001000, AST_VIDEO_SCAL_FACTOR);
+	video_write(pAstRVAS, ctrl, AST_VIDEO_CTRL);
+
+	video_set_Window(pAstRVAS);
+}
+
+
+
+void video_ctrl_init(struct AstRVAS *pAstRVAS)
+{
+	VIDEO_ENG_DBG("\n");
+	VIDEO_ENG_DBG("reg address: %#x\n", pAstRVAS->video_reg_base);
+	video_write(pAstRVAS, (u32)vem.captureBuf0.phy, AST_VIDEO_SOURCE_BUFF0);//44h
+	video_write(pAstRVAS, (u32)vem.captureBuf1.phy, AST_VIDEO_SOURCE_BUFF1);//4Ch
+	video_write(pAstRVAS, (u32)vem.jpegTable.phy, AST_VIDEO_JPEG_HEADER_BUFF); //40h
+	video_write(pAstRVAS, 0, AST_VIDEO_COMPRESS_READ); //3Ch
+
+	//clr int sts
+	video_write(pAstRVAS, 0xffffffff, AST_VIDEO_INT_STS);
+	video_write(pAstRVAS, 0, AST_VIDEO_BCD_CTRL);
+
+	// =============================  JPEG init ===========================================
+	init_jpeg_table();
+	VIDEO_ENG_DBG("JpegTable in Memory:%#x\n", vem.jpegTable.pVirt);
+	dump_buffer(vem.jpegTable.phy, 80);
+
+	// ===================================================================================
+	//Specification define bit 12:13 must always 0;
+	video_write(pAstRVAS, (video_read(pAstRVAS, AST_VIDEO_PASS_CTRL) &
+								~(VIDEO_DUAL_EDGE_MODE | VIDEO_18BIT_SINGLE_EDGE)) |
+								VIDEO_DVO_INPUT_DELAY(0x4),
+								AST_VIDEO_PASS_CTRL);
+
+	video_write(pAstRVAS, VIDEO_STREAM_PKT_N(STREAM_32_PKTS) |
+					VIDEO_STREAM_PKT_SIZE(STREAM_128KB), AST_VIDEO_STREAM_SIZE);
+
+
+	//rc4 init reset ..
+	video_write(pAstRVAS, video_read(pAstRVAS, AST_VIDEO_CTRL) | VIDEO_CTRL_RC4_RST, AST_VIDEO_CTRL);
+	video_write(pAstRVAS, video_read(pAstRVAS, AST_VIDEO_CTRL) & ~VIDEO_CTRL_RC4_RST, AST_VIDEO_CTRL);
+
+	//CRC/REDUCE_BIT register clear
+	video_write(pAstRVAS, 0, AST_VIDEO_CRC1);
+	video_write(pAstRVAS, 0, AST_VIDEO_CRC2);
+	video_write(pAstRVAS, 0, AST_VIDEO_DATA_TRUNCA);
+	video_write(pAstRVAS, 0, AST_VIDEO_COMPRESS_READ);
+}
+
+
+static int reserve_video_engine_memory(struct AstRVAS *pAstRVAS)
+{
+	u32 size;
+	u32 phys_add = 0;
+	u32 virt_add = 0;
+
+	memset(&vem, 0, sizeof(struct VideoEngineMem));
+	vem.captureBuf0.size = VIDEO_CAPTURE_BUFFER_SIZE; //size 10M
+	vem.captureBuf1.size = VIDEO_CAPTURE_BUFFER_SIZE; //size 10M
+	vem.jpegTable.size =  VIDEO_JPEG_TABLE_SIZE; //size 1M
+
+	size = vem.captureBuf0.size + vem.captureBuf1.size + vem.jpegTable.size;
+	VIDEO_ENG_DBG("Allocating memory size: 0x%x\n", size);
+	virt_add = (u32)dma_alloc_coherent(pAstRVAS->pdev, size, &phys_add,
+				  GFP_KERNEL);
+
+	if (!virt_add) {
+		pr_err("Cannot alloc buffer for video engine\n");
+		return -ENOMEM;
+	}
+
+	vem.captureBuf0.phy =  phys_add;
+	vem.captureBuf1.phy =  phys_add + vem.captureBuf0.size;
+	vem.jpegTable.phy = phys_add + vem.captureBuf0.size + vem.captureBuf1.size;
+
+	vem.captureBuf0.pVirt = (void *)virt_add;
+	vem.captureBuf1.pVirt = (void *)(virt_add + vem.captureBuf0.size);
+	vem.jpegTable.pVirt = (void *)(virt_add + vem.captureBuf0.size + vem.captureBuf1.size);
+
+	VIDEO_ENG_DBG("Allocated: phys: %#x\n", phys_add);
+	VIDEO_ENG_DBG("Phy: Buf0:%#x; Buf1:%#x; jpegT:%#x\n", vem.captureBuf0.phy, vem.captureBuf1.phy, vem.jpegTable.phy);
+	VIDEO_ENG_DBG("Virt: Buf0:%#x; Buf1:%#x; JpegT:%#x\n", vem.captureBuf0.pVirt, vem.captureBuf1.pVirt, vem.jpegTable.pVirt);
+
+	return 0;
+
+}
+
+
+
+/************************************************ JPEG ***************************************************************************************/
+static void init_jpeg_table(void)
+{
+	int i = 0;
+	int base = 0;
+	u32 *tlb_table = vem.jpegTable.pVirt;
+
+	//JPEG header default value:
+	for (i = 0; i < 12; i++) {
+		base = (256 * i);
+		tlb_table[base + 0] = 0xE0FFD8FF;
+		tlb_table[base + 1] = 0x464A1000;
+		tlb_table[base + 2] = 0x01004649;
+		tlb_table[base + 3] = 0x60000101;
+		tlb_table[base + 4] = 0x00006000;
+		tlb_table[base + 5] = 0x0F00FEFF;
+		tlb_table[base + 6] = 0x00002D05;
+		tlb_table[base + 7] = 0x00000000;
+		tlb_table[base + 8] = 0x00000000;
+		tlb_table[base + 9] = 0x00DBFF00;
+		tlb_table[base + 44] = 0x081100C0;
+		tlb_table[base + 45] = 0x00000000;
+		tlb_table[base + 47] = 0x03011102;
+		tlb_table[base + 48] = 0xC4FF0111;
+		tlb_table[base + 49] = 0x00001F00;
+		tlb_table[base + 50] = 0x01010501;
+		tlb_table[base + 51] = 0x01010101;
+		tlb_table[base + 52] = 0x00000000;
+		tlb_table[base + 53] = 0x00000000;
+		tlb_table[base + 54] = 0x04030201;
+		tlb_table[base + 55] = 0x08070605;
+		tlb_table[base + 56] = 0xFF0B0A09;
+		tlb_table[base + 57] = 0x10B500C4;
+		tlb_table[base + 58] = 0x03010200;
+		tlb_table[base + 59] = 0x03040203;
+		tlb_table[base + 60] = 0x04040505;
+		tlb_table[base + 61] = 0x7D010000;
+		tlb_table[base + 62] = 0x00030201;
+		tlb_table[base + 63] = 0x12051104;
+		tlb_table[base + 64] = 0x06413121;
+		tlb_table[base + 65] = 0x07615113;
+		tlb_table[base + 66] = 0x32147122;
+		tlb_table[base + 67] = 0x08A19181;
+		tlb_table[base + 68] = 0xC1B14223;
+		tlb_table[base + 69] = 0xF0D15215;
+		tlb_table[base + 70] = 0x72623324;
+		tlb_table[base + 71] = 0x160A0982;
+		tlb_table[base + 72] = 0x1A191817;
+		tlb_table[base + 73] = 0x28272625;
+		tlb_table[base + 74] = 0x35342A29;
+		tlb_table[base + 75] = 0x39383736;
+		tlb_table[base + 76] = 0x4544433A;
+		tlb_table[base + 77] = 0x49484746;
+		tlb_table[base + 78] = 0x5554534A;
+		tlb_table[base + 79] = 0x59585756;
+		tlb_table[base + 80] = 0x6564635A;
+		tlb_table[base + 81] = 0x69686766;
+		tlb_table[base + 82] = 0x7574736A;
+		tlb_table[base + 83] = 0x79787776;
+		tlb_table[base + 84] = 0x8584837A;
+		tlb_table[base + 85] = 0x89888786;
+		tlb_table[base + 86] = 0x9493928A;
+		tlb_table[base + 87] = 0x98979695;
+		tlb_table[base + 88] = 0xA3A29A99;
+		tlb_table[base + 89] = 0xA7A6A5A4;
+		tlb_table[base + 90] = 0xB2AAA9A8;
+		tlb_table[base + 91] = 0xB6B5B4B3;
+		tlb_table[base + 92] = 0xBAB9B8B7;
+		tlb_table[base + 93] = 0xC5C4C3C2;
+		tlb_table[base + 94] = 0xC9C8C7C6;
+		tlb_table[base + 95] = 0xD4D3D2CA;
+		tlb_table[base + 96] = 0xD8D7D6D5;
+		tlb_table[base + 97] = 0xE2E1DAD9;
+		tlb_table[base + 98] = 0xE6E5E4E3;
+		tlb_table[base + 99] = 0xEAE9E8E7;
+		tlb_table[base + 100] = 0xF4F3F2F1;
+		tlb_table[base + 101] = 0xF8F7F6F5;
+		tlb_table[base + 102] = 0xC4FFFAF9;
+		tlb_table[base + 103] = 0x00011F00;
+		tlb_table[base + 104] = 0x01010103;
+		tlb_table[base + 105] = 0x01010101;
+		tlb_table[base + 106] = 0x00000101;
+		tlb_table[base + 107] = 0x00000000;
+		tlb_table[base + 108] = 0x04030201;
+		tlb_table[base + 109] = 0x08070605;
+		tlb_table[base + 110] = 0xFF0B0A09;
+		tlb_table[base + 111] = 0x11B500C4;
+		tlb_table[base + 112] = 0x02010200;
+		tlb_table[base + 113] = 0x04030404;
+		tlb_table[base + 114] = 0x04040507;
+		tlb_table[base + 115] = 0x77020100;
+		tlb_table[base + 116] = 0x03020100;
+		tlb_table[base + 117] = 0x21050411;
+		tlb_table[base + 118] = 0x41120631;
+		tlb_table[base + 119] = 0x71610751;
+		tlb_table[base + 120] = 0x81322213;
+		tlb_table[base + 121] = 0x91421408;
+		tlb_table[base + 122] = 0x09C1B1A1;
+		tlb_table[base + 123] = 0xF0523323;
+		tlb_table[base + 124] = 0xD1726215;
+		tlb_table[base + 125] = 0x3424160A;
+		tlb_table[base + 126] = 0x17F125E1;
+		tlb_table[base + 127] = 0x261A1918;
+		tlb_table[base + 128] = 0x2A292827;
+		tlb_table[base + 129] = 0x38373635;
+		tlb_table[base + 130] = 0x44433A39;
+		tlb_table[base + 131] = 0x48474645;
+		tlb_table[base + 132] = 0x54534A49;
+		tlb_table[base + 133] = 0x58575655;
+		tlb_table[base + 134] = 0x64635A59;
+		tlb_table[base + 135] = 0x68676665;
+		tlb_table[base + 136] = 0x74736A69;
+		tlb_table[base + 137] = 0x78777675;
+		tlb_table[base + 138] = 0x83827A79;
+		tlb_table[base + 139] = 0x87868584;
+		tlb_table[base + 140] = 0x928A8988;
+		tlb_table[base + 141] = 0x96959493;
+		tlb_table[base + 142] = 0x9A999897;
+		tlb_table[base + 143] = 0xA5A4A3A2;
+		tlb_table[base + 144] = 0xA9A8A7A6;
+		tlb_table[base + 145] = 0xB4B3B2AA;
+		tlb_table[base + 146] = 0xB8B7B6B5;
+		tlb_table[base + 147] = 0xC3C2BAB9;
+		tlb_table[base + 148] = 0xC7C6C5C4;
+		tlb_table[base + 149] = 0xD2CAC9C8;
+		tlb_table[base + 150] = 0xD6D5D4D3;
+		tlb_table[base + 151] = 0xDAD9D8D7;
+		tlb_table[base + 152] = 0xE5E4E3E2;
+		tlb_table[base + 153] = 0xE9E8E7E6;
+		tlb_table[base + 154] = 0xF4F3F2EA;
+		tlb_table[base + 155] = 0xF8F7F6F5;
+		tlb_table[base + 156] = 0xDAFFFAF9;
+		tlb_table[base + 157] = 0x01030C00;
+		tlb_table[base + 158] = 0x03110200;
+		tlb_table[base + 159] = 0x003F0011;
+
+		//Table 0
+		if (i == 0) {
+			tlb_table[base + 10] = 0x0D140043;
+			tlb_table[base + 11] = 0x0C0F110F;
+			tlb_table[base + 12] = 0x11101114;
+			tlb_table[base + 13] = 0x17141516;
+			tlb_table[base + 14] = 0x1E20321E;
+			tlb_table[base + 15] = 0x3D1E1B1B;
+			tlb_table[base + 16] = 0x32242E2B;
+			tlb_table[base + 17] = 0x4B4C3F48;
+			tlb_table[base + 18] = 0x44463F47;
+			tlb_table[base + 19] = 0x61735A50;
+			tlb_table[base + 20] = 0x566C5550;
+			tlb_table[base + 21] = 0x88644644;
+			tlb_table[base + 22] = 0x7A766C65;
+			tlb_table[base + 23] = 0x4D808280;
+			tlb_table[base + 24] = 0x8C978D60;
+			tlb_table[base + 25] = 0x7E73967D;
+			tlb_table[base + 26] = 0xDBFF7B80;
+			tlb_table[base + 27] = 0x1F014300;
+			tlb_table[base + 28] = 0x272D2121;
+			tlb_table[base + 29] = 0x3030582D;
+			tlb_table[base + 30] = 0x697BB958;
+			tlb_table[base + 31] = 0xB8B9B97B;
+			tlb_table[base + 32] = 0xB9B8A6A6;
+			tlb_table[base + 33] = 0xB9B9B9B9;
+			tlb_table[base + 34] = 0xB9B9B9B9;
+			tlb_table[base + 35] = 0xB9B9B9B9;
+			tlb_table[base + 36] = 0xB9B9B9B9;
+			tlb_table[base + 37] = 0xB9B9B9B9;
+			tlb_table[base + 38] = 0xB9B9B9B9;
+			tlb_table[base + 39] = 0xB9B9B9B9;
+			tlb_table[base + 40] = 0xB9B9B9B9;
+			tlb_table[base + 41] = 0xB9B9B9B9;
+			tlb_table[base + 42] = 0xB9B9B9B9;
+			tlb_table[base + 43] = 0xFFB9B9B9;
+		}
+		//Table 1
+		if (i == 1) {
+			tlb_table[base + 10] = 0x0C110043;
+			tlb_table[base + 11] = 0x0A0D0F0D;
+			tlb_table[base + 12] = 0x0F0E0F11;
+			tlb_table[base + 13] = 0x14111213;
+			tlb_table[base + 14] = 0x1A1C2B1A;
+			tlb_table[base + 15] = 0x351A1818;
+			tlb_table[base + 16] = 0x2B1F2826;
+			tlb_table[base + 17] = 0x4142373F;
+			tlb_table[base + 18] = 0x3C3D373E;
+			tlb_table[base + 19] = 0x55644E46;
+			tlb_table[base + 20] = 0x4B5F4A46;
+			tlb_table[base + 21] = 0x77573D3C;
+			tlb_table[base + 22] = 0x6B675F58;
+			tlb_table[base + 23] = 0x43707170;
+			tlb_table[base + 24] = 0x7A847B54;
+			tlb_table[base + 25] = 0x6E64836D;
+			tlb_table[base + 26] = 0xDBFF6C70;
+			tlb_table[base + 27] = 0x1B014300;
+			tlb_table[base + 28] = 0x22271D1D;
+			tlb_table[base + 29] = 0x2A2A4C27;
+			tlb_table[base + 30] = 0x5B6BA04C;
+			tlb_table[base + 31] = 0xA0A0A06B;
+			tlb_table[base + 32] = 0xA0A0A0A0;
+			tlb_table[base + 33] = 0xA0A0A0A0;
+			tlb_table[base + 34] = 0xA0A0A0A0;
+			tlb_table[base + 35] = 0xA0A0A0A0;
+			tlb_table[base + 36] = 0xA0A0A0A0;
+			tlb_table[base + 37] = 0xA0A0A0A0;
+			tlb_table[base + 38] = 0xA0A0A0A0;
+			tlb_table[base + 39] = 0xA0A0A0A0;
+			tlb_table[base + 40] = 0xA0A0A0A0;
+			tlb_table[base + 41] = 0xA0A0A0A0;
+			tlb_table[base + 42] = 0xA0A0A0A0;
+			tlb_table[base + 43] = 0xFFA0A0A0;
+		}
+		//Table 2
+		if (i == 2) {
+			tlb_table[base + 10] = 0x090E0043;
+			tlb_table[base + 11] = 0x090A0C0A;
+			tlb_table[base + 12] = 0x0C0B0C0E;
+			tlb_table[base + 13] = 0x110E0F10;
+			tlb_table[base + 14] = 0x15172415;
+			tlb_table[base + 15] = 0x2C151313;
+			tlb_table[base + 16] = 0x241A211F;
+			tlb_table[base + 17] = 0x36372E34;
+			tlb_table[base + 18] = 0x31322E33;
+			tlb_table[base + 19] = 0x4653413A;
+			tlb_table[base + 20] = 0x3E4E3D3A;
+			tlb_table[base + 21] = 0x62483231;
+			tlb_table[base + 22] = 0x58564E49;
+			tlb_table[base + 23] = 0x385D5E5D;
+			tlb_table[base + 24] = 0x656D6645;
+			tlb_table[base + 25] = 0x5B536C5A;
+			tlb_table[base + 26] = 0xDBFF595D;
+			tlb_table[base + 27] = 0x16014300;
+			tlb_table[base + 28] = 0x1C201818;
+			tlb_table[base + 29] = 0x22223F20;
+			tlb_table[base + 30] = 0x4B58853F;
+			tlb_table[base + 31] = 0x85858558;
+			tlb_table[base + 32] = 0x85858585;
+			tlb_table[base + 33] = 0x85858585;
+			tlb_table[base + 34] = 0x85858585;
+			tlb_table[base + 35] = 0x85858585;
+			tlb_table[base + 36] = 0x85858585;
+			tlb_table[base + 37] = 0x85858585;
+			tlb_table[base + 38] = 0x85858585;
+			tlb_table[base + 39] = 0x85858585;
+			tlb_table[base + 40] = 0x85858585;
+			tlb_table[base + 41] = 0x85858585;
+			tlb_table[base + 42] = 0x85858585;
+			tlb_table[base + 43] = 0xFF858585;
+		}
+		//Table 3
+		if (i == 3) {
+			tlb_table[base + 10] = 0x070B0043;
+			tlb_table[base + 11] = 0x07080A08;
+			tlb_table[base + 12] = 0x0A090A0B;
+			tlb_table[base + 13] = 0x0D0B0C0C;
+			tlb_table[base + 14] = 0x11121C11;
+			tlb_table[base + 15] = 0x23110F0F;
+			tlb_table[base + 16] = 0x1C141A19;
+			tlb_table[base + 17] = 0x2B2B2429;
+			tlb_table[base + 18] = 0x27282428;
+			tlb_table[base + 19] = 0x3842332E;
+			tlb_table[base + 20] = 0x313E302E;
+			tlb_table[base + 21] = 0x4E392827;
+			tlb_table[base + 22] = 0x46443E3A;
+			tlb_table[base + 23] = 0x2C4A4A4A;
+			tlb_table[base + 24] = 0x50565137;
+			tlb_table[base + 25] = 0x48425647;
+			tlb_table[base + 26] = 0xDBFF474A;
+			tlb_table[base + 27] = 0x12014300;
+			tlb_table[base + 28] = 0x161A1313;
+			tlb_table[base + 29] = 0x1C1C331A;
+			tlb_table[base + 30] = 0x3D486C33;
+			tlb_table[base + 31] = 0x6C6C6C48;
+			tlb_table[base + 32] = 0x6C6C6C6C;
+			tlb_table[base + 33] = 0x6C6C6C6C;
+			tlb_table[base + 34] = 0x6C6C6C6C;
+			tlb_table[base + 35] = 0x6C6C6C6C;
+			tlb_table[base + 36] = 0x6C6C6C6C;
+			tlb_table[base + 37] = 0x6C6C6C6C;
+			tlb_table[base + 38] = 0x6C6C6C6C;
+			tlb_table[base + 39] = 0x6C6C6C6C;
+			tlb_table[base + 40] = 0x6C6C6C6C;
+			tlb_table[base + 41] = 0x6C6C6C6C;
+			tlb_table[base + 42] = 0x6C6C6C6C;
+			tlb_table[base + 43] = 0xFF6C6C6C;
+		}
+		//Table 4
+		if (i == 4) {
+			tlb_table[base + 10] = 0x06090043;
+			tlb_table[base + 11] = 0x05060706;
+			tlb_table[base + 12] = 0x07070709;
+			tlb_table[base + 13] = 0x0A09090A;
+			tlb_table[base + 14] = 0x0D0E160D;
+			tlb_table[base + 15] = 0x1B0D0C0C;
+			tlb_table[base + 16] = 0x16101413;
+			tlb_table[base + 17] = 0x21221C20;
+			tlb_table[base + 18] = 0x1E1F1C20;
+			tlb_table[base + 19] = 0x2B332824;
+			tlb_table[base + 20] = 0x26302624;
+			tlb_table[base + 21] = 0x3D2D1F1E;
+			tlb_table[base + 22] = 0x3735302D;
+			tlb_table[base + 23] = 0x22393A39;
+			tlb_table[base + 24] = 0x3F443F2B;
+			tlb_table[base + 25] = 0x38334338;
+			tlb_table[base + 26] = 0xDBFF3739;
+			tlb_table[base + 27] = 0x0D014300;
+			tlb_table[base + 28] = 0x11130E0E;
+			tlb_table[base + 29] = 0x15152613;
+			tlb_table[base + 30] = 0x2D355026;
+			tlb_table[base + 31] = 0x50505035;
+			tlb_table[base + 32] = 0x50505050;
+			tlb_table[base + 33] = 0x50505050;
+			tlb_table[base + 34] = 0x50505050;
+			tlb_table[base + 35] = 0x50505050;
+			tlb_table[base + 36] = 0x50505050;
+			tlb_table[base + 37] = 0x50505050;
+			tlb_table[base + 38] = 0x50505050;
+			tlb_table[base + 39] = 0x50505050;
+			tlb_table[base + 40] = 0x50505050;
+			tlb_table[base + 41] = 0x50505050;
+			tlb_table[base + 42] = 0x50505050;
+			tlb_table[base + 43] = 0xFF505050;
+		}
+		//Table 5
+		if (i == 5) {
+			tlb_table[base + 10] = 0x04060043;
+			tlb_table[base + 11] = 0x03040504;
+			tlb_table[base + 12] = 0x05040506;
+			tlb_table[base + 13] = 0x07060606;
+			tlb_table[base + 14] = 0x09090F09;
+			tlb_table[base + 15] = 0x12090808;
+			tlb_table[base + 16] = 0x0F0A0D0D;
+			tlb_table[base + 17] = 0x16161315;
+			tlb_table[base + 18] = 0x14151315;
+			tlb_table[base + 19] = 0x1D221B18;
+			tlb_table[base + 20] = 0x19201918;
+			tlb_table[base + 21] = 0x281E1514;
+			tlb_table[base + 22] = 0x2423201E;
+			tlb_table[base + 23] = 0x17262726;
+			tlb_table[base + 24] = 0x2A2D2A1C;
+			tlb_table[base + 25] = 0x25222D25;
+			tlb_table[base + 26] = 0xDBFF2526;
+			tlb_table[base + 27] = 0x09014300;
+			tlb_table[base + 28] = 0x0B0D0A0A;
+			tlb_table[base + 29] = 0x0E0E1A0D;
+			tlb_table[base + 30] = 0x1F25371A;
+			tlb_table[base + 31] = 0x37373725;
+			tlb_table[base + 32] = 0x37373737;
+			tlb_table[base + 33] = 0x37373737;
+			tlb_table[base + 34] = 0x37373737;
+			tlb_table[base + 35] = 0x37373737;
+			tlb_table[base + 36] = 0x37373737;
+			tlb_table[base + 37] = 0x37373737;
+			tlb_table[base + 38] = 0x37373737;
+			tlb_table[base + 39] = 0x37373737;
+			tlb_table[base + 40] = 0x37373737;
+			tlb_table[base + 41] = 0x37373737;
+			tlb_table[base + 42] = 0x37373737;
+			tlb_table[base + 43] = 0xFF373737;
+		}
+		//Table 6
+		if (i == 6) {
+			tlb_table[base + 10] = 0x02030043;
+			tlb_table[base + 11] = 0x01020202;
+			tlb_table[base + 12] = 0x02020203;
+			tlb_table[base + 13] = 0x03030303;
+			tlb_table[base + 14] = 0x04040704;
+			tlb_table[base + 15] = 0x09040404;
+			tlb_table[base + 16] = 0x07050606;
+			tlb_table[base + 17] = 0x0B0B090A;
+			tlb_table[base + 18] = 0x0A0A090A;
+			tlb_table[base + 19] = 0x0E110D0C;
+			tlb_table[base + 20] = 0x0C100C0C;
+			tlb_table[base + 21] = 0x140F0A0A;
+			tlb_table[base + 22] = 0x1211100F;
+			tlb_table[base + 23] = 0x0B131313;
+			tlb_table[base + 24] = 0x1516150E;
+			tlb_table[base + 25] = 0x12111612;
+			tlb_table[base + 26] = 0xDBFF1213;
+			tlb_table[base + 27] = 0x04014300;
+			tlb_table[base + 28] = 0x05060505;
+			tlb_table[base + 29] = 0x07070D06;
+			tlb_table[base + 30] = 0x0F121B0D;
+			tlb_table[base + 31] = 0x1B1B1B12;
+			tlb_table[base + 32] = 0x1B1B1B1B;
+			tlb_table[base + 33] = 0x1B1B1B1B;
+			tlb_table[base + 34] = 0x1B1B1B1B;
+			tlb_table[base + 35] = 0x1B1B1B1B;
+			tlb_table[base + 36] = 0x1B1B1B1B;
+			tlb_table[base + 37] = 0x1B1B1B1B;
+			tlb_table[base + 38] = 0x1B1B1B1B;
+			tlb_table[base + 39] = 0x1B1B1B1B;
+			tlb_table[base + 40] = 0x1B1B1B1B;
+			tlb_table[base + 41] = 0x1B1B1B1B;
+			tlb_table[base + 42] = 0x1B1B1B1B;
+			tlb_table[base + 43] = 0xFF1B1B1B;
+		}
+		//Table 7
+		if (i == 7) {
+			tlb_table[base + 10] = 0x01020043;
+			tlb_table[base + 11] = 0x01010101;
+			tlb_table[base + 12] = 0x01010102;
+			tlb_table[base + 13] = 0x02020202;
+			tlb_table[base + 14] = 0x03030503;
+			tlb_table[base + 15] = 0x06030202;
+			tlb_table[base + 16] = 0x05030404;
+			tlb_table[base + 17] = 0x07070607;
+			tlb_table[base + 18] = 0x06070607;
+			tlb_table[base + 19] = 0x090B0908;
+			tlb_table[base + 20] = 0x080A0808;
+			tlb_table[base + 21] = 0x0D0A0706;
+			tlb_table[base + 22] = 0x0C0B0A0A;
+			tlb_table[base + 23] = 0x070C0D0C;
+			tlb_table[base + 24] = 0x0E0F0E09;
+			tlb_table[base + 25] = 0x0C0B0F0C;
+			tlb_table[base + 26] = 0xDBFF0C0C;
+			tlb_table[base + 27] = 0x03014300;
+			tlb_table[base + 28] = 0x03040303;
+			tlb_table[base + 29] = 0x04040804;
+			tlb_table[base + 30] = 0x0A0C1208;
+			tlb_table[base + 31] = 0x1212120C;
+			tlb_table[base + 32] = 0x12121212;
+			tlb_table[base + 33] = 0x12121212;
+			tlb_table[base + 34] = 0x12121212;
+			tlb_table[base + 35] = 0x12121212;
+			tlb_table[base + 36] = 0x12121212;
+			tlb_table[base + 37] = 0x12121212;
+			tlb_table[base + 38] = 0x12121212;
+			tlb_table[base + 39] = 0x12121212;
+			tlb_table[base + 40] = 0x12121212;
+			tlb_table[base + 41] = 0x12121212;
+			tlb_table[base + 42] = 0x12121212;
+			tlb_table[base + 43] = 0xFF121212;
+		}
+		//Table 8
+		if (i == 8) {
+			tlb_table[base + 10] = 0x01020043;
+			tlb_table[base + 11] = 0x01010101;
+			tlb_table[base + 12] = 0x01010102;
+			tlb_table[base + 13] = 0x02020202;
+			tlb_table[base + 14] = 0x03030503;
+			tlb_table[base + 15] = 0x06030202;
+			tlb_table[base + 16] = 0x05030404;
+			tlb_table[base + 17] = 0x07070607;
+			tlb_table[base + 18] = 0x06070607;
+			tlb_table[base + 19] = 0x090B0908;
+			tlb_table[base + 20] = 0x080A0808;
+			tlb_table[base + 21] = 0x0D0A0706;
+			tlb_table[base + 22] = 0x0C0B0A0A;
+			tlb_table[base + 23] = 0x070C0D0C;
+			tlb_table[base + 24] = 0x0E0F0E09;
+			tlb_table[base + 25] = 0x0C0B0F0C;
+			tlb_table[base + 26] = 0xDBFF0C0C;
+			tlb_table[base + 27] = 0x02014300;
+			tlb_table[base + 28] = 0x03030202;
+			tlb_table[base + 29] = 0x04040703;
+			tlb_table[base + 30] = 0x080A0F07;
+			tlb_table[base + 31] = 0x0F0F0F0A;
+			tlb_table[base + 32] = 0x0F0F0F0F;
+			tlb_table[base + 33] = 0x0F0F0F0F;
+			tlb_table[base + 34] = 0x0F0F0F0F;
+			tlb_table[base + 35] = 0x0F0F0F0F;
+			tlb_table[base + 36] = 0x0F0F0F0F;
+			tlb_table[base + 37] = 0x0F0F0F0F;
+			tlb_table[base + 38] = 0x0F0F0F0F;
+			tlb_table[base + 39] = 0x0F0F0F0F;
+			tlb_table[base + 40] = 0x0F0F0F0F;
+			tlb_table[base + 41] = 0x0F0F0F0F;
+			tlb_table[base + 42] = 0x0F0F0F0F;
+			tlb_table[base + 43] = 0xFF0F0F0F;
+		}
+		//Table 9
+		if (i == 9) {
+			tlb_table[base + 10] = 0x01010043;
+			tlb_table[base + 11] = 0x01010101;
+			tlb_table[base + 12] = 0x01010101;
+			tlb_table[base + 13] = 0x01010101;
+			tlb_table[base + 14] = 0x02020302;
+			tlb_table[base + 15] = 0x04020202;
+			tlb_table[base + 16] = 0x03020303;
+			tlb_table[base + 17] = 0x05050405;
+			tlb_table[base + 18] = 0x05050405;
+			tlb_table[base + 19] = 0x07080606;
+			tlb_table[base + 20] = 0x06080606;
+			tlb_table[base + 21] = 0x0A070505;
+			tlb_table[base + 22] = 0x09080807;
+			tlb_table[base + 23] = 0x05090909;
+			tlb_table[base + 24] = 0x0A0B0A07;
+			tlb_table[base + 25] = 0x09080B09;
+			tlb_table[base + 26] = 0xDBFF0909;
+			tlb_table[base + 27] = 0x02014300;
+			tlb_table[base + 28] = 0x02030202;
+			tlb_table[base + 29] = 0x03030503;
+			tlb_table[base + 30] = 0x07080C05;
+			tlb_table[base + 31] = 0x0C0C0C08;
+			tlb_table[base + 32] = 0x0C0C0C0C;
+			tlb_table[base + 33] = 0x0C0C0C0C;
+			tlb_table[base + 34] = 0x0C0C0C0C;
+			tlb_table[base + 35] = 0x0C0C0C0C;
+			tlb_table[base + 36] = 0x0C0C0C0C;
+			tlb_table[base + 37] = 0x0C0C0C0C;
+			tlb_table[base + 38] = 0x0C0C0C0C;
+			tlb_table[base + 39] = 0x0C0C0C0C;
+			tlb_table[base + 40] = 0x0C0C0C0C;
+			tlb_table[base + 41] = 0x0C0C0C0C;
+			tlb_table[base + 42] = 0x0C0C0C0C;
+			tlb_table[base + 43] = 0xFF0C0C0C;
+		}
+		//Table 10
+		if (i == 10) {
+			tlb_table[base + 10] = 0x01010043;
+			tlb_table[base + 11] = 0x01010101;
+			tlb_table[base + 12] = 0x01010101;
+			tlb_table[base + 13] = 0x01010101;
+			tlb_table[base + 14] = 0x01010201;
+			tlb_table[base + 15] = 0x03010101;
+			tlb_table[base + 16] = 0x02010202;
+			tlb_table[base + 17] = 0x03030303;
+			tlb_table[base + 18] = 0x03030303;
+			tlb_table[base + 19] = 0x04050404;
+			tlb_table[base + 20] = 0x04050404;
+			tlb_table[base + 21] = 0x06050303;
+			tlb_table[base + 22] = 0x06050505;
+			tlb_table[base + 23] = 0x03060606;
+			tlb_table[base + 24] = 0x07070704;
+			tlb_table[base + 25] = 0x06050706;
+			tlb_table[base + 26] = 0xDBFF0606;
+			tlb_table[base + 27] = 0x01014300;
+			tlb_table[base + 28] = 0x01020101;
+			tlb_table[base + 29] = 0x02020402;
+			tlb_table[base + 30] = 0x05060904;
+			tlb_table[base + 31] = 0x09090906;
+			tlb_table[base + 32] = 0x09090909;
+			tlb_table[base + 33] = 0x09090909;
+			tlb_table[base + 34] = 0x09090909;
+			tlb_table[base + 35] = 0x09090909;
+			tlb_table[base + 36] = 0x09090909;
+			tlb_table[base + 37] = 0x09090909;
+			tlb_table[base + 38] = 0x09090909;
+			tlb_table[base + 39] = 0x09090909;
+			tlb_table[base + 40] = 0x09090909;
+			tlb_table[base + 41] = 0x09090909;
+			tlb_table[base + 42] = 0x09090909;
+			tlb_table[base + 43] = 0xFF090909;
+		}
+		//Table 11
+		if (i == 11) {
+			tlb_table[base + 10] = 0x01010043;
+			tlb_table[base + 11] = 0x01010101;
+			tlb_table[base + 12] = 0x01010101;
+			tlb_table[base + 13] = 0x01010101;
+			tlb_table[base + 14] = 0x01010101;
+			tlb_table[base + 15] = 0x01010101;
+			tlb_table[base + 16] = 0x01010101;
+			tlb_table[base + 17] = 0x01010101;
+			tlb_table[base + 18] = 0x01010101;
+			tlb_table[base + 19] = 0x02020202;
+			tlb_table[base + 20] = 0x02020202;
+			tlb_table[base + 21] = 0x03020101;
+			tlb_table[base + 22] = 0x03020202;
+			tlb_table[base + 23] = 0x01030303;
+			tlb_table[base + 24] = 0x03030302;
+			tlb_table[base + 25] = 0x03020303;
+			tlb_table[base + 26] = 0xDBFF0403;
+			tlb_table[base + 27] = 0x01014300;
+			tlb_table[base + 28] = 0x01010101;
+			tlb_table[base + 29] = 0x01010201;
+			tlb_table[base + 30] = 0x03040602;
+			tlb_table[base + 31] = 0x06060604;
+			tlb_table[base + 32] = 0x06060606;
+			tlb_table[base + 33] = 0x06060606;
+			tlb_table[base + 34] = 0x06060606;
+			tlb_table[base + 35] = 0x06060606;
+			tlb_table[base + 36] = 0x06060606;
+			tlb_table[base + 37] = 0x06060606;
+			tlb_table[base + 38] = 0x06060606;
+			tlb_table[base + 39] = 0x06060606;
+			tlb_table[base + 40] = 0x06060606;
+			tlb_table[base + 41] = 0x06060606;
+			tlb_table[base + 42] = 0x06060606;
+			tlb_table[base + 43] = 0xFF060606;
+		}
+	}
+
+
+}
+
+static inline void
+video_write(struct AstRVAS *pAstRVAS, u32 val, u32 reg)
+{
+	VIDEO_ENG_DBG("write offset: %x, val: %x\n", reg, val);
+	//Video is lock after reset, need always unlock
+	//unlock
+	writel(VIDEO_PROTECT_UNLOCK, (void *)pAstRVAS->video_reg_base);
+	writel(val, (void *)(pAstRVAS->video_reg_base + reg));
+
+}
+
+static inline u32
+video_read(struct AstRVAS *pAstRVAS, u32 reg)
+{
+	u32 val = readl((void *)(pAstRVAS->video_reg_base + reg));
+
+	VIDEO_ENG_DBG("read offset: %x, val: %x\n", reg, val);
+	return val;
+}
+
diff --git a/drivers/soc/aspeed/rvas/video_engine.h b/drivers/soc/aspeed/rvas/video_engine.h
new file mode 100644
index 000000000000..0b6670816fa3
--- /dev/null
+++ b/drivers/soc/aspeed/rvas/video_engine.h
@@ -0,0 +1,293 @@
+/* SPDX-License-Identifier: GPL-2.0+ */
+/*
+ * File Name     : video_engines.h
+ * Description   : AST2600 video  engines
+ *
+ * Copyright (C) 2019-2021 ASPEED Technology Inc. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ */
+#ifndef __VIDEO_ENGINE_H__
+#define __VIDEO_ENGINE_H__
+
+#include "video_ioctl.h"
+#include "hardware_engines.h"
+
+
+#define VIDEO_STREAM_BUFFER_SIZE	(0x400000) //4M
+#define VIDEO_CAPTURE_BUFFER_SIZE	(0xA00000) //10M
+#define VIDEO_JPEG_TABLE_SIZE		(0x100000) //1M
+
+#define SCU_VIDEO_ENGINE_BIT						BIT(6)
+#define SCU_VIDEO_CAPTURE_STOP_CLOCK_BIT			BIT(3)
+#define SCU_VIDEO_ENGINE_STOP_CLOCK_BIT				BIT(1)
+/***********************************************************************/
+/* Register for VIDEO */
+#define AST_VIDEO_PROTECT			0x000		/*	protection key register	*/
+#define AST_VIDEO_SEQ_CTRL			0x004		/*	Video Sequence Control register	*/
+#define AST_VIDEO_PASS_CTRL		0x008		/*	Video Pass 1 Control register	*/
+
+//VR008[5]=1
+#define AST_VIDEO_DIRECT_BASE		0x00C		/*	Video Direct Frame buffer mode control Register VR008[5]=1 */
+#define AST_VIDEO_DIRECT_CTRL		0x010		/*	Video Direct Frame buffer mode control Register VR008[5]=1 */
+
+//VR008[5]=0
+#define AST_VIDEO_TIMING_H			0x00C		/*	Video Timing Generation Setting Register */
+#define AST_VIDEO_TIMING_V			0x010		/*	Video Timing Generation Setting Register */
+#define AST_VIDEO_SCAL_FACTOR		0x014		/*	Video Scaling Factor Register */
+
+#define AST_VIDEO_SCALING0		0x018		/*	Video Scaling Filter Parameter Register #0 */
+#define AST_VIDEO_SCALING1		0x01C		/*	Video Scaling Filter Parameter Register #1 */
+#define AST_VIDEO_SCALING2		0x020		/*	Video Scaling Filter Parameter Register #2 */
+#define AST_VIDEO_SCALING3		0x024		/*	Video Scaling Filter Parameter Register #3 */
+
+
+#define AST_VIDEO_BCD_CTRL			0x02C		/*	Video BCD Control Register */
+#define AST_VIDEO_CAPTURE_WIN		0x030		/*	 Video Capturing Window Setting Register */
+#define AST_VIDEO_COMPRESS_WIN	0x034		/*	 Video Compression Window Setting Register */
+
+
+
+#define AST_VIDEO_COMPRESS_PRO	0x038		/* Video Compression Stream Buffer Processing Offset Register */
+#define AST_VIDEO_COMPRESS_READ	0x03C		/* Video Compression Stream Buffer Read Offset Register */
+
+#define AST_VIDEO_JPEG_HEADER_BUFF	0x040		/*	Video Based Address of JPEG Header Buffer Register */
+#define AST_VIDEO_SOURCE_BUFF0		0x044		/*	Video Based Address of Video Source Buffer #1 Register */
+#define AST_VIDEO_SOURCE_SCAN_LINE	0x048		/*	Video Scan Line Offset of Video Source Buffer Register */
+#define AST_VIDEO_SOURCE_BUFF1		0x04C		/*	Video Based Address of Video Source Buffer #2 Register */
+#define AST_VIDEO_BCD_BUFF				0x050		/*	Video Base Address of BCD Flag Buffer Register */
+#define AST_VIDEO_STREAM_BUFF			0x054		/*	Video Base Address of Compressed Video Stream Buffer Register */
+#define AST_VIDEO_STREAM_SIZE			0x058		/*	Video Stream Buffer Size Register */
+
+
+#define AST_VIDEO_COMPRESS_CTRL				0x060		/* Video Compression Control Register */
+
+#define AST_VIDEO_COMPRESS_DATA_COUNT		0x070		/* Video Total Size of Compressed Video Stream Read Back Register */
+#define AST_VIDEO_COMPRESS_BLOCK_COUNT		0x074		/* Video Total Number of Compressed Video Block Read Back Register */
+#define AST_VIDEO_COMPRESS_FRAME_END		0x078		/* Video Frame-end offset of compressed video stream buffer read back Register */
+
+
+#define AST_VIDEO_CTRL			0x300		/* Video Control Register */
+#define AST_VIDEO_INT_EN		0x304		/* Video interrupt Enable */
+#define AST_VIDEO_INT_STS		0x308		/* Video interrupt status */
+#define AST_VIDEO_MODE_DETECT	0x30C		/* Video Mode Detection Parameter Register */
+
+#define AST_VIDEO_CRC1			0x320		/* Primary CRC Parameter Register */
+#define AST_VIDEO_CRC2			0x324		/* Second CRC Parameter Register */
+#define AST_VIDEO_DATA_TRUNCA	0x328		/* Video Data Truncation Register */
+
+#define AST_VIDEO_E_SCRATCH_34C	0x34C		/* Video Scratch Remap Read Back */
+#define AST_VIDEO_E_SCRATCH_350	0x350		/* Video Scratch Remap Read Back */
+#define AST_VIDEO_E_SCRATCH_354	0x354		/* Video Scratch Remap Read Back */
+
+//multi jpeg
+#define AST_VIDEO_ENCRYPT_SRAM		0x400		/* Video RC4/AES128 Encryption Key Register #0 ~ #63 */
+#define AST_VIDEO_MULTI_JPEG_SRAM	(AST_VIDEO_ENCRYPT_SRAM)		/* Multi JPEG registers */
+
+#define REG_32_BIT_SZ_IN_BYTES (sizeof(u32))
+
+#define SET_FRAME_W_H(w, h) ((((u32) (h)) & 0x1fff) | ((((u32) (w)) & 0x1fff) << 13))
+#define SET_FRAME_START_ADDR(addr) ((addr) & 0x7fffff80)
+
+/////////////////////////////////////////////////////////////////////////////
+
+/*	AST_VIDEO_PROTECT: 0x000  - protection key register */
+#define VIDEO_PROTECT_UNLOCK				0x1A038AA8
+
+/*	AST_VIDEO_SEQ_CTRL		0x004		Video Sequence Control register	*/
+#define VIDEO_HALT_ENG_STS					(1 << 21)
+#define VIDEO_COMPRESS_BUSY				(1 << 18)
+#define VIDEO_CAPTURE_BUSY					(1 << 16)
+#define VIDEO_HALT_ENG_TRIGGER			(1 << 12)
+#define VIDEO_COMPRESS_FORMAT_MASK		(3 << 10)
+#define VIDEO_GET_COMPRESS_FORMAT(x)		((x >> 10) & 0x3)   // 0 YUV444
+#define VIDEO_COMPRESS_FORMAT(x)			(x << 10)	// 0 YUV444
+#define YUV420		1
+
+#define G5_VIDEO_COMPRESS_JPEG_MODE		(1 << 13)
+#define VIDEO_YUV2RGB_DITHER_EN			(1 << 8)
+
+#define VIDEO_COMPRESS_JPEG_MODE			(1 << 8)
+
+//if bit 0 : 1
+#define VIDEO_INPUT_MODE_CHG_WDT			(1 << 7)
+#define VIDEO_INSERT_FULL_COMPRESS		(1 << 6)
+#define VIDEO_AUTO_COMPRESS				(1 << 5)
+#define VIDEO_COMPRESS_TRIGGER			(1 << 4)
+#define VIDEO_CAPTURE_MULTI_FRAME		(1 << 3)
+#define VIDEO_COMPRESS_FORCE_IDLE		(1 << 2)
+#define VIDEO_CAPTURE_TRIGGER				(1 << 1)
+#define VIDEO_DETECT_TRIGGER				(1 << 0)
+
+
+#define VIDEO_HALT_ENG_RB					(1 << 21)
+
+#define VIDEO_ABCD_CHG_EN					(1 << 1)
+#define VIDEO_BCD_CHG_EN					(1)
+
+/*	AST_VIDEO_PASS_CTRL			0x008		Video Pass1 Control register	*/
+#define G6_VIDEO_MULTI_JPEG_FLAG_MODE	(1 << 31)
+#define G6_VIDEO_MULTI_JPEG_MODE			(1 << 30)
+#define G6_VIDEO_JPEG__COUNT(x)			((x) << 24)
+#define G6_VIDEO_FRAME_CT_MASK			(0x3f << 24)
+
+/*	AST_VIDEO_DIRECT_CTRL	0x010		Video Direct Frame buffer mode control Register VR008[5]=1 */
+#define VIDEO_FETCH_TIMING(x)			((x) << 16)
+#define VIDEO_FETCH_LINE_OFFSET(x)		(x & 0xffff)
+
+//x * source frame rate / 60
+#define VIDEO_FRAME_RATE_CTRL(x)			(x << 16)
+#define VIDEO_HSYNC_POLARITY_CTRL		(1 << 15)
+#define VIDEO_INTERLANCE_MODE				(1 << 14)
+#define VIDEO_DUAL_EDGE_MODE				(1 << 13)	//0 : Single edage
+#define VIDEO_18BIT_SINGLE_EDGE			(1 << 12)	//0: 24bits
+#define VIDEO_DVO_INPUT_DELAY_MASK		(7 << 9)
+#define VIDEO_DVO_INPUT_DELAY(x)			(x << 9) //0 : no delay , 1: 1ns, 2: 2ns, 3:3ns, 4: inversed clock but no delay
+// if bit 5 : 0
+#define VIDEO_HW_CURSOR_DIS				(1 << 8)
+// if bit 5 : 1
+#define VIDEO_AUTO_FETCH					(1 << 8)	//
+#define VIDEO_CAPTURE_FORMATE_MASK		(3 << 6)
+
+#define VIDEO_SET_CAPTURE_FORMAT(x)		(x << 6)
+#define JPEG_MODE		1
+#define RGB_MODE		2
+#define GRAY_MODE		3
+#define VIDEO_DIRECT_FETCH				(1 << 5)
+// if bit 5 : 0
+#define VIDEO_INTERNAL_DE				(1 << 4)
+#define VIDEO_EXT_ADC_ATTRIBUTE		(1 << 3)
+
+
+/*	 AST_VIDEO_CAPTURE_WIN	0x030		Video Capturing Window Setting Register */
+#define VIDEO_CAPTURE_V(x)				(x & 0x7ff)
+#define VIDEO_CAPTURE_H(x)				((x & 0x7ff) << 16)
+
+/*	 AST_VIDEO_COMPRESS_WIN	0x034		Video Compression Window Setting Register */
+#define VIDEO_COMPRESS_V(x)			(x & 0x7ff)
+#define VIDEO_GET_COMPRESS_V(x)		(x & 0x7ff)
+#define VIDEO_COMPRESS_H(x)			((x & 0x7ff) << 16)
+#define VIDEO_GET_COMPRESS_H(x)		((x >> 16) & 0x7ff)
+
+/*	AST_VIDEO_STREAM_SIZE	0x058		Video Stream Buffer Size Register */
+#define VIDEO_STREAM_PKT_N(x)			(x << 3)
+#define STREAM_4_PKTS		0
+#define STREAM_8_PKTS		1
+#define STREAM_16_PKTS		2
+#define STREAM_32_PKTS		3
+#define STREAM_64_PKTS		4
+#define STREAM_128_PKTS		5
+
+#define VIDEO_STREAM_PKT_SIZE(x)		(x)
+#define STREAM_1KB		0
+#define STREAM_2KB		1
+#define STREAM_4KB		2
+#define STREAM_8KB		3
+#define STREAM_16KB		4
+#define STREAM_32KB		5
+#define STREAM_64KB		6
+#define STREAM_128KB	7
+
+
+/* AST_VIDEO_COMPRESS_CTRL	0x060		Video Compression Control Register */
+#define VIDEO_DCT_CQT_SELECTION			(0xf<<6)  // bit 6-9, bit 10 for which quantization is referred
+#define VIDEO_DCT_HQ_CQT_SELECTION		(0xf<<27) // bit 27-30, bit 31 for which quantization is referred
+
+#define VIDEO_HQ_DCT_LUM(x)				((x) << 27)
+#define VIDEO_GET_HQ_DCT_LUM(x)			((x >> 27) & 0x1f)
+#define VIDEO_HQ_DCT_CHROM(x)				((x) << 22)
+#define VIDEO_GET_HQ_DCT_CHROM(x)			((x >> 22) & 0x1f)
+#define VIDEO_HQ_DCT_MASK					(0x3ff << 22)
+#define VIDEO_DCT_HUFFMAN_ENCODE(x)		((x) << 20)
+#define VIDEO_DCT_RESET						(1 << 17)
+#define VIDEO_HQ_ENABLE						(1 << 16)
+#define VIDEO_GET_HQ_ENABLE(x)			((x >> 16) & 0x1)
+#define VIDEO_DCT_LUM(x)					((x) << 11)
+#define VIDEO_GET_DCT_LUM(x)				((x >> 11) & 0x1f)
+#define VIDEO_DCT_CHROM(x)					((x) << 6)
+#define VIDEO_GET_DCT_CHROM(x)			((x >> 6) & 0x1f)
+#define VIDEO_DCT_MASK						(0x3ff << 6)
+#define VIDEO_ENCRYP_ENABLE				(1 << 5)
+#define VIDEO_COMPRESS_QUANTIZ_MODE		(1 << 2)
+#define VIDEO_4COLOR_VQ_ENCODE			(1 << 1)
+#define VIDEO_DCT_ONLY_ENCODE				(1)
+#define VIDEO_DCT_VQ_MASK					(0x3)
+
+#define VIDEO_CTRL_RC4_TEST_MODE		(1 << 9)
+#define VIDEO_CTRL_RC4_RST				(1 << 8)
+
+#define VIDEO_CTRL_ADDRESS_MAP_MULTI_JPEG	(0x3 << 30)
+
+#define VIDEO_CTRL_DWN_SCALING_MASK		(0x3 << 4)
+#define VIDEO_CTRL_DWN_SCALING_ENABLE_LINE_BUFFER		(0x1 << 4)
+
+/* AST_VIDEO_INT_EN			0x304		Video interrupt Enable */
+/* AST_VIDEO_INT_STS		0x308		Video interrupt status */
+#define VM_COMPRESS_COMPLETE			(1 << 17)
+#define VM_CAPTURE_COMPLETE			(1 << 16)
+
+#define VIDEO_FRAME_COMPLETE			(1 << 5)
+#define VIDEO_MODE_DETECT_RDY			(1 << 4)
+#define VIDEO_COMPRESS_COMPLETE		(1 << 3)
+#define VIDEO_COMPRESS_PKT_COMPLETE	(1 << 2)
+#define VIDEO_CAPTURE_COMPLETE		(1 << 1)
+#define VIDEO_MODE_DETECT_WDT			(1 << 0)
+
+/***********************************************************************/
+struct VideoMem {
+	u32	phy;
+	void *pVirt;
+	u32 size;
+};
+
+struct VideoEngineMem {
+	struct VideoMem captureBuf0;
+	struct VideoMem captureBuf1;
+	struct VideoMem jpegTable;
+};
+
+struct ast_capture_mode {
+	u8	engine_idx;					//set 0: engine 0, engine 1
+	u8	differential;					//set 0: full, 1:diff frame
+	u8	mode_change;				//get 0: no, 1:change
+};
+
+struct ast_compression_mode {
+	u8	engine_idx;					//set 0: engine 0, engine 1
+	u8	mode_change;				//get 0: no, 1:change
+	u32	total_size;					//get
+	u32	block_count;					//get
+};
+
+
+/***********************************************************************/
+struct INTERNAL_MODE {
+	u16 HorizontalActive;
+	u16 VerticalActive;
+	u16 RefreshRateIndex;
+	u32 PixelClock;
+};
+
+
+
+// ioctl functions
+void ioctl_get_video_engine_config(struct VideoConfig  *pVideoConfig, struct AstRVAS *pAstRVAS);
+void ioctl_set_video_engine_config(struct VideoConfig  *pVideoConfig, struct AstRVAS *pAstRVAS);
+void ioctl_get_video_engine_data(struct MultiJpegConfig *pArrayMJConfig, struct AstRVAS *pAstRVAS,  u32 dwPhyStreamAddress);
+
+//local functions
+irqreturn_t ast_video_isr(int this_irq, void *dev_id);
+int video_engine_reserveMem(struct AstRVAS *pAstRVAS);
+void enable_video_interrupt(struct AstRVAS *pAstRVAS);
+void disable_video_interrupt(struct AstRVAS *pAstRVAS);
+void video_set_Window(struct AstRVAS *pAstRVAS);
+int free_video_engine_memory(struct AstRVAS *pAstRVAS);
+void video_ctrl_init(struct AstRVAS *pAstRVAS);
+void video_engine_rc4Reset(struct AstRVAS *pAstRVAS);
+void set_direct_mode(struct AstRVAS *pAstRVAS);
+
+
+#endif // __VIDEO_ENGINE_H__
diff --git a/drivers/soc/aspeed/rvas/video_ioctl.h b/drivers/soc/aspeed/rvas/video_ioctl.h
new file mode 100644
index 000000000000..e49d37ed7db1
--- /dev/null
+++ b/drivers/soc/aspeed/rvas/video_ioctl.h
@@ -0,0 +1,275 @@
+/* SPDX-License-Identifier: GPL-2.0+ */
+/*
+ * This file is part of the ASPEED Linux Device Driver for ASPEED Baseboard Management Controller.
+ * Refer to the README file included with this package for driver version and adapter compatibility.
+ *
+ * Copyright (C) 2019-2021 ASPEED Technology Inc. All rights reserved.
+ *
+ */
+
+#ifndef _VIDEO_IOCTL_H
+#define _VIDEO_IOCTL_H
+
+#include <linux/types.h>
+
+#define RVAS_MAGIC				('b')
+#define CMD_IOCTL_TURN_LOCAL_MONITOR_ON		_IOR(RVAS_MAGIC, IOCTL_TURN_LOCAL_MONITOR_ON, struct RvasIoctl)
+#define CMD_IOCTL_TURN_LOCAL_MONITOR_OFF	_IOR(RVAS_MAGIC, IOCTL_TURN_LOCAL_MONITOR_OFF, struct RvasIoctl)
+#define CMD_IOCTL_IS_LOCAL_MONITOR_ENABLED	_IOR(RVAS_MAGIC, IOCTL_IS_LOCAL_MONITOR_ENABLED, struct RvasIoctl)
+#define CMD_IOCTL_GET_VIDEO_GEOMETRY		_IOWR(RVAS_MAGIC, IOCTL_GET_VIDEO_GEOMETRY, struct RvasIoctl)
+#define CMD_IOCTL_WAIT_FOR_VIDEO_EVENT		_IOWR(RVAS_MAGIC, IOCTL_WAIT_FOR_VIDEO_EVENT, struct RvasIoctl)
+#define CMD_IOCTL_GET_GRC_REGIESTERS		_IOWR(RVAS_MAGIC, IOCTL_GET_GRC_REGIESTERS, struct RvasIoctl)
+#define CMD_IOCTL_READ_SNOOP_MAP		_IOWR(RVAS_MAGIC, IOCTL_READ_SNOOP_MAP, struct RvasIoctl)
+#define CMD_IOCTL_READ_SNOOP_AGGREGATE		_IOWR(RVAS_MAGIC, IOCTL_READ_SNOOP_AGGREGATE, struct RvasIoctl)
+#define CMD_IOCTL_FETCH_VIDEO_TILES		_IOWR(RVAS_MAGIC, IOCTL_FETCH_VIDEO_TILES, struct RvasIoctl)
+#define CMD_IOCTL_FETCH_VIDEO_SLICES		_IOWR(RVAS_MAGIC, IOCTL_FETCH_VIDEO_SLICES, struct RvasIoctl)
+#define CMD_IOCTL_RUN_LENGTH_ENCODE_DATA	_IOWR(RVAS_MAGIC, IOCTL_RUN_LENGTH_ENCODE_DATA, struct RvasIoctl)
+#define CMD_IOCTL_FETCH_TEXT_DATA		_IOWR(RVAS_MAGIC, IOCTL_FETCH_TEXT_DATA, struct RvasIoctl)
+#define CMD_IOCTL_FETCH_MODE13_DATA		_IOWR(RVAS_MAGIC, IOCTL_FETCH_MODE13_DATA, struct RvasIoctl)
+#define CMD_IOCTL_NEW_CONTEXT			_IOWR(RVAS_MAGIC, IOCTL_NEW_CONTEXT, struct RvasIoctl)
+#define CMD_IOCTL_DEL_CONTEXT			_IOWR(RVAS_MAGIC, IOCTL_DEL_CONTEXT, struct RvasIoctl)
+#define CMD_IOCTL_ALLOC				_IOWR(RVAS_MAGIC, IOCTL_ALLOC, struct RvasIoctl)
+#define CMD_IOCTL_FREE				_IOWR(RVAS_MAGIC, IOCTL_FREE, struct RvasIoctl)
+#define CMD_IOCTL_SET_TSE_COUNTER		_IOWR(RVAS_MAGIC, IOCTL_SET_TSE_COUNTER, struct RvasIoctl)
+#define CMD_IOCTL_GET_TSE_COUNTER		_IOWR(RVAS_MAGIC, IOCTL_GET_TSE_COUNTER, struct RvasIoctl)
+#define CMD_IOCTL_VIDEO_ENGINE_RESET		_IOWR(RVAS_MAGIC, IOCTL_VIDEO_ENGINE_RESET, struct RvasIoctl)
+//jpeg
+#define CMD_IOCTL_SET_VIDEO_ENGINE_CONFIG	_IOW(RVAS_MAGIC, IOCTL_SET_VIDEO_ENGINE_CONFIG, struct VideoConfig*)
+#define CMD_IOCTL_GET_VIDEO_ENGINE_CONFIG	_IOW(RVAS_MAGIC, IOCTL_GET_VIDEO_ENGINE_CONFIG, struct VideoConfig*)
+#define CMD_IOCTL_GET_VIDEO_ENGINE_DATA		_IOWR(RVAS_MAGIC, IOCTL_GET_VIDEO_ENGINE_DATA, struct MultiJpegConfig*)
+
+enum  HARD_WARE_ENGINE_IOCTL {
+	IOCTL_TURN_LOCAL_MONITOR_ON = 20, //REMOTE VIDEO GENERAL IOCTL
+	IOCTL_TURN_LOCAL_MONITOR_OFF,
+	IOCTL_IS_LOCAL_MONITOR_ENABLED,
+
+	IOCTL_GET_VIDEO_GEOMETRY = 40, // REMOTE VIDEO
+	IOCTL_WAIT_FOR_VIDEO_EVENT,
+	IOCTL_GET_GRC_REGIESTERS,
+	IOCTL_READ_SNOOP_MAP,
+	IOCTL_READ_SNOOP_AGGREGATE,
+	IOCTL_FETCH_VIDEO_TILES,
+	IOCTL_FETCH_VIDEO_SLICES,
+	IOCTL_RUN_LENGTH_ENCODE_DATA,
+	IOCTL_FETCH_TEXT_DATA,
+	IOCTL_FETCH_MODE13_DATA,
+	IOCTL_NEW_CONTEXT,
+	IOCTL_DEL_CONTEXT,
+	IOCTL_ALLOC,
+	IOCTL_FREE,
+	IOCTL_SET_TSE_COUNTER,
+	IOCTL_GET_TSE_COUNTER,
+	IOCTL_VIDEO_ENGINE_RESET,
+	IOCTL_SET_VIDEO_ENGINE_CONFIG,
+	IOCTL_GET_VIDEO_ENGINE_CONFIG,
+	IOCTL_GET_VIDEO_ENGINE_DATA,
+};
+
+enum GraphicsModeType {
+	InvalidMode = 0, TextMode = 1, VGAGraphicsMode = 2, AGAGraphicsMode = 3
+};
+
+enum RVASStatus {
+	SuccessStatus = 0,
+	GenericError = 1,
+	MemoryAllocError = 2,
+	InvalidMemoryHandle = 3,
+	CannotMapMemory = 4,
+	CannotUnMapMemory = 5,
+	TimedOut = 6,
+	InvalidContextHandle = 7,
+	CaptureTimedOut = 8,
+	CompressionTimedOut = 9,
+	HostSuspended
+};
+
+enum SelectedByteMode {
+	AllBytesMode = 0,
+	SkipMode = 1,
+	PlanarToPackedMode,
+	PackedToPackedMode,
+	LowByteMode,
+	MiddleByteMode,
+	TopByteMode
+};
+
+enum DataProccessMode {
+	NormalTileMode = 0,
+	FourBitPlanarMode = 1,
+	FourBitPackedMode = 2,
+	AttrMode = 3,
+	AsciiOnlyMode = 4,
+	FontFetchMode = 5,
+	SplitByteMode = 6
+};
+
+enum ResetEngineMode {
+	ResetAll = 0,
+	ResetRvasEngine = 1,
+	ResetVeEngine = 2
+};
+
+struct VideoGeometry {
+	u16 wScreenWidth;
+	u16 wScreenHeight;
+	u16 wStride;
+	u8 byBitsPerPixel;
+	u8 byModeID;
+	enum GraphicsModeType gmt;
+};
+
+struct EventMap {
+	u32 bPaletteChanged :1;
+	u32 bATTRChanged :1;
+	u32 bSEQChanged :1;
+	u32 bGCTLChanged :1;
+	u32 bCRTCChanged :1;
+	u32 bCRTCEXTChanged :1;
+	u32 bPLTRAMChanged :1;
+	u32 bXCURCOLChanged :1;
+	u32 bXCURCTLChanged :1;
+	u32 bXCURPOSChanged :1;
+	u32 bDoorbellA :1;
+	u32 bDoorbellB :1;
+	u32 bGeometryChanged :1;
+	u32 bSnoopChanged :1;
+	u32 bTextFontChanged :1;
+	u32 bTextATTRChanged :1;
+	u32 bTextASCIIChanged :1;
+};
+
+struct FetchMap {
+	//in parameters
+	bool bEnableRLE;
+	u8 bTextAlignDouble; // 0 - 8 byte, 1 - 16 byte
+	u8 byRLETripletCode;
+	u8 byRLERepeatCode;
+	enum DataProccessMode dpm;
+	//out parameters
+	u32 dwFetchSize;
+	u32 dwFetchRLESize;
+	u32 dwCheckSum;
+	bool bRLEFailed;
+	u8 rsvd[3];
+};
+
+struct SnoopAggregate {
+	u64 qwRow;
+	u64 qwCol;
+};
+
+struct FetchRegion {
+	u16 wTopY;
+	u16 wLeftX;
+	u16 wBottomY;
+	u16 wRightX;
+};
+
+struct FetchOperation {
+	struct FetchRegion fr;
+	enum SelectedByteMode sbm;
+	u32 dwFetchSize;
+	u32 dwFetchRLESize;
+	u32 dwCheckSum;
+	bool bRLEFailed;
+	bool bEnableRLE;
+	u8 byRLETripletCode;
+	u8 byRLERepeatCode;
+	u8 byVGATextAlignment; //0-8bytes, 1-16bytes.
+};
+
+struct FetchVideoTilesArg {
+	struct VideoGeometry vg;
+	u32 dwTotalOutputSize;
+	u32 cfo;
+	struct FetchOperation pfo[4];
+};
+
+struct FetchVideoSlicesArg {
+	struct VideoGeometry vg;
+	u32 dwSlicedSize;
+	u32 dwSlicedRLESize;
+	u32 dwCheckSum;
+	bool bEnableRLE;
+	bool bRLEFailed;
+	u8 byRLETripletCode;
+	u8 byRLERepeatCode;
+	u8 cBuckets;
+	u8 abyBitIndexes[24];
+	u32 cfr;
+	struct FetchRegion pfr[4];
+};
+
+struct RVASBuffer {
+	void *pv;
+	size_t cb;
+};
+
+
+struct RvasIoctl {
+	enum RVASStatus rs;
+	void *rc;
+	struct RVASBuffer rvb;
+	void *rmh;
+	void *rmh1;
+	void *rmh2;
+	u32 rmh_mem_size;
+	u32 rmh1_mem_size;
+	u32 rmh2_mem_size;
+	struct VideoGeometry vg;
+	struct EventMap em;
+	struct SnoopAggregate sa;
+	union {
+		u32 tse_counter;
+		u32 req_mem_size;
+		u32 encode;
+		u32 time_out;
+	};
+	u32 rle_len;  // RLE Length
+	u32 rle_checksum;
+	struct FetchMap tfm;
+	u8 flag;
+	u8 lms;
+	u8 resetMode;
+	u8 rsvd[1];
+};
+
+
+//
+// Video Engine
+//
+
+#define MAX_MULTI_FRAME_CT (32)
+
+struct VideoConfig {
+	u8 engine;					//0: engine 0 - normal engine, engine 1 - VM legacy engine
+	u8 compression_mode;	//0:DCT, 1:DCT_VQ mix VQ-2 color, 2:DCT_VQ mix VQ-4 color		9:
+	u8 compression_format;	//0:ASPEED 1:JPEG
+	u8 capture_format;		//0:CCIR601-2 YUV, 1:JPEG YUV, 2:RGB for ASPEED mode only, 3:Gray
+	u8 rc4_enable;				//0:disable 1:enable
+	u8 YUV420_mode;			//0:YUV444, 1:YUV420
+	u8 Visual_Lossless;
+	u8 Y_JPEGTableSelector;
+	u8 AdvanceTableSelector;
+	u8 AutoMode;
+	u8 rsvd[2];
+	enum RVASStatus rs;
+};
+
+struct MultiJpegFrame {
+	u32 dwSizeInBytes;			// Image size in bytes
+	u32 dwOffsetInBytes;			// Offset in bytes
+	u16 wXPixels;					// In: X coordinate
+	u16 wYPixels;					// In: Y coordinate
+	u16 wWidthPixels;				// In: Width for Fetch
+	u16 wHeightPixels;			// In: Height for Fetch
+};
+
+struct MultiJpegConfig {
+	unsigned char multi_jpeg_frames;				// frame count
+	struct MultiJpegFrame frame[MAX_MULTI_FRAME_CT];	// The Multi Frames
+	void *aStreamHandle;
+	enum RVASStatus rs;
+};
+
+#endif // _VIDEO_IOCTL_H
diff --git a/drivers/soc/aspeed/rvas/video_main.c b/drivers/soc/aspeed/rvas/video_main.c
new file mode 100644
index 000000000000..845470162a96
--- /dev/null
+++ b/drivers/soc/aspeed/rvas/video_main.c
@@ -0,0 +1,1618 @@
+// SPDX-License-Identifier: GPL-2.0+
+/*
+ * File Name     : video_main.c
+ * Description   : AST2600 RVAS hardware engines
+ *
+ * Copyright (C) ASPEED Technology Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ */
+
+
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/version.h>
+#include <linux/interrupt.h>
+#include <linux/device.h>
+#include <linux/reset.h>
+#include <asm/uaccess.h>
+#include <linux/string.h>
+#include <linux/errno.h>
+#include <linux/fs.h>
+#include <linux/platform_device.h>
+#include <linux/cdev.h>
+#include <linux/dma-mapping.h>
+#include <linux/miscdevice.h>
+#include <linux/slab.h>
+#include <linux/wait.h>
+#include <linux/sched.h>
+#include <linux/of.h>
+#include <linux/of_address.h>
+#include <linux/mm.h>
+#include <stdbool.h>
+#include <linux/of_reserved_mem.h>
+#include <linux/regmap.h>
+#include <linux/mfd/syscon.h>
+#include <linux/clk.h>
+
+#include "video_ioctl.h"
+#include "hardware_engines.h"
+#include "video.h"
+#include "video_debug.h"
+#include "video_engine.h"
+
+
+#define TEST_GRCE_DETECT_RESOLUTION_CHG
+
+static long video_ioctl(struct file *file, unsigned int cmd, unsigned long arg);
+static int video_open(struct inode *pInode, struct file *pFile);
+static int video_release(struct inode *pInode, struct file *pFile);
+static irqreturn_t fge_handler(int irq, void *dev_id);
+
+static void video_os_init_sleep_struct(struct Video_OsSleepStruct *Sleep);
+static void video_ss_wakeup_on_timeout(struct Video_OsSleepStruct *Sleep);
+static void enable_rvas_engines(struct AstRVAS *pAstRVAS);
+static void video_engine_init(void);
+static void rvas_init(void);
+static void reset_rvas_engine(struct AstRVAS *pAstRVAS);
+static void reset_video_engine(struct AstRVAS *pAstRVAS);
+static void set_FBInfo_size(struct AstRVAS *pAstRVAS, void __iomem *mcr_base);
+
+static long video_os_sleep_on_timeout(struct Video_OsSleepStruct *Sleep, u8 *Var, long msecs);
+
+static struct AstRVAS *pAstRVAS;
+static void __iomem *dp_base;
+
+static long video_ioctl(struct file *file, unsigned int cmd, unsigned long arg)
+{
+	int iResult = 0;
+	struct RvasIoctl ri;
+	struct VideoConfig video_config;
+	struct MultiJpegConfig multi_jpeg;
+	u8 bVideoCmd = 0;
+	u32 dw_phys = 0;
+
+	VIDEO_DBG("Start\n");
+	VIDEO_DBG("pAstRVAS: 0x%p\n", pAstRVAS);
+	memset(&ri, 0, sizeof(ri));
+
+	if ((cmd != CMD_IOCTL_SET_VIDEO_ENGINE_CONFIG) &&
+		(cmd != CMD_IOCTL_GET_VIDEO_ENGINE_CONFIG) &&
+		(cmd != CMD_IOCTL_GET_VIDEO_ENGINE_DATA)) {
+		if (raw_copy_from_user(&ri, (void *) arg, sizeof(struct RvasIoctl))) {
+			dev_err(pAstRVAS->pdev, "Copy from user buffer Failed\n");
+			return -EINVAL;
+		}
+
+		ri.rs = SuccessStatus;
+		bVideoCmd = 0;
+	} else {
+		bVideoCmd = 1;
+	}
+
+
+	VIDEO_DBG(" Command = 0x%x\n", cmd);
+
+	switch (cmd) {
+	case CMD_IOCTL_TURN_LOCAL_MONITOR_ON:
+		ioctl_update_lms(0x1, pAstRVAS);
+		break;
+
+	case CMD_IOCTL_TURN_LOCAL_MONITOR_OFF:
+		ioctl_update_lms(0x0, pAstRVAS);
+		break;
+
+	case CMD_IOCTL_IS_LOCAL_MONITOR_ENABLED:
+		if (ioctl_get_lm_status(pAstRVAS))
+			ri.lms = 0x1;
+		else
+			ri.lms = 0x0;
+		break;
+
+	case CMD_IOCTL_GET_VIDEO_GEOMETRY:
+		VIDEO_DBG(" Command CMD_IOCTL_GET_VIDEO_GEOMETRY\n");
+		ioctl_get_video_geometry(&ri, pAstRVAS);
+		break;
+
+	case CMD_IOCTL_WAIT_FOR_VIDEO_EVENT:
+		VIDEO_DBG(" Command CMD_IOCTL_WAIT_FOR_VIDEO_EVENT\n");
+		ioctl_wait_for_video_event(&ri, pAstRVAS);
+		break;
+
+	case CMD_IOCTL_GET_GRC_REGIESTERS:
+		VIDEO_DBG(" Command CMD_IOCTL_GET_GRC_REGIESTERS\n");
+		ioctl_get_grc_register(&ri, pAstRVAS);
+		break;
+
+	case CMD_IOCTL_READ_SNOOP_MAP:
+		VIDEO_DBG(" Command CMD_IOCTL_READ_SNOOP_MAP\n");
+		ioctl_read_snoop_map(&ri, pAstRVAS);
+		break;
+
+	case CMD_IOCTL_READ_SNOOP_AGGREGATE:
+		VIDEO_DBG(" Command CMD_IOCTL_READ_SNOOP_AGGREGATE\n");
+		ioctl_read_snoop_aggregate(&ri, pAstRVAS);
+		break;
+
+	case CMD_IOCTL_FETCH_VIDEO_TILES: ///
+		VIDEO_DBG("CMD_IOCTL_FETCH_VIDEO_TILES\n");
+		ioctl_fetch_video_tiles(&ri, pAstRVAS);
+		break;
+
+	case CMD_IOCTL_FETCH_VIDEO_SLICES:
+		VIDEO_DBG(" Command CMD_IOCTL_FETCH_VIDEO_SLICES\n");
+		ioctl_fetch_video_slices(&ri, pAstRVAS);
+		break;
+
+	case CMD_IOCTL_RUN_LENGTH_ENCODE_DATA:
+		VIDEO_DBG(" Command CMD_IOCTL_RUN_LENGTH_ENCODE_DATA\n");
+		ioctl_run_length_encode_data(&ri, pAstRVAS);
+		break;
+
+	case CMD_IOCTL_FETCH_TEXT_DATA:
+		VIDEO_DBG(" Command CMD_IOCTL_FETCH_TEXT_DATA\n");
+		ioctl_fetch_text_data(&ri, pAstRVAS);
+		break;
+
+	case CMD_IOCTL_FETCH_MODE13_DATA:
+		VIDEO_DBG(" Command CMD_IOCTL_FETCH_MODE13_DATA\n");
+		ioctl_fetch_mode_13_data(&ri, pAstRVAS);
+		break;
+
+	case CMD_IOCTL_ALLOC:
+		VIDEO_DBG(" Command CMD_IOCTL_ALLOC\n");
+		ioctl_alloc(file, &ri, pAstRVAS);
+		break;
+
+	case CMD_IOCTL_FREE:
+		VIDEO_DBG(" Command CMD_IOCTL_FREE\n");
+		ioctl_free(&ri, pAstRVAS);
+		break;
+
+	case CMD_IOCTL_NEW_CONTEXT:
+		VIDEO_DBG(" Command CMD_IOCTL_NEW_CONTEXT\n");
+		ioctl_new_context(file, &ri, pAstRVAS);
+		break;
+
+	case CMD_IOCTL_DEL_CONTEXT:
+		VIDEO_DBG(" Command CMD_IOCTL_DEL_CONTEXT\n");
+		ioctl_delete_context(&ri, pAstRVAS);
+		break;
+
+	case CMD_IOCTL_SET_TSE_COUNTER:
+		VIDEO_DBG(" Command CMD_IOCTL_SET_TSE_COUNTER\n");
+		ioctl_set_tse_tsicr(&ri, pAstRVAS);
+		break;
+
+	case CMD_IOCTL_GET_TSE_COUNTER:
+		VIDEO_DBG(" Command CMD_IOCTL_GET_TSE_COUNTER\n");
+		ioctl_get_tse_tsicr(&ri, pAstRVAS);
+		break;
+
+	case CMD_IOCTL_VIDEO_ENGINE_RESET:
+		VIDEO_ENG_DBG(" Command CMD_IOCTL_VIDEO_ENGINE_RESET\n");
+		ioctl_reset_video_engine(&ri, pAstRVAS);
+		break;
+	case CMD_IOCTL_GET_VIDEO_ENGINE_CONFIG:
+		VIDEO_DBG(" Command CMD_IOCTL_GET_VIDEO_ENGINE_CONFIG\n");
+		ioctl_get_video_engine_config(&video_config, pAstRVAS);
+
+		iResult = raw_copy_to_user((void *) arg, &video_config, sizeof(video_config));
+		break;
+	case CMD_IOCTL_SET_VIDEO_ENGINE_CONFIG:
+		VIDEO_DBG(" Command CMD_IOCTL_SET_VIDEO_ENGINE_CONFIG\n");
+		iResult = raw_copy_from_user(&video_config, (void *) arg, sizeof(video_config));
+
+		ioctl_set_video_engine_config(&video_config, pAstRVAS);
+		break;
+	case CMD_IOCTL_GET_VIDEO_ENGINE_DATA:
+		VIDEO_DBG(" Command CMD_IOCTL_GET_VIDEO_ENGINE_DATA\n");
+		iResult = raw_copy_from_user(&multi_jpeg, (void *) arg, sizeof(multi_jpeg));
+		dw_phys = get_phys_add_rsvd_mem((u32)multi_jpeg.aStreamHandle, pAstRVAS);
+		VIDEO_DBG("physical stream address: %#x\n", dw_phys);
+
+		if (dw_phys == 0)
+			dev_err(pAstRVAS->pdev, "Error of getting stream buffer address\n");
+		else
+			ioctl_get_video_engine_data(&multi_jpeg, pAstRVAS, dw_phys);
+
+		iResult = raw_copy_to_user((void *) arg, &multi_jpeg, sizeof(multi_jpeg));
+		break;
+	default:
+		dev_err(pAstRVAS->pdev, "Unknown Ioctl: %#x\n", cmd);
+		iResult = -EINVAL;
+		break;
+	}
+
+	if (!iResult && !bVideoCmd)
+		if (raw_copy_to_user((void *) arg, &ri, sizeof(struct RvasIoctl))) {
+			dev_err(pAstRVAS->pdev, "Copy to user buffer Failed\n");
+			iResult = -EINVAL;
+		}
+
+	return iResult;
+}
+
+static int video_mmap(struct file *file, struct vm_area_struct *vma)
+{
+	size_t size;
+	u32 dw_index;
+	u8 found = 0;
+
+	struct MemoryMapTable **pmmt = pAstRVAS->ppmmtMemoryTable;
+
+	size = vma->vm_end - vma->vm_start;
+	vma->vm_private_data = pAstRVAS;
+	VIDEO_DBG("vma->vm_start 0x%lx, vma->vm_end 0x%lx, vma->vm_pgoff=0x%x\n",
+			vma->vm_start,
+			vma->vm_end,
+			(u32) vma->vm_pgoff);
+	VIDEO_DBG("(vma->vm_pgoff << PAGE_SHIFT) = 0x%lx\n", (vma->vm_pgoff << PAGE_SHIFT));
+	for (dw_index = 0; dw_index < MAX_NUM_MEM_TBL; ++dw_index) {
+		if (pmmt[dw_index]) {
+			VIDEO_DBG("index %d, phys_addr=0x%x, virt_addr0x%x, length=0x%x\n",
+					dw_index,
+					pmmt[dw_index]->dwPhysicalAddr,
+					(u32)pmmt[dw_index]->pvVirtualAddr,
+					pmmt[dw_index]->dwLength);
+			if ((vma->vm_pgoff<<PAGE_SHIFT) == (u32)pmmt[dw_index]->pvVirtualAddr) {
+				found = 1;
+				if (size > pmmt[dw_index]->dwLength) {
+					pr_err("required size exceed alloc size\n");
+					return -EAGAIN;
+				}
+				break;
+			}
+		}
+	}
+	if (!found) {
+		pr_err("no match mem entry\n");
+		return -EAGAIN;
+	}
+
+	vma->vm_flags |= VM_IO;
+	vma->vm_page_prot = pgprot_noncached(vma->vm_page_prot);
+
+	if (io_remap_pfn_range(vma, vma->vm_start, ((u32) vma->vm_pgoff), size,
+		vma->vm_page_prot)) {
+		pr_err("remap_pfn_range fail at %s()\n", __func__);
+		return -EAGAIN;
+	}
+
+	return 0;
+}
+
+static int video_open(struct inode *pin, struct file *pf)
+{
+	VIDEO_DBG("%s Called\n", __func__);
+
+	// make sure the rvas clk is running.
+	//	 if it's already enabled, clk_enable will just return.
+	clk_enable(pAstRVAS->rvasclk);
+
+	return 0;
+}
+
+void free_all_mem_entries(struct AstRVAS *pAstRVAS)
+{
+	u32 dw_index;
+	struct MemoryMapTable **pmmt = pAstRVAS->ppmmtMemoryTable;
+	void *virt_add;
+	u32 dw_phys, len;
+
+	VIDEO_DBG("Removing mem map entries...\n");
+	for (dw_index = 0; dw_index < MAX_NUM_MEM_TBL; ++dw_index) {
+		if (pmmt[dw_index]) {
+			if (pmmt[dw_index]->dwPhysicalAddr) {
+				virt_add = get_virt_add_rsvd_mem(dw_index, pAstRVAS);
+				dw_phys = get_phys_add_rsvd_mem(dw_index, pAstRVAS);
+				len = get_len_rsvd_mem(dw_index, pAstRVAS);
+				dma_free_coherent(pAstRVAS->pdev, len, virt_add, dw_phys);
+			}
+			pmmt[dw_index]->pf = NULL;
+			kfree(pmmt[dw_index]);
+			pmmt[dw_index] = NULL;
+		}
+	}
+}
+
+static int video_release(struct inode *inode, struct file *filp)
+{
+	u32 dw_index;
+	struct ContextTable **ppctContextTable = pAstRVAS->ppctContextTable;
+
+	VIDEO_DBG("Start\n");
+
+	free_all_mem_entries(pAstRVAS);
+
+	VIDEO_DBG("ppctContextTable: 0x%p\n", ppctContextTable);
+
+	disable_grce_tse_interrupt(pAstRVAS);
+
+	for (dw_index = 0; dw_index < MAX_NUM_CONTEXT; ++dw_index) {
+
+		if (ppctContextTable[dw_index]) {
+			VIDEO_DBG("Releasing Context dw_index: %u\n", dw_index);
+			kfree(ppctContextTable[dw_index]);
+			ppctContextTable[dw_index] = NULL;
+		}
+	}
+	enable_grce_tse_interrupt(pAstRVAS);
+	VIDEO_DBG("End\n");
+
+	return 0;
+}
+
+static struct file_operations video_module_ops = { .compat_ioctl = video_ioctl,
+	.unlocked_ioctl = video_ioctl, .open = video_open, .release =
+		video_release, .mmap = video_mmap, .owner = THIS_MODULE, };
+
+static struct miscdevice video_misc = { .minor = MISC_DYNAMIC_MINOR, .name =
+	RVAS_DRIVER_NAME, .fops = &video_module_ops, };
+
+void ioctl_new_context(struct file *file, struct RvasIoctl *pri, struct AstRVAS *pAstRVAS)
+{
+	struct ContextTable *pct;
+
+	VIDEO_DBG("Start\n");
+	pct = get_new_context_table_entry(pAstRVAS);
+
+	if (pct) {
+		pct->desc_virt = dma_alloc_coherent(pAstRVAS->pdev, PAGE_SIZE, (dma_addr_t *) &pct->desc_phy, GFP_KERNEL);
+		if (!pct->desc_virt) {
+			pri->rs = MemoryAllocError;
+			return;
+		}
+		pri->rc = pct->rc;
+
+	} else {
+		pri->rs = MemoryAllocError;
+	}
+	VIDEO_DBG("end: return status: %d\n", pri->rs);
+
+}
+
+void ioctl_delete_context(struct RvasIoctl *pri, struct AstRVAS *pAstRVAS)
+{
+	VIDEO_DBG("Start\n");
+
+	VIDEO_DBG("pri->rc: %d\n", pri->rc);
+	if (remove_context_table_entry(pri->rc, pAstRVAS)) {
+		VIDEO_DBG("Success in removing\n");
+		pri->rs = SuccessStatus;
+	} else {
+		VIDEO_DBG("Failed in removing\n");
+		pri->rs = InvalidMemoryHandle;
+	}
+}
+
+int get_mem_entry(struct AstRVAS *pAstRVAS)
+{
+	int index = 0;
+	bool found = false;
+
+	down(&pAstRVAS->mem_sem);
+	do {
+		if (pAstRVAS->ppmmtMemoryTable[index])
+			index++;
+		else {
+			found = true;
+			break;
+		}
+
+	} while (!found && (index < MAX_NUM_MEM_TBL));
+
+	if (found) {
+		pAstRVAS->ppmmtMemoryTable[index] = kmalloc(sizeof(struct MemoryMapTable), GFP_KERNEL);
+		if (!pAstRVAS->ppmmtMemoryTable[index])
+			index = -1;
+	} else
+		index = -1;
+
+	up(&pAstRVAS->mem_sem);
+	return index;
+}
+
+bool delete_mem_entry(const void *crmh, struct AstRVAS *pAstRVAS)
+{
+	bool b_ret = false;
+	u32 dw_index = (u32) crmh;
+
+	VIDEO_DBG("Start, dw_index: %#x\n", dw_index);
+
+	down(&pAstRVAS->mem_sem);
+	if ((dw_index < MAX_NUM_MEM_TBL) && pAstRVAS->ppmmtMemoryTable[dw_index]) {
+		VIDEO_DBG("mem: 0x%p\n", pAstRVAS->ppmmtMemoryTable[dw_index]);
+		kfree(pAstRVAS->ppmmtMemoryTable[dw_index]);
+		pAstRVAS->ppmmtMemoryTable[dw_index] = NULL;
+		b_ret = true;
+	}
+	up(&pAstRVAS->mem_sem);
+	VIDEO_DBG("End\n");
+	return b_ret;
+}
+
+void *get_virt_add_rsvd_mem(u32 index, struct AstRVAS *pAstRVAS)
+{
+	if (index < MAX_NUM_MEM_TBL && pAstRVAS->ppmmtMemoryTable[index])
+		return pAstRVAS->ppmmtMemoryTable[index]->pvVirtualAddr;
+
+	return 0;
+}
+
+u32 get_phys_add_rsvd_mem(u32 index, struct AstRVAS *pAstRVAS)
+{
+	if (index < MAX_NUM_MEM_TBL && pAstRVAS->ppmmtMemoryTable[index])
+		return pAstRVAS->ppmmtMemoryTable[index]->dwPhysicalAddr;
+
+	return 0;
+}
+
+u32 get_len_rsvd_mem(u32 index, struct AstRVAS *pAstRVAS)
+{
+	u32 len = 0;
+
+	if (index < MAX_NUM_MEM_TBL && pAstRVAS->ppmmtMemoryTable[index])
+		len = pAstRVAS->ppmmtMemoryTable[index]->dwLength;
+
+	return len;
+}
+
+bool virt_is_valid_rsvd_mem(u32 index, u32 size, struct AstRVAS *pAstRVAS)
+{
+	if (index < MAX_NUM_MEM_TBL &&
+		pAstRVAS->ppmmtMemoryTable[index] &&
+		pAstRVAS->ppmmtMemoryTable[index]->dwLength)
+		return true;
+
+	return false;
+}
+
+void ioctl_alloc(struct file *pfile, struct RvasIoctl *pri, struct AstRVAS *pAstRVAS)
+{
+	u32 size;
+	u32 phys_add = 0;
+	u32 virt_add = 0;
+	u32 index = get_mem_entry(pAstRVAS);
+
+	if (index < 0 || index >= MAX_NUM_MEM_TBL) {
+		pri->rs = MemoryAllocError;
+		return;
+	}
+	if (pri->req_mem_size < PAGE_SIZE)
+		pri->req_mem_size = PAGE_SIZE;
+
+	size = pri->req_mem_size;
+
+	VIDEO_DBG("Allocating memory size: 0x%x\n", size);
+	virt_add = (u32)dma_alloc_coherent(pAstRVAS->pdev, size, &phys_add,
+			GFP_KERNEL);
+	if (virt_add) {
+		pri->rmh = (void *)index;
+		pri->rvb.pv = (void *) phys_add;
+		pri->rvb.cb = size;
+		pri->rs = SuccessStatus;
+		pAstRVAS->ppmmtMemoryTable[index]->pf = pfile;
+		pAstRVAS->ppmmtMemoryTable[index]->dwPhysicalAddr = phys_add;
+		pAstRVAS->ppmmtMemoryTable[index]->pvVirtualAddr = (void *)virt_add;
+		pAstRVAS->ppmmtMemoryTable[index]->dwLength = size;
+		pAstRVAS->ppmmtMemoryTable[index]->byDmaAlloc = 1;
+	} else {
+		if (pAstRVAS->ppmmtMemoryTable[index])
+			delete_mem_entry((void *)index, pAstRVAS);
+
+		pr_err("Cannot alloc video destination data buffer\n");
+		pri->rs = MemoryAllocError;
+	}
+
+	VIDEO_DBG("Allocated: index: 0x%x phys: %#x cb: 0x%x\n", index,
+			phys_add, pri->rvb.cb);
+}
+
+void ioctl_free(struct RvasIoctl *pri, struct AstRVAS *pAstRVAS)
+{
+	void *virt_add = get_virt_add_rsvd_mem((u32)pri->rmh, pAstRVAS);
+	u32 dw_phys = get_phys_add_rsvd_mem((u32) pri->rmh, pAstRVAS);
+	u32 len = get_len_rsvd_mem((u32) pri->rmh, pAstRVAS);
+
+	VIDEO_DBG("Start\n");
+	VIDEO_DBG("Freeing: rmh: 0x%p, phys: 0x%x, size 0x%x virt_add: 0x%p len: %u\n",
+		pri->rmh, dw_phys, pri->rvb.cb, virt_add, len);
+
+	delete_mem_entry(pri->rmh, pAstRVAS);
+	VIDEO_DBG("After delete_mem_entry\n");
+
+	dma_free_coherent(pAstRVAS->pdev, len,
+			virt_add,
+			dw_phys);
+	VIDEO_DBG("After dma_free_coherent\n");
+}
+
+
+void ioctl_update_lms(u8 lms_on, struct AstRVAS *pAstRVAS)
+{
+	u32 reg_scu418 = 0;
+	u32 reg_scu0C0 = 0;
+	u32 reg_scu0D0 = 0;
+	u32 reg_dptx100 = 0;
+	u32 reg_dptx104 = 0;
+
+	regmap_read(pAstRVAS->scu, SCU418_Pin_Ctrl, &reg_scu418);
+	regmap_read(pAstRVAS->scu, SCU0C0_Misc1_Ctrl, &reg_scu0C0);
+	regmap_read(pAstRVAS->scu, SCU0D0_Misc3_Ctrl, &reg_scu0D0);
+	if (dp_base) {
+		reg_dptx100 = readl(dp_base + DPTX_Configuration_Register);
+		reg_dptx104 = readl(dp_base + DPTX_PHY_Configuration_Register);
+	}
+
+	if (lms_on) {
+		if (!(reg_scu418 & (VGAVS_ENBL|VGAHS_ENBL))) {
+			reg_scu418 |= (VGAVS_ENBL|VGAHS_ENBL);
+			regmap_write(pAstRVAS->scu, SCU418_Pin_Ctrl, reg_scu418);
+		}
+		if (reg_scu0C0 & VGA_CRT_DISBL) {
+			reg_scu0C0 &= ~VGA_CRT_DISBL;
+			regmap_write(pAstRVAS->scu, SCU0C0_Misc1_Ctrl, reg_scu0C0);
+		}
+		if (reg_scu0D0 & PWR_OFF_VDAC) {
+			reg_scu0D0 &= ~PWR_OFF_VDAC;
+			regmap_write(pAstRVAS->scu, SCU0D0_Misc3_Ctrl, reg_scu0D0);
+		}
+		//dp output
+		if (dp_base) {
+			reg_dptx100 |= 1 << AUX_RESETN;
+			writel(reg_dptx100, dp_base + DPTX_Configuration_Register);
+		}
+	} else { //turn off
+		if (reg_scu418 & (VGAVS_ENBL|VGAHS_ENBL)) {
+			reg_scu418 &= ~(VGAVS_ENBL|VGAHS_ENBL);
+			regmap_write(pAstRVAS->scu, SCU418_Pin_Ctrl, reg_scu418);
+		}
+		if (!(reg_scu0C0 & VGA_CRT_DISBL)) {
+			reg_scu0C0 |= VGA_CRT_DISBL;
+			regmap_write(pAstRVAS->scu, SCU0C0_Misc1_Ctrl, reg_scu0C0);
+		}
+		if (!(reg_scu0D0 & PWR_OFF_VDAC)) {
+			reg_scu0D0 |= PWR_OFF_VDAC;
+			regmap_write(pAstRVAS->scu, SCU0D0_Misc3_Ctrl, reg_scu0D0);
+		}
+		//dp output
+		if (dp_base) {
+			reg_dptx100 &= ~(1 << AUX_RESETN);
+			writel(reg_dptx100, dp_base + DPTX_Configuration_Register);
+			reg_dptx104 &= ~(1 << DP_TX_I_MAIN_ON);
+			writel(reg_dptx104, dp_base + DPTX_PHY_Configuration_Register);
+		}
+	}
+}
+
+u32 ioctl_get_lm_status(struct AstRVAS *pAstRVAS)
+{
+	u32 reg_val = 0;
+
+	regmap_read(pAstRVAS->scu, SCU418_Pin_Ctrl, &reg_val);
+	if (reg_val & (VGAVS_ENBL|VGAHS_ENBL)) {
+		regmap_read(pAstRVAS->scu, SCU0C0_Misc1_Ctrl, &reg_val);
+		if (!(reg_val & VGA_CRT_DISBL)) {
+			regmap_read(pAstRVAS->scu, SCU0D0_Misc3_Ctrl, &reg_val);
+			if (!(reg_val & PWR_OFF_VDAC))
+				return 1;
+
+		}
+	}
+	return 0;
+}
+
+void init_osr_es(struct AstRVAS *pAstRVAS)
+{
+	VIDEO_DBG("Start\n");
+	sema_init(&pAstRVAS->mem_sem, 1);
+	sema_init(&pAstRVAS->context_sem, 1);
+
+	video_os_init_sleep_struct(&pAstRVAS->video_wait);
+
+	memset(&pAstRVAS->tfe_engine, 0x00, sizeof(struct EngineInfo));
+	memset(&pAstRVAS->bse_engine, 0x00, sizeof(struct EngineInfo));
+	memset(&pAstRVAS->ldma_engine, 0x00, sizeof(struct EngineInfo));
+	sema_init(&pAstRVAS->tfe_engine.sem, 1);
+	sema_init(&pAstRVAS->bse_engine.sem, 1);
+	sema_init(&pAstRVAS->ldma_engine.sem, 1);
+	video_os_init_sleep_struct(&pAstRVAS->tfe_engine.wait);
+	video_os_init_sleep_struct(&pAstRVAS->bse_engine.wait);
+	video_os_init_sleep_struct(&pAstRVAS->ldma_engine.wait);
+
+	memset(pAstRVAS->ppctContextTable, 0x00, MAX_NUM_CONTEXT * sizeof(u32));
+	pAstRVAS->dwMemoryTableSize = MAX_NUM_MEM_TBL;
+	memset(pAstRVAS->ppmmtMemoryTable, 0x00, MAX_NUM_MEM_TBL * sizeof(u32));
+	VIDEO_DBG("End\n");
+}
+
+void release_osr_es(struct AstRVAS *pAstRVAS)
+{
+	u32 dw_index;
+	struct ContextTable **ppctContextTable = pAstRVAS->ppctContextTable;
+
+	VIDEO_DBG("Removing contexts...\n");
+	for (dw_index = 0; dw_index < MAX_NUM_CONTEXT; ++dw_index) {
+		//if (ppctContextTable[dw_index]) {
+		kfree(ppctContextTable[dw_index]);
+		ppctContextTable[dw_index] = NULL;
+		//} // kfree(NULL) is safe and this check is probably not require
+	}
+
+	free_all_mem_entries(pAstRVAS);
+}
+
+//Retrieve a context entry
+struct ContextTable *get_context_entry(const void *crc, struct AstRVAS *pAstRVAS)
+{
+	struct ContextTable *pct = NULL;
+	u32 dw_index = (u32) crc;
+	struct ContextTable **ppctContextTable = pAstRVAS->ppctContextTable;
+
+	if ((dw_index < MAX_NUM_CONTEXT) && ppctContextTable[dw_index]
+		&& (ppctContextTable[dw_index]->rc == crc))
+		pct = ppctContextTable[dw_index];
+
+	return pct;
+}
+
+struct ContextTable *get_new_context_table_entry(struct AstRVAS *pAstRVAS)
+{
+	struct ContextTable *pct = NULL;
+	u32 dw_index = 0;
+	bool b_found = false;
+	struct ContextTable **ppctContextTable = pAstRVAS->ppctContextTable;
+
+	disable_grce_tse_interrupt(pAstRVAS);
+	down(&pAstRVAS->context_sem);
+	while (!b_found && (dw_index < MAX_NUM_CONTEXT)) {
+		if (!(ppctContextTable[dw_index]))
+			b_found = true;
+		else
+			++dw_index;
+	}
+	if (b_found) {
+		pct = kmalloc(sizeof(struct ContextTable), GFP_KERNEL);
+
+		if (pct) {
+			memset(pct, 0x00, sizeof(struct ContextTable));
+			pct->rc = (void *) dw_index;
+			memset(&(pct->aqwSnoopMap), 0xff,
+				sizeof(pct->aqwSnoopMap));
+			memset(&(pct->sa), 0xff, sizeof(pct->sa));
+			ppctContextTable[dw_index] = pct;
+		}
+	}
+	up(&pAstRVAS->context_sem);
+	enable_grce_tse_interrupt(pAstRVAS);
+
+	return pct;
+}
+
+bool remove_context_table_entry(const void *crc, struct AstRVAS *pAstRVAS)
+{
+	bool b_ret = false;
+	u32 dw_index = (u32) crc;
+	struct ContextTable *ctx_entry;
+
+	VIDEO_DBG("Start\n");
+
+	VIDEO_DBG("dw_index: %u\n", dw_index);
+
+	if (dw_index < MAX_NUM_CONTEXT) {
+		ctx_entry = pAstRVAS->ppctContextTable[dw_index];
+		VIDEO_DBG("ctx_entry: 0x%p\n", ctx_entry);
+
+		if (ctx_entry) {
+			disable_grce_tse_interrupt(pAstRVAS);
+			if (!ctx_entry->desc_virt) {
+				VIDEO_DBG("Removing memory, virt: 0x%p phys: %#x\n",
+					ctx_entry->desc_virt,
+					ctx_entry->desc_phy);
+
+				dma_free_coherent(pAstRVAS->pdev, PAGE_SIZE, ctx_entry->desc_virt, ctx_entry->desc_phy);
+			}
+			VIDEO_DBG("Removing memory: 0x%p\n", ctx_entry);
+			pAstRVAS->ppctContextTable[dw_index] = NULL;
+			kfree(ctx_entry);
+			b_ret = true;
+			enable_grce_tse_interrupt(pAstRVAS);
+		}
+	}
+	return b_ret;
+}
+
+void display_event_map(const struct EventMap *pem)
+{
+	VIDEO_DBG("EM:\n");
+	VIDEO_DBG("*************************\n");
+	VIDEO_DBG("  bATTRChanged=      %u\n", pem->bATTRChanged);
+	VIDEO_DBG("  bCRTCChanged=      %u\n", pem->bCRTCChanged);
+	VIDEO_DBG("  bCRTCEXTChanged=   %u\n", pem->bCRTCEXTChanged);
+	VIDEO_DBG("  bDoorbellA=        %u\n", pem->bDoorbellA);
+	VIDEO_DBG("  bDoorbellB=        %u\n", pem->bDoorbellB);
+	VIDEO_DBG("  bGCTLChanged=      %u\n", pem->bGCTLChanged);
+	VIDEO_DBG("  bGeometryChanged=  %u\n", pem->bGeometryChanged);
+	VIDEO_DBG("  bPLTRAMChanged=    %u\n", pem->bPLTRAMChanged);
+	VIDEO_DBG("  bPaletteChanged=   %u\n", pem->bPaletteChanged);
+	VIDEO_DBG("  bSEQChanged=       %u\n", pem->bSEQChanged);
+	VIDEO_DBG("  bSnoopChanged=     %u\n", pem->bSnoopChanged);
+	VIDEO_DBG("  bTextASCIIChanged= %u\n", pem->bTextASCIIChanged);
+	VIDEO_DBG("  bTextATTRChanged=  %u\n", pem->bTextATTRChanged);
+	VIDEO_DBG("  bTextFontChanged=  %u\n", pem->bTextFontChanged);
+	VIDEO_DBG("  bXCURCOLChanged=   %u\n", pem->bXCURCOLChanged);
+	VIDEO_DBG("  bXCURCTLChanged=   %u\n", pem->bXCURCTLChanged);
+	VIDEO_DBG("  bXCURPOSChanged=   %u\n", pem->bXCURPOSChanged);
+	VIDEO_DBG("*************************\n");
+}
+
+void ioctl_wait_for_video_event(struct RvasIoctl *ri, struct AstRVAS *pAstRVAS)
+{
+	union EmDwordUnion eduRequested;
+	union EmDwordUnion eduReturned;
+	union EmDwordUnion eduChanged;
+	struct EventMap anEm;
+	u32 result = 1;
+	int iTimerRemaining = ri->time_out;
+	unsigned long ulTimeStart, ulTimeEnd, ulElapsedTime;
+	struct ContextTable **ppctContextTable = pAstRVAS->ppctContextTable;
+
+
+	memset(&anEm, 0x0, sizeof(struct EventMap));
+
+	VIDEO_DBG("Calling VideoSleepOnTimeout\n");
+
+	eduRequested.em = ri->em;
+	VIDEO_DBG("eduRequested.em:\n");
+	display_event_map(&eduRequested.em);
+	eduChanged.em = ppctContextTable[(int) ri->rc]->emEventReceived;
+	VIDEO_DBG("eduChanged.em:\n");
+	display_event_map(&eduChanged.em);
+
+	// While event has not occurred and there is still time remaining for wait
+	while (!(eduChanged.dw & eduRequested.dw) && (iTimerRemaining > 0)
+		&& result) {
+		pAstRVAS->video_intr_occurred = 0;
+		ulTimeStart = jiffies_to_msecs(jiffies);
+		result = video_os_sleep_on_timeout(&pAstRVAS->video_wait,
+			&pAstRVAS->video_intr_occurred, iTimerRemaining);
+		ulTimeEnd = jiffies_to_msecs(jiffies);
+		ulElapsedTime = (ulTimeEnd - ulTimeStart);
+		iTimerRemaining -= (int) ulElapsedTime;
+		eduChanged.em = ppctContextTable[(int) ri->rc]->emEventReceived;
+//    VIDEO_DBG("Elapsedtime [%u], timestart[%u], timeend[%u]\n", dwElapsedTime, dwTimeStart, dwTimeEnd);
+
+		VIDEO_DBG("ulElapsedTime [%lu], ulTimeStart[%lu], ulTimeEnd[%lu]\n",
+			ulElapsedTime, ulTimeStart, ulTimeEnd);
+		VIDEO_DBG("HZ [%ul]\n", HZ);
+		VIDEO_DBG("result [%u], iTimerRemaining [%d]\n", result,
+			iTimerRemaining);
+	}
+
+	if (result == 0 && ri->time_out != 0) {
+		VIDEO_DBG("IOCTL Timedout\n");
+		ri->rs = TimedOut;
+		memset(&(ri->em), 0x0, sizeof(struct EventMap));
+	} else {
+		eduChanged.em = ppctContextTable[(int) ri->rc]->emEventReceived;
+		VIDEO_DBG("Event Received[%X]\n", eduChanged.dw);
+		// Mask out the changes we are waiting on
+		eduReturned.dw = eduChanged.dw & eduRequested.dw;
+
+		// Reset flags of changes that have been returned
+		eduChanged.dw &= ~(eduReturned.dw);
+		VIDEO_DBG("Event Reset[%X]\n", eduChanged.dw);
+		ppctContextTable[(int) ri->rc]->emEventReceived = eduChanged.em;
+
+		// Copy changes back to ri
+		ri->em = eduReturned.em;
+		VIDEO_DBG("ri->em:\n");
+		display_event_map(&ri->em);
+		ri->rs = SuccessStatus;
+		VIDEO_DBG("Success [%x]\n",
+			eduReturned.dw);
+	}
+}
+
+static void update_context_events(struct AstRVAS *pAstRVAS,
+		union EmDwordUnion eduFge_status)
+{
+	union EmDwordUnion eduEmReceived;
+	u32 dwIter = 0;
+	struct ContextTable **ppctContextTable = pAstRVAS->ppctContextTable;
+	// VIDEO_DBG("Setting up context\n");
+	for (dwIter = 0; dwIter < MAX_NUM_CONTEXT; ++dwIter) {
+		if (ppctContextTable[dwIter] != NULL) {
+			//          VIDEO_DBG ("Copying EventMap to RVAS Context\n");
+			memcpy((void *) &eduEmReceived,
+					(void *) &(ppctContextTable[dwIter]->emEventReceived),
+					sizeof(union EmDwordUnion));
+			eduEmReceived.dw |= eduFge_status.dw;
+			memcpy(
+					(void *) &(ppctContextTable[dwIter]->emEventReceived),
+					(void *) &eduEmReceived,
+					sizeof(union EmDwordUnion));
+		}
+	}
+	pAstRVAS->video_intr_occurred = 1;
+	video_ss_wakeup_on_timeout(&pAstRVAS->video_wait);
+}
+
+static irqreturn_t fge_handler(int irq, void *dev_id)
+{
+	union EmDwordUnion eduFge_status;
+	u32 tse_sts = 0;
+	u32 dwGRCEStatus = 0;
+	bool bFgeItr = false;
+	bool bTfeItr = false;
+	bool bBSEItr = false;
+	bool bLdmaItr = false;
+	bool vg_changed = false;
+	u32 dw_screen_offset = 0;
+	struct AstRVAS *pAstRVAS = (struct AstRVAS *) dev_id;
+	struct VideoGeometry *cur_vg = NULL;
+
+	memset(&eduFge_status, 0x0, sizeof(union EmDwordUnion));
+	bFgeItr = false;
+	VIDEO_DBG("fge_handler");
+	// Checking for GRC status changes
+	dwGRCEStatus = readl((void *)(pAstRVAS->grce_reg_base + GRCE_STATUS_REGISTER));
+	if (dwGRCEStatus & GRC_INT_STS_MASK) {
+		VIDEO_DBG("GRC Status Changed: %#x\n", dwGRCEStatus);
+		eduFge_status.dw |= dwGRCEStatus & GRC_INT_STS_MASK;
+		bFgeItr = true;
+
+		if (dwGRCEStatus & 0x30) {
+			dw_screen_offset = get_screen_offset(pAstRVAS);
+
+			if (pAstRVAS->dwScreenOffset != dw_screen_offset) {
+				pAstRVAS->dwScreenOffset = dw_screen_offset;
+				vg_changed = true;
+			}
+		}
+	}
+	vg_changed |= video_geometry_change(pAstRVAS, dwGRCEStatus);
+	if (vg_changed) {
+		eduFge_status.em.bGeometryChanged = true;
+		bFgeItr = true;
+		set_snoop_engine(vg_changed, pAstRVAS);
+		video_set_Window(pAstRVAS);
+		VIDEO_DBG("Geometry has changed\n");
+		VIDEO_DBG("Reconfigure TSE\n");
+	}
+	// Checking and clear TSE Intr Status
+	tse_sts = clear_tse_interrupt(pAstRVAS);
+
+	if (tse_sts & TSSTS_ALL) {
+		bFgeItr = true;
+		if (tse_sts & (TSSTS_TC_SCREEN0|TSSTS_TC_SCREEN1)) {
+			eduFge_status.em.bSnoopChanged = 1;
+			cur_vg = &(pAstRVAS->current_vg);
+
+			if (cur_vg->gmt == TextMode) {
+				eduFge_status.em.bTextASCIIChanged = 1;
+				eduFge_status.em.bTextATTRChanged = 1;
+				eduFge_status.em.bTextFontChanged = 1;
+			}
+		}
+		if (tse_sts & TSSTS_ASCII) {
+			//VIDEO_DBG("Text Ascii Changed\n");
+			eduFge_status.em.bTextASCIIChanged = 1;
+		}
+
+		if (tse_sts & TSSTS_ATTR) {
+			//VIDEO_DBG("Text Attr Changed\n");
+			eduFge_status.em.bTextATTRChanged = 1;
+		}
+
+		if (tse_sts & TSSTS_FONT) {
+			//VIDEO_DBG("Text Font Changed\n");
+			eduFge_status.em.bTextFontChanged = 1;
+		}
+	}
+
+	if (clear_ldma_interrupt(pAstRVAS)) {
+		bLdmaItr = true;
+		pAstRVAS->ldma_engine.finished = 1;
+		video_ss_wakeup_on_timeout(&pAstRVAS->ldma_engine.wait);
+	}
+
+	if (clear_tfe_interrupt(pAstRVAS)) {
+		bTfeItr = true;
+		pAstRVAS->tfe_engine.finished = 1;
+		video_ss_wakeup_on_timeout(&pAstRVAS->tfe_engine.wait);
+	}
+
+	if (clear_bse_interrupt(pAstRVAS)) {
+		bBSEItr = true;
+		pAstRVAS->bse_engine.finished = 1;
+		video_ss_wakeup_on_timeout(&pAstRVAS->bse_engine.wait);
+	}
+
+	if ((!bFgeItr) && (!bTfeItr) && (!bBSEItr) && (!bLdmaItr)) {
+		//VIDEO_DBG(" Unknown Interrupt\n");
+//      VIDEO_DBG("TFE CRT [%#x].", *fge_intr);
+		return IRQ_NONE;
+	}
+
+	if (bFgeItr) {
+		update_context_events(pAstRVAS, eduFge_status);
+		pAstRVAS->video_intr_occurred = 1;
+		video_ss_wakeup_on_timeout(&pAstRVAS->video_wait);
+	}
+
+	return IRQ_HANDLED;
+}
+
+/*Sleep and Wakeup Functions*/
+
+void video_os_init_sleep_struct(struct Video_OsSleepStruct *Sleep)
+{
+	init_waitqueue_head(&(Sleep->queue));
+	Sleep->Timeout = 0;
+}
+
+void video_ss_wakeup_on_timeout(struct Video_OsSleepStruct *Sleep)
+{
+	/* Wakeup Process and Kill timeout handler */
+	wake_up(&(Sleep->queue));
+}
+
+long video_os_sleep_on_timeout(struct Video_OsSleepStruct *Sleep, u8 *Var, long msecs)
+{
+	long timeout; /* In jiffies */
+	u8 *Condition = Var;
+	/* Sleep on the Condition for a wakeup */
+	timeout = wait_event_interruptible_timeout(Sleep->queue,
+		(*Condition == 1), msecs_to_jiffies(msecs));
+
+	return timeout;
+}
+
+void disable_video_engines(struct AstRVAS *pAstRVAS)
+{
+	clk_disable(pAstRVAS->eclk);
+	clk_disable(pAstRVAS->vclk);
+}
+
+void enable_video_engines(struct AstRVAS *pAstRVAS)
+{
+	clk_enable(pAstRVAS->eclk);
+	clk_enable(pAstRVAS->vclk);
+}
+
+
+void disable_rvas_engines(struct AstRVAS *pAstRVAS)
+{
+	clk_disable(pAstRVAS->rvasclk);
+}
+
+void enable_rvas_engines(struct AstRVAS *pAstRVAS)
+{
+	// clk enable does
+	//	reset engine reset at SCU040
+	//	delay 100 us
+	//	enable clock at SCU080
+	//	delay 10ms
+	//	disable engine reset at SCU040
+	clk_enable(pAstRVAS->rvasclk);
+}
+
+
+static void reset_rvas_engine(struct AstRVAS *pAstRVAS)
+{
+	disable_rvas_engines(pAstRVAS);
+	enable_rvas_engines(pAstRVAS);
+	rvas_init();
+}
+
+static void reset_video_engine(struct AstRVAS *pAstRVAS)
+{
+	disable_video_engines(pAstRVAS);
+	enable_video_engines(pAstRVAS);
+	video_engine_init();
+}
+
+void ioctl_reset_video_engine(struct RvasIoctl *ri, struct AstRVAS *pAstRVAS)
+{
+	enum ResetEngineMode resetMode = ri->resetMode;
+
+	switch (resetMode) {
+	case  ResetAll:
+		VIDEO_ENG_DBG("reset all engine\n");
+		reset_rvas_engine(pAstRVAS);
+		reset_video_engine(pAstRVAS);
+		break;
+	case ResetRvasEngine:
+		VIDEO_ENG_DBG("reset rvas engine\n");
+		reset_rvas_engine(pAstRVAS);
+		break;
+	case ResetVeEngine:
+		VIDEO_ENG_DBG("reset video engine\n");
+		reset_video_engine(pAstRVAS);
+		break;
+	default:
+		dev_err(pAstRVAS->pdev, "Error resetting: no such mode: %d\n", resetMode);
+		break;
+	}
+
+	if (ri)
+		ri->rs = SuccessStatus;
+
+}
+
+static ssize_t rvas_reset_store(struct device *dev, struct device_attribute *attr, const char *buf, size_t count)
+{
+	struct AstRVAS *pAstRVAS = dev_get_drvdata(dev);
+	u32 val = kstrtoul(buf, 10, NULL);
+
+	if (val)
+		ioctl_reset_video_engine(NULL, pAstRVAS);
+
+	return count;
+}
+
+static DEVICE_ATTR_WO(rvas_reset);
+
+static struct attribute *ast_rvas_attributes[] = {
+	&dev_attr_rvas_reset.attr,
+	NULL
+};
+
+static const struct attribute_group rvas_attribute_group = {
+	.attrs = ast_rvas_attributes
+};
+
+bool sleep_on_tfe_busy(struct AstRVAS *pAstRVAS, u32 dwTFEDescriptorAddr,
+	u32 dwTFEControlR, u32 dwTFERleLimitor, u32 *pdwRLESize,
+	u32 *pdwCheckSum)
+{
+	u32 addrTFEDTBR = pAstRVAS->fg_reg_base + TFE_Descriptor_Table_Offset;
+	u32 addrTFECR = pAstRVAS->fg_reg_base + TFE_Descriptor_Control_Resgister;
+	u32 addrTFERleL = pAstRVAS->fg_reg_base + TFE_RLE_LIMITOR;
+	u32 addrTFERSTS = pAstRVAS->fg_reg_base + TFE_Status_Register;
+	bool bResult = true;
+
+	down(&pAstRVAS->tfe_engine.sem);
+	VIDEO_DBG("In Busy Semaphore......\n");
+
+	VIDEO_DBG("Before change, TFECR: %#x\n", readl((void *)addrTFECR));
+	writel(dwTFEControlR, (void *)addrTFECR);
+	VIDEO_DBG("After change, TFECR: %#x\n", readl((void *)addrTFECR));
+	writel(dwTFERleLimitor, (void *)addrTFERleL);
+	VIDEO_DBG("dwTFEControlR: %#x\n", dwTFEControlR);
+	VIDEO_DBG("dwTFERleLimitor: %#x\n", dwTFERleLimitor);
+	VIDEO_DBG("dwTFEDescriptorAddr: %#x\n", dwTFEDescriptorAddr);
+	// put descriptor add to TBR and Fetch start
+	writel(dwTFEDescriptorAddr, (void *)addrTFEDTBR);
+	//wTFETiles = 1;
+	pAstRVAS->tfe_engine.finished = 0;
+	video_os_sleep_on_timeout(&pAstRVAS->tfe_engine.wait,
+		&pAstRVAS->tfe_engine.finished, TFE_TIMEOUT_IN_MS);
+
+	if (!pAstRVAS->tfe_engine.finished) {
+		dev_err(pAstRVAS->pdev, "Video TFE failed\n");
+		writel(0x00, (void *)addrTFERSTS);
+		pAstRVAS->tfe_engine.finished = 1;
+		bResult = false;
+	}
+
+	writel((readl((void *)addrTFECR)&(~0x3)), (void *)addrTFECR); // Disable IRQ and Turn off TFE when done
+	*pdwRLESize = readl((void *)(pAstRVAS->fg_reg_base + TFE_RLE_Byte_Count));
+	*pdwCheckSum = readl((void *)(pAstRVAS->fg_reg_base + TFE_RLE_CheckSum));
+
+	up(&pAstRVAS->tfe_engine.sem);
+	VIDEO_DBG("Done Busy: bResult: %d\n", bResult);
+
+	return bResult;
+}
+
+bool sleep_on_tfe_text_busy(struct AstRVAS *pAstRVAS, u32 dwTFEDescriptorAddr,
+	u32 dwTFEControlR, u32 dwTFERleLimitor, u32 *pdwRLESize,
+	u32 *pdwCheckSum)
+{
+	u32 addrTFEDTBR = pAstRVAS->fg_reg_base + TFE_Descriptor_Table_Offset;
+	u32 addrTFECR = pAstRVAS->fg_reg_base + TFE_Descriptor_Control_Resgister;
+	u32 addrTFERleL = pAstRVAS->fg_reg_base + TFE_RLE_LIMITOR;
+	u32 addrTFERSTS = pAstRVAS->fg_reg_base + TFE_Status_Register;
+	bool bResult = true;
+
+	down(&pAstRVAS->tfe_engine.sem);
+	VIDEO_DBG("In Busy Semaphore......\n");
+
+	VIDEO_DBG("Before change, TFECR: %#x\n", readl((void *)addrTFECR));
+	writel(dwTFEControlR, (void *)addrTFECR);
+	VIDEO_DBG("After change, TFECR: %#x\n", readl((void *)addrTFECR));
+	writel(dwTFERleLimitor, (void *)addrTFERleL);
+	VIDEO_DBG("dwTFEControlR: %#x\n", dwTFEControlR);
+	VIDEO_DBG("dwTFERleLimitor: %#x\n", dwTFERleLimitor);
+	VIDEO_DBG("dwTFEDescriptorAddr: %#x\n", dwTFEDescriptorAddr);
+	// put descriptor add to TBR and Fetch start
+	writel(dwTFEDescriptorAddr, (void *)addrTFEDTBR);
+	//wTFETiles = 1;
+	pAstRVAS->tfe_engine.finished = 0;
+	video_os_sleep_on_timeout(&pAstRVAS->tfe_engine.wait,
+		&pAstRVAS->tfe_engine.finished, TFE_TIMEOUT_IN_MS);
+
+	if (!pAstRVAS->tfe_engine.finished) {
+		dev_err(pAstRVAS->pdev, "Video TFE failed\n");
+		writel(0x00, (void *)addrTFERSTS);
+		pAstRVAS->tfe_engine.finished = 1;
+		bResult = false;
+	}
+
+	writel((readl((void *)addrTFECR)&(~0x3)), (void *)addrTFECR);// Disable IRQ and Turn off TFE when done
+	writel((readl((void *)addrTFERSTS)|0x2), (void *)addrTFERSTS); // clear status bit
+	*pdwRLESize = readl((void *)(pAstRVAS->fg_reg_base + TFE_RLE_Byte_Count));
+	*pdwCheckSum = readl((void *)(pAstRVAS->fg_reg_base + TFE_RLE_CheckSum));
+
+	up(&pAstRVAS->tfe_engine.sem);
+	VIDEO_DBG("Done Busy: bResult: %d\n", bResult);
+
+	return bResult;
+}
+
+bool sleep_on_bse_busy(struct AstRVAS *pAstRVAS, u32 dwBSEDescriptorAddr,
+		struct BSEAggregateRegister aBSEAR, u32 size)
+{
+	u32 addrBSEDTBR = pAstRVAS->fg_reg_base + BSE_Descriptor_Table_Base_Register;
+	u32 addrBSCR = pAstRVAS->fg_reg_base + BSE_Command_Register;
+	u32 addrBSDBS = pAstRVAS->fg_reg_base + BSE_Destination_Buket_Size_Resgister;
+	u32 addrBSBPS0 = pAstRVAS->fg_reg_base + BSE_Bit_Position_Register_0;
+	u32 addrBSBPS1 = pAstRVAS->fg_reg_base + BSE_Bit_Position_Register_1;
+	u32 addrBSBPS2 = pAstRVAS->fg_reg_base + BSE_Bit_Position_Register_2;
+	u32 addrBSESSTS = pAstRVAS->fg_reg_base + BSE_Status_Register;
+	u8 byCounter = 0;
+	bool bResult = true;
+
+	down(&pAstRVAS->bse_engine.sem);
+	pAstRVAS->bse_engine.finished = 0;
+
+    // Set BSE Temp buffer address, and clear lower u16
+	writel(BSE_LMEM_Temp_Buffer_Offset << 16, (void *)addrBSCR);
+	writel(readl((void *)addrBSCR)|(aBSEAR.dwBSCR & 0X00000FFF), (void *)addrBSCR);
+	writel(aBSEAR.dwBSDBS, (void *)addrBSDBS);
+	writel(aBSEAR.adwBSBPS[0], (void *)addrBSBPS0);
+	writel(aBSEAR.adwBSBPS[1], (void *)addrBSBPS1);
+	writel(aBSEAR.adwBSBPS[2], (void *)addrBSBPS2);
+
+	writel(dwBSEDescriptorAddr, (void *)addrBSEDTBR);
+
+	while (!pAstRVAS->bse_engine.finished) {
+		VIDEO_DBG("BSE Sleeping...\n");
+		video_os_sleep_on_timeout(&pAstRVAS->bse_engine.wait,
+			&pAstRVAS->bse_engine.finished, 1000); // loop if bse timedout
+		byCounter++;
+		VIDEO_DBG("Back from BSE Sleeping, finished: %u\n",
+			pAstRVAS->bse_engine.finished);
+
+		if (byCounter == ENGINE_TIMEOUT_IN_SECONDS) {
+			writel(0x00, (void *)addrBSESSTS);
+			pAstRVAS->bse_engine.finished = 1;
+			dev_err(pAstRVAS->pdev, "TIMEOUT::Waiting BSE\n");
+			bResult = false;
+		}
+	}
+
+	VIDEO_DBG("*pdwBSESSTS = %#x\n", readl((void *)addrBSESSTS));
+	writel(readl((void *)addrBSCR)&(~0x3), (void *)addrBSCR);
+
+	up(&pAstRVAS->bse_engine.sem);
+
+	return bResult;
+}
+
+void sleep_on_ldma_busy(struct AstRVAS *pAstRVAS, u32 dwDescriptorAddress)
+{
+	u32 addrLDMADTBR = pAstRVAS->fg_reg_base + LDMA_Descriptor_Table_Base_Register;
+	u32 addrLDMAControlR = pAstRVAS->fg_reg_base + LDMA_Control_Register;
+
+	VIDEO_DBG("In sleepONldma busy\n");
+
+	down(&pAstRVAS->ldma_engine.sem);
+
+	pAstRVAS->ldma_engine.finished = 0;
+
+	writel(0x83, (void *)addrLDMAControlR);// descriptor can only in LMEM FOR LDMA
+	writel(dwDescriptorAddress, (void *)addrLDMADTBR);
+	VIDEO_DBG("LDMA: control [%#x]\n", readl((void *)addrLDMAControlR));
+	VIDEO_DBG("LDMA:  DTBR  [%#x]\n", readl((void *)addrLDMADTBR));
+
+	while (!pAstRVAS->ldma_engine.finished)
+		video_os_sleep_on_timeout(&pAstRVAS->ldma_engine.wait, (u8 *)&pAstRVAS->ldma_engine.finished, 1000); // loop if bse timedout
+
+	VIDEO_DBG("LDMA wake up\n");
+	writel(readl((void *)addrLDMAControlR)&(~0x3), (void *)addrLDMAControlR);
+	up(&pAstRVAS->ldma_engine.sem);
+}
+
+static int video_drv_get_resources(struct platform_device *pdev)
+{
+	int result = 0;
+
+	struct resource *io_fg;
+	struct resource *io_grc;
+	struct resource *io_video;
+
+	//get resources from platform
+	io_fg = platform_get_resource(pdev, IORESOURCE_MEM, 0);
+	VIDEO_DBG("io_fg: 0x%p\n", io_fg);
+
+	if (io_fg == NULL) {
+		dev_err(&pdev->dev, "No Frame Grabber IORESOURCE_MEM entry\n");
+		return -ENOENT;
+	}
+	io_grc = platform_get_resource(pdev, IORESOURCE_MEM, 1);
+	VIDEO_DBG("io_grc: 0x%p\n", io_grc);
+	if (io_grc == NULL) {
+		dev_err(&pdev->dev, "No GRCE IORESOURCE_MEM entry\n");
+		return -ENOENT;
+	}
+	io_video = platform_get_resource(pdev, IORESOURCE_MEM, 2);
+	VIDEO_DBG("io_video: 0x%p\n", io_video);
+	if (io_video == NULL) {
+		dev_err(&pdev->dev, "No video compression IORESOURCE_MEM entry\n");
+		return -ENOENT;
+	}
+
+	//map resource by device
+	pAstRVAS->fg_reg_base = (u32) devm_ioremap_resource(&pdev->dev, io_fg);
+	VIDEO_DBG("fg_reg_base: %#x\n", pAstRVAS->fg_reg_base);
+	if (IS_ERR((void *) pAstRVAS->fg_reg_base)) {
+		result = PTR_ERR((void *) pAstRVAS->fg_reg_base);
+		dev_err(&pdev->dev, "Cannot map FG registers\n");
+		pAstRVAS->fg_reg_base = 0;
+		return result;
+	}
+	pAstRVAS->grce_reg_base = (u32) devm_ioremap_resource(&pdev->dev,
+			  io_grc);
+	VIDEO_DBG("grce_reg_base: %#x\n", pAstRVAS->grce_reg_base);
+	if (IS_ERR((void *) pAstRVAS->grce_reg_base)) {
+		result = PTR_ERR((void *) pAstRVAS->grce_reg_base);
+		dev_err(&pdev->dev, "Cannot map GRC registers\n");
+		pAstRVAS->grce_reg_base = 0;
+		return result;
+	}
+	pAstRVAS->video_reg_base = (u32) devm_ioremap_resource(&pdev->dev,
+				  io_video);
+	VIDEO_DBG("video_reg_base: %#x\n", pAstRVAS->video_reg_base);
+	if (IS_ERR((void *) pAstRVAS->video_reg_base)) {
+		result = PTR_ERR((void *) pAstRVAS->video_reg_base);
+		dev_err(&pdev->dev, "Cannot map video registers\n");
+		pAstRVAS->video_reg_base = 0;
+		return result;
+	}
+	return 0;
+}
+
+static int video_drv_get_irqs(struct platform_device *pdev)
+{
+	pAstRVAS->irq_fge = platform_get_irq(pdev, 0);
+	VIDEO_DBG("irq_fge: %#x\n", pAstRVAS->irq_fge);
+	if (pAstRVAS->irq_fge < 0) {
+		dev_err(&pdev->dev, "NO FGE irq entry\n");
+		return -ENOENT;
+	}
+	pAstRVAS->irq_vga = platform_get_irq(pdev, 1);
+	VIDEO_DBG("irq_vga: %#x\n", pAstRVAS->irq_vga);
+		if (pAstRVAS->irq_vga < 0) {
+			dev_err(&pdev->dev, "NO VGA irq entry\n");
+			return -ENOENT;
+		}
+	pAstRVAS->irq_video = platform_get_irq(pdev, 2);
+	VIDEO_DBG("irq_video: %#x\n", pAstRVAS->irq_video);
+	if (pAstRVAS->irq_video < 0) {
+		dev_err(&pdev->dev, "NO video compression entry\n");
+		return -ENOENT;
+	}
+	return 0;
+}
+
+static int video_drv_get_clock(struct platform_device *pdev)
+{
+
+	pAstRVAS->eclk = devm_clk_get(&pdev->dev, "eclk");
+	if (IS_ERR(pAstRVAS->eclk)) {
+		dev_err(&pdev->dev, "no eclk clock defined\n");
+		return PTR_ERR(pAstRVAS->eclk);
+	}
+
+	clk_prepare_enable(pAstRVAS->eclk);
+
+	pAstRVAS->vclk = devm_clk_get(&pdev->dev, "vclk");
+	if (IS_ERR(pAstRVAS->vclk)) {
+		dev_err(&pdev->dev, "no vclk clock defined\n");
+		return PTR_ERR(pAstRVAS->vclk);
+	}
+
+	clk_prepare_enable(pAstRVAS->vclk);
+
+	pAstRVAS->rvasclk = devm_clk_get(&pdev->dev, "rvasclk-gate");
+	if (IS_ERR(pAstRVAS->rvasclk)) {
+		dev_err(&pdev->dev, "no rvasclk clock defined\n");
+		return PTR_ERR(pAstRVAS->rvasclk);
+	}
+
+	clk_prepare_enable(pAstRVAS->rvasclk);
+	return 0;
+}
+
+static int video_drv_map_irqs(struct platform_device *pdev)
+{
+	int result = 0;
+	//Map IRQS to handler
+	VIDEO_DBG("Requesting IRQs, irq_fge: %d, irq_vga: %d, irq_video: %d\n",
+			  pAstRVAS->irq_fge, pAstRVAS->irq_vga, pAstRVAS->irq_video);
+
+	result = devm_request_irq(&pdev->dev, pAstRVAS->irq_fge, fge_handler, 0,
+			  dev_name(&pdev->dev), pAstRVAS);
+	if (result) {
+		pr_err("Error in requesting IRQ\n");
+		pr_err("RVAS: Failed request FGE irq %d\n", pAstRVAS->irq_fge);
+		misc_deregister(&video_misc);
+		return result;
+	}
+
+	result = devm_request_irq(&pdev->dev, pAstRVAS->irq_vga, fge_handler, 0,
+				  dev_name(&pdev->dev), pAstRVAS);
+	if (result) {
+		pr_err("Error in requesting IRQ\n");
+		pr_err("RVAS: Failed request vga irq %d\n", pAstRVAS->irq_vga);
+		misc_deregister(&video_misc);
+		return result;
+	}
+
+	result = devm_request_irq(&pdev->dev, pAstRVAS->irq_video, ast_video_isr, 0,
+				  dev_name(&pdev->dev), pAstRVAS);
+	if (result) {
+		pr_err("Error in requesting IRQ\n");
+		pr_err("RVAS: Failed request video irq %d\n", pAstRVAS->irq_video);
+		misc_deregister(&video_misc);
+		return result;
+	}
+
+	return result;
+}
+//
+//
+//
+static int video_drv_probe(struct platform_device *pdev)
+{
+	int result = 0;
+
+	struct regmap *sdram_scu;
+	struct device_node *dp_node;
+	struct device_node *edac_node;
+	void __iomem *mcr_base;
+
+	VIDEO_DBG("RVAS driver probe\n");
+	pAstRVAS = devm_kzalloc(&pdev->dev, sizeof(struct AstRVAS), GFP_KERNEL);
+	VIDEO_DBG("pAstRVAS: 0x%p\n", pAstRVAS);
+
+	if (!pAstRVAS) {
+		dev_err(pAstRVAS->pdev, "Cannot allocate device structure\n");
+		return -ENOMEM;
+	}
+	pAstRVAS->pdev = (void *)&pdev->dev;
+
+
+	// Get resources
+	result = video_drv_get_resources(pdev);
+	if (result < 0) {
+		dev_err(pAstRVAS->pdev, "video_probe: Error getting resources\n");
+		return result;
+	}
+
+	//get irqs
+	result = video_drv_get_irqs(pdev);
+	if (result < 0) {
+		dev_err(pAstRVAS->pdev, "video_probe: Error getting irqs\n");
+		return result;
+	}
+
+
+	pAstRVAS->rvas_reset = devm_reset_control_get_by_index(&pdev->dev, 0);
+	if (IS_ERR(pAstRVAS->rvas_reset)) {
+		dev_err(&pdev->dev, "can't get rvas reset\n");
+		return -ENOENT;
+	}
+
+	pAstRVAS->video_engine_reset = devm_reset_control_get_by_index(&pdev->dev, 1);
+	if (IS_ERR(pAstRVAS->video_engine_reset)) {
+		dev_err(&pdev->dev, "can't get video engine reset\n");
+		return -ENOENT;
+	}
+
+	//prepare video engine clock
+	result = video_drv_get_clock(pdev);
+	if (result < 0) {
+		dev_err(pAstRVAS->pdev, "video_probe: Error getting clocks\n");
+		return result;
+	}
+
+	dp_node = of_find_compatible_node(NULL, NULL, "aspeed,ast2600-displayport");
+	if (!dp_node)
+		dev_err(&pdev->dev, "cannot find dp node\n");
+	else {
+		dp_base = of_iomap(dp_node, 0);
+		if (!dp_base)
+			dev_err(&pdev->dev, "failed to iomem of display port\n");
+	}
+
+	edac_node = of_find_compatible_node(NULL, NULL, "aspeed,ast2600-sdram-edac");
+	if (!edac_node)
+		dev_err(&pdev->dev, "cannot find edac node\n");
+	else {
+		mcr_base = of_iomap(edac_node, 0);
+		if (!mcr_base)
+			dev_err(&pdev->dev, "failed to iomem of MCR\n");
+	}
+
+	set_FBInfo_size(pAstRVAS, mcr_base);
+
+	//scu
+	sdram_scu = syscon_regmap_lookup_by_compatible("aspeed,ast2600-scu");
+	VIDEO_DBG("sdram_scu: 0x%p\n", sdram_scu);
+	if (IS_ERR(sdram_scu)) {
+		dev_err(&pdev->dev, "failed to find ast2600-scu regmap\n");
+		return PTR_ERR(sdram_scu);
+	}
+	pAstRVAS->scu = sdram_scu;
+
+	result = misc_register(&video_misc);
+	if (result) {
+		pr_err("Failed in miscellaneous register (err: %d)\n", result);
+		return result;
+	}
+	pr_info("Video misc minor %d\n", video_misc.minor);
+
+	if (sysfs_create_group(&pdev->dev.kobj, &rvas_attribute_group)) {
+		pr_err("Failed in creating group\n");
+		return -1;
+	}
+
+	VIDEO_DBG("Disabling interrupts...\n");
+	disable_grce_tse_interrupt(pAstRVAS);
+
+	//reserve memory
+	of_reserved_mem_device_init(&pdev->dev);
+	result = dma_set_mask_and_coherent(&pdev->dev, DMA_BIT_MASK(32));
+	if (result) {
+		dev_err(&pdev->dev, "Failed to set DMA mask\n");
+		of_reserved_mem_device_release(&pdev->dev);
+	}
+
+	// map irqs to irq_handlers
+	result = video_drv_map_irqs(pdev);
+	if (result < 0) {
+		dev_err(pAstRVAS->pdev, "video_probe: Error mapping irqs\n");
+		return result;
+	}
+	VIDEO_DBG("After IRQ registration\n");
+
+
+	platform_set_drvdata(pdev, pAstRVAS);
+	pAstRVAS->rvas_dev = &video_misc;
+	VIDEO_DBG("pdev: 0x%p dev: 0x%p pAstRVAS: 0x%p rvas_dev: 0x%p\n", pdev,
+		&pdev->dev, pAstRVAS, pAstRVAS->rvas_dev);
+
+	init_osr_es(pAstRVAS);
+	rvas_init();
+	video_engine_reserveMem(pAstRVAS);
+	video_engine_init();
+
+
+	pr_info("RVAS: driver successfully loaded.\n");
+	return result;
+}
+
+static void rvas_init(void)
+{
+	VIDEO_ENG_DBG("\n");
+
+	reset_snoop_engine(pAstRVAS);
+	update_video_geometry(pAstRVAS);
+
+	set_snoop_engine(true, pAstRVAS);
+	enable_grce_tse_interrupt(pAstRVAS);
+}
+
+static void video_engine_init(void)
+{
+	VIDEO_ENG_DBG("\n");
+	// video engine
+	disable_video_interrupt(pAstRVAS);
+	video_ctrl_init(pAstRVAS);
+	video_engine_rc4Reset(pAstRVAS);
+	set_direct_mode(pAstRVAS);
+	video_set_Window(pAstRVAS);
+	enable_video_interrupt(pAstRVAS);
+}
+
+static int video_drv_remove(struct platform_device *pdev)
+{
+	struct AstRVAS *pAstRVAS = NULL;
+
+	VIDEO_DBG("\n");
+	pAstRVAS = platform_get_drvdata(pdev);
+
+	VIDEO_DBG("disable_grce_tse_interrupt...\n");
+	disable_grce_tse_interrupt(pAstRVAS);
+	disable_video_interrupt(pAstRVAS);
+
+	sysfs_remove_group(&pdev->dev.kobj, &rvas_attribute_group);
+
+	VIDEO_DBG("misc_deregister...\n");
+	misc_deregister(&video_misc);
+
+	VIDEO_DBG("Releasing OSRes...\n");
+	release_osr_es(pAstRVAS);
+	pr_info("RVAS: driver successfully unloaded.\n");
+	free_video_engine_memory(pAstRVAS);
+	return 0;
+}
+
+static const u32 ast2400_dram_table[] = {
+	0x04000000,     //64MB
+	0x08000000,     //128MB
+	0x10000000,     //256MB
+	0x20000000,     //512MB
+};
+
+static const u32 ast2500_dram_table[] = {
+	0x08000000,     //128MB
+	0x10000000,     //256MB
+	0x20000000,     //512MB
+	0x40000000,     //1024MB
+};
+
+static const u32 ast2600_dram_table[] = {
+	0x10000000,     //256MB
+	0x20000000,     //512MB
+	0x40000000,     //1024MB
+	0x80000000,     //2048MB
+};
+
+static const u32 aspeed_vga_table[] = {
+	0x800000,       //8MB
+	0x1000000,      //16MB
+	0x2000000,      //32MB
+	0x4000000,      //64MB
+};
+
+static void set_FBInfo_size(struct AstRVAS *pAstRVAS, void __iomem *mcr_base)
+{
+	u32 reg_mcr004 = readl(mcr_base + MCR_CONF);
+
+#if defined(CONFIG_MACH_ASPEED_G6)
+	pAstRVAS->FBInfo.dwDRAMSize = ast2600_dram_table[reg_mcr004 & 0x3];
+#elif defined(CONFIG_MACH_ASPEED_G5)
+	pAstRVAS->FBInfo.dwDRAMSize = ast2500_dram_table[reg_mcr004 & 0x3];
+#else
+	pAstRVAS->FBInfo.dwDRAMSize = ast2400_dram_table[reg_mcr004 & 0x3];
+#endif
+
+	pAstRVAS->FBInfo.dwVGASize = aspeed_vga_table[((reg_mcr004 & 0xC) >> 2)];
+
+}
+
+static const struct of_device_id ast_rvas_match[] = { { .compatible =
+	"aspeed,ast2600-rvas", }, { }, };
+
+MODULE_DEVICE_TABLE(of, ast_rvas_match);
+
+static struct platform_driver video_driver = {
+	.probe = video_drv_probe,
+	.remove = video_drv_remove,
+	.driver = { .of_match_table = of_match_ptr(ast_rvas_match), .name =
+		RVAS_DRIVER_NAME, .owner = THIS_MODULE, }, };
+
+module_platform_driver(video_driver);
+
+MODULE_AUTHOR("ASPEED Technology");
+MODULE_DESCRIPTION("RVAS video driver module for AST2600");
+MODULE_LICENSE("GPL");
diff --git a/fs/ext4/file.c b/fs/ext4/file.c
index ac0e11bbb445..b25c1f8f7c4f 100644
--- a/fs/ext4/file.c
+++ b/fs/ext4/file.c
@@ -74,7 +74,7 @@ static ssize_t ext4_dio_read_iter(struct kiocb *iocb, struct iov_iter *to)
 		return generic_file_read_iter(iocb, to);
 	}
 
-	ret = iomap_dio_rw(iocb, to, &ext4_iomap_ops, NULL, 0);
+	ret = iomap_dio_rw(iocb, to, &ext4_iomap_ops, NULL, 0, 0);
 	inode_unlock_shared(inode);
 
 	file_accessed(iocb->ki_filp);
@@ -566,7 +566,8 @@ static ssize_t ext4_dio_write_iter(struct kiocb *iocb, struct iov_iter *from)
 	if (ilock_shared)
 		iomap_ops = &ext4_iomap_overwrite_ops;
 	ret = iomap_dio_rw(iocb, from, iomap_ops, &ext4_dio_write_ops,
-			   (unaligned_io || extend) ? IOMAP_DIO_FORCE_WAIT : 0);
+			   (unaligned_io || extend) ? IOMAP_DIO_FORCE_WAIT : 0,
+			   0);
 	if (ret == -ENOTBLK)
 		ret = 0;
 
diff --git a/fs/iomap/buffered-io.c b/fs/iomap/buffered-io.c
index 97119ec3b850..fe10d8a30f6b 100644
--- a/fs/iomap/buffered-io.c
+++ b/fs/iomap/buffered-io.c
@@ -757,7 +757,7 @@ static loff_t iomap_write_iter(struct iomap_iter *iter, struct iov_iter *i)
 		 * same page as we're writing to, without it being marked
 		 * up-to-date.
 		 */
-		if (unlikely(iov_iter_fault_in_readable(i, bytes))) {
+		if (unlikely(fault_in_iov_iter_readable(i, bytes))) {
 			status = -EFAULT;
 			break;
 		}
diff --git a/fs/iomap/direct-io.c b/fs/iomap/direct-io.c
index 4ecd255e0511..468dcbba45bc 100644
--- a/fs/iomap/direct-io.c
+++ b/fs/iomap/direct-io.c
@@ -31,6 +31,7 @@ struct iomap_dio {
 	atomic_t		ref;
 	unsigned		flags;
 	int			error;
+	size_t			done_before;
 	bool			wait_for_completion;
 
 	union {
@@ -124,6 +125,9 @@ ssize_t iomap_dio_complete(struct iomap_dio *dio)
 	if (ret > 0 && (dio->flags & IOMAP_DIO_NEED_SYNC))
 		ret = generic_write_sync(iocb, ret);
 
+	if (ret > 0)
+		ret += dio->done_before;
+
 	kfree(dio);
 
 	return ret;
@@ -371,6 +375,8 @@ static loff_t iomap_dio_hole_iter(const struct iomap_iter *iter,
 	loff_t length = iov_iter_zero(iomap_length(iter), dio->submit.iter);
 
 	dio->size += length;
+	if (!length)
+		return -EFAULT;
 	return length;
 }
 
@@ -402,6 +408,8 @@ static loff_t iomap_dio_inline_iter(const struct iomap_iter *iomi,
 		copied = copy_to_iter(inline_data, length, iter);
 	}
 	dio->size += copied;
+	if (!copied)
+		return -EFAULT;
 	return copied;
 }
 
@@ -446,13 +454,21 @@ static loff_t iomap_dio_iter(const struct iomap_iter *iter,
  * may be pure data writes. In that case, we still need to do a full data sync
  * completion.
  *
+ * When page faults are disabled and @dio_flags includes IOMAP_DIO_PARTIAL,
+ * __iomap_dio_rw can return a partial result if it encounters a non-resident
+ * page in @iter after preparing a transfer.  In that case, the non-resident
+ * pages can be faulted in and the request resumed with @done_before set to the
+ * number of bytes previously transferred.  The request will then complete with
+ * the correct total number of bytes transferred; this is essential for
+ * completing partial requests asynchronously.
+ *
  * Returns -ENOTBLK In case of a page invalidation invalidation failure for
  * writes.  The callers needs to fall back to buffered I/O in this case.
  */
 struct iomap_dio *
 __iomap_dio_rw(struct kiocb *iocb, struct iov_iter *iter,
 		const struct iomap_ops *ops, const struct iomap_dio_ops *dops,
-		unsigned int dio_flags)
+		unsigned int dio_flags, size_t done_before)
 {
 	struct address_space *mapping = iocb->ki_filp->f_mapping;
 	struct inode *inode = file_inode(iocb->ki_filp);
@@ -482,6 +498,7 @@ __iomap_dio_rw(struct kiocb *iocb, struct iov_iter *iter,
 	dio->dops = dops;
 	dio->error = 0;
 	dio->flags = 0;
+	dio->done_before = done_before;
 
 	dio->submit.iter = iter;
 	dio->submit.waiter = current;
@@ -577,6 +594,12 @@ __iomap_dio_rw(struct kiocb *iocb, struct iov_iter *iter,
 	if (iov_iter_rw(iter) == READ && iomi.pos >= dio->i_size)
 		iov_iter_revert(iter, iomi.pos - dio->i_size);
 
+	if (ret == -EFAULT && dio->size && (dio_flags & IOMAP_DIO_PARTIAL)) {
+		if (!(iocb->ki_flags & IOCB_NOWAIT))
+			wait_for_completion = true;
+		ret = 0;
+	}
+
 	/* magic error code to fall back to buffered I/O */
 	if (ret == -ENOTBLK) {
 		wait_for_completion = true;
@@ -642,11 +665,11 @@ EXPORT_SYMBOL_GPL(__iomap_dio_rw);
 ssize_t
 iomap_dio_rw(struct kiocb *iocb, struct iov_iter *iter,
 		const struct iomap_ops *ops, const struct iomap_dio_ops *dops,
-		unsigned int dio_flags)
+		unsigned int dio_flags, size_t done_before)
 {
 	struct iomap_dio *dio;
 
-	dio = __iomap_dio_rw(iocb, iter, ops, dops, dio_flags);
+	dio = __iomap_dio_rw(iocb, iter, ops, dops, dio_flags, done_before);
 	if (IS_ERR_OR_NULL(dio))
 		return PTR_ERR_OR_ZERO(dio);
 	return iomap_dio_complete(dio);
diff --git a/include/linux/aspeed-mctp.h b/include/linux/aspeed-mctp.h
new file mode 100644
index 000000000000..7fbbf301be3b
--- /dev/null
+++ b/include/linux/aspeed-mctp.h
@@ -0,0 +1,155 @@
+/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
+/* Copyright (c) 2020 Intel Corporation */
+
+#ifndef __LINUX_ASPEED_MCTP_H
+#define __LINUX_ASPEED_MCTP_H
+
+#include <linux/types.h>
+
+struct mctp_client;
+struct aspeed_mctp;
+
+struct pcie_transport_hdr {
+	u8 fmt_type;
+	u8 mbz;
+	u8 mbz_attr_len_hi;
+	u8 len_lo;
+	u16 requester;
+	u8 tag;
+	u8 code;
+	u16 target;
+	u16 vendor;
+} __packed;
+
+struct mctp_protocol_hdr {
+	u8 ver;
+	u8 dest;
+	u8 src;
+	u8 flags_seq_tag;
+} __packed;
+
+#define PCIE_VDM_HDR_SIZE 16
+#define MCTP_BTU_SIZE 64
+/* The MTU of the ASPEED MCTP can be 64/128/256 */
+#define ASPEED_MCTP_MTU MCTP_BTU_SIZE
+#define PCIE_VDM_DATA_SIZE_DW (ASPEED_MCTP_MTU / 4)
+#define PCIE_VDM_HDR_SIZE_DW (PCIE_VDM_HDR_SIZE / 4)
+
+#define PCIE_MCTP_MIN_PACKET_SIZE (PCIE_VDM_HDR_SIZE + 4)
+
+struct mctp_pcie_packet_data_2500 {
+	u32 data[32];
+};
+
+struct mctp_pcie_packet_data {
+	u32 hdr[PCIE_VDM_HDR_SIZE_DW];
+	u32 payload[PCIE_VDM_DATA_SIZE_DW];
+};
+
+struct mctp_pcie_packet {
+	struct mctp_pcie_packet_data data;
+	u32 size;
+};
+
+/**
+ * aspeed_mctp_add_type_handler() - register for the given MCTP message type
+ * @client: pointer to the existing mctp_client context
+ * @mctp_type: message type code according to DMTF DSP0239 spec.
+ * @pci_vendor_id: vendor ID (non-zero if msg_type is Vendor Defined PCI,
+ * otherwise it should be set to 0)
+ * @vdm_type: vendor defined message type (it should be set to 0 for non-Vendor
+ * Defined PCI message type)
+ * @vdm_mask: vendor defined message mask (it should be set to 0 for non-Vendor
+ * Defined PCI message type)
+ *
+ * Return:
+ * * 0 - success,
+ * * -EINVAL - arguments passed are incorrect,
+ * * -ENOMEM - cannot alloc a new handler,
+ * * -EBUSY - given message has already registered handler.
+ */
+
+int aspeed_mctp_add_type_handler(struct mctp_client *client, u8 mctp_type,
+				 u16 pci_vendor_id, u16 vdm_type, u16 vdm_mask);
+
+/**
+ * aspeed_mctp_create_client() - create mctp_client context
+ * @priv pointer to aspeed-mctp context
+ *
+ * Returns struct mctp_client or NULL.
+ */
+struct mctp_client *aspeed_mctp_create_client(struct aspeed_mctp *priv);
+
+/**
+ * aspeed_mctp_delete_client()- delete mctp_client context
+ * @client: pointer to existing mctp_client context
+ */
+void aspeed_mctp_delete_client(struct mctp_client *client);
+
+/**
+ * aspeed_mctp_send_packet() - send mctp_packet
+ * @client: pointer to existing mctp_client context
+ * @tx_packet: the allocated packet that needs to be send via aspeed-mctp
+ *
+ * After the function returns success, the packet is no longer owned by the
+ * caller, and as such, the caller should not attempt to free it.
+ *
+ * Return:
+ * * 0 - success,
+ * * -ENOSPC - failed to send packet due to lack of available space.
+ */
+int aspeed_mctp_send_packet(struct mctp_client *client,
+			    struct mctp_pcie_packet *tx_packet);
+
+/**
+ * aspeed_mctp_receive_packet() - receive mctp_packet
+ * @client: pointer to existing mctp_client context
+ * @timeout: timeout, in jiffies
+ *
+ * The function will sleep for up to @timeout if no packet is ready to read.
+ *
+ * After the function returns valid packet, the caller takes its ownership and
+ * is responsible for freeing it.
+ *
+ * Returns struct mctp_pcie_packet from or ERR_PTR in case of error or the
+ * @timeout elapsed.
+ */
+struct mctp_pcie_packet *aspeed_mctp_receive_packet(struct mctp_client *client,
+						    unsigned long timeout);
+
+/**
+ * aspeed_mctp_flush_rx_queue() - remove all mctp_packets from rx queue
+ * @client: pointer to existing mctp_client context
+ */
+void aspeed_mctp_flush_rx_queue(struct mctp_client *client);
+
+/**
+ * aspeed_mctp_get_eid_bdf() - return PCIe address for requested endpoint ID
+ * @client: pointer to existing mctp_client context
+ * @eid: requested eid
+ * @bdf: pointer to store BDF value
+ *
+ * Return:
+ * * 0 - success,
+ * * -ENOENT - there is no record for requested endpoint id.
+ */
+int aspeed_mctp_get_eid_bdf(struct mctp_client *client, u8 eid, u16 *bdf);
+
+/**
+ * aspeed_mctp_get_eid() - return EID for requested BDF and domainId.
+ * @client: pointer to existing mctp_client context
+ * @bdf: requested BDF value
+ * @domain_id: requested domainId
+ * @eid: pointer to store EID value
+ *
+ * Return:
+ * * 0 - success,
+ * * -ENOENT - there is no record for requested bdf/domainId.
+ */
+int aspeed_mctp_get_eid(struct mctp_client *client, u16 bdf,
+			u8 domain_id, u8 *eid);
+
+void *aspeed_mctp_packet_alloc(gfp_t flags);
+void aspeed_mctp_packet_free(void *packet);
+
+#endif /* __LINUX_ASPEED_MCTP_H */
diff --git a/include/linux/aspeed_pcie_io.h b/include/linux/aspeed_pcie_io.h
new file mode 100644
index 000000000000..e4b1f806aca8
--- /dev/null
+++ b/include/linux/aspeed_pcie_io.h
@@ -0,0 +1,9 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
+
+#ifndef __ASM_ARM_ARCH_IO_H
+#define __ASM_ARM_ARCH_IO_H
+
+extern u8 aspeed_pcie_inb(u32 addr);
+extern void aspeed_pcie_outb(u8 value, u32 addr);
+
+#endif
diff --git a/include/linux/bpf.h b/include/linux/bpf.h
index 15b690a0cecb..c5c4b6f09e23 100644
--- a/include/linux/bpf.h
+++ b/include/linux/bpf.h
@@ -293,6 +293,34 @@ bool bpf_map_meta_equal(const struct bpf_map *meta0,
 
 extern const struct bpf_map_ops bpf_map_offload_ops;
 
+/* bpf_type_flag contains a set of flags that are applicable to the values of
+ * arg_type, ret_type and reg_type. For example, a pointer value may be null,
+ * or a memory is read-only. We classify types into two categories: base types
+ * and extended types. Extended types are base types combined with a type flag.
+ *
+ * Currently there are no more than 32 base types in arg_type, ret_type and
+ * reg_types.
+ */
+#define BPF_BASE_TYPE_BITS	8
+
+enum bpf_type_flag {
+	/* PTR may be NULL. */
+	PTR_MAYBE_NULL		= BIT(0 + BPF_BASE_TYPE_BITS),
+
+	/* MEM is read-only. When applied on bpf_arg, it indicates the arg is
+	 * compatible with both mutable and immutable memory.
+	 */
+	MEM_RDONLY		= BIT(1 + BPF_BASE_TYPE_BITS),
+
+	__BPF_TYPE_LAST_FLAG	= MEM_RDONLY,
+};
+
+/* Max number of base types. */
+#define BPF_BASE_TYPE_LIMIT	(1UL << BPF_BASE_TYPE_BITS)
+
+/* Max number of all types. */
+#define BPF_TYPE_LIMIT		(__BPF_TYPE_LAST_FLAG | (__BPF_TYPE_LAST_FLAG - 1))
+
 /* function argument constraints */
 enum bpf_arg_type {
 	ARG_DONTCARE = 0,	/* unused argument in helper function */
@@ -304,13 +332,11 @@ enum bpf_arg_type {
 	ARG_PTR_TO_MAP_KEY,	/* pointer to stack used as map key */
 	ARG_PTR_TO_MAP_VALUE,	/* pointer to stack used as map value */
 	ARG_PTR_TO_UNINIT_MAP_VALUE,	/* pointer to valid memory used to store a map value */
-	ARG_PTR_TO_MAP_VALUE_OR_NULL,	/* pointer to stack used as map value or NULL */
 
 	/* the following constraints used to prototype bpf_memcmp() and other
 	 * functions that access data on eBPF program stack
 	 */
 	ARG_PTR_TO_MEM,		/* pointer to valid memory (stack, packet, map value) */
-	ARG_PTR_TO_MEM_OR_NULL, /* pointer to valid memory or NULL */
 	ARG_PTR_TO_UNINIT_MEM,	/* pointer to memory does not need to be initialized,
 				 * helper function must fill all bytes or clear
 				 * them in error case.
@@ -320,42 +346,65 @@ enum bpf_arg_type {
 	ARG_CONST_SIZE_OR_ZERO,	/* number of bytes accessed from memory or 0 */
 
 	ARG_PTR_TO_CTX,		/* pointer to context */
-	ARG_PTR_TO_CTX_OR_NULL,	/* pointer to context or NULL */
 	ARG_ANYTHING,		/* any (initialized) argument is ok */
 	ARG_PTR_TO_SPIN_LOCK,	/* pointer to bpf_spin_lock */
 	ARG_PTR_TO_SOCK_COMMON,	/* pointer to sock_common */
 	ARG_PTR_TO_INT,		/* pointer to int */
 	ARG_PTR_TO_LONG,	/* pointer to long */
 	ARG_PTR_TO_SOCKET,	/* pointer to bpf_sock (fullsock) */
-	ARG_PTR_TO_SOCKET_OR_NULL,	/* pointer to bpf_sock (fullsock) or NULL */
 	ARG_PTR_TO_BTF_ID,	/* pointer to in-kernel struct */
 	ARG_PTR_TO_ALLOC_MEM,	/* pointer to dynamically allocated memory */
-	ARG_PTR_TO_ALLOC_MEM_OR_NULL,	/* pointer to dynamically allocated memory or NULL */
 	ARG_CONST_ALLOC_SIZE_OR_ZERO,	/* number of allocated bytes requested */
 	ARG_PTR_TO_BTF_ID_SOCK_COMMON,	/* pointer to in-kernel sock_common or bpf-mirrored bpf_sock */
 	ARG_PTR_TO_PERCPU_BTF_ID,	/* pointer to in-kernel percpu type */
 	ARG_PTR_TO_FUNC,	/* pointer to a bpf program function */
-	ARG_PTR_TO_STACK_OR_NULL,	/* pointer to stack or NULL */
+	ARG_PTR_TO_STACK,	/* pointer to stack */
 	ARG_PTR_TO_CONST_STR,	/* pointer to a null terminated read-only string */
 	ARG_PTR_TO_TIMER,	/* pointer to bpf_timer */
 	__BPF_ARG_TYPE_MAX,
+
+	/* Extended arg_types. */
+	ARG_PTR_TO_MAP_VALUE_OR_NULL	= PTR_MAYBE_NULL | ARG_PTR_TO_MAP_VALUE,
+	ARG_PTR_TO_MEM_OR_NULL		= PTR_MAYBE_NULL | ARG_PTR_TO_MEM,
+	ARG_PTR_TO_CTX_OR_NULL		= PTR_MAYBE_NULL | ARG_PTR_TO_CTX,
+	ARG_PTR_TO_SOCKET_OR_NULL	= PTR_MAYBE_NULL | ARG_PTR_TO_SOCKET,
+	ARG_PTR_TO_ALLOC_MEM_OR_NULL	= PTR_MAYBE_NULL | ARG_PTR_TO_ALLOC_MEM,
+	ARG_PTR_TO_STACK_OR_NULL	= PTR_MAYBE_NULL | ARG_PTR_TO_STACK,
+
+	/* This must be the last entry. Its purpose is to ensure the enum is
+	 * wide enough to hold the higher bits reserved for bpf_type_flag.
+	 */
+	__BPF_ARG_TYPE_LIMIT	= BPF_TYPE_LIMIT,
 };
+static_assert(__BPF_ARG_TYPE_MAX <= BPF_BASE_TYPE_LIMIT);
 
 /* type of values returned from helper functions */
 enum bpf_return_type {
 	RET_INTEGER,			/* function returns integer */
 	RET_VOID,			/* function doesn't return anything */
 	RET_PTR_TO_MAP_VALUE,		/* returns a pointer to map elem value */
-	RET_PTR_TO_MAP_VALUE_OR_NULL,	/* returns a pointer to map elem value or NULL */
-	RET_PTR_TO_SOCKET_OR_NULL,	/* returns a pointer to a socket or NULL */
-	RET_PTR_TO_TCP_SOCK_OR_NULL,	/* returns a pointer to a tcp_sock or NULL */
-	RET_PTR_TO_SOCK_COMMON_OR_NULL,	/* returns a pointer to a sock_common or NULL */
-	RET_PTR_TO_ALLOC_MEM_OR_NULL,	/* returns a pointer to dynamically allocated memory or NULL */
-	RET_PTR_TO_BTF_ID_OR_NULL,	/* returns a pointer to a btf_id or NULL */
-	RET_PTR_TO_MEM_OR_BTF_ID_OR_NULL, /* returns a pointer to a valid memory or a btf_id or NULL */
+	RET_PTR_TO_SOCKET,		/* returns a pointer to a socket */
+	RET_PTR_TO_TCP_SOCK,		/* returns a pointer to a tcp_sock */
+	RET_PTR_TO_SOCK_COMMON,		/* returns a pointer to a sock_common */
+	RET_PTR_TO_ALLOC_MEM,		/* returns a pointer to dynamically allocated memory */
 	RET_PTR_TO_MEM_OR_BTF_ID,	/* returns a pointer to a valid memory or a btf_id */
 	RET_PTR_TO_BTF_ID,		/* returns a pointer to a btf_id */
+	__BPF_RET_TYPE_MAX,
+
+	/* Extended ret_types. */
+	RET_PTR_TO_MAP_VALUE_OR_NULL	= PTR_MAYBE_NULL | RET_PTR_TO_MAP_VALUE,
+	RET_PTR_TO_SOCKET_OR_NULL	= PTR_MAYBE_NULL | RET_PTR_TO_SOCKET,
+	RET_PTR_TO_TCP_SOCK_OR_NULL	= PTR_MAYBE_NULL | RET_PTR_TO_TCP_SOCK,
+	RET_PTR_TO_SOCK_COMMON_OR_NULL	= PTR_MAYBE_NULL | RET_PTR_TO_SOCK_COMMON,
+	RET_PTR_TO_ALLOC_MEM_OR_NULL	= PTR_MAYBE_NULL | RET_PTR_TO_ALLOC_MEM,
+	RET_PTR_TO_BTF_ID_OR_NULL	= PTR_MAYBE_NULL | RET_PTR_TO_BTF_ID,
+
+	/* This must be the last entry. Its purpose is to ensure the enum is
+	 * wide enough to hold the higher bits reserved for bpf_type_flag.
+	 */
+	__BPF_RET_TYPE_LIMIT	= BPF_TYPE_LIMIT,
 };
+static_assert(__BPF_RET_TYPE_MAX <= BPF_BASE_TYPE_LIMIT);
 
 /* eBPF function prototype used by verifier to allow BPF_CALLs from eBPF programs
  * to in-kernel helper functions and for adjusting imm32 field in BPF_CALL
@@ -417,18 +466,15 @@ enum bpf_reg_type {
 	PTR_TO_CTX,		 /* reg points to bpf_context */
 	CONST_PTR_TO_MAP,	 /* reg points to struct bpf_map */
 	PTR_TO_MAP_VALUE,	 /* reg points to map element value */
-	PTR_TO_MAP_VALUE_OR_NULL,/* points to map elem value or NULL */
+	PTR_TO_MAP_KEY,		 /* reg points to a map element key */
 	PTR_TO_STACK,		 /* reg == frame_pointer + offset */
 	PTR_TO_PACKET_META,	 /* skb->data - meta_len */
 	PTR_TO_PACKET,		 /* reg points to skb->data */
 	PTR_TO_PACKET_END,	 /* skb->data + headlen */
 	PTR_TO_FLOW_KEYS,	 /* reg points to bpf_flow_keys */
 	PTR_TO_SOCKET,		 /* reg points to struct bpf_sock */
-	PTR_TO_SOCKET_OR_NULL,	 /* reg points to struct bpf_sock or NULL */
 	PTR_TO_SOCK_COMMON,	 /* reg points to sock_common */
-	PTR_TO_SOCK_COMMON_OR_NULL, /* reg points to sock_common or NULL */
 	PTR_TO_TCP_SOCK,	 /* reg points to struct tcp_sock */
-	PTR_TO_TCP_SOCK_OR_NULL, /* reg points to struct tcp_sock or NULL */
 	PTR_TO_TP_BUFFER,	 /* reg points to a writable raw tp's buffer */
 	PTR_TO_XDP_SOCK,	 /* reg points to struct xdp_sock */
 	/* PTR_TO_BTF_ID points to a kernel struct that does not need
@@ -446,18 +492,25 @@ enum bpf_reg_type {
 	 * been checked for null. Used primarily to inform the verifier
 	 * an explicit null check is required for this struct.
 	 */
-	PTR_TO_BTF_ID_OR_NULL,
 	PTR_TO_MEM,		 /* reg points to valid memory region */
-	PTR_TO_MEM_OR_NULL,	 /* reg points to valid memory region or NULL */
-	PTR_TO_RDONLY_BUF,	 /* reg points to a readonly buffer */
-	PTR_TO_RDONLY_BUF_OR_NULL, /* reg points to a readonly buffer or NULL */
-	PTR_TO_RDWR_BUF,	 /* reg points to a read/write buffer */
-	PTR_TO_RDWR_BUF_OR_NULL, /* reg points to a read/write buffer or NULL */
+	PTR_TO_BUF,		 /* reg points to a read/write buffer */
 	PTR_TO_PERCPU_BTF_ID,	 /* reg points to a percpu kernel variable */
 	PTR_TO_FUNC,		 /* reg points to a bpf program function */
-	PTR_TO_MAP_KEY,		 /* reg points to a map element key */
 	__BPF_REG_TYPE_MAX,
+
+	/* Extended reg_types. */
+	PTR_TO_MAP_VALUE_OR_NULL	= PTR_MAYBE_NULL | PTR_TO_MAP_VALUE,
+	PTR_TO_SOCKET_OR_NULL		= PTR_MAYBE_NULL | PTR_TO_SOCKET,
+	PTR_TO_SOCK_COMMON_OR_NULL	= PTR_MAYBE_NULL | PTR_TO_SOCK_COMMON,
+	PTR_TO_TCP_SOCK_OR_NULL		= PTR_MAYBE_NULL | PTR_TO_TCP_SOCK,
+	PTR_TO_BTF_ID_OR_NULL		= PTR_MAYBE_NULL | PTR_TO_BTF_ID,
+
+	/* This must be the last entry. Its purpose is to ensure the enum is
+	 * wide enough to hold the higher bits reserved for bpf_type_flag.
+	 */
+	__BPF_REG_TYPE_LIMIT	= BPF_TYPE_LIMIT,
 };
+static_assert(__BPF_REG_TYPE_MAX <= BPF_BASE_TYPE_LIMIT);
 
 /* The information passed from prog-specific *_is_valid_access
  * back to the verifier.
diff --git a/include/linux/bpf_verifier.h b/include/linux/bpf_verifier.h
index 364550dd19c4..bb1cc3fbc4ba 100644
--- a/include/linux/bpf_verifier.h
+++ b/include/linux/bpf_verifier.h
@@ -18,6 +18,8 @@
  * that converting umax_value to int cannot overflow.
  */
 #define BPF_MAX_VAR_SIZ	(1 << 29)
+/* size of type_str_buf in bpf_verifier. */
+#define TYPE_STR_BUF_LEN 64
 
 /* Liveness marks, used for registers and spilled-regs (in stack slots).
  * Read marks propagate upwards until they find a write mark; they record that
@@ -474,6 +476,8 @@ struct bpf_verifier_env {
 	/* longest register parentage chain walked for liveness marking */
 	u32 longest_mark_read_walk;
 	bpfptr_t fd_array;
+	/* buffer used in reg_type_str() to generate reg_type string */
+	char type_str_buf[TYPE_STR_BUF_LEN];
 };
 
 __printf(2, 0) void bpf_verifier_vlog(struct bpf_verifier_log *log,
@@ -535,4 +539,18 @@ int bpf_check_attach_target(struct bpf_verifier_log *log,
 			    u32 btf_id,
 			    struct bpf_attach_target_info *tgt_info);
 
+#define BPF_BASE_TYPE_MASK	GENMASK(BPF_BASE_TYPE_BITS - 1, 0)
+
+/* extract base type from bpf_{arg, return, reg}_type. */
+static inline u32 base_type(u32 type)
+{
+	return type & BPF_BASE_TYPE_MASK;
+}
+
+/* extract flags from an extended type. See bpf_type_flag in bpf.h. */
+static inline u32 type_flag(u32 type)
+{
+	return type & ~BPF_BASE_TYPE_MASK;
+}
+
 #endif /* _LINUX_BPF_VERIFIER_H */
diff --git a/include/linux/i3c/ccc.h b/include/linux/i3c/ccc.h
index 73b0982cc519..0e078d2fd1ca 100644
--- a/include/linux/i3c/ccc.h
+++ b/include/linux/i3c/ccc.h
@@ -32,6 +32,9 @@
 #define I3C_CCC_DEFSLVS			I3C_CCC_ID(0x8, true)
 #define I3C_CCC_ENTTM			I3C_CCC_ID(0xb, true)
 #define I3C_CCC_ENTHDR(x)		I3C_CCC_ID(0x20 + (x), true)
+#define I3C_CCC_SETAASA			I3C_CCC_ID(0x29, true)
+#define I3C_CCC_SETHID			I3C_CCC_ID(0x61, true)
+#define I3C_CCC_DEVCTRL			I3C_CCC_ID(0x62, true)
 
 /* Unicast-only commands */
 #define I3C_CCC_SETDASA			I3C_CCC_ID(0x7, false)
@@ -243,6 +246,15 @@ struct i3c_ccc_setbrgtgt {
 	struct i3c_ccc_bridged_slave_desc bslaves[0];
 } __packed;
 
+
+/**
+ * struct i3c_ccc_sethid - payload passed to SETHID CCC
+ *
+ * @hid: 3-bit HID
+ */
+struct i3c_ccc_sethid {
+	u8 hid;
+};
 /**
  * enum i3c_sdr_max_data_rate - max data rate values for private SDR transfers
  */
@@ -369,6 +381,8 @@ struct i3c_ccc_cmd_dest {
  * @rnw: true if the CCC should retrieve data from the device. Only valid for
  *	 unicast commands
  * @id: CCC command id
+ * @dbp: true if the defining byte present
+ * @db: the defining byte
  * @ndests: number of destinations. Should always be one for broadcast commands
  * @dests: array of destinations and associated payload for this CCC. Most of
  *	   the time, only one destination is provided
@@ -377,6 +391,8 @@ struct i3c_ccc_cmd_dest {
 struct i3c_ccc_cmd {
 	u8 rnw;
 	u8 id;
+	u8 dbp;
+	u8 db;
 	unsigned int ndests;
 	struct i3c_ccc_cmd_dest *dests;
 	enum i3c_error_code err;
diff --git a/include/linux/i3c/device.h b/include/linux/i3c/device.h
index 8242e13e7b0b..6c39c403961c 100644
--- a/include/linux/i3c/device.h
+++ b/include/linux/i3c/device.h
@@ -49,6 +49,27 @@ enum i3c_hdr_mode {
 	I3C_HDR_TSL,
 };
 
+/**
+ * struct i3c_hdr_cmd - I3C HDR command
+ * @mode: HDR mode selected for this command
+ * @code: command opcode
+ * @addr: I3C dynamic address
+ * @ndatawords: number of data words (a word is 16bits wide)
+ * @data: input/output buffer
+ * @err: I3C error code
+ */
+struct i3c_hdr_cmd {
+	enum i3c_hdr_mode mode;
+	u8 code;
+	u8 addr;
+	int ndatawords;
+	union {
+		void *in;
+		const void *out;
+	} data;
+	enum i3c_error_code err;
+};
+
 /**
  * struct i3c_priv_xfer - I3C SDR private transfer
  * @rnw: encodes the transfer direction. true for a read, false for a write
@@ -71,11 +92,26 @@ struct i3c_priv_xfer {
 /**
  * enum i3c_dcr - I3C DCR values
  * @I3C_DCR_GENERIC_DEVICE: generic I3C device
+ * @I3C_DCR_HUB: I3C HUB device
  */
 enum i3c_dcr {
 	I3C_DCR_GENERIC_DEVICE = 0,
+	I3C_DCR_HUB = 194,
+	I3C_DCR_JESD403_BEGIN = 208,
+	I3C_DCR_THERMAL_SENSOR_FIRST = 210,
+	I3C_DCR_THERMAL_SENSOR_SECOND = 214,
+	I3C_DCR_PMIC_SECOND = 216,
+	I3C_DCR_PMIC_FIRST = 217,
+	I3C_DCR_SPD_HUB = 218,
+	I3C_DCR_RCD = 219,
+	I3C_DCR_PMIC_THIRD = 220,
+	I3C_DCR_JESD403_END = 223,
+	I3C_DCR_MAX = 228,
 };
 
+#define I3C_DCR_IS_JESD403_COMPLIANT(dcr)                                      \
+	(dcr >= I3C_DCR_JESD403_BEGIN && dcr <= I3C_DCR_JESD403_END)
+
 #define I3C_PID_MANUF_ID(pid)		(((pid) & GENMASK_ULL(47, 33)) >> 33)
 #define I3C_PID_RND_LOWER_32BITS(pid)	(!!((pid) & BIT_ULL(32)))
 #define I3C_PID_RND_VAL(pid)		((pid) & GENMASK_ULL(31, 0))
@@ -93,6 +129,22 @@ enum i3c_dcr {
 #define I3C_BCR_IBI_REQ_CAP		BIT(1)
 #define I3C_BCR_MAX_DATA_SPEED_LIM	BIT(0)
 
+/*
+ * MIPI I3C MDB definition
+ * see https://www.mipi.org/MIPI_I3C_mandatory_data_byte_values_public
+ */
+#define IBI_MDB_ID(grp, id)                                                    \
+	((((grp) << 5) & GENMASK(7, 5)) | ((id)&GENMASK(4, 0)))
+#define IBI_MDB_GET_GRP(m) (((m)&GENMASK(7, 5)) >> 5)
+#define IBI_MDB_GET_ID(m) ((m)&GENMASK(4, 0))
+
+#define IBI_MDB_GRP_PENDING_READ_NOTIF 0x5
+#define IS_MDB_PENDING_READ_NOTIFY(m)                                          \
+	(IBI_MDB_GET_GRP(m) == IBI_MDB_GRP_PENDING_READ_NOTIF)
+#define IBI_MDB_MIPI_DBGDATAREADY                                              \
+	IBI_MDB_ID(IBI_MDB_GRP_PENDING_READ_NOTIF, 0xd)
+#define IBI_MDB_MCTP IBI_MDB_ID(IBI_MDB_GRP_PENDING_READ_NOTIF, 0xe)
+
 /**
  * struct i3c_device_info - I3C device information
  * @pid: Provisional ID
@@ -107,6 +159,8 @@ enum i3c_dcr {
  * @max_read_turnaround: max read turn-around time in micro-seconds
  * @max_read_len: max private SDR read length in bytes
  * @max_write_len: max private SDR write length in bytes
+ * @pec: flag telling whether PEC (Packet Error Check) generation and verification for read
+ *       and write transaction is enabled
  *
  * These are all basic information that should be advertised by an I3C device.
  * Some of them are optional depending on the device type and device
@@ -128,6 +182,8 @@ struct i3c_device_info {
 	u32 max_read_turnaround;
 	u16 max_read_len;
 	u16 max_write_len;
+	u8 pec;
+	__be16 status;
 };
 
 /*
@@ -178,6 +234,7 @@ struct i3c_driver {
 	int (*probe)(struct i3c_device *dev);
 	void (*remove)(struct i3c_device *dev);
 	const struct i3c_device_id *id_table;
+	bool target;
 };
 
 static inline struct i3c_driver *drv_to_i3cdrv(struct device_driver *drv)
@@ -293,6 +350,12 @@ int i3c_device_do_priv_xfers(struct i3c_device *dev,
 			     struct i3c_priv_xfer *xfers,
 			     int nxfers);
 
+int i3c_device_send_hdr_cmds(struct i3c_device *dev,
+			     struct i3c_hdr_cmd *cmds,
+			     int ncmds);
+
+int i3c_device_generate_ibi(struct i3c_device *dev, const u8 *data, int len);
+
 void i3c_device_get_info(struct i3c_device *dev, struct i3c_device_info *info);
 
 struct i3c_ibi_payload {
@@ -331,5 +394,21 @@ int i3c_device_request_ibi(struct i3c_device *dev,
 void i3c_device_free_ibi(struct i3c_device *dev);
 int i3c_device_enable_ibi(struct i3c_device *dev);
 int i3c_device_disable_ibi(struct i3c_device *dev);
+int i3c_device_send_ccc_cmd(struct i3c_device *dev, u8 ccc_id);
+
+int i3c_device_getstatus_ccc(struct i3c_device *dev, struct i3c_device_info *info);
+int i3c_device_setmrl_ccc(struct i3c_device *dev, struct i3c_device_info *info, u16 read_len,
+			  u8 ibi_len);
+int i3c_device_setmwl_ccc(struct i3c_device *dev, struct i3c_device_info *info, u16 write_len);
+int i3c_device_getmrl_ccc(struct i3c_device *dev, struct i3c_device_info *info);
+int i3c_device_getmwl_ccc(struct i3c_device *dev, struct i3c_device_info *info);
+
+struct i3c_target_read_setup {
+	void (*handler)(struct i3c_device *dev, const u8 *data, size_t len);
+};
+
+int i3c_target_read_register(struct i3c_device *dev, const struct i3c_target_read_setup *setup);
+
+int i3c_device_control_pec(struct i3c_device *dev, bool pec);
 
 #endif /* I3C_DEV_H */
diff --git a/include/linux/i3c/master.h b/include/linux/i3c/master.h
index 9cb39d901cd5..03fe3377d993 100644
--- a/include/linux/i3c/master.h
+++ b/include/linux/i3c/master.h
@@ -22,10 +22,13 @@
 #define I3C_BROADCAST_ADDR		0x7e
 #define I3C_MAX_ADDR			GENMASK(6, 0)
 
+struct i3c_target_ops;
 struct i3c_master_controller;
 struct i3c_bus;
 struct i2c_device;
 struct i3c_device;
+struct i3c_slave_setup;
+struct i3c_slave_payload;
 
 /**
  * struct i3c_i2c_dev_desc - Common part of the I3C/I2C device descriptor
@@ -85,7 +88,6 @@ struct i2c_dev_boardinfo {
  */
 struct i2c_dev_desc {
 	struct i3c_i2c_dev_desc common;
-	const struct i2c_dev_boardinfo *boardinfo;
 	struct i2c_client *dev;
 	u16 addr;
 	u8 lvr;
@@ -181,14 +183,26 @@ struct i3c_dev_boardinfo {
 	u8 init_dyn_addr;
 	u8 static_addr;
 	u64 pid;
+	u8 bcr;
+	u8 dcr;
 	struct device_node *of_node;
 };
 
+/**
+ * struct i3c_target_info - target information attached to a specific device
+ * @read handler: handler specified at i3c_target_read_register() call time.
+ */
+
+struct i3c_target_info {
+	void (*read_handler)(struct i3c_device *dev, const u8 *data, size_t len);
+};
+
 /**
  * struct i3c_dev_desc - I3C device descriptor
  * @common: common part of the I3C device descriptor
  * @info: I3C device information. Will be automatically filled when you create
  *	  your device with i3c_master_add_i3c_dev_locked()
+ * @target_info: I3C target information.
  * @ibi_lock: lock used to protect the &struct_i3c_device->ibi
  * @ibi: IBI info attached to a device. Should be NULL until
  *	 i3c_device_request_ibi() is called
@@ -207,6 +221,7 @@ struct i3c_dev_boardinfo {
 struct i3c_dev_desc {
 	struct i3c_i2c_dev_desc common;
 	struct i3c_device_info info;
+	struct i3c_target_info target_info;
 	struct mutex ibi_lock;
 	struct i3c_device_ibi_info *ibi;
 	struct i3c_device *dev;
@@ -384,6 +399,9 @@ struct i3c_bus {
  *		      all CCC commands are supported.
  * @send_ccc_cmd: send a CCC command
  *		  This method is mandatory.
+ * @send_hdr_cmds: send one or several HDR commands. If there is more than one
+ *		   command, they should ideally be sent in the same HDR
+ *		   transaction
  * @priv_xfers: do one or several private I3C SDR transfers
  *		This method is mandatory.
  * @attach_i2c_dev: called every time an I2C device is attached to the bus.
@@ -430,6 +448,7 @@ struct i3c_bus {
 struct i3c_master_controller_ops {
 	int (*bus_init)(struct i3c_master_controller *master);
 	void (*bus_cleanup)(struct i3c_master_controller *master);
+	void (*bus_reset)(struct i3c_master_controller *master);
 	int (*attach_i3c_dev)(struct i3c_dev_desc *dev);
 	int (*reattach_i3c_dev)(struct i3c_dev_desc *dev, u8 old_dyn_addr);
 	void (*detach_i3c_dev)(struct i3c_dev_desc *dev);
@@ -438,6 +457,9 @@ struct i3c_master_controller_ops {
 				 const struct i3c_ccc_cmd *cmd);
 	int (*send_ccc_cmd)(struct i3c_master_controller *master,
 			    struct i3c_ccc_cmd *cmd);
+	int (*send_hdr_cmds)(struct i3c_master_controller *master,
+			     struct i3c_hdr_cmd *cmds,
+			     int ncmds);
 	int (*priv_xfers)(struct i3c_dev_desc *dev,
 			  struct i3c_priv_xfer *xfers,
 			  int nxfers);
@@ -452,6 +474,14 @@ struct i3c_master_controller_ops {
 	int (*disable_ibi)(struct i3c_dev_desc *dev);
 	void (*recycle_ibi_slot)(struct i3c_dev_desc *dev,
 				 struct i3c_ibi_slot *slot);
+	int (*register_slave)(struct i3c_master_controller *master,
+			      const struct i3c_slave_setup *req);
+	int (*unregister_slave)(struct i3c_master_controller *master);
+	int (*send_sir)(struct i3c_master_controller *master,
+			struct i3c_slave_payload *payload);
+	int (*put_read_data)(struct i3c_master_controller *master,
+			     struct i3c_slave_payload *data,
+			     struct i3c_slave_payload *ibi_notify);
 };
 
 /**
@@ -463,6 +493,8 @@ struct i3c_master_controller_ops {
  *	 registered to the I2C subsystem to be as transparent as possible to
  *	 existing I2C drivers
  * @ops: master operations. See &struct i3c_master_controller_ops
+ * @target_ops: target operations. See &struct i3c_target_ops
+ * @target: true if the underlying I3C device acts as a target on I3C bus
  * @secondary: true if the master is a secondary master
  * @init_done: true when the bus initialization is done
  * @boardinfo.i3c: list of I3C  boardinfo objects
@@ -485,8 +517,12 @@ struct i3c_master_controller {
 	struct i3c_dev_desc *this;
 	struct i2c_adapter i2c;
 	const struct i3c_master_controller_ops *ops;
+	const struct i3c_target_ops *target_ops;
+	unsigned int pec_supported : 1;
+	unsigned int target : 1;
 	unsigned int secondary : 1;
 	unsigned int init_done : 1;
+	unsigned int jdec_spd : 1;
 	struct {
 		struct list_head i3c;
 		struct list_head i2c;
@@ -525,15 +561,20 @@ int i3c_master_disec_locked(struct i3c_master_controller *master, u8 addr,
 			    u8 evts);
 int i3c_master_enec_locked(struct i3c_master_controller *master, u8 addr,
 			   u8 evts);
+int i3c_master_setmrl_locked(struct i3c_master_controller *master,
+			     struct i3c_device_info *info, u16 read_len,
+			     u8 ibi_len);
 int i3c_master_entdaa_locked(struct i3c_master_controller *master);
 int i3c_master_defslvs_locked(struct i3c_master_controller *master);
-
+int i3c_master_rstdaa_locked(struct i3c_master_controller *master,
+				    u8 addr);
 int i3c_master_get_free_addr(struct i3c_master_controller *master,
 			     u8 start_addr);
 
 int i3c_master_add_i3c_dev_locked(struct i3c_master_controller *master,
 				  u8 addr);
 int i3c_master_do_daa(struct i3c_master_controller *master);
+int i3c_master_enable_hj(struct i3c_master_controller *master);
 
 int i3c_master_set_info(struct i3c_master_controller *master,
 			const struct i3c_device_info *info);
@@ -544,6 +585,13 @@ int i3c_master_register(struct i3c_master_controller *master,
 			bool secondary);
 int i3c_master_unregister(struct i3c_master_controller *master);
 
+int i3c_register(struct i3c_master_controller *master,
+		 struct device *parent,
+		 const struct i3c_master_controller_ops *master_ops,
+		 const struct i3c_target_ops *target_ops,
+		 bool secondary);
+int i3c_unregister(struct i3c_master_controller *master);
+
 /**
  * i3c_dev_get_master_data() - get master private data attached to an I3C
  *			       device descriptor
@@ -636,6 +684,18 @@ i3c_master_get_bus(struct i3c_master_controller *master)
 	return &master->bus;
 }
 
+struct i3c_slave_payload {
+	unsigned int len;
+	const void *data;
+};
+
+struct i3c_slave_setup {
+	unsigned int max_payload_len;
+	unsigned int num_slots;
+	void (*handler)(struct i3c_master_controller *m,
+			const struct i3c_slave_payload *payload);
+};
+
 struct i3c_generic_ibi_pool;
 
 struct i3c_generic_ibi_pool *
@@ -652,4 +712,32 @@ void i3c_master_queue_ibi(struct i3c_dev_desc *dev, struct i3c_ibi_slot *slot);
 
 struct i3c_ibi_slot *i3c_master_get_free_ibi_slot(struct i3c_dev_desc *dev);
 
+int i3c_master_register_slave(struct i3c_master_controller *master,
+			      const struct i3c_slave_setup *req);
+int i3c_master_unregister_slave(struct i3c_master_controller *master);
+int i3c_master_send_sir(struct i3c_master_controller *master,
+			struct i3c_slave_payload *payload);
+int i3c_master_send_hdr_cmds(struct i3c_master_controller *master,
+			     struct i3c_hdr_cmd *cmds, int ncmds);
+/**
+ * i3c_master_put_read_data() - put read data and optionally notify primary master
+ * @master: master object in slave mode
+ * @data: data structure to be read
+ * @ibi_notify: IBI data (including MDB) to notify primary master device
+ */
+int i3c_master_put_read_data(struct i3c_master_controller *master,
+			     struct i3c_slave_payload *data,
+			     struct i3c_slave_payload *ibi_notify);
+/*
+ * Slave message queue driver API
+ */
+#ifdef CONFIG_I3C_SLAVE_MQUEUE
+int i3c_slave_mqueue_probe(struct i3c_master_controller *master);
+int i3c_slave_mqueue_remove(struct i3c_master_controller *master);
+#endif
+
+#ifdef CONFIG_I3C_SLAVE_EEPROM
+int i3c_slave_eeprom_probe(struct i3c_master_controller *master);
+int i3c_slave_eeprom_remove(struct i3c_master_controller *master);
+#endif
 #endif /* I3C_MASTER_H */
diff --git a/include/linux/i3c/mctp/i3c-mctp.h b/include/linux/i3c/mctp/i3c-mctp.h
new file mode 100644
index 000000000000..dd20750d79d8
--- /dev/null
+++ b/include/linux/i3c/mctp/i3c-mctp.h
@@ -0,0 +1,50 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/* Copyright (C) 2022 Intel Corporation.*/
+
+#ifndef I3C_MCTP_H
+#define I3C_MCTP_H
+
+#define I3C_MCTP_PACKET_SIZE	68
+#define I3C_MCTP_PAYLOAD_SIZE	64
+#define I3C_MCTP_HDR_SIZE	4
+
+/* PECI MCTP Intel VDM definitions */
+#define MCTP_MSG_TYPE_VDM_PCI		0x7E
+#define MCTP_VDM_PCI_INTEL_VENDOR_ID	0x8086
+#define MCTP_VDM_PCI_INTEL_PECI		0x2
+
+/* MCTP message header offsets */
+#define MCTP_MSG_HDR_MSG_TYPE_OFFSET	0
+#define MCTP_MSG_HDR_VENDOR_OFFSET	1
+#define MCTP_MSG_HDR_OPCODE_OFFSET	4
+
+struct i3c_mctp_client;
+
+struct mctp_protocol_hdr {
+	u8 ver;
+	u8 dest;
+	u8 src;
+	u8 flags_seq_tag;
+} __packed;
+
+struct i3c_mctp_packet_data {
+	u8 protocol_hdr[I3C_MCTP_HDR_SIZE];
+	u8 payload[I3C_MCTP_PAYLOAD_SIZE];
+};
+
+struct i3c_mctp_packet {
+	struct i3c_mctp_packet_data data;
+	u32 size;
+};
+
+void *i3c_mctp_packet_alloc(gfp_t flags);
+void i3c_mctp_packet_free(void *packet);
+
+int i3c_mctp_get_eid(struct i3c_mctp_client *client, u8 domain_id, u8 *eid);
+int i3c_mctp_send_packet(struct i3c_device *i3c, struct i3c_mctp_packet *tx_packet);
+struct i3c_mctp_packet *i3c_mctp_receive_packet(struct i3c_mctp_client *client,
+						unsigned long timeout);
+struct i3c_mctp_client *i3c_mctp_add_peci_client(struct i3c_device *i3c);
+void i3c_mctp_remove_peci_client(struct i3c_mctp_client *client);
+
+#endif /* I3C_MCTP_H */
diff --git a/include/linux/i3c/target.h b/include/linux/i3c/target.h
new file mode 100644
index 000000000000..9e71124b5325
--- /dev/null
+++ b/include/linux/i3c/target.h
@@ -0,0 +1,23 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/* Copyright (c) 2022, Intel Corporation */
+
+#ifndef I3C_TARGET_H
+#define I3C_TARGET_H
+
+#include <linux/device.h>
+#include <linux/i3c/device.h>
+
+struct i3c_master_controller;
+
+struct i3c_target_ops {
+	int (*bus_init)(struct i3c_master_controller *master);
+	void (*bus_cleanup)(struct i3c_master_controller *master);
+	int (*priv_xfers)(struct i3c_dev_desc *dev, struct i3c_priv_xfer *xfers, int nxfers);
+	int (*generate_ibi)(struct i3c_dev_desc *dev, const u8 *data, int len);
+};
+
+int i3c_target_register(struct i3c_master_controller *master, struct device *parent,
+			const struct i3c_target_ops *ops);
+int i3c_target_unregister(struct i3c_master_controller *master);
+
+#endif
diff --git a/include/linux/iomap.h b/include/linux/iomap.h
index 24f8489583ca..829f2325ecba 100644
--- a/include/linux/iomap.h
+++ b/include/linux/iomap.h
@@ -330,12 +330,19 @@ struct iomap_dio_ops {
   */
 #define IOMAP_DIO_OVERWRITE_ONLY	(1 << 1)
 
+/*
+ * When a page fault occurs, return a partial synchronous result and allow
+ * the caller to retry the rest of the operation after dealing with the page
+ * fault.
+ */
+#define IOMAP_DIO_PARTIAL		(1 << 2)
+
 ssize_t iomap_dio_rw(struct kiocb *iocb, struct iov_iter *iter,
 		const struct iomap_ops *ops, const struct iomap_dio_ops *dops,
-		unsigned int dio_flags);
+		unsigned int dio_flags, size_t done_before);
 struct iomap_dio *__iomap_dio_rw(struct kiocb *iocb, struct iov_iter *iter,
 		const struct iomap_ops *ops, const struct iomap_dio_ops *dops,
-		unsigned int dio_flags);
+		unsigned int dio_flags, size_t done_before);
 ssize_t iomap_dio_complete(struct iomap_dio *dio);
 int iomap_dio_iopoll(struct kiocb *kiocb, bool spin);
 
diff --git a/include/linux/jtag.h b/include/linux/jtag.h
new file mode 100644
index 000000000000..abb442a2c805
--- /dev/null
+++ b/include/linux/jtag.h
@@ -0,0 +1,49 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/* Copyright (c) 2018 Mellanox Technologies. All rights reserved. */
+/* Copyright (c) 2018 Oleksandr Shamray <oleksandrs@mellanox.com> */
+/* Copyright (c) 2019 Intel Corporation */
+
+#ifndef __LINUX_JTAG_H
+#define __LINUX_JTAG_H
+
+#include <linux/types.h>
+#include <uapi/linux/jtag.h>
+
+#define JTAG_MAX_XFER_DATA_LEN 65535
+
+struct jtag;
+/**
+ * struct jtag_ops - callbacks for JTAG control functions:
+ *
+ * @freq_get: get frequency function. Filled by dev driver
+ * @freq_set: set frequency function. Filled by dev driver
+ * @status_get: get JTAG TAPC state function. Mandatory, Filled by dev driver
+ * @status_set: set JTAG TAPC state function. Mandatory, Filled by dev driver
+ * @xfer: send JTAG xfer function. Mandatory func. Filled by dev driver
+ * @mode_set: set specific work mode for JTAG. Filled by dev driver
+ * @trst_set: set TRST pin active(pull low) for JTAG. Filled by dev driver
+ * @bitbang: set low level bitbang operations. Filled by dev driver
+ * @enable: enables JTAG interface in master mode. Filled by dev driver
+ * @disable: disables JTAG interface master mode. Filled by dev driver
+ */
+struct jtag_ops {
+	int (*freq_get)(struct jtag *jtag, u32 *freq);
+	int (*freq_set)(struct jtag *jtag, u32 freq);
+	int (*status_get)(struct jtag *jtag, u32 *state);
+	int (*status_set)(struct jtag *jtag, struct jtag_tap_state *endst);
+	int (*xfer)(struct jtag *jtag, struct jtag_xfer *xfer, u8 *xfer_data);
+	int (*mode_set)(struct jtag *jtag, struct jtag_mode *jtag_mode);
+	int (*trst_set)(struct jtag *jtag, u32 active);
+	int (*bitbang)(struct jtag *jtag, struct bitbang_packet *bitbang,
+		       struct tck_bitbang *bitbang_data);
+	int (*enable)(struct jtag *jtag);
+	int (*disable)(struct jtag *jtag);
+};
+
+void *jtag_priv(struct jtag *jtag);
+int devm_jtag_register(struct device *dev, struct jtag *jtag);
+struct jtag *jtag_alloc(struct device *host, size_t priv_size,
+			const struct jtag_ops *ops);
+void jtag_free(struct jtag *jtag);
+
+#endif /* __LINUX_JTAG_H */
diff --git a/include/linux/kernel.h b/include/linux/kernel.h
index 2776423a587e..f56cd8879a59 100644
--- a/include/linux/kernel.h
+++ b/include/linux/kernel.h
@@ -277,7 +277,7 @@ static inline char *hex_byte_pack_upper(char *buf, u8 byte)
 	return buf;
 }
 
-extern int hex_to_bin(char ch);
+extern int hex_to_bin(unsigned char ch);
 extern int __must_check hex2bin(u8 *dst, const char *src, size_t count);
 extern char *bin2hex(char *dst, const void *src, size_t count);
 
diff --git a/include/linux/mm.h b/include/linux/mm.h
index 90c2d7f3c7a8..04345ff97f8c 100644
--- a/include/linux/mm.h
+++ b/include/linux/mm.h
@@ -2858,7 +2858,8 @@ struct page *follow_page(struct vm_area_struct *vma, unsigned long address,
 #define FOLL_FORCE	0x10	/* get_user_pages read/write w/o permission */
 #define FOLL_NOWAIT	0x20	/* if a disk transfer is needed, start the IO
 				 * and return without waiting upon it */
-#define FOLL_POPULATE	0x40	/* fault in page */
+#define FOLL_POPULATE	0x40	/* fault in pages (with FOLL_MLOCK) */
+#define FOLL_NOFAULT	0x80	/* do not fault in pages */
 #define FOLL_HWPOISON	0x100	/* check page is hwpoisoned */
 #define FOLL_NUMA	0x200	/* force NUMA hinting page fault */
 #define FOLL_MIGRATION	0x400	/* wait for page to replace migration entry */
diff --git a/include/linux/mmc/host.h b/include/linux/mmc/host.h
index 0c0c9a0fdf57..3c13010683e0 100644
--- a/include/linux/mmc/host.h
+++ b/include/linux/mmc/host.h
@@ -82,7 +82,9 @@ struct mmc_ios {
 
 struct mmc_clk_phase {
 	bool valid;
+	bool inv_in_deg;
 	u16 in_deg;
+	bool inv_out_deg;
 	u16 out_deg;
 };
 
diff --git a/include/linux/mtd/mtd.h b/include/linux/mtd/mtd.h
index 88227044fc86..8a2c60235ebb 100644
--- a/include/linux/mtd/mtd.h
+++ b/include/linux/mtd/mtd.h
@@ -394,10 +394,8 @@ struct mtd_info {
 	/* List of partitions attached to this MTD device */
 	struct list_head partitions;
 
-	union {
-		struct mtd_part part;
-		struct mtd_master master;
-	};
+	struct mtd_part part;
+	struct mtd_master master;
 };
 
 static inline struct mtd_info *mtd_get_master(struct mtd_info *mtd)
diff --git a/include/linux/netdev_features.h b/include/linux/netdev_features.h
index 2c6b9e416225..7c2d77d75a88 100644
--- a/include/linux/netdev_features.h
+++ b/include/linux/netdev_features.h
@@ -169,7 +169,7 @@ enum {
 #define NETIF_F_HW_HSR_FWD	__NETIF_F(HW_HSR_FWD)
 #define NETIF_F_HW_HSR_DUP	__NETIF_F(HW_HSR_DUP)
 
-/* Finds the next feature with the highest number of the range of start till 0.
+/* Finds the next feature with the highest number of the range of start-1 till 0.
  */
 static inline int find_next_netdev_feature(u64 feature, unsigned long start)
 {
@@ -188,7 +188,7 @@ static inline int find_next_netdev_feature(u64 feature, unsigned long start)
 	for ((bit) = find_next_netdev_feature((mask_addr),		\
 					      NETDEV_FEATURE_COUNT);	\
 	     (bit) >= 0;						\
-	     (bit) = find_next_netdev_feature((mask_addr), (bit) - 1))
+	     (bit) = find_next_netdev_feature((mask_addr), (bit)))
 
 /* Features valid for ethtool to change */
 /* = all defined minus driver/device-class-related */
diff --git a/include/linux/pagemap.h b/include/linux/pagemap.h
index 62db6b0176b9..2f7dd14083d9 100644
--- a/include/linux/pagemap.h
+++ b/include/linux/pagemap.h
@@ -733,61 +733,11 @@ int wait_on_page_private_2_killable(struct page *page);
 extern void add_page_wait_queue(struct page *page, wait_queue_entry_t *waiter);
 
 /*
- * Fault everything in given userspace address range in.
+ * Fault in userspace address range.
  */
-static inline int fault_in_pages_writeable(char __user *uaddr, size_t size)
-{
-	char __user *end = uaddr + size - 1;
-
-	if (unlikely(size == 0))
-		return 0;
-
-	if (unlikely(uaddr > end))
-		return -EFAULT;
-	/*
-	 * Writing zeroes into userspace here is OK, because we know that if
-	 * the zero gets there, we'll be overwriting it.
-	 */
-	do {
-		if (unlikely(__put_user(0, uaddr) != 0))
-			return -EFAULT;
-		uaddr += PAGE_SIZE;
-	} while (uaddr <= end);
-
-	/* Check whether the range spilled into the next page. */
-	if (((unsigned long)uaddr & PAGE_MASK) ==
-			((unsigned long)end & PAGE_MASK))
-		return __put_user(0, end);
-
-	return 0;
-}
-
-static inline int fault_in_pages_readable(const char __user *uaddr, size_t size)
-{
-	volatile char c;
-	const char __user *end = uaddr + size - 1;
-
-	if (unlikely(size == 0))
-		return 0;
-
-	if (unlikely(uaddr > end))
-		return -EFAULT;
-
-	do {
-		if (unlikely(__get_user(c, uaddr) != 0))
-			return -EFAULT;
-		uaddr += PAGE_SIZE;
-	} while (uaddr <= end);
-
-	/* Check whether the range spilled into the next page. */
-	if (((unsigned long)uaddr & PAGE_MASK) ==
-			((unsigned long)end & PAGE_MASK)) {
-		return __get_user(c, end);
-	}
-
-	(void)c;
-	return 0;
-}
+size_t fault_in_writeable(char __user *uaddr, size_t size);
+size_t fault_in_safe_writeable(const char __user *uaddr, size_t size);
+size_t fault_in_readable(const char __user *uaddr, size_t size);
 
 int add_to_page_cache_locked(struct page *page, struct address_space *mapping,
 				pgoff_t index, gfp_t gfp_mask);
diff --git a/include/linux/pci_ids.h b/include/linux/pci_ids.h
index 011f2f1ea5bb..4048dfb6c498 100644
--- a/include/linux/pci_ids.h
+++ b/include/linux/pci_ids.h
@@ -3113,4 +3113,7 @@
 
 #define PCI_VENDOR_ID_NCUBE		0x10ff
 
+#define PCI_VENDOR_ID_ASPEED		0x1a03
+#define PCI_DEVICE_ID_ASPEED_EHCI	0x2603
+
 #endif /* _LINUX_PCI_IDS_H */
diff --git a/include/linux/soc/aspeed/aspeed-udma.h b/include/linux/soc/aspeed/aspeed-udma.h
new file mode 100644
index 000000000000..33acea745f1c
--- /dev/null
+++ b/include/linux/soc/aspeed/aspeed-udma.h
@@ -0,0 +1,30 @@
+#ifndef __ASPEED_UDMA_H__
+#define __ASPEED_UDMA_H__
+
+#include <linux/circ_buf.h>
+
+typedef void (*aspeed_udma_cb_t)(int rb_rwptr, void *id);
+
+enum aspeed_udma_ops {
+	ASPEED_UDMA_OP_ENABLE,
+	ASPEED_UDMA_OP_DISABLE,
+	ASPEED_UDMA_OP_RESET,
+};
+
+void aspeed_udma_set_tx_wptr(u32 ch_no, u32 wptr);
+void aspeed_udma_set_rx_rptr(u32 ch_no, u32 rptr);
+
+void aspeed_udma_tx_chan_ctrl(u32 ch_no, enum aspeed_udma_ops op);
+void aspeed_udma_rx_chan_ctrl(u32 ch_no, enum aspeed_udma_ops op);
+
+int aspeed_udma_request_tx_chan(u32 ch_no, dma_addr_t addr,
+				struct circ_buf *rb, u32 rb_sz,
+				aspeed_udma_cb_t cb, void *id, bool en_tmout);
+int aspeed_udma_request_rx_chan(u32 ch_no, dma_addr_t addr,
+				struct circ_buf *rb, u32 rb_sz,
+				aspeed_udma_cb_t cb, void *id, bool en_tmout);
+
+int aspeed_udma_free_tx_chan(u32 ch_no);
+int aspeed_udma_free_rx_chan(u32 ch_no);
+
+#endif
diff --git a/include/linux/stmmac.h b/include/linux/stmmac.h
index 1450397fc0bc..48d015ed2175 100644
--- a/include/linux/stmmac.h
+++ b/include/linux/stmmac.h
@@ -269,5 +269,6 @@ struct plat_stmmacenet_data {
 	int msi_rx_base_vec;
 	int msi_tx_base_vec;
 	bool use_phy_wol;
+	bool sph_disable;
 };
 #endif
diff --git a/include/linux/sunrpc/clnt.h b/include/linux/sunrpc/clnt.h
index a4661646adc9..9fcf5ffc4f9a 100644
--- a/include/linux/sunrpc/clnt.h
+++ b/include/linux/sunrpc/clnt.h
@@ -159,6 +159,7 @@ struct rpc_add_xprt_test {
 #define RPC_CLNT_CREATE_NO_RETRANS_TIMEOUT	(1UL << 9)
 #define RPC_CLNT_CREATE_SOFTERR		(1UL << 10)
 #define RPC_CLNT_CREATE_REUSEPORT	(1UL << 11)
+#define RPC_CLNT_CREATE_CONNECTED	(1UL << 12)
 
 struct rpc_clnt *rpc_create(struct rpc_create_args *args);
 struct rpc_clnt	*rpc_bind_new_program(struct rpc_clnt *,
diff --git a/include/linux/uio.h b/include/linux/uio.h
index 207101a9c5c3..6350354f97e9 100644
--- a/include/linux/uio.h
+++ b/include/linux/uio.h
@@ -35,6 +35,7 @@ struct iov_iter_state {
 
 struct iov_iter {
 	u8 iter_type;
+	bool nofault;
 	bool data_source;
 	size_t iov_offset;
 	size_t count;
@@ -133,7 +134,8 @@ size_t copy_page_from_iter_atomic(struct page *page, unsigned offset,
 				  size_t bytes, struct iov_iter *i);
 void iov_iter_advance(struct iov_iter *i, size_t bytes);
 void iov_iter_revert(struct iov_iter *i, size_t bytes);
-int iov_iter_fault_in_readable(const struct iov_iter *i, size_t bytes);
+size_t fault_in_iov_iter_readable(const struct iov_iter *i, size_t bytes);
+size_t fault_in_iov_iter_writeable(const struct iov_iter *i, size_t bytes);
 size_t iov_iter_single_seg_count(const struct iov_iter *i);
 size_t copy_page_to_iter(struct page *page, size_t offset, size_t bytes,
 			 struct iov_iter *i);
diff --git a/include/uapi/linux/aspeed-mctp.h b/include/uapi/linux/aspeed-mctp.h
new file mode 100644
index 000000000000..ffa90009d258
--- /dev/null
+++ b/include/uapi/linux/aspeed-mctp.h
@@ -0,0 +1,136 @@
+/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
+/* Copyright (c) 2020 Intel Corporation */
+
+#ifndef _UAPI_LINUX_ASPEED_MCTP_H
+#define _UAPI_LINUX_ASPEED_MCTP_H
+
+#include <linux/ioctl.h>
+#include <linux/types.h>
+
+/*
+ * aspeed-mctp is a simple device driver exposing a read/write interface:
+ *  +----------------------+
+ *  | PCIe VDM Header      | 16 bytes (Big Endian)
+ *  +----------------------+
+ *  | MCTP Message Payload | 64/128/256/512 bytes (Big Endian)
+ *  +----------------------+
+ *
+ * MCTP packet description can be found in DMTF DSP0238,
+ * MCTP PCIe VDM Transport Specification.
+ */
+
+#define ASPEED_MCTP_PCIE_VDM_HDR_SIZE 16
+
+/*
+ * uevents generated by aspeed-mctp driver
+ */
+#define ASPEED_MCTP_READY "PCIE_READY"
+
+/*
+ * maximum possible number of struct eid_info elements stored in list
+ */
+#define ASPEED_MCTP_EID_INFO_MAX 256
+
+/*
+ * MCTP operations
+ * @ASPEED_MCTP_IOCTL_FILTER_EID: enable/disable filter incoming packets based
+ * on Endpoint ID (BROKEN)
+ * @ASPEED_MCTP_IOCTL_GET_BDF: read PCI bus/device/function of MCTP Controller
+ * @ASPEED_MCTP_IOCTL_GET_MEDIUM_ID: read MCTP physical medium identifier
+ * related to PCIe revision
+ * @ASPEED_MCTP_IOCTL_GET_MTU: read max transmission unit (in bytes)
+ * @ASPEED_MCTP_IOCTL_REGISTER_DEFAULT_HANDLER Register client as default
+ * handler that receives all MCTP messages that were not dispatched to other
+ * clients
+ * @ASPEED_MCTP_IOCTL_REGISTER_TYPE_HANDLER Register client to receive all
+ * messages of specified MCTP type or PCI vendor defined type
+ * @ASPEED_MCTP_IOCTL_UNREGISTER_TYPE_HANDLER Unregister client as handler
+ * for specified MCTP type or PCI vendor defined message type
+ * @ASPEED_MCTP_GET_EID_INFO - deprecated, use ASPEED_MCTP_GET_EID_EXT instead
+ * @ASPEED_MCTP_SET_EID_INFO - deprecated, use ASPEED_MCTP_SET_EID_EXT instead
+ * @ASPEED_MCTP_GET_EID_EXT_INFO: read list of existing CPU EID and Domain ID
+ * mappings and return count which is lesser of the two: requested count and existing count
+ * @ASPEED_MCTP_SET_EID_EXT_INFO: write or overwrite already existing list of
+ * CPU EID and Domain ID mappings
+ * @ASPEED_MCTP_SET_OWN_EID: write/overwrite own EID information
+ */
+
+struct aspeed_mctp_filter_eid {
+	__u8 eid;
+	bool enable;
+};
+
+struct aspeed_mctp_get_bdf {
+	__u16 bdf;
+};
+
+struct aspeed_mctp_get_medium_id {
+	__u8 medium_id;
+};
+
+struct aspeed_mctp_get_mtu {
+	__u16 mtu;
+};
+
+struct aspeed_mctp_type_handler_ioctl {
+	__u8 mctp_type;		/* MCTP message type as per DSP239*/
+	/* Below params must be 0 if mctp_type is not Vendor Defined PCI */
+	__u16 pci_vendor_id;	/* PCI Vendor ID */
+	__u16 vendor_type;	/* Vendor specific type */
+	__u16 vendor_type_mask; /* Mask applied to vendor type */
+};
+
+struct aspeed_mctp_eid_info {
+	__u8 eid;
+	__u16 bdf;
+};
+
+struct aspeed_mctp_eid_ext_info {
+	__u8 eid;
+	__u16 bdf;
+	__u8 domain_id;
+};
+
+struct aspeed_mctp_get_eid_info {
+	__u64 ptr;
+	__u16 count;
+	__u8 start_eid;
+};
+
+struct aspeed_mctp_set_eid_info {
+	__u64 ptr;
+	__u16 count;
+};
+
+struct aspeed_mctp_set_own_eid {
+	__u8 eid;
+};
+
+#define ASPEED_MCTP_IOCTL_BASE	0x4d
+
+#define ASPEED_MCTP_IOCTL_FILTER_EID \
+	_IOW(ASPEED_MCTP_IOCTL_BASE, 0, struct aspeed_mctp_filter_eid)
+#define ASPEED_MCTP_IOCTL_GET_BDF \
+	_IOR(ASPEED_MCTP_IOCTL_BASE, 1, struct aspeed_mctp_get_bdf)
+#define ASPEED_MCTP_IOCTL_GET_MEDIUM_ID \
+	_IOR(ASPEED_MCTP_IOCTL_BASE, 2, struct aspeed_mctp_get_medium_id)
+#define ASPEED_MCTP_IOCTL_GET_MTU \
+	_IOR(ASPEED_MCTP_IOCTL_BASE, 3, struct aspeed_mctp_get_mtu)
+#define ASPEED_MCTP_IOCTL_REGISTER_DEFAULT_HANDLER \
+	_IO(ASPEED_MCTP_IOCTL_BASE, 4)
+#define ASPEED_MCTP_IOCTL_REGISTER_TYPE_HANDLER \
+	_IOW(ASPEED_MCTP_IOCTL_BASE, 6, struct aspeed_mctp_type_handler_ioctl)
+#define ASPEED_MCTP_IOCTL_UNREGISTER_TYPE_HANDLER \
+	_IOW(ASPEED_MCTP_IOCTL_BASE, 7, struct aspeed_mctp_type_handler_ioctl)
+#define ASPEED_MCTP_IOCTL_GET_EID_INFO \
+	_IOWR(ASPEED_MCTP_IOCTL_BASE, 8, struct aspeed_mctp_get_eid_info) /* deprecated */
+#define ASPEED_MCTP_IOCTL_SET_EID_INFO \
+	_IOW(ASPEED_MCTP_IOCTL_BASE, 9, struct aspeed_mctp_set_eid_info) /* deprecated */
+#define ASPEED_MCTP_IOCTL_GET_EID_EXT_INFO \
+	_IOW(ASPEED_MCTP_IOCTL_BASE, 10, struct aspeed_mctp_get_eid_info)
+#define ASPEED_MCTP_IOCTL_SET_EID_EXT_INFO \
+	_IOW(ASPEED_MCTP_IOCTL_BASE, 11, struct aspeed_mctp_set_eid_info)
+#define ASPEED_MCTP_IOCTL_SET_OWN_EID \
+	_IOW(ASPEED_MCTP_IOCTL_BASE, 12, struct aspeed_mctp_set_own_eid)
+
+#endif /* _UAPI_LINUX_ASPEED_MCTP_H */
diff --git a/include/uapi/linux/aspeed-otp.h b/include/uapi/linux/aspeed-otp.h
new file mode 100644
index 000000000000..cbb4d26fc804
--- /dev/null
+++ b/include/uapi/linux/aspeed-otp.h
@@ -0,0 +1,39 @@
+/* SPDX-License-Identifier: GPL-2.0-or-later WITH Linux-syscall-note */
+/*
+ * Copyright (C) 2021 ASPEED Technology Inc.
+ */
+
+#ifndef _UAPI_LINUX_ASPEED_OTP_H
+#define _UAPI_LINUX_ASPEED_OTP_H
+
+#include <linux/ioctl.h>
+#include <linux/types.h>
+
+struct otp_read {
+	unsigned int offset;
+	unsigned int len;
+	unsigned int *data;
+};
+
+struct otp_prog {
+	unsigned int dw_offset;
+	unsigned int bit_offset;
+	unsigned int value;
+};
+
+#define OTP_A0	0
+#define OTP_A1	1
+#define OTP_A2	2
+#define OTP_A3	3
+
+#define OTPIOC_BASE 'O'
+
+#define ASPEED_OTP_READ_DATA _IOR(OTPIOC_BASE, 0, struct otp_read)
+#define ASPEED_OTP_READ_CONF _IOR(OTPIOC_BASE, 1, struct otp_read)
+#define ASPEED_OTP_PROG_DATA _IOW(OTPIOC_BASE, 2, struct otp_prog)
+#define ASPEED_OTP_PROG_CONF _IOW(OTPIOC_BASE, 3, struct otp_prog)
+#define ASPEED_OTP_VER _IOR(OTPIOC_BASE, 4, unsigned int)
+#define ASPEED_OTP_SW_RID _IOR(OTPIOC_BASE, 5, u32*)
+#define ASPEED_SEC_KEY_NUM _IOR(OTPIOC_BASE, 6, u32*)
+
+#endif /* _UAPI_LINUX_ASPEED_JTAG_H */
diff --git a/include/uapi/linux/aspeed-video.h b/include/uapi/linux/aspeed-video.h
new file mode 100644
index 000000000000..867f87a583cb
--- /dev/null
+++ b/include/uapi/linux/aspeed-video.h
@@ -0,0 +1,23 @@
+/* SPDX-License-Identifier: GPL-2.0+ WITH Linux-syscall-note */
+/*
+ * Copyright (C) 2021 ASPEED Technology Inc.
+ */
+
+#ifndef _UAPI_LINUX_ASPEED_VIDEO_H
+#define _UAPI_LINUX_ASPEED_VIDEO_H
+
+#include <linux/v4l2-controls.h>
+
+// enum for aspeed video's v4l2 s_input
+enum aspeed_video_input {
+	VIDEO_INPUT_VGA = 0,
+	VIDEO_INPUT_GFX,
+	VIDEO_INPUT_MEM,
+	VIDEO_INPUT_MAX
+};
+
+#define V4L2_CID_ASPEED_COMPRESSION_SCHEME	(V4L2_CID_USER_ASPEED_BASE  + 1)
+#define V4L2_CID_ASPEED_HQ_MODE			(V4L2_CID_USER_ASPEED_BASE  + 2)
+#define V4L2_CID_ASPEED_HQ_JPEG_QUALITY		(V4L2_CID_USER_ASPEED_BASE  + 3)
+
+#endif /* _UAPI_LINUX_ASPEED_VIDEO_H */
diff --git a/include/uapi/linux/i3c/i3cdev.h b/include/uapi/linux/i3c/i3cdev.h
new file mode 100644
index 000000000000..5adc1e3e7c4f
--- /dev/null
+++ b/include/uapi/linux/i3c/i3cdev.h
@@ -0,0 +1,37 @@
+/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
+/*
+ * Copyright (c) 2020 Synopsys, Inc. and/or its affiliates.
+ *
+ * Author: Vitor Soares <vitor.soares@synopsys.com>
+ */
+
+#ifndef _UAPI_I3C_DEV_H_
+#define _UAPI_I3C_DEV_H_
+
+#include <linux/types.h>
+#include <linux/ioctl.h>
+
+/* IOCTL commands */
+#define I3C_DEV_IOC_MAGIC	0x07
+
+/**
+ * struct i3c_ioc_priv_xfer - I3C SDR ioctl private transfer
+ * @data: Holds pointer to userspace buffer with transmit data.
+ * @len: Length of data buffer buffers, in bytes.
+ * @rnw: encodes the transfer direction. true for a read, false for a write
+ */
+struct i3c_ioc_priv_xfer {
+	__u64 data;
+	__u16 len;
+	__u8 rnw;
+	__u8 pad[5];
+};
+
+#define I3C_PRIV_XFER_SIZE(N)	\
+	((((sizeof(struct i3c_ioc_priv_xfer)) * (N)) < (1 << _IOC_SIZEBITS)) \
+	? ((sizeof(struct i3c_ioc_priv_xfer)) * (N)) : 0)
+
+#define I3C_IOC_PRIV_XFER(N)	\
+	_IOC(_IOC_READ|_IOC_WRITE, I3C_DEV_IOC_MAGIC, 30, I3C_PRIV_XFER_SIZE(N))
+
+#endif
diff --git a/include/uapi/linux/if_alg.h b/include/uapi/linux/if_alg.h
index dc52a11ba6d1..6f334041057c 100644
--- a/include/uapi/linux/if_alg.h
+++ b/include/uapi/linux/if_alg.h
@@ -52,9 +52,12 @@ struct af_alg_iv {
 #define ALG_SET_AEAD_ASSOCLEN		4
 #define ALG_SET_AEAD_AUTHSIZE		5
 #define ALG_SET_DRBG_ENTROPY		6
+#define ALG_SET_PUBKEY			7
 
 /* Operations */
 #define ALG_OP_DECRYPT			0
 #define ALG_OP_ENCRYPT			1
+#define ALG_OP_SIGN			2
+#define ALG_OP_VERIFY			3
 
 #endif	/* _LINUX_IF_ALG_H */
diff --git a/include/uapi/linux/jtag.h b/include/uapi/linux/jtag.h
new file mode 100644
index 000000000000..77d0b471efd3
--- /dev/null
+++ b/include/uapi/linux/jtag.h
@@ -0,0 +1,370 @@
+/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
+/* Copyright (c) 2018 Mellanox Technologies. All rights reserved. */
+/* Copyright (c) 2018 Oleksandr Shamray <oleksandrs@mellanox.com> */
+/* Copyright (c) 2019 Intel Corporation */
+
+#ifndef __UAPI_LINUX_JTAG_H
+#define __UAPI_LINUX_JTAG_H
+
+#include <linux/types.h>
+#include <linux/ioctl.h>
+
+/*
+ * JTAG_XFER_MODE: JTAG transfer mode. Used to set JTAG controller transfer mode
+ * This is bitmask for feature param in jtag_mode for ioctl JTAG_SIOCMODE
+ */
+#define  JTAG_XFER_MODE 0
+/*
+ * JTAG_CONTROL_MODE: JTAG controller mode. Used to set JTAG controller mode
+ * This is bitmask for feature param in jtag_mode for ioctl JTAG_SIOCMODE
+ */
+#define  JTAG_CONTROL_MODE 1
+/*
+ * JTAG_MASTER_OUTPUT_DISABLE: JTAG master mode output disable, it is used to
+ * enable other devices to own the JTAG bus.
+ * This is bitmask for mode param in jtag_mode for ioctl JTAG_SIOCMODE
+ */
+#define  JTAG_MASTER_OUTPUT_DISABLE 0
+/*
+ * JTAG_MASTER_MODE: JTAG master mode. Used to set JTAG controller master mode
+ * This is bitmask for mode param in jtag_mode for ioctl JTAG_SIOCMODE
+ */
+#define  JTAG_MASTER_MODE 1
+/*
+ * JTAG_XFER_HW_MODE: JTAG hardware mode. Used to set HW drived or bitbang
+ * mode. This is bitmask for mode param in jtag_mode for ioctl JTAG_SIOCMODE
+ */
+#define  JTAG_XFER_HW_MODE 1
+/*
+ * JTAG_XFER_SW_MODE: JTAG software mode. Used to set SW drived or bitbang
+ * mode. This is bitmask for mode param in jtag_mode for ioctl JTAG_SIOCMODE
+ */
+#define  JTAG_XFER_SW_MODE 0
+
+/**
+ * enum jtag_tapstate:
+ *
+ * @JTAG_STATE_TLRESET: JTAG state machine Test Logic Reset state
+ * @JTAG_STATE_IDLE: JTAG state machine IDLE state
+ * @JTAG_STATE_SELECTDR: JTAG state machine SELECT_DR state
+ * @JTAG_STATE_CAPTUREDR: JTAG state machine CAPTURE_DR state
+ * @JTAG_STATE_SHIFTDR: JTAG state machine SHIFT_DR state
+ * @JTAG_STATE_EXIT1DR: JTAG state machine EXIT-1 DR state
+ * @JTAG_STATE_PAUSEDR: JTAG state machine PAUSE_DR state
+ * @JTAG_STATE_EXIT2DR: JTAG state machine EXIT-2 DR state
+ * @JTAG_STATE_UPDATEDR: JTAG state machine UPDATE DR state
+ * @JTAG_STATE_SELECTIR: JTAG state machine SELECT_IR state
+ * @JTAG_STATE_CAPTUREIR: JTAG state machine CAPTURE_IR state
+ * @JTAG_STATE_SHIFTIR: JTAG state machine SHIFT_IR state
+ * @JTAG_STATE_EXIT1IR: JTAG state machine EXIT-1 IR state
+ * @JTAG_STATE_PAUSEIR: JTAG state machine PAUSE_IR state
+ * @JTAG_STATE_EXIT2IR: JTAG state machine EXIT-2 IR state
+ * @JTAG_STATE_UPDATEIR: JTAG state machine UPDATE IR state
+ * @JTAG_STATE_CURRENT: JTAG current state, saved by driver
+ */
+enum jtag_tapstate {
+	JTAG_STATE_TLRESET,
+	JTAG_STATE_IDLE,
+	JTAG_STATE_SELECTDR,
+	JTAG_STATE_CAPTUREDR,
+	JTAG_STATE_SHIFTDR,
+	JTAG_STATE_EXIT1DR,
+	JTAG_STATE_PAUSEDR,
+	JTAG_STATE_EXIT2DR,
+	JTAG_STATE_UPDATEDR,
+	JTAG_STATE_SELECTIR,
+	JTAG_STATE_CAPTUREIR,
+	JTAG_STATE_SHIFTIR,
+	JTAG_STATE_EXIT1IR,
+	JTAG_STATE_PAUSEIR,
+	JTAG_STATE_EXIT2IR,
+	JTAG_STATE_UPDATEIR,
+	JTAG_STATE_CURRENT
+};
+
+/**
+ * enum jtag_reset:
+ *
+ * @JTAG_NO_RESET: JTAG run TAP from current state
+ * @JTAG_FORCE_RESET: JTAG force TAP to reset state
+ */
+enum jtag_reset {
+	JTAG_NO_RESET = 0,
+	JTAG_FORCE_RESET = 1,
+};
+
+/**
+ * enum jtag_xfer_type:
+ *
+ * @JTAG_SIR_XFER: SIR transfer
+ * @JTAG_SDR_XFER: SDR transfer
+ */
+enum jtag_xfer_type {
+	JTAG_SIR_XFER = 0,
+	JTAG_SDR_XFER = 1,
+};
+
+/**
+ * enum jtag_xfer_direction:
+ *
+ * @JTAG_READ_XFER: read transfer
+ * @JTAG_WRITE_XFER: write transfer
+ * @JTAG_READ_WRITE_XFER: read & write transfer
+ */
+enum jtag_xfer_direction {
+	JTAG_READ_XFER = 1,
+	JTAG_WRITE_XFER = 2,
+	JTAG_READ_WRITE_XFER = 3,
+};
+
+/**
+ * struct jtag_tap_state - forces JTAG state machine to go into a TAPC
+ * state
+ *
+ * @reset: 0 - run IDLE/PAUSE from current state
+ *         1 - go through TEST_LOGIC/RESET state before  IDLE/PAUSE
+ * @end: completion flag
+ * @tck: clock counter
+ *
+ * Structure provide interface to JTAG device for JTAG set state execution.
+ */
+struct jtag_tap_state {
+	__u8	reset;
+	__u8	from;
+	__u8	endstate;
+	__u8	tck;
+};
+
+/**
+ * union pad_config - Padding Configuration:
+ *
+ * @type: transfer type
+ * @pre_pad_number: Number of prepadding bits bit[11:0]
+ * @post_pad_number: Number of prepadding bits bit[23:12]
+ * @pad_data : Bit value to be used by pre and post padding bit[24]
+ * @int_value: unsigned int packed padding configuration value bit[32:0]
+ *
+ * Structure provide pre and post padding configuration in a single __u32
+ */
+union pad_config {
+	struct {
+		__u32 pre_pad_number	: 12;
+		__u32 post_pad_number	: 12;
+		__u32 pad_data		: 1;
+		__u32 rsvd		: 7;
+	};
+	__u32 int_value;
+};
+
+/**
+ * struct jtag_xfer - jtag xfer:
+ *
+ * @type: transfer type
+ * @direction: xfer direction
+ * @from: xfer current state
+ * @endstate: xfer end state
+ * @padding: xfer padding
+ * @length: xfer bits length
+ * @tdio : xfer data array
+ *
+ * Structure provide interface to JTAG device for JTAG SDR/SIR xfer execution.
+ */
+struct jtag_xfer {
+	__u8	type;
+	__u8	direction;
+	__u8	from;
+	__u8	endstate;
+	__u32	padding;
+	__u32	length;
+	__u64	tdio;
+};
+
+/**
+ * struct bitbang_packet - jtag bitbang array packet:
+ *
+ * @data:   JTAG Bitbang struct array pointer(input/output)
+ * @length: array size (input)
+ *
+ * Structure provide interface to JTAG device for JTAG bitbang bundle execution
+ */
+struct bitbang_packet {
+	struct tck_bitbang *data;
+	__u32	length;
+} __attribute__((__packed__));
+
+/**
+ * struct jtag_bitbang - jtag bitbang:
+ *
+ * @tms: JTAG TMS
+ * @tdi: JTAG TDI (input)
+ * @tdo: JTAG TDO (output)
+ *
+ * Structure provide interface to JTAG device for JTAG bitbang execution.
+ */
+struct tck_bitbang {
+	__u8	tms;
+	__u8	tdi;
+	__u8	tdo;
+} __attribute__((__packed__));
+
+/**
+ * struct jtag_mode - jtag mode:
+ *
+ * @feature: 0 - JTAG feature setting selector for JTAG controller HW/SW
+ *           1 - JTAG feature setting selector for controller bus master
+ *               mode output (enable / disable).
+ * @mode:    (0 - SW / 1 - HW) for JTAG_XFER_MODE feature(0)
+ *           (0 - output disable / 1 - output enable) for JTAG_CONTROL_MODE
+ *                                                    feature(1)
+ *
+ * Structure provide configuration modes to JTAG device.
+ */
+struct jtag_mode {
+	__u32	feature;
+	__u32	mode;
+};
+
+/* ioctl interface */
+#define __JTAG_IOCTL_MAGIC	0xb2
+
+#define JTAG_SIOCSTATE	_IOW(__JTAG_IOCTL_MAGIC, 0, struct jtag_tap_state)
+#define JTAG_SIOCFREQ	_IOW(__JTAG_IOCTL_MAGIC, 1, unsigned int)
+#define JTAG_GIOCFREQ	_IOR(__JTAG_IOCTL_MAGIC, 2, unsigned int)
+#define JTAG_IOCXFER	_IOWR(__JTAG_IOCTL_MAGIC, 3, struct jtag_xfer)
+#define JTAG_GIOCSTATUS _IOWR(__JTAG_IOCTL_MAGIC, 4, enum jtag_tapstate)
+#define JTAG_SIOCMODE	_IOW(__JTAG_IOCTL_MAGIC, 5, unsigned int)
+#define JTAG_IOCBITBANG	_IOW(__JTAG_IOCTL_MAGIC, 6, unsigned int)
+#define JTAG_SIOCTRST	_IOW(__JTAG_IOCTL_MAGIC, 7, unsigned int)
+
+/**
+ * struct tms_cycle - This structure represents a tms cycle state.
+ *
+ * @tmsbits: is the bitwise representation of the needed tms transitions to
+ *           move from one state to another.
+ * @count:   number of jumps needed to move to the needed state.
+ *
+ */
+struct tms_cycle {
+	unsigned char tmsbits;
+	unsigned char count;
+};
+
+/*
+ * This is the complete set TMS cycles for going from any TAP state to any
+ * other TAP state, following a "shortest path" rule.
+ */
+static const struct tms_cycle _tms_cycle_lookup[][16] = {
+/*	    TLR        RTI        SelDR      CapDR      SDR        Ex1DR*/
+/* TLR  */{{0x00, 0}, {0x00, 1}, {0x02, 2}, {0x02, 3}, {0x02, 4}, {0x0a, 4},
+/*	    PDR        Ex2DR      UpdDR      SelIR      CapIR      SIR*/
+	    {0x0a, 5}, {0x2a, 6}, {0x1a, 5}, {0x06, 3}, {0x06, 4}, {0x06, 5},
+/*	    Ex1IR      PIR        Ex2IR      UpdIR*/
+	    {0x16, 5}, {0x16, 6}, {0x56, 7}, {0x36, 6} },
+
+/*	    TLR        RTI        SelDR      CapDR      SDR        Ex1DR*/
+/* RTI  */{{0x07, 3}, {0x00, 0}, {0x01, 1}, {0x01, 2}, {0x01, 3}, {0x05, 3},
+/*	    PDR        Ex2DR      UpdDR      SelIR      CapIR      SIR*/
+	    {0x05, 4}, {0x15, 5}, {0x0d, 4}, {0x03, 2}, {0x03, 3}, {0x03, 4},
+/*	    Ex1IR      PIR        Ex2IR      UpdIR*/
+	    {0x0b, 4}, {0x0b, 5}, {0x2b, 6}, {0x1b, 5} },
+
+/*	    TLR        RTI        SelDR      CapDR      SDR        Ex1DR*/
+/* SelDR*/{{0x03, 2}, {0x03, 3}, {0x00, 0}, {0x00, 1}, {0x00, 2}, {0x02, 2},
+/*	    PDR        Ex2DR      UpdDR      SelIR      CapIR      SIR*/
+	    {0x02, 3}, {0x0a, 4}, {0x06, 3}, {0x01, 1}, {0x01, 2}, {0x01, 3},
+/*	    Ex1IR      PIR        Ex2IR      UpdIR*/
+	    {0x05, 3}, {0x05, 4}, {0x15, 5}, {0x0d, 4} },
+
+/*	    TLR        RTI        SelDR      CapDR      SDR        Ex1DR*/
+/* CapDR*/{{0x1f, 5}, {0x03, 3}, {0x07, 3}, {0x00, 0}, {0x00, 1}, {0x01, 1},
+/*	    PDR        Ex2DR      UpdDR      SelIR      CapIR      SIR*/
+	    {0x01, 2}, {0x05, 3}, {0x03, 2}, {0x0f, 4}, {0x0f, 5}, {0x0f, 6},
+/*	    Ex1IR      PIR        Ex2IR      UpdIR*/
+	    {0x2f, 6}, {0x2f, 7}, {0xaf, 8}, {0x6f, 7} },
+
+/*	    TLR        RTI        SelDR      CapDR      SDR        Ex1DR*/
+/* SDR  */{{0x1f, 5}, {0x03, 3}, {0x07, 3}, {0x07, 4}, {0x00, 0}, {0x01, 1},
+/*	    PDR        Ex2DR      UpdDR      SelIR      CapIR      SIR*/
+	    {0x01, 2}, {0x05, 3}, {0x03, 2}, {0x0f, 4}, {0x0f, 5}, {0x0f, 6},
+/*	    Ex1IR      PIR        Ex2IR      UpdIR*/
+	    {0x2f, 6}, {0x2f, 7}, {0xaf, 8}, {0x6f, 7} },
+
+/*	    TLR        RTI        SelDR      CapDR      SDR        Ex1DR*/
+/* Ex1DR*/{{0x0f, 4}, {0x01, 2}, {0x03, 2}, {0x03, 3}, {0x02, 3}, {0x00, 0},
+/*	    PDR        Ex2DR      UpdDR      SelIR      CapIR      SIR*/
+	    {0x00, 1}, {0x02, 2}, {0x01, 1}, {0x07, 3}, {0x07, 4}, {0x07, 5},
+/*	    Ex1IR      PIR        Ex2IR      UpdIR*/
+	    {0x17, 5}, {0x17, 6}, {0x57, 7}, {0x37, 6} },
+
+/*	    TLR        RTI        SelDR      CapDR      SDR        Ex1DR*/
+/* PDR  */{{0x1f, 5}, {0x03, 3}, {0x07, 3}, {0x07, 4}, {0x01, 2}, {0x05, 3},
+/*	    PDR        Ex2DR      UpdDR      SelIR      CapIR      SIR*/
+	    {0x00, 0}, {0x01, 1}, {0x03, 2}, {0x0f, 4}, {0x0f, 5}, {0x0f, 6},
+/*	    Ex1IR      PIR        Ex2IR      UpdIR*/
+	    {0x2f, 6}, {0x2f, 7}, {0xaf, 8}, {0x6f, 7} },
+
+/*	    TLR        RTI        SelDR      CapDR      SDR        Ex1DR*/
+/* Ex2DR*/{{0x0f, 4}, {0x01, 2}, {0x03, 2}, {0x03, 3}, {0x00, 1}, {0x02, 2},
+/*	    PDR        Ex2DR      UpdDR      SelIR      CapIR      SIR*/
+	    {0x02, 3}, {0x00, 0}, {0x01, 1}, {0x07, 3}, {0x07, 4}, {0x07, 5},
+/*	    Ex1IR      PIR        Ex2IR      UpdIR*/
+	    {0x17, 5}, {0x17, 6}, {0x57, 7}, {0x37, 6} },
+
+/*	    TLR        RTI        SelDR      CapDR      SDR        Ex1DR*/
+/* UpdDR*/{{0x07, 3}, {0x00, 1}, {0x01, 1}, {0x01, 2}, {0x01, 3}, {0x05, 3},
+/*	    PDR        Ex2DR      UpdDR      SelIR      CapIR      SIR*/
+	    {0x05, 4}, {0x15, 5}, {0x00, 0}, {0x03, 2}, {0x03, 3}, {0x03, 4},
+/*	    Ex1IR      PIR        Ex2IR      UpdIR*/
+	    {0x0b, 4}, {0x0b, 5}, {0x2b, 6}, {0x1b, 5} },
+
+/*	    TLR        RTI        SelDR      CapDR      SDR        Ex1DR*/
+/* SelIR*/{{0x01, 1}, {0x01, 2}, {0x05, 3}, {0x05, 4}, {0x05, 5}, {0x15, 5},
+/*	    PDR        Ex2DR      UpdDR      SelIR      CapIR      SIR*/
+	    {0x15, 6}, {0x55, 7}, {0x35, 6}, {0x00, 0}, {0x00, 1}, {0x00, 2},
+/*	    Ex1IR      PIR        Ex2IR      UpdIR*/
+	    {0x02, 2}, {0x02, 3}, {0x0a, 4}, {0x06, 3} },
+
+/*	    TLR        RTI        SelDR      CapDR      SDR        Ex1DR*/
+/* CapIR*/{{0x1f, 5}, {0x03, 3}, {0x07, 3}, {0x07, 4}, {0x07, 5}, {0x17, 5},
+/*	    PDR        Ex2DR      UpdDR      SelIR      CapIR      SIR*/
+	    {0x17, 6}, {0x57, 7}, {0x37, 6}, {0x0f, 4}, {0x00, 0}, {0x00, 1},
+/*	    Ex1IR      PIR        Ex2IR      UpdIR*/
+	    {0x01, 1}, {0x01, 2}, {0x05, 3}, {0x03, 2} },
+
+/*	    TLR        RTI        SelDR      CapDR      SDR        Ex1DR*/
+/* SIR  */{{0x1f, 5}, {0x03, 3}, {0x07, 3}, {0x07, 4}, {0x07, 5}, {0x17, 5},
+/*	    PDR        Ex2DR      UpdDR      SelIR      CapIR      SIR*/
+	    {0x17, 6}, {0x57, 7}, {0x37, 6}, {0x0f, 4}, {0x0f, 5}, {0x00, 0},
+/*	    Ex1IR      PIR        Ex2IR      UpdIR*/
+	    {0x01, 1}, {0x01, 2}, {0x05, 3}, {0x03, 2} },
+
+/*	    TLR        RTI        SelDR      CapDR      SDR        Ex1DR*/
+/* Ex1IR*/{{0x0f, 4}, {0x01, 2}, {0x03, 2}, {0x03, 3}, {0x03, 4}, {0x0b, 4},
+/*	    PDR        Ex2DR      UpdDR      SelIR      CapIR      SIR*/
+	    {0x0b, 5}, {0x2b, 6}, {0x1b, 5}, {0x07, 3}, {0x07, 4}, {0x02, 3},
+/*	    Ex1IR      PIR        Ex2IR      UpdIR*/
+	    {0x00, 0}, {0x00, 1}, {0x02, 2}, {0x01, 1} },
+
+/*	    TLR        RTI        SelDR      CapDR      SDR        Ex1DR*/
+/* PIR  */{{0x1f, 5}, {0x03, 3}, {0x07, 3}, {0x07, 4}, {0x07, 5}, {0x17, 5},
+/*	    PDR        Ex2DR      UpdDR      SelIR      CapIR      SIR*/
+	    {0x17, 6}, {0x57, 7}, {0x37, 6}, {0x0f, 4}, {0x0f, 5}, {0x01, 2},
+/*	    Ex1IR      PIR        Ex2IR      UpdIR*/
+	    {0x05, 3}, {0x00, 0}, {0x01, 1}, {0x03, 2} },
+
+/*	    TLR        RTI        SelDR      CapDR      SDR        Ex1DR*/
+/* Ex2IR*/{{0x0f, 4}, {0x01, 2}, {0x03, 2}, {0x03, 3}, {0x03, 4}, {0x0b, 4},
+/*	    PDR        Ex2DR      UpdDR      SelIR      CapIR      SIR*/
+	    {0x0b, 5}, {0x2b, 6}, {0x1b, 5}, {0x07, 3}, {0x07, 4}, {0x00, 1},
+/*	    Ex1IR      PIR        Ex2IR      UpdIR*/
+	    {0x02, 2}, {0x02, 3}, {0x00, 0}, {0x01, 1} },
+
+/*	    TLR        RTI        SelDR      CapDR      SDR        Ex1DR*/
+/* UpdIR*/{{0x07, 3}, {0x00, 1}, {0x01, 1}, {0x01, 2}, {0x01, 3}, {0x05, 3},
+/*	    PDR        Ex2DR      UpdDR      SelIR      CapIR      SIR*/
+	    {0x05, 4}, {0x15, 5}, {0x0d, 4}, {0x03, 2}, {0x03, 3}, {0x03, 4},
+/*	    Ex1IR      PIR        Ex2IR      UpdIR*/
+	    {0x0b, 4}, {0x0b, 5}, {0x2b, 6}, {0x00, 0} },
+};
+
+#endif /* __UAPI_LINUX_JTAG_H */
diff --git a/include/uapi/linux/rfkill.h b/include/uapi/linux/rfkill.h
index 283c5a7b3f2c..db6c8588c1d0 100644
--- a/include/uapi/linux/rfkill.h
+++ b/include/uapi/linux/rfkill.h
@@ -184,7 +184,7 @@ struct rfkill_event_ext {
 #define RFKILL_IOC_NOINPUT	1
 #define RFKILL_IOCTL_NOINPUT	_IO(RFKILL_IOC_MAGIC, RFKILL_IOC_NOINPUT)
 #define RFKILL_IOC_MAX_SIZE	2
-#define RFKILL_IOCTL_MAX_SIZE	_IOW(RFKILL_IOC_MAGIC, RFKILL_IOC_EXT_SIZE, __u32)
+#define RFKILL_IOCTL_MAX_SIZE	_IOW(RFKILL_IOC_MAGIC, RFKILL_IOC_MAX_SIZE, __u32)
 
 /* and that's all userspace gets */
 
diff --git a/include/uapi/linux/v4l2-controls.h b/include/uapi/linux/v4l2-controls.h
index 5532b5f68493..62f8b0dca337 100644
--- a/include/uapi/linux/v4l2-controls.h
+++ b/include/uapi/linux/v4l2-controls.h
@@ -212,6 +212,12 @@ enum v4l2_colorfx {
  */
 #define V4L2_CID_USER_CCS_BASE			(V4L2_CID_USER_BASE + 0x10f0)
 
+/*
+ * The base for the aspeed driver controls.
+ * We reserve 16 controls for this driver.
+ */
+#define V4L2_CID_USER_ASPEED_BASE		(V4L2_CID_USER_BASE + 0x1170)
+
 /* MPEG-class control IDs */
 /* The MPEG controls are applicable to all codec controls
  * and the 'MPEG' part of the define is historical */
diff --git a/include/uapi/linux/videodev2.h b/include/uapi/linux/videodev2.h
index 9260791b8438..205dd4c11ed0 100644
--- a/include/uapi/linux/videodev2.h
+++ b/include/uapi/linux/videodev2.h
@@ -737,6 +737,7 @@ struct v4l2_pix_format {
 #define V4L2_PIX_FMT_SUNXI_TILED_NV12 v4l2_fourcc('S', 'T', '1', '2') /* Sunxi Tiled NV12 Format */
 #define V4L2_PIX_FMT_CNF4     v4l2_fourcc('C', 'N', 'F', '4') /* Intel 4-bit packed depth confidence information */
 #define V4L2_PIX_FMT_HI240    v4l2_fourcc('H', 'I', '2', '4') /* BTTV 8-bit dithered RGB */
+#define V4L2_PIX_FMT_AJPG     v4l2_fourcc('A', 'J', 'P', 'G') /* Aspeed JPEG */
 
 /* 10bit raw bayer packed, 32 bytes for every 25 pixels, last LSB 6 bits unused */
 #define V4L2_PIX_FMT_IPU3_SBGGR10	v4l2_fourcc('i', 'p', '3', 'b') /* IPU3 packed 10-bit BGGR bayer */
@@ -777,6 +778,7 @@ struct v4l2_pix_format {
 /* Flags */
 #define V4L2_PIX_FMT_FLAG_PREMUL_ALPHA	0x00000001
 #define V4L2_PIX_FMT_FLAG_SET_CSC	0x00000002
+#define V4L2_PIX_FMT_FLAG_PARTIAL_JPG	0x00000004
 
 /*
  *	F O R M A T   E N U M E R A T I O N
diff --git a/include/uapi/linux/virtio_ids.h b/include/uapi/linux/virtio_ids.h
index 80d76b75bccd..7aa2eb766205 100644
--- a/include/uapi/linux/virtio_ids.h
+++ b/include/uapi/linux/virtio_ids.h
@@ -73,12 +73,12 @@
  * Virtio Transitional IDs
  */
 
-#define VIRTIO_TRANS_ID_NET		1000 /* transitional virtio net */
-#define VIRTIO_TRANS_ID_BLOCK		1001 /* transitional virtio block */
-#define VIRTIO_TRANS_ID_BALLOON		1002 /* transitional virtio balloon */
-#define VIRTIO_TRANS_ID_CONSOLE		1003 /* transitional virtio console */
-#define VIRTIO_TRANS_ID_SCSI		1004 /* transitional virtio SCSI */
-#define VIRTIO_TRANS_ID_RNG		1005 /* transitional virtio rng */
-#define VIRTIO_TRANS_ID_9P		1009 /* transitional virtio 9p console */
+#define VIRTIO_TRANS_ID_NET		0x1000 /* transitional virtio net */
+#define VIRTIO_TRANS_ID_BLOCK		0x1001 /* transitional virtio block */
+#define VIRTIO_TRANS_ID_BALLOON		0x1002 /* transitional virtio balloon */
+#define VIRTIO_TRANS_ID_CONSOLE		0x1003 /* transitional virtio console */
+#define VIRTIO_TRANS_ID_SCSI		0x1004 /* transitional virtio SCSI */
+#define VIRTIO_TRANS_ID_RNG		0x1005 /* transitional virtio rng */
+#define VIRTIO_TRANS_ID_9P		0x1009 /* transitional virtio 9p console */
 
 #endif /* _LINUX_VIRTIO_IDS_H */
diff --git a/kernel/bpf/btf.c b/kernel/bpf/btf.c
index 09406b0e215e..40df35088cdb 100644
--- a/kernel/bpf/btf.c
+++ b/kernel/bpf/btf.c
@@ -4800,10 +4800,12 @@ bool btf_ctx_access(int off, int size, enum bpf_access_type type,
 	/* check for PTR_TO_RDONLY_BUF_OR_NULL or PTR_TO_RDWR_BUF_OR_NULL */
 	for (i = 0; i < prog->aux->ctx_arg_info_size; i++) {
 		const struct bpf_ctx_arg_aux *ctx_arg_info = &prog->aux->ctx_arg_info[i];
+		u32 type, flag;
 
-		if (ctx_arg_info->offset == off &&
-		    (ctx_arg_info->reg_type == PTR_TO_RDONLY_BUF_OR_NULL ||
-		     ctx_arg_info->reg_type == PTR_TO_RDWR_BUF_OR_NULL)) {
+		type = base_type(ctx_arg_info->reg_type);
+		flag = type_flag(ctx_arg_info->reg_type);
+		if (ctx_arg_info->offset == off && type == PTR_TO_BUF &&
+		    (flag & PTR_MAYBE_NULL)) {
 			info->reg_type = ctx_arg_info->reg_type;
 			return true;
 		}
@@ -5508,9 +5510,9 @@ static int btf_check_func_arg_match(struct bpf_verifier_env *env,
 			if (reg->type == PTR_TO_BTF_ID) {
 				reg_btf = reg->btf;
 				reg_ref_id = reg->btf_id;
-			} else if (reg2btf_ids[reg->type]) {
+			} else if (reg2btf_ids[base_type(reg->type)]) {
 				reg_btf = btf_vmlinux;
-				reg_ref_id = *reg2btf_ids[reg->type];
+				reg_ref_id = *reg2btf_ids[base_type(reg->type)];
 			} else {
 				bpf_log(log, "kernel function %s args#%d expected pointer to %s %s but R%d is not a pointer to btf_id\n",
 					func_name, i,
@@ -5717,7 +5719,7 @@ int btf_prepare_func_args(struct bpf_verifier_env *env, int subprog,
 				return -EINVAL;
 			}
 
-			reg->type = PTR_TO_MEM_OR_NULL;
+			reg->type = PTR_TO_MEM | PTR_MAYBE_NULL;
 			reg->id = ++env->id_gen;
 
 			continue;
@@ -6229,7 +6231,7 @@ const struct bpf_func_proto bpf_btf_find_by_name_kind_proto = {
 	.func		= bpf_btf_find_by_name_kind,
 	.gpl_only	= false,
 	.ret_type	= RET_INTEGER,
-	.arg1_type	= ARG_PTR_TO_MEM,
+	.arg1_type	= ARG_PTR_TO_MEM | MEM_RDONLY,
 	.arg2_type	= ARG_CONST_SIZE,
 	.arg3_type	= ARG_ANYTHING,
 	.arg4_type	= ARG_ANYTHING,
diff --git a/kernel/bpf/helpers.c b/kernel/bpf/helpers.c
index 6f600cc95ccd..a711ffe23893 100644
--- a/kernel/bpf/helpers.c
+++ b/kernel/bpf/helpers.c
@@ -530,7 +530,7 @@ const struct bpf_func_proto bpf_strtol_proto = {
 	.func		= bpf_strtol,
 	.gpl_only	= false,
 	.ret_type	= RET_INTEGER,
-	.arg1_type	= ARG_PTR_TO_MEM,
+	.arg1_type	= ARG_PTR_TO_MEM | MEM_RDONLY,
 	.arg2_type	= ARG_CONST_SIZE,
 	.arg3_type	= ARG_ANYTHING,
 	.arg4_type	= ARG_PTR_TO_LONG,
@@ -558,7 +558,7 @@ const struct bpf_func_proto bpf_strtoul_proto = {
 	.func		= bpf_strtoul,
 	.gpl_only	= false,
 	.ret_type	= RET_INTEGER,
-	.arg1_type	= ARG_PTR_TO_MEM,
+	.arg1_type	= ARG_PTR_TO_MEM | MEM_RDONLY,
 	.arg2_type	= ARG_CONST_SIZE,
 	.arg3_type	= ARG_ANYTHING,
 	.arg4_type	= ARG_PTR_TO_LONG,
@@ -630,7 +630,7 @@ const struct bpf_func_proto bpf_event_output_data_proto =  {
 	.arg1_type      = ARG_PTR_TO_CTX,
 	.arg2_type      = ARG_CONST_MAP_PTR,
 	.arg3_type      = ARG_ANYTHING,
-	.arg4_type      = ARG_PTR_TO_MEM,
+	.arg4_type      = ARG_PTR_TO_MEM | MEM_RDONLY,
 	.arg5_type      = ARG_CONST_SIZE_OR_ZERO,
 };
 
@@ -667,7 +667,7 @@ BPF_CALL_2(bpf_per_cpu_ptr, const void *, ptr, u32, cpu)
 const struct bpf_func_proto bpf_per_cpu_ptr_proto = {
 	.func		= bpf_per_cpu_ptr,
 	.gpl_only	= false,
-	.ret_type	= RET_PTR_TO_MEM_OR_BTF_ID_OR_NULL,
+	.ret_type	= RET_PTR_TO_MEM_OR_BTF_ID | PTR_MAYBE_NULL | MEM_RDONLY,
 	.arg1_type	= ARG_PTR_TO_PERCPU_BTF_ID,
 	.arg2_type	= ARG_ANYTHING,
 };
@@ -680,7 +680,7 @@ BPF_CALL_1(bpf_this_cpu_ptr, const void *, percpu_ptr)
 const struct bpf_func_proto bpf_this_cpu_ptr_proto = {
 	.func		= bpf_this_cpu_ptr,
 	.gpl_only	= false,
-	.ret_type	= RET_PTR_TO_MEM_OR_BTF_ID,
+	.ret_type	= RET_PTR_TO_MEM_OR_BTF_ID | MEM_RDONLY,
 	.arg1_type	= ARG_PTR_TO_PERCPU_BTF_ID,
 };
 
@@ -1013,7 +1013,7 @@ const struct bpf_func_proto bpf_snprintf_proto = {
 	.arg1_type	= ARG_PTR_TO_MEM_OR_NULL,
 	.arg2_type	= ARG_CONST_SIZE_OR_ZERO,
 	.arg3_type	= ARG_PTR_TO_CONST_STR,
-	.arg4_type	= ARG_PTR_TO_MEM_OR_NULL,
+	.arg4_type	= ARG_PTR_TO_MEM | PTR_MAYBE_NULL | MEM_RDONLY,
 	.arg5_type	= ARG_CONST_SIZE_OR_ZERO,
 };
 
diff --git a/kernel/bpf/map_iter.c b/kernel/bpf/map_iter.c
index 6a9542af4212..b0fa190b0979 100644
--- a/kernel/bpf/map_iter.c
+++ b/kernel/bpf/map_iter.c
@@ -174,9 +174,9 @@ static const struct bpf_iter_reg bpf_map_elem_reg_info = {
 	.ctx_arg_info_size	= 2,
 	.ctx_arg_info		= {
 		{ offsetof(struct bpf_iter__bpf_map_elem, key),
-		  PTR_TO_RDONLY_BUF_OR_NULL },
+		  PTR_TO_BUF | PTR_MAYBE_NULL | MEM_RDONLY },
 		{ offsetof(struct bpf_iter__bpf_map_elem, value),
-		  PTR_TO_RDWR_BUF_OR_NULL },
+		  PTR_TO_BUF | PTR_MAYBE_NULL },
 	},
 };
 
diff --git a/kernel/bpf/verifier.c b/kernel/bpf/verifier.c
index 670721e39c0e..d2b119b4fbe7 100644
--- a/kernel/bpf/verifier.c
+++ b/kernel/bpf/verifier.c
@@ -445,18 +445,6 @@ static bool reg_type_not_null(enum bpf_reg_type type)
 		type == PTR_TO_SOCK_COMMON;
 }
 
-static bool reg_type_may_be_null(enum bpf_reg_type type)
-{
-	return type == PTR_TO_MAP_VALUE_OR_NULL ||
-	       type == PTR_TO_SOCKET_OR_NULL ||
-	       type == PTR_TO_SOCK_COMMON_OR_NULL ||
-	       type == PTR_TO_TCP_SOCK_OR_NULL ||
-	       type == PTR_TO_BTF_ID_OR_NULL ||
-	       type == PTR_TO_MEM_OR_NULL ||
-	       type == PTR_TO_RDONLY_BUF_OR_NULL ||
-	       type == PTR_TO_RDWR_BUF_OR_NULL;
-}
-
 static bool reg_may_point_to_spin_lock(const struct bpf_reg_state *reg)
 {
 	return reg->type == PTR_TO_MAP_VALUE &&
@@ -465,12 +453,14 @@ static bool reg_may_point_to_spin_lock(const struct bpf_reg_state *reg)
 
 static bool reg_type_may_be_refcounted_or_null(enum bpf_reg_type type)
 {
-	return type == PTR_TO_SOCKET ||
-		type == PTR_TO_SOCKET_OR_NULL ||
-		type == PTR_TO_TCP_SOCK ||
-		type == PTR_TO_TCP_SOCK_OR_NULL ||
-		type == PTR_TO_MEM ||
-		type == PTR_TO_MEM_OR_NULL;
+	return base_type(type) == PTR_TO_SOCKET ||
+		base_type(type) == PTR_TO_TCP_SOCK ||
+		base_type(type) == PTR_TO_MEM;
+}
+
+static bool type_is_rdonly_mem(u32 type)
+{
+	return type & MEM_RDONLY;
 }
 
 static bool arg_type_may_be_refcounted(enum bpf_arg_type type)
@@ -478,14 +468,9 @@ static bool arg_type_may_be_refcounted(enum bpf_arg_type type)
 	return type == ARG_PTR_TO_SOCK_COMMON;
 }
 
-static bool arg_type_may_be_null(enum bpf_arg_type type)
+static bool type_may_be_null(u32 type)
 {
-	return type == ARG_PTR_TO_MAP_VALUE_OR_NULL ||
-	       type == ARG_PTR_TO_MEM_OR_NULL ||
-	       type == ARG_PTR_TO_CTX_OR_NULL ||
-	       type == ARG_PTR_TO_SOCKET_OR_NULL ||
-	       type == ARG_PTR_TO_ALLOC_MEM_OR_NULL ||
-	       type == ARG_PTR_TO_STACK_OR_NULL;
+	return type & PTR_MAYBE_NULL;
 }
 
 /* Determine whether the function releases some resources allocated by another
@@ -545,39 +530,54 @@ static bool is_cmpxchg_insn(const struct bpf_insn *insn)
 	       insn->imm == BPF_CMPXCHG;
 }
 
-/* string representation of 'enum bpf_reg_type' */
-static const char * const reg_type_str[] = {
-	[NOT_INIT]		= "?",
-	[SCALAR_VALUE]		= "inv",
-	[PTR_TO_CTX]		= "ctx",
-	[CONST_PTR_TO_MAP]	= "map_ptr",
-	[PTR_TO_MAP_VALUE]	= "map_value",
-	[PTR_TO_MAP_VALUE_OR_NULL] = "map_value_or_null",
-	[PTR_TO_STACK]		= "fp",
-	[PTR_TO_PACKET]		= "pkt",
-	[PTR_TO_PACKET_META]	= "pkt_meta",
-	[PTR_TO_PACKET_END]	= "pkt_end",
-	[PTR_TO_FLOW_KEYS]	= "flow_keys",
-	[PTR_TO_SOCKET]		= "sock",
-	[PTR_TO_SOCKET_OR_NULL] = "sock_or_null",
-	[PTR_TO_SOCK_COMMON]	= "sock_common",
-	[PTR_TO_SOCK_COMMON_OR_NULL] = "sock_common_or_null",
-	[PTR_TO_TCP_SOCK]	= "tcp_sock",
-	[PTR_TO_TCP_SOCK_OR_NULL] = "tcp_sock_or_null",
-	[PTR_TO_TP_BUFFER]	= "tp_buffer",
-	[PTR_TO_XDP_SOCK]	= "xdp_sock",
-	[PTR_TO_BTF_ID]		= "ptr_",
-	[PTR_TO_BTF_ID_OR_NULL]	= "ptr_or_null_",
-	[PTR_TO_PERCPU_BTF_ID]	= "percpu_ptr_",
-	[PTR_TO_MEM]		= "mem",
-	[PTR_TO_MEM_OR_NULL]	= "mem_or_null",
-	[PTR_TO_RDONLY_BUF]	= "rdonly_buf",
-	[PTR_TO_RDONLY_BUF_OR_NULL] = "rdonly_buf_or_null",
-	[PTR_TO_RDWR_BUF]	= "rdwr_buf",
-	[PTR_TO_RDWR_BUF_OR_NULL] = "rdwr_buf_or_null",
-	[PTR_TO_FUNC]		= "func",
-	[PTR_TO_MAP_KEY]	= "map_key",
-};
+/* string representation of 'enum bpf_reg_type'
+ *
+ * Note that reg_type_str() can not appear more than once in a single verbose()
+ * statement.
+ */
+static const char *reg_type_str(struct bpf_verifier_env *env,
+				enum bpf_reg_type type)
+{
+	char postfix[16] = {0}, prefix[16] = {0};
+	static const char * const str[] = {
+		[NOT_INIT]		= "?",
+		[SCALAR_VALUE]		= "inv",
+		[PTR_TO_CTX]		= "ctx",
+		[CONST_PTR_TO_MAP]	= "map_ptr",
+		[PTR_TO_MAP_VALUE]	= "map_value",
+		[PTR_TO_STACK]		= "fp",
+		[PTR_TO_PACKET]		= "pkt",
+		[PTR_TO_PACKET_META]	= "pkt_meta",
+		[PTR_TO_PACKET_END]	= "pkt_end",
+		[PTR_TO_FLOW_KEYS]	= "flow_keys",
+		[PTR_TO_SOCKET]		= "sock",
+		[PTR_TO_SOCK_COMMON]	= "sock_common",
+		[PTR_TO_TCP_SOCK]	= "tcp_sock",
+		[PTR_TO_TP_BUFFER]	= "tp_buffer",
+		[PTR_TO_XDP_SOCK]	= "xdp_sock",
+		[PTR_TO_BTF_ID]		= "ptr_",
+		[PTR_TO_PERCPU_BTF_ID]	= "percpu_ptr_",
+		[PTR_TO_MEM]		= "mem",
+		[PTR_TO_BUF]		= "buf",
+		[PTR_TO_FUNC]		= "func",
+		[PTR_TO_MAP_KEY]	= "map_key",
+	};
+
+	if (type & PTR_MAYBE_NULL) {
+		if (base_type(type) == PTR_TO_BTF_ID ||
+		    base_type(type) == PTR_TO_PERCPU_BTF_ID)
+			strncpy(postfix, "or_null_", 16);
+		else
+			strncpy(postfix, "_or_null", 16);
+	}
+
+	if (type & MEM_RDONLY)
+		strncpy(prefix, "rdonly_", 16);
+
+	snprintf(env->type_str_buf, TYPE_STR_BUF_LEN, "%s%s%s",
+		 prefix, str[base_type(type)], postfix);
+	return env->type_str_buf;
+}
 
 static char slot_type_char[] = {
 	[STACK_INVALID]	= '?',
@@ -628,7 +628,7 @@ static void print_verifier_state(struct bpf_verifier_env *env,
 			continue;
 		verbose(env, " R%d", i);
 		print_liveness(env, reg->live);
-		verbose(env, "=%s", reg_type_str[t]);
+		verbose(env, "=%s", reg_type_str(env, t));
 		if (t == SCALAR_VALUE && reg->precise)
 			verbose(env, "P");
 		if ((t == SCALAR_VALUE || t == PTR_TO_STACK) &&
@@ -636,9 +636,8 @@ static void print_verifier_state(struct bpf_verifier_env *env,
 			/* reg->off should be 0 for SCALAR_VALUE */
 			verbose(env, "%lld", reg->var_off.value + reg->off);
 		} else {
-			if (t == PTR_TO_BTF_ID ||
-			    t == PTR_TO_BTF_ID_OR_NULL ||
-			    t == PTR_TO_PERCPU_BTF_ID)
+			if (base_type(t) == PTR_TO_BTF_ID ||
+			    base_type(t) == PTR_TO_PERCPU_BTF_ID)
 				verbose(env, "%s", kernel_type_name(reg->btf, reg->btf_id));
 			verbose(env, "(id=%d", reg->id);
 			if (reg_type_may_be_refcounted_or_null(t))
@@ -647,10 +646,9 @@ static void print_verifier_state(struct bpf_verifier_env *env,
 				verbose(env, ",off=%d", reg->off);
 			if (type_is_pkt_pointer(t))
 				verbose(env, ",r=%d", reg->range);
-			else if (t == CONST_PTR_TO_MAP ||
-				 t == PTR_TO_MAP_KEY ||
-				 t == PTR_TO_MAP_VALUE ||
-				 t == PTR_TO_MAP_VALUE_OR_NULL)
+			else if (base_type(t) == CONST_PTR_TO_MAP ||
+				 base_type(t) == PTR_TO_MAP_KEY ||
+				 base_type(t) == PTR_TO_MAP_VALUE)
 				verbose(env, ",ks=%d,vs=%d",
 					reg->map_ptr->key_size,
 					reg->map_ptr->value_size);
@@ -720,7 +718,7 @@ static void print_verifier_state(struct bpf_verifier_env *env,
 		if (state->stack[i].slot_type[0] == STACK_SPILL) {
 			reg = &state->stack[i].spilled_ptr;
 			t = reg->type;
-			verbose(env, "=%s", reg_type_str[t]);
+			verbose(env, "=%s", reg_type_str(env, t));
 			if (t == SCALAR_VALUE && reg->precise)
 				verbose(env, "P");
 			if (t == SCALAR_VALUE && tnum_is_const(reg->var_off))
@@ -1133,8 +1131,7 @@ static void mark_reg_known_zero(struct bpf_verifier_env *env,
 
 static void mark_ptr_not_null_reg(struct bpf_reg_state *reg)
 {
-	switch (reg->type) {
-	case PTR_TO_MAP_VALUE_OR_NULL: {
+	if (base_type(reg->type) == PTR_TO_MAP_VALUE) {
 		const struct bpf_map *map = reg->map_ptr;
 
 		if (map->inner_map_meta) {
@@ -1153,32 +1150,10 @@ static void mark_ptr_not_null_reg(struct bpf_reg_state *reg)
 		} else {
 			reg->type = PTR_TO_MAP_VALUE;
 		}
-		break;
-	}
-	case PTR_TO_SOCKET_OR_NULL:
-		reg->type = PTR_TO_SOCKET;
-		break;
-	case PTR_TO_SOCK_COMMON_OR_NULL:
-		reg->type = PTR_TO_SOCK_COMMON;
-		break;
-	case PTR_TO_TCP_SOCK_OR_NULL:
-		reg->type = PTR_TO_TCP_SOCK;
-		break;
-	case PTR_TO_BTF_ID_OR_NULL:
-		reg->type = PTR_TO_BTF_ID;
-		break;
-	case PTR_TO_MEM_OR_NULL:
-		reg->type = PTR_TO_MEM;
-		break;
-	case PTR_TO_RDONLY_BUF_OR_NULL:
-		reg->type = PTR_TO_RDONLY_BUF;
-		break;
-	case PTR_TO_RDWR_BUF_OR_NULL:
-		reg->type = PTR_TO_RDWR_BUF;
-		break;
-	default:
-		WARN_ONCE(1, "unknown nullable register type");
+		return;
 	}
+
+	reg->type &= ~PTR_MAYBE_NULL;
 }
 
 static bool reg_is_pkt_pointer(const struct bpf_reg_state *reg)
@@ -1906,7 +1881,7 @@ static int mark_reg_read(struct bpf_verifier_env *env,
 			break;
 		if (parent->live & REG_LIVE_DONE) {
 			verbose(env, "verifier BUG type %s var_off %lld off %d\n",
-				reg_type_str[parent->type],
+				reg_type_str(env, parent->type),
 				parent->var_off.value, parent->off);
 			return -EFAULT;
 		}
@@ -2564,9 +2539,8 @@ static int mark_chain_precision_stack(struct bpf_verifier_env *env, int spi)
 
 static bool is_spillable_regtype(enum bpf_reg_type type)
 {
-	switch (type) {
+	switch (base_type(type)) {
 	case PTR_TO_MAP_VALUE:
-	case PTR_TO_MAP_VALUE_OR_NULL:
 	case PTR_TO_STACK:
 	case PTR_TO_CTX:
 	case PTR_TO_PACKET:
@@ -2575,21 +2549,13 @@ static bool is_spillable_regtype(enum bpf_reg_type type)
 	case PTR_TO_FLOW_KEYS:
 	case CONST_PTR_TO_MAP:
 	case PTR_TO_SOCKET:
-	case PTR_TO_SOCKET_OR_NULL:
 	case PTR_TO_SOCK_COMMON:
-	case PTR_TO_SOCK_COMMON_OR_NULL:
 	case PTR_TO_TCP_SOCK:
-	case PTR_TO_TCP_SOCK_OR_NULL:
 	case PTR_TO_XDP_SOCK:
 	case PTR_TO_BTF_ID:
-	case PTR_TO_BTF_ID_OR_NULL:
-	case PTR_TO_RDONLY_BUF:
-	case PTR_TO_RDONLY_BUF_OR_NULL:
-	case PTR_TO_RDWR_BUF:
-	case PTR_TO_RDWR_BUF_OR_NULL:
+	case PTR_TO_BUF:
 	case PTR_TO_PERCPU_BTF_ID:
 	case PTR_TO_MEM:
-	case PTR_TO_MEM_OR_NULL:
 	case PTR_TO_FUNC:
 	case PTR_TO_MAP_KEY:
 		return true;
@@ -3405,7 +3371,7 @@ static int check_ctx_access(struct bpf_verifier_env *env, int insn_idx, int off,
 		 */
 		*reg_type = info.reg_type;
 
-		if (*reg_type == PTR_TO_BTF_ID || *reg_type == PTR_TO_BTF_ID_OR_NULL) {
+		if (base_type(*reg_type) == PTR_TO_BTF_ID) {
 			*btf = info.btf;
 			*btf_id = info.btf_id;
 		} else {
@@ -3473,7 +3439,7 @@ static int check_sock_access(struct bpf_verifier_env *env, int insn_idx,
 	}
 
 	verbose(env, "R%d invalid %s access off=%d size=%d\n",
-		regno, reg_type_str[reg->type], off, size);
+		regno, reg_type_str(env, reg->type), off, size);
 
 	return -EACCES;
 }
@@ -4200,15 +4166,30 @@ static int check_mem_access(struct bpf_verifier_env *env, int insn_idx, u32 regn
 				mark_reg_unknown(env, regs, value_regno);
 			}
 		}
-	} else if (reg->type == PTR_TO_MEM) {
+	} else if (base_type(reg->type) == PTR_TO_MEM) {
+		bool rdonly_mem = type_is_rdonly_mem(reg->type);
+
+		if (type_may_be_null(reg->type)) {
+			verbose(env, "R%d invalid mem access '%s'\n", regno,
+				reg_type_str(env, reg->type));
+			return -EACCES;
+		}
+
+		if (t == BPF_WRITE && rdonly_mem) {
+			verbose(env, "R%d cannot write into %s\n",
+				regno, reg_type_str(env, reg->type));
+			return -EACCES;
+		}
+
 		if (t == BPF_WRITE && value_regno >= 0 &&
 		    is_pointer_value(env, value_regno)) {
 			verbose(env, "R%d leaks addr into mem\n", value_regno);
 			return -EACCES;
 		}
+
 		err = check_mem_region_access(env, regno, off, size,
 					      reg->mem_size, false);
-		if (!err && t == BPF_READ && value_regno >= 0)
+		if (!err && value_regno >= 0 && (t == BPF_READ || rdonly_mem))
 			mark_reg_unknown(env, regs, value_regno);
 	} else if (reg->type == PTR_TO_CTX) {
 		enum bpf_reg_type reg_type = SCALAR_VALUE;
@@ -4238,7 +4219,7 @@ static int check_mem_access(struct bpf_verifier_env *env, int insn_idx, u32 regn
 			} else {
 				mark_reg_known_zero(env, regs,
 						    value_regno);
-				if (reg_type_may_be_null(reg_type))
+				if (type_may_be_null(reg_type))
 					regs[value_regno].id = ++env->id_gen;
 				/* A load of ctx field could have different
 				 * actual load size with the one encoded in the
@@ -4246,8 +4227,7 @@ static int check_mem_access(struct bpf_verifier_env *env, int insn_idx, u32 regn
 				 * a sub-register.
 				 */
 				regs[value_regno].subreg_def = DEF_NOT_SUBREG;
-				if (reg_type == PTR_TO_BTF_ID ||
-				    reg_type == PTR_TO_BTF_ID_OR_NULL) {
+				if (base_type(reg_type) == PTR_TO_BTF_ID) {
 					regs[value_regno].btf = btf;
 					regs[value_regno].btf_id = btf_id;
 				}
@@ -4300,7 +4280,7 @@ static int check_mem_access(struct bpf_verifier_env *env, int insn_idx, u32 regn
 	} else if (type_is_sk_pointer(reg->type)) {
 		if (t == BPF_WRITE) {
 			verbose(env, "R%d cannot write into %s\n",
-				regno, reg_type_str[reg->type]);
+				regno, reg_type_str(env, reg->type));
 			return -EACCES;
 		}
 		err = check_sock_access(env, insn_idx, regno, off, size, t);
@@ -4316,26 +4296,32 @@ static int check_mem_access(struct bpf_verifier_env *env, int insn_idx, u32 regn
 	} else if (reg->type == CONST_PTR_TO_MAP) {
 		err = check_ptr_to_map_access(env, regs, regno, off, size, t,
 					      value_regno);
-	} else if (reg->type == PTR_TO_RDONLY_BUF) {
-		if (t == BPF_WRITE) {
-			verbose(env, "R%d cannot write into %s\n",
-				regno, reg_type_str[reg->type]);
-			return -EACCES;
+	} else if (base_type(reg->type) == PTR_TO_BUF) {
+		bool rdonly_mem = type_is_rdonly_mem(reg->type);
+		const char *buf_info;
+		u32 *max_access;
+
+		if (rdonly_mem) {
+			if (t == BPF_WRITE) {
+				verbose(env, "R%d cannot write into %s\n",
+					regno, reg_type_str(env, reg->type));
+				return -EACCES;
+			}
+			buf_info = "rdonly";
+			max_access = &env->prog->aux->max_rdonly_access;
+		} else {
+			buf_info = "rdwr";
+			max_access = &env->prog->aux->max_rdwr_access;
 		}
+
 		err = check_buffer_access(env, reg, regno, off, size, false,
-					  "rdonly",
-					  &env->prog->aux->max_rdonly_access);
-		if (!err && value_regno >= 0)
-			mark_reg_unknown(env, regs, value_regno);
-	} else if (reg->type == PTR_TO_RDWR_BUF) {
-		err = check_buffer_access(env, reg, regno, off, size, false,
-					  "rdwr",
-					  &env->prog->aux->max_rdwr_access);
-		if (!err && t == BPF_READ && value_regno >= 0)
+					  buf_info, max_access);
+
+		if (!err && value_regno >= 0 && (rdonly_mem || t == BPF_READ))
 			mark_reg_unknown(env, regs, value_regno);
 	} else {
 		verbose(env, "R%d invalid mem access '%s'\n", regno,
-			reg_type_str[reg->type]);
+			reg_type_str(env, reg->type));
 		return -EACCES;
 	}
 
@@ -4409,7 +4395,7 @@ static int check_atomic(struct bpf_verifier_env *env, int insn_idx, struct bpf_i
 	    is_sk_reg(env, insn->dst_reg)) {
 		verbose(env, "BPF_ATOMIC stores into R%d %s is not allowed\n",
 			insn->dst_reg,
-			reg_type_str[reg_state(env, insn->dst_reg)->type]);
+			reg_type_str(env, reg_state(env, insn->dst_reg)->type));
 		return -EACCES;
 	}
 
@@ -4592,8 +4578,10 @@ static int check_helper_mem_access(struct bpf_verifier_env *env, int regno,
 				   struct bpf_call_arg_meta *meta)
 {
 	struct bpf_reg_state *regs = cur_regs(env), *reg = &regs[regno];
+	const char *buf_info;
+	u32 *max_access;
 
-	switch (reg->type) {
+	switch (base_type(reg->type)) {
 	case PTR_TO_PACKET:
 	case PTR_TO_PACKET_META:
 		return check_packet_access(env, regno, reg->off, access_size,
@@ -4612,18 +4600,20 @@ static int check_helper_mem_access(struct bpf_verifier_env *env, int regno,
 		return check_mem_region_access(env, regno, reg->off,
 					       access_size, reg->mem_size,
 					       zero_size_allowed);
-	case PTR_TO_RDONLY_BUF:
-		if (meta && meta->raw_mode)
-			return -EACCES;
-		return check_buffer_access(env, reg, regno, reg->off,
-					   access_size, zero_size_allowed,
-					   "rdonly",
-					   &env->prog->aux->max_rdonly_access);
-	case PTR_TO_RDWR_BUF:
+	case PTR_TO_BUF:
+		if (type_is_rdonly_mem(reg->type)) {
+			if (meta && meta->raw_mode)
+				return -EACCES;
+
+			buf_info = "rdonly";
+			max_access = &env->prog->aux->max_rdonly_access;
+		} else {
+			buf_info = "rdwr";
+			max_access = &env->prog->aux->max_rdwr_access;
+		}
 		return check_buffer_access(env, reg, regno, reg->off,
 					   access_size, zero_size_allowed,
-					   "rdwr",
-					   &env->prog->aux->max_rdwr_access);
+					   buf_info, max_access);
 	case PTR_TO_STACK:
 		return check_stack_range_initialized(
 				env,
@@ -4635,9 +4625,9 @@ static int check_helper_mem_access(struct bpf_verifier_env *env, int regno,
 		    register_is_null(reg))
 			return 0;
 
-		verbose(env, "R%d type=%s expected=%s\n", regno,
-			reg_type_str[reg->type],
-			reg_type_str[PTR_TO_STACK]);
+		verbose(env, "R%d type=%s ", regno,
+			reg_type_str(env, reg->type));
+		verbose(env, "expected=%s\n", reg_type_str(env, PTR_TO_STACK));
 		return -EACCES;
 	}
 }
@@ -4648,7 +4638,7 @@ int check_mem_reg(struct bpf_verifier_env *env, struct bpf_reg_state *reg,
 	if (register_is_null(reg))
 		return 0;
 
-	if (reg_type_may_be_null(reg->type)) {
+	if (type_may_be_null(reg->type)) {
 		/* Assuming that the register contains a value check if the memory
 		 * access is safe. Temporarily save and restore the register's state as
 		 * the conversion shouldn't be visible to a caller.
@@ -4796,9 +4786,8 @@ static int process_timer_func(struct bpf_verifier_env *env, int regno,
 
 static bool arg_type_is_mem_ptr(enum bpf_arg_type type)
 {
-	return type == ARG_PTR_TO_MEM ||
-	       type == ARG_PTR_TO_MEM_OR_NULL ||
-	       type == ARG_PTR_TO_UNINIT_MEM;
+	return base_type(type) == ARG_PTR_TO_MEM ||
+	       base_type(type) == ARG_PTR_TO_UNINIT_MEM;
 }
 
 static bool arg_type_is_mem_size(enum bpf_arg_type type)
@@ -4900,8 +4889,7 @@ static const struct bpf_reg_types mem_types = {
 		PTR_TO_MAP_KEY,
 		PTR_TO_MAP_VALUE,
 		PTR_TO_MEM,
-		PTR_TO_RDONLY_BUF,
-		PTR_TO_RDWR_BUF,
+		PTR_TO_BUF,
 	},
 };
 
@@ -4932,31 +4920,26 @@ static const struct bpf_reg_types *compatible_reg_types[__BPF_ARG_TYPE_MAX] = {
 	[ARG_PTR_TO_MAP_KEY]		= &map_key_value_types,
 	[ARG_PTR_TO_MAP_VALUE]		= &map_key_value_types,
 	[ARG_PTR_TO_UNINIT_MAP_VALUE]	= &map_key_value_types,
-	[ARG_PTR_TO_MAP_VALUE_OR_NULL]	= &map_key_value_types,
 	[ARG_CONST_SIZE]		= &scalar_types,
 	[ARG_CONST_SIZE_OR_ZERO]	= &scalar_types,
 	[ARG_CONST_ALLOC_SIZE_OR_ZERO]	= &scalar_types,
 	[ARG_CONST_MAP_PTR]		= &const_map_ptr_types,
 	[ARG_PTR_TO_CTX]		= &context_types,
-	[ARG_PTR_TO_CTX_OR_NULL]	= &context_types,
 	[ARG_PTR_TO_SOCK_COMMON]	= &sock_types,
 #ifdef CONFIG_NET
 	[ARG_PTR_TO_BTF_ID_SOCK_COMMON]	= &btf_id_sock_common_types,
 #endif
 	[ARG_PTR_TO_SOCKET]		= &fullsock_types,
-	[ARG_PTR_TO_SOCKET_OR_NULL]	= &fullsock_types,
 	[ARG_PTR_TO_BTF_ID]		= &btf_ptr_types,
 	[ARG_PTR_TO_SPIN_LOCK]		= &spin_lock_types,
 	[ARG_PTR_TO_MEM]		= &mem_types,
-	[ARG_PTR_TO_MEM_OR_NULL]	= &mem_types,
 	[ARG_PTR_TO_UNINIT_MEM]		= &mem_types,
 	[ARG_PTR_TO_ALLOC_MEM]		= &alloc_mem_types,
-	[ARG_PTR_TO_ALLOC_MEM_OR_NULL]	= &alloc_mem_types,
 	[ARG_PTR_TO_INT]		= &int_ptr_types,
 	[ARG_PTR_TO_LONG]		= &int_ptr_types,
 	[ARG_PTR_TO_PERCPU_BTF_ID]	= &percpu_btf_ptr_types,
 	[ARG_PTR_TO_FUNC]		= &func_ptr_types,
-	[ARG_PTR_TO_STACK_OR_NULL]	= &stack_ptr_types,
+	[ARG_PTR_TO_STACK]		= &stack_ptr_types,
 	[ARG_PTR_TO_CONST_STR]		= &const_str_ptr_types,
 	[ARG_PTR_TO_TIMER]		= &timer_types,
 };
@@ -4970,12 +4953,27 @@ static int check_reg_type(struct bpf_verifier_env *env, u32 regno,
 	const struct bpf_reg_types *compatible;
 	int i, j;
 
-	compatible = compatible_reg_types[arg_type];
+	compatible = compatible_reg_types[base_type(arg_type)];
 	if (!compatible) {
 		verbose(env, "verifier internal error: unsupported arg type %d\n", arg_type);
 		return -EFAULT;
 	}
 
+	/* ARG_PTR_TO_MEM + RDONLY is compatible with PTR_TO_MEM and PTR_TO_MEM + RDONLY,
+	 * but ARG_PTR_TO_MEM is compatible only with PTR_TO_MEM and NOT with PTR_TO_MEM + RDONLY
+	 *
+	 * Same for MAYBE_NULL:
+	 *
+	 * ARG_PTR_TO_MEM + MAYBE_NULL is compatible with PTR_TO_MEM and PTR_TO_MEM + MAYBE_NULL,
+	 * but ARG_PTR_TO_MEM is compatible only with PTR_TO_MEM but NOT with PTR_TO_MEM + MAYBE_NULL
+	 *
+	 * Therefore we fold these flags depending on the arg_type before comparison.
+	 */
+	if (arg_type & MEM_RDONLY)
+		type &= ~MEM_RDONLY;
+	if (arg_type & PTR_MAYBE_NULL)
+		type &= ~PTR_MAYBE_NULL;
+
 	for (i = 0; i < ARRAY_SIZE(compatible->types); i++) {
 		expected = compatible->types[i];
 		if (expected == NOT_INIT)
@@ -4985,14 +4983,14 @@ static int check_reg_type(struct bpf_verifier_env *env, u32 regno,
 			goto found;
 	}
 
-	verbose(env, "R%d type=%s expected=", regno, reg_type_str[type]);
+	verbose(env, "R%d type=%s expected=", regno, reg_type_str(env, reg->type));
 	for (j = 0; j + 1 < i; j++)
-		verbose(env, "%s, ", reg_type_str[compatible->types[j]]);
-	verbose(env, "%s\n", reg_type_str[compatible->types[j]]);
+		verbose(env, "%s, ", reg_type_str(env, compatible->types[j]));
+	verbose(env, "%s\n", reg_type_str(env, compatible->types[j]));
 	return -EACCES;
 
 found:
-	if (type == PTR_TO_BTF_ID) {
+	if (reg->type == PTR_TO_BTF_ID) {
 		if (!arg_btf_id) {
 			if (!compatible->btf_id) {
 				verbose(env, "verifier internal error: missing arg compatible BTF ID\n");
@@ -5051,15 +5049,14 @@ static int check_func_arg(struct bpf_verifier_env *env, u32 arg,
 		return -EACCES;
 	}
 
-	if (arg_type == ARG_PTR_TO_MAP_VALUE ||
-	    arg_type == ARG_PTR_TO_UNINIT_MAP_VALUE ||
-	    arg_type == ARG_PTR_TO_MAP_VALUE_OR_NULL) {
+	if (base_type(arg_type) == ARG_PTR_TO_MAP_VALUE ||
+	    base_type(arg_type) == ARG_PTR_TO_UNINIT_MAP_VALUE) {
 		err = resolve_map_arg_type(env, meta, &arg_type);
 		if (err)
 			return err;
 	}
 
-	if (register_is_null(reg) && arg_type_may_be_null(arg_type))
+	if (register_is_null(reg) && type_may_be_null(arg_type))
 		/* A NULL register has a SCALAR_VALUE type, so skip
 		 * type checking.
 		 */
@@ -5128,10 +5125,11 @@ static int check_func_arg(struct bpf_verifier_env *env, u32 arg,
 		err = check_helper_mem_access(env, regno,
 					      meta->map_ptr->key_size, false,
 					      NULL);
-	} else if (arg_type == ARG_PTR_TO_MAP_VALUE ||
-		   (arg_type == ARG_PTR_TO_MAP_VALUE_OR_NULL &&
-		    !register_is_null(reg)) ||
-		   arg_type == ARG_PTR_TO_UNINIT_MAP_VALUE) {
+	} else if (base_type(arg_type) == ARG_PTR_TO_MAP_VALUE ||
+		   base_type(arg_type) == ARG_PTR_TO_UNINIT_MAP_VALUE) {
+		if (type_may_be_null(arg_type) && register_is_null(reg))
+			return 0;
+
 		/* bpf_map_xxx(..., map_ptr, ..., value) call:
 		 * check [value, value + map->value_size) validity
 		 */
@@ -6206,6 +6204,8 @@ static int check_helper_call(struct bpf_verifier_env *env, struct bpf_insn *insn
 			     int *insn_idx_p)
 {
 	const struct bpf_func_proto *fn = NULL;
+	enum bpf_return_type ret_type;
+	enum bpf_type_flag ret_flag;
 	struct bpf_reg_state *regs;
 	struct bpf_call_arg_meta meta;
 	int insn_idx = *insn_idx_p;
@@ -6339,13 +6339,14 @@ static int check_helper_call(struct bpf_verifier_env *env, struct bpf_insn *insn
 	regs[BPF_REG_0].subreg_def = DEF_NOT_SUBREG;
 
 	/* update return register (already marked as written above) */
-	if (fn->ret_type == RET_INTEGER) {
+	ret_type = fn->ret_type;
+	ret_flag = type_flag(fn->ret_type);
+	if (ret_type == RET_INTEGER) {
 		/* sets type to SCALAR_VALUE */
 		mark_reg_unknown(env, regs, BPF_REG_0);
-	} else if (fn->ret_type == RET_VOID) {
+	} else if (ret_type == RET_VOID) {
 		regs[BPF_REG_0].type = NOT_INIT;
-	} else if (fn->ret_type == RET_PTR_TO_MAP_VALUE_OR_NULL ||
-		   fn->ret_type == RET_PTR_TO_MAP_VALUE) {
+	} else if (base_type(ret_type) == RET_PTR_TO_MAP_VALUE) {
 		/* There is no offset yet applied, variable or fixed */
 		mark_reg_known_zero(env, regs, BPF_REG_0);
 		/* remember map_ptr, so that check_map_access()
@@ -6359,28 +6360,25 @@ static int check_helper_call(struct bpf_verifier_env *env, struct bpf_insn *insn
 		}
 		regs[BPF_REG_0].map_ptr = meta.map_ptr;
 		regs[BPF_REG_0].map_uid = meta.map_uid;
-		if (fn->ret_type == RET_PTR_TO_MAP_VALUE) {
-			regs[BPF_REG_0].type = PTR_TO_MAP_VALUE;
-			if (map_value_has_spin_lock(meta.map_ptr))
-				regs[BPF_REG_0].id = ++env->id_gen;
-		} else {
-			regs[BPF_REG_0].type = PTR_TO_MAP_VALUE_OR_NULL;
+		regs[BPF_REG_0].type = PTR_TO_MAP_VALUE | ret_flag;
+		if (!type_may_be_null(ret_type) &&
+		    map_value_has_spin_lock(meta.map_ptr)) {
+			regs[BPF_REG_0].id = ++env->id_gen;
 		}
-	} else if (fn->ret_type == RET_PTR_TO_SOCKET_OR_NULL) {
+	} else if (base_type(ret_type) == RET_PTR_TO_SOCKET) {
 		mark_reg_known_zero(env, regs, BPF_REG_0);
-		regs[BPF_REG_0].type = PTR_TO_SOCKET_OR_NULL;
-	} else if (fn->ret_type == RET_PTR_TO_SOCK_COMMON_OR_NULL) {
+		regs[BPF_REG_0].type = PTR_TO_SOCKET | ret_flag;
+	} else if (base_type(ret_type) == RET_PTR_TO_SOCK_COMMON) {
 		mark_reg_known_zero(env, regs, BPF_REG_0);
-		regs[BPF_REG_0].type = PTR_TO_SOCK_COMMON_OR_NULL;
-	} else if (fn->ret_type == RET_PTR_TO_TCP_SOCK_OR_NULL) {
+		regs[BPF_REG_0].type = PTR_TO_SOCK_COMMON | ret_flag;
+	} else if (base_type(ret_type) == RET_PTR_TO_TCP_SOCK) {
 		mark_reg_known_zero(env, regs, BPF_REG_0);
-		regs[BPF_REG_0].type = PTR_TO_TCP_SOCK_OR_NULL;
-	} else if (fn->ret_type == RET_PTR_TO_ALLOC_MEM_OR_NULL) {
+		regs[BPF_REG_0].type = PTR_TO_TCP_SOCK | ret_flag;
+	} else if (base_type(ret_type) == RET_PTR_TO_ALLOC_MEM) {
 		mark_reg_known_zero(env, regs, BPF_REG_0);
-		regs[BPF_REG_0].type = PTR_TO_MEM_OR_NULL;
+		regs[BPF_REG_0].type = PTR_TO_MEM | ret_flag;
 		regs[BPF_REG_0].mem_size = meta.mem_size;
-	} else if (fn->ret_type == RET_PTR_TO_MEM_OR_BTF_ID_OR_NULL ||
-		   fn->ret_type == RET_PTR_TO_MEM_OR_BTF_ID) {
+	} else if (base_type(ret_type) == RET_PTR_TO_MEM_OR_BTF_ID) {
 		const struct btf_type *t;
 
 		mark_reg_known_zero(env, regs, BPF_REG_0);
@@ -6398,29 +6396,30 @@ static int check_helper_call(struct bpf_verifier_env *env, struct bpf_insn *insn
 					tname, PTR_ERR(ret));
 				return -EINVAL;
 			}
-			regs[BPF_REG_0].type =
-				fn->ret_type == RET_PTR_TO_MEM_OR_BTF_ID ?
-				PTR_TO_MEM : PTR_TO_MEM_OR_NULL;
+			regs[BPF_REG_0].type = PTR_TO_MEM | ret_flag;
 			regs[BPF_REG_0].mem_size = tsize;
 		} else {
-			regs[BPF_REG_0].type =
-				fn->ret_type == RET_PTR_TO_MEM_OR_BTF_ID ?
-				PTR_TO_BTF_ID : PTR_TO_BTF_ID_OR_NULL;
+			/* MEM_RDONLY may be carried from ret_flag, but it
+			 * doesn't apply on PTR_TO_BTF_ID. Fold it, otherwise
+			 * it will confuse the check of PTR_TO_BTF_ID in
+			 * check_mem_access().
+			 */
+			ret_flag &= ~MEM_RDONLY;
+
+			regs[BPF_REG_0].type = PTR_TO_BTF_ID | ret_flag;
 			regs[BPF_REG_0].btf = meta.ret_btf;
 			regs[BPF_REG_0].btf_id = meta.ret_btf_id;
 		}
-	} else if (fn->ret_type == RET_PTR_TO_BTF_ID_OR_NULL ||
-		   fn->ret_type == RET_PTR_TO_BTF_ID) {
+	} else if (base_type(ret_type) == RET_PTR_TO_BTF_ID) {
 		int ret_btf_id;
 
 		mark_reg_known_zero(env, regs, BPF_REG_0);
-		regs[BPF_REG_0].type = fn->ret_type == RET_PTR_TO_BTF_ID ?
-						     PTR_TO_BTF_ID :
-						     PTR_TO_BTF_ID_OR_NULL;
+		regs[BPF_REG_0].type = PTR_TO_BTF_ID | ret_flag;
 		ret_btf_id = *fn->ret_btf_id;
 		if (ret_btf_id == 0) {
-			verbose(env, "invalid return type %d of func %s#%d\n",
-				fn->ret_type, func_id_name(func_id), func_id);
+			verbose(env, "invalid return type %u of func %s#%d\n",
+				base_type(ret_type), func_id_name(func_id),
+				func_id);
 			return -EINVAL;
 		}
 		/* current BPF helper definitions are only coming from
@@ -6429,12 +6428,12 @@ static int check_helper_call(struct bpf_verifier_env *env, struct bpf_insn *insn
 		regs[BPF_REG_0].btf = btf_vmlinux;
 		regs[BPF_REG_0].btf_id = ret_btf_id;
 	} else {
-		verbose(env, "unknown return type %d of func %s#%d\n",
-			fn->ret_type, func_id_name(func_id), func_id);
+		verbose(env, "unknown return type %u of func %s#%d\n",
+			base_type(ret_type), func_id_name(func_id), func_id);
 		return -EINVAL;
 	}
 
-	if (reg_type_may_be_null(regs[BPF_REG_0].type))
+	if (type_may_be_null(regs[BPF_REG_0].type))
 		regs[BPF_REG_0].id = ++env->id_gen;
 
 	if (is_ptr_cast_function(func_id)) {
@@ -6633,25 +6632,25 @@ static bool check_reg_sane_offset(struct bpf_verifier_env *env,
 
 	if (known && (val >= BPF_MAX_VAR_OFF || val <= -BPF_MAX_VAR_OFF)) {
 		verbose(env, "math between %s pointer and %lld is not allowed\n",
-			reg_type_str[type], val);
+			reg_type_str(env, type), val);
 		return false;
 	}
 
 	if (reg->off >= BPF_MAX_VAR_OFF || reg->off <= -BPF_MAX_VAR_OFF) {
 		verbose(env, "%s pointer offset %d is not allowed\n",
-			reg_type_str[type], reg->off);
+			reg_type_str(env, type), reg->off);
 		return false;
 	}
 
 	if (smin == S64_MIN) {
 		verbose(env, "math between %s pointer and register with unbounded min value is not allowed\n",
-			reg_type_str[type]);
+			reg_type_str(env, type));
 		return false;
 	}
 
 	if (smin >= BPF_MAX_VAR_OFF || smin <= -BPF_MAX_VAR_OFF) {
 		verbose(env, "value %lld makes %s pointer be out of bounds\n",
-			smin, reg_type_str[type]);
+			smin, reg_type_str(env, type));
 		return false;
 	}
 
@@ -7028,11 +7027,13 @@ static int adjust_ptr_min_max_vals(struct bpf_verifier_env *env,
 		return -EACCES;
 	}
 
-	switch (ptr_reg->type) {
-	case PTR_TO_MAP_VALUE_OR_NULL:
+	if (ptr_reg->type & PTR_MAYBE_NULL) {
 		verbose(env, "R%d pointer arithmetic on %s prohibited, null-check it first\n",
-			dst, reg_type_str[ptr_reg->type]);
+			dst, reg_type_str(env, ptr_reg->type));
 		return -EACCES;
+	}
+
+	switch (base_type(ptr_reg->type)) {
 	case CONST_PTR_TO_MAP:
 		/* smin_val represents the known value */
 		if (known && smin_val == 0 && opcode == BPF_ADD)
@@ -7045,10 +7046,10 @@ static int adjust_ptr_min_max_vals(struct bpf_verifier_env *env,
 	case PTR_TO_XDP_SOCK:
 reject:
 		verbose(env, "R%d pointer arithmetic on %s prohibited\n",
-			dst, reg_type_str[ptr_reg->type]);
+			dst, reg_type_str(env, ptr_reg->type));
 		return -EACCES;
 	default:
-		if (reg_type_may_be_null(ptr_reg->type))
+		if (type_may_be_null(ptr_reg->type))
 			goto reject;
 		break;
 	}
@@ -8770,7 +8771,7 @@ static void mark_ptr_or_null_reg(struct bpf_func_state *state,
 				 struct bpf_reg_state *reg, u32 id,
 				 bool is_null)
 {
-	if (reg_type_may_be_null(reg->type) && reg->id == id &&
+	if (type_may_be_null(reg->type) && reg->id == id &&
 	    !WARN_ON_ONCE(!reg->id)) {
 		if (WARN_ON_ONCE(reg->smin_value || reg->smax_value ||
 				 !tnum_equals_const(reg->var_off, 0) ||
@@ -9148,7 +9149,7 @@ static int check_cond_jmp_op(struct bpf_verifier_env *env,
 	 */
 	if (!is_jmp32 && BPF_SRC(insn->code) == BPF_K &&
 	    insn->imm == 0 && (opcode == BPF_JEQ || opcode == BPF_JNE) &&
-	    reg_type_may_be_null(dst_reg->type)) {
+	    type_may_be_null(dst_reg->type)) {
 		/* Mark all identical registers in each branch as either
 		 * safe or unknown depending R == 0 or R != 0 conditional.
 		 */
@@ -9207,7 +9208,7 @@ static int check_ld_imm(struct bpf_verifier_env *env, struct bpf_insn *insn)
 
 	if (insn->src_reg == BPF_PSEUDO_BTF_ID) {
 		dst_reg->type = aux->btf_var.reg_type;
-		switch (dst_reg->type) {
+		switch (base_type(dst_reg->type)) {
 		case PTR_TO_MEM:
 			dst_reg->mem_size = aux->btf_var.mem_size;
 			break;
@@ -9404,7 +9405,7 @@ static int check_return_code(struct bpf_verifier_env *env)
 		/* enforce return zero from async callbacks like timer */
 		if (reg->type != SCALAR_VALUE) {
 			verbose(env, "In async callback the register R0 is not a known value (%s)\n",
-				reg_type_str[reg->type]);
+				reg_type_str(env, reg->type));
 			return -EINVAL;
 		}
 
@@ -9418,7 +9419,7 @@ static int check_return_code(struct bpf_verifier_env *env)
 	if (is_subprog) {
 		if (reg->type != SCALAR_VALUE) {
 			verbose(env, "At subprogram exit the register R0 is not a scalar value (%s)\n",
-				reg_type_str[reg->type]);
+				reg_type_str(env, reg->type));
 			return -EINVAL;
 		}
 		return 0;
@@ -9482,7 +9483,7 @@ static int check_return_code(struct bpf_verifier_env *env)
 
 	if (reg->type != SCALAR_VALUE) {
 		verbose(env, "At program exit the register R0 is not a known value (%s)\n",
-			reg_type_str[reg->type]);
+			reg_type_str(env, reg->type));
 		return -EINVAL;
 	}
 
@@ -10263,7 +10264,7 @@ static bool regsafe(struct bpf_verifier_env *env, struct bpf_reg_state *rold,
 		return true;
 	if (rcur->type == NOT_INIT)
 		return false;
-	switch (rold->type) {
+	switch (base_type(rold->type)) {
 	case SCALAR_VALUE:
 		if (env->explore_alu_limits)
 			return false;
@@ -10285,6 +10286,22 @@ static bool regsafe(struct bpf_verifier_env *env, struct bpf_reg_state *rold,
 		}
 	case PTR_TO_MAP_KEY:
 	case PTR_TO_MAP_VALUE:
+		/* a PTR_TO_MAP_VALUE could be safe to use as a
+		 * PTR_TO_MAP_VALUE_OR_NULL into the same map.
+		 * However, if the old PTR_TO_MAP_VALUE_OR_NULL then got NULL-
+		 * checked, doing so could have affected others with the same
+		 * id, and we can't check for that because we lost the id when
+		 * we converted to a PTR_TO_MAP_VALUE.
+		 */
+		if (type_may_be_null(rold->type)) {
+			if (!type_may_be_null(rcur->type))
+				return false;
+			if (memcmp(rold, rcur, offsetof(struct bpf_reg_state, id)))
+				return false;
+			/* Check our ids match any regs they're supposed to */
+			return check_ids(rold->id, rcur->id, idmap);
+		}
+
 		/* If the new min/max/var_off satisfy the old ones and
 		 * everything else matches, we are OK.
 		 * 'id' is not compared, since it's only used for maps with
@@ -10296,20 +10313,6 @@ static bool regsafe(struct bpf_verifier_env *env, struct bpf_reg_state *rold,
 		return memcmp(rold, rcur, offsetof(struct bpf_reg_state, id)) == 0 &&
 		       range_within(rold, rcur) &&
 		       tnum_in(rold->var_off, rcur->var_off);
-	case PTR_TO_MAP_VALUE_OR_NULL:
-		/* a PTR_TO_MAP_VALUE could be safe to use as a
-		 * PTR_TO_MAP_VALUE_OR_NULL into the same map.
-		 * However, if the old PTR_TO_MAP_VALUE_OR_NULL then got NULL-
-		 * checked, doing so could have affected others with the same
-		 * id, and we can't check for that because we lost the id when
-		 * we converted to a PTR_TO_MAP_VALUE.
-		 */
-		if (rcur->type != PTR_TO_MAP_VALUE_OR_NULL)
-			return false;
-		if (memcmp(rold, rcur, offsetof(struct bpf_reg_state, id)))
-			return false;
-		/* Check our ids match any regs they're supposed to */
-		return check_ids(rold->id, rcur->id, idmap);
 	case PTR_TO_PACKET_META:
 	case PTR_TO_PACKET:
 		if (rcur->type != rold->type)
@@ -10338,11 +10341,8 @@ static bool regsafe(struct bpf_verifier_env *env, struct bpf_reg_state *rold,
 	case PTR_TO_PACKET_END:
 	case PTR_TO_FLOW_KEYS:
 	case PTR_TO_SOCKET:
-	case PTR_TO_SOCKET_OR_NULL:
 	case PTR_TO_SOCK_COMMON:
-	case PTR_TO_SOCK_COMMON_OR_NULL:
 	case PTR_TO_TCP_SOCK:
-	case PTR_TO_TCP_SOCK_OR_NULL:
 	case PTR_TO_XDP_SOCK:
 		/* Only valid matches are exact, which memcmp() above
 		 * would have accepted
@@ -10868,17 +10868,13 @@ static int is_state_visited(struct bpf_verifier_env *env, int insn_idx)
 /* Return true if it's OK to have the same insn return a different type. */
 static bool reg_type_mismatch_ok(enum bpf_reg_type type)
 {
-	switch (type) {
+	switch (base_type(type)) {
 	case PTR_TO_CTX:
 	case PTR_TO_SOCKET:
-	case PTR_TO_SOCKET_OR_NULL:
 	case PTR_TO_SOCK_COMMON:
-	case PTR_TO_SOCK_COMMON_OR_NULL:
 	case PTR_TO_TCP_SOCK:
-	case PTR_TO_TCP_SOCK_OR_NULL:
 	case PTR_TO_XDP_SOCK:
 	case PTR_TO_BTF_ID:
-	case PTR_TO_BTF_ID_OR_NULL:
 		return false;
 	default:
 		return true;
@@ -11102,7 +11098,7 @@ static int do_check(struct bpf_verifier_env *env)
 			if (is_ctx_reg(env, insn->dst_reg)) {
 				verbose(env, "BPF_ST stores into R%d %s is not allowed\n",
 					insn->dst_reg,
-					reg_type_str[reg_state(env, insn->dst_reg)->type]);
+					reg_type_str(env, reg_state(env, insn->dst_reg)->type));
 				return -EACCES;
 			}
 
@@ -11353,7 +11349,7 @@ static int check_pseudo_btf_id(struct bpf_verifier_env *env,
 			err = -EINVAL;
 			goto err_put;
 		}
-		aux->btf_var.reg_type = PTR_TO_MEM;
+		aux->btf_var.reg_type = PTR_TO_MEM | MEM_RDONLY;
 		aux->btf_var.mem_size = tsize;
 	} else {
 		aux->btf_var.reg_type = PTR_TO_BTF_ID;
@@ -13175,7 +13171,7 @@ static int do_check_common(struct bpf_verifier_env *env, int subprog)
 				mark_reg_known_zero(env, regs, i);
 			else if (regs[i].type == SCALAR_VALUE)
 				mark_reg_unknown(env, regs, i);
-			else if (regs[i].type == PTR_TO_MEM_OR_NULL) {
+			else if (base_type(regs[i].type) == PTR_TO_MEM) {
 				const u32 mem_size = regs[i].mem_size;
 
 				mark_reg_known_zero(env, regs, i);
diff --git a/lib/hexdump.c b/lib/hexdump.c
index 9301578f98e8..06833d404398 100644
--- a/lib/hexdump.c
+++ b/lib/hexdump.c
@@ -22,15 +22,33 @@ EXPORT_SYMBOL(hex_asc_upper);
  *
  * hex_to_bin() converts one hex digit to its actual value or -1 in case of bad
  * input.
+ *
+ * This function is used to load cryptographic keys, so it is coded in such a
+ * way that there are no conditions or memory accesses that depend on data.
+ *
+ * Explanation of the logic:
+ * (ch - '9' - 1) is negative if ch <= '9'
+ * ('0' - 1 - ch) is negative if ch >= '0'
+ * we "and" these two values, so the result is negative if ch is in the range
+ *	'0' ... '9'
+ * we are only interested in the sign, so we do a shift ">> 8"; note that right
+ *	shift of a negative value is implementation-defined, so we cast the
+ *	value to (unsigned) before the shift --- we have 0xffffff if ch is in
+ *	the range '0' ... '9', 0 otherwise
+ * we "and" this value with (ch - '0' + 1) --- we have a value 1 ... 10 if ch is
+ *	in the range '0' ... '9', 0 otherwise
+ * we add this value to -1 --- we have a value 0 ... 9 if ch is in the range '0'
+ *	... '9', -1 otherwise
+ * the next line is similar to the previous one, but we need to decode both
+ *	uppercase and lowercase letters, so we use (ch & 0xdf), which converts
+ *	lowercase to uppercase
  */
-int hex_to_bin(char ch)
+int hex_to_bin(unsigned char ch)
 {
-	if ((ch >= '0') && (ch <= '9'))
-		return ch - '0';
-	ch = tolower(ch);
-	if ((ch >= 'a') && (ch <= 'f'))
-		return ch - 'a' + 10;
-	return -1;
+	unsigned char cu = ch & 0xdf;
+	return -1 +
+		((ch - '0' +  1) & (unsigned)((ch - '9' - 1) & ('0' - 1 - ch)) >> 8) +
+		((cu - 'A' + 11) & (unsigned)((cu - 'F' - 1) & ('A' - 1 - cu)) >> 8);
 }
 EXPORT_SYMBOL(hex_to_bin);
 
@@ -45,10 +63,13 @@ EXPORT_SYMBOL(hex_to_bin);
 int hex2bin(u8 *dst, const char *src, size_t count)
 {
 	while (count--) {
-		int hi = hex_to_bin(*src++);
-		int lo = hex_to_bin(*src++);
+		int hi, lo;
 
-		if ((hi < 0) || (lo < 0))
+		hi = hex_to_bin(*src++);
+		if (unlikely(hi < 0))
+			return -EINVAL;
+		lo = hex_to_bin(*src++);
+		if (unlikely(lo < 0))
 			return -EINVAL;
 
 		*dst++ = (hi << 4) | lo;
diff --git a/lib/iov_iter.c b/lib/iov_iter.c
index c5b2f0f4b8a8..6d146f77601d 100644
--- a/lib/iov_iter.c
+++ b/lib/iov_iter.c
@@ -191,7 +191,7 @@ static size_t copy_page_to_iter_iovec(struct page *page, size_t offset, size_t b
 	buf = iov->iov_base + skip;
 	copy = min(bytes, iov->iov_len - skip);
 
-	if (IS_ENABLED(CONFIG_HIGHMEM) && !fault_in_pages_writeable(buf, copy)) {
+	if (IS_ENABLED(CONFIG_HIGHMEM) && !fault_in_writeable(buf, copy)) {
 		kaddr = kmap_atomic(page);
 		from = kaddr + offset;
 
@@ -275,7 +275,7 @@ static size_t copy_page_from_iter_iovec(struct page *page, size_t offset, size_t
 	buf = iov->iov_base + skip;
 	copy = min(bytes, iov->iov_len - skip);
 
-	if (IS_ENABLED(CONFIG_HIGHMEM) && !fault_in_pages_readable(buf, copy)) {
+	if (IS_ENABLED(CONFIG_HIGHMEM) && !fault_in_readable(buf, copy)) {
 		kaddr = kmap_atomic(page);
 		to = kaddr + offset;
 
@@ -431,35 +431,81 @@ static size_t copy_page_to_iter_pipe(struct page *page, size_t offset, size_t by
 }
 
 /*
+ * fault_in_iov_iter_readable - fault in iov iterator for reading
+ * @i: iterator
+ * @size: maximum length
+ *
  * Fault in one or more iovecs of the given iov_iter, to a maximum length of
- * bytes.  For each iovec, fault in each page that constitutes the iovec.
+ * @size.  For each iovec, fault in each page that constitutes the iovec.
+ *
+ * Returns the number of bytes not faulted in (like copy_to_user() and
+ * copy_from_user()).
  *
- * Return 0 on success, or non-zero if the memory could not be accessed (i.e.
- * because it is an invalid address).
+ * Always returns 0 for non-userspace iterators.
  */
-int iov_iter_fault_in_readable(const struct iov_iter *i, size_t bytes)
+size_t fault_in_iov_iter_readable(const struct iov_iter *i, size_t size)
 {
 	if (iter_is_iovec(i)) {
+		size_t count = min(size, iov_iter_count(i));
 		const struct iovec *p;
 		size_t skip;
 
-		if (bytes > i->count)
-			bytes = i->count;
-		for (p = i->iov, skip = i->iov_offset; bytes; p++, skip = 0) {
-			size_t len = min(bytes, p->iov_len - skip);
-			int err;
+		size -= count;
+		for (p = i->iov, skip = i->iov_offset; count; p++, skip = 0) {
+			size_t len = min(count, p->iov_len - skip);
+			size_t ret;
 
 			if (unlikely(!len))
 				continue;
-			err = fault_in_pages_readable(p->iov_base + skip, len);
-			if (unlikely(err))
-				return err;
-			bytes -= len;
+			ret = fault_in_readable(p->iov_base + skip, len);
+			count -= len - ret;
+			if (ret)
+				break;
 		}
+		return count + size;
 	}
 	return 0;
 }
-EXPORT_SYMBOL(iov_iter_fault_in_readable);
+EXPORT_SYMBOL(fault_in_iov_iter_readable);
+
+/*
+ * fault_in_iov_iter_writeable - fault in iov iterator for writing
+ * @i: iterator
+ * @size: maximum length
+ *
+ * Faults in the iterator using get_user_pages(), i.e., without triggering
+ * hardware page faults.  This is primarily useful when we already know that
+ * some or all of the pages in @i aren't in memory.
+ *
+ * Returns the number of bytes not faulted in, like copy_to_user() and
+ * copy_from_user().
+ *
+ * Always returns 0 for non-user-space iterators.
+ */
+size_t fault_in_iov_iter_writeable(const struct iov_iter *i, size_t size)
+{
+	if (iter_is_iovec(i)) {
+		size_t count = min(size, iov_iter_count(i));
+		const struct iovec *p;
+		size_t skip;
+
+		size -= count;
+		for (p = i->iov, skip = i->iov_offset; count; p++, skip = 0) {
+			size_t len = min(count, p->iov_len - skip);
+			size_t ret;
+
+			if (unlikely(!len))
+				continue;
+			ret = fault_in_safe_writeable(p->iov_base + skip, len);
+			count -= len - ret;
+			if (ret)
+				break;
+		}
+		return count + size;
+	}
+	return 0;
+}
+EXPORT_SYMBOL(fault_in_iov_iter_writeable);
 
 void iov_iter_init(struct iov_iter *i, unsigned int direction,
 			const struct iovec *iov, unsigned long nr_segs,
@@ -468,6 +514,7 @@ void iov_iter_init(struct iov_iter *i, unsigned int direction,
 	WARN_ON(direction & ~(READ | WRITE));
 	*i = (struct iov_iter) {
 		.iter_type = ITER_IOVEC,
+		.nofault = false,
 		.data_source = direction,
 		.iov = iov,
 		.nr_segs = nr_segs,
@@ -1483,13 +1530,17 @@ ssize_t iov_iter_get_pages(struct iov_iter *i,
 		return 0;
 
 	if (likely(iter_is_iovec(i))) {
+		unsigned int gup_flags = 0;
 		unsigned long addr;
 
+		if (iov_iter_rw(i) != WRITE)
+			gup_flags |= FOLL_WRITE;
+		if (i->nofault)
+			gup_flags |= FOLL_NOFAULT;
+
 		addr = first_iovec_segment(i, &len, start, maxsize, maxpages);
 		n = DIV_ROUND_UP(len, PAGE_SIZE);
-		res = get_user_pages_fast(addr, n,
-				iov_iter_rw(i) != WRITE ?  FOLL_WRITE : 0,
-				pages);
+		res = get_user_pages_fast(addr, n, gup_flags, pages);
 		if (unlikely(res <= 0))
 			return res;
 		return (res == n ? len : res * PAGE_SIZE) - *start;
@@ -1605,15 +1656,20 @@ ssize_t iov_iter_get_pages_alloc(struct iov_iter *i,
 		return 0;
 
 	if (likely(iter_is_iovec(i))) {
+		unsigned int gup_flags = 0;
 		unsigned long addr;
 
+		if (iov_iter_rw(i) != WRITE)
+			gup_flags |= FOLL_WRITE;
+		if (i->nofault)
+			gup_flags |= FOLL_NOFAULT;
+
 		addr = first_iovec_segment(i, &len, start, maxsize, ~0U);
 		n = DIV_ROUND_UP(len, PAGE_SIZE);
 		p = get_pages_array(n);
 		if (!p)
 			return -ENOMEM;
-		res = get_user_pages_fast(addr, n,
-				iov_iter_rw(i) != WRITE ?  FOLL_WRITE : 0, p);
+		res = get_user_pages_fast(addr, n, gup_flags, p);
 		if (unlikely(res <= 0)) {
 			kvfree(p);
 			*pages = NULL;
diff --git a/mm/filemap.c b/mm/filemap.c
index 1293c3409e42..ab9de2720576 100644
--- a/mm/filemap.c
+++ b/mm/filemap.c
@@ -90,7 +90,7 @@
  *      ->lock_page		(filemap_fault, access_process_vm)
  *
  *  ->i_rwsem			(generic_perform_write)
- *    ->mmap_lock		(fault_in_pages_readable->do_page_fault)
+ *    ->mmap_lock              (fault_in_readable->do_page_fault)  
  *
  *  bdi->wb.list_lock
  *    sb_lock			(fs/fs-writeback.c)
@@ -3760,7 +3760,7 @@ ssize_t generic_perform_write(struct file *file,
 		 * same page as we're writing to, without it being marked
 		 * up-to-date.
 		 */
-		if (unlikely(iov_iter_fault_in_readable(i, bytes))) {
+		if (unlikely(fault_in_iov_iter_readable(i, bytes))) {
 			status = -EFAULT;
 			break;
 		}
diff --git a/mm/gup.c b/mm/gup.c
index 52f08e3177e9..05068d3d2557 100644
--- a/mm/gup.c
+++ b/mm/gup.c
@@ -465,7 +465,7 @@ static int follow_pfn_pte(struct vm_area_struct *vma, unsigned long address,
 		pte_t *pte, unsigned int flags)
 {
 	/* No page to get reference */
-	if (flags & FOLL_GET)
+	if (flags & (FOLL_GET | FOLL_PIN))
 		return -EFAULT;
 
 	if (flags & FOLL_TOUCH) {
@@ -943,6 +943,8 @@ static int faultin_page(struct vm_area_struct *vma,
 	/* mlock all present pages, but do not fault in new pages */
 	if ((*flags & (FOLL_POPULATE | FOLL_MLOCK)) == FOLL_MLOCK)
 		return -ENOENT;
+	if (*flags & FOLL_NOFAULT)
+		return -EFAULT;
 	if (*flags & FOLL_WRITE)
 		fault_flags |= FAULT_FLAG_WRITE;
 	if (*flags & FOLL_REMOTE)
@@ -1681,6 +1683,122 @@ static long __get_user_pages_locked(struct mm_struct *mm, unsigned long start,
 }
 #endif /* !CONFIG_MMU */
 
+/**
+ * fault_in_writeable - fault in userspace address range for writing
+ * @uaddr: start of address range
+ * @size: size of address range
+ *
+ * Returns the number of bytes not faulted in (like copy_to_user() and
+ * copy_from_user()).
+ */
+size_t fault_in_writeable(char __user *uaddr, size_t size)
+{
+	char __user *start = uaddr, *end;
+
+	if (unlikely(size == 0))
+		return 0;
+	if (!PAGE_ALIGNED(uaddr)) {
+		if (unlikely(__put_user(0, uaddr) != 0))
+			return size;
+		uaddr = (char __user *)PAGE_ALIGN((unsigned long)uaddr);
+	}
+	end = (char __user *)PAGE_ALIGN((unsigned long)start + size);
+	if (unlikely(end < start))
+		end = NULL;
+	while (uaddr != end) {
+		if (unlikely(__put_user(0, uaddr) != 0))
+			goto out;
+		uaddr += PAGE_SIZE;
+	}
+
+out:
+	if (size > uaddr - start)
+		return size - (uaddr - start);
+	return 0;
+}
+EXPORT_SYMBOL(fault_in_writeable);
+
+/*
+ * fault_in_safe_writeable - fault in an address range for writing
+ * @uaddr: start of address range
+ * @size: length of address range
+ *
+ * Faults in an address range for writing.  This is primarily useful when we
+ * already know that some or all of the pages in the address range aren't in
+ * memory.
+ *
+ * Unlike fault_in_writeable(), this function is non-destructive.
+ *
+ * Note that we don't pin or otherwise hold the pages referenced that we fault
+ * in.  There's no guarantee that they'll stay in memory for any duration of
+ * time.
+ *
+ * Returns the number of bytes not faulted in, like copy_to_user() and
+ * copy_from_user().
+ */
+size_t fault_in_safe_writeable(const char __user *uaddr, size_t size)
+{
+	unsigned long start = (unsigned long)uaddr, end;
+	struct mm_struct *mm = current->mm;
+	bool unlocked = false;
+
+	if (unlikely(size == 0))
+		return 0;
+	end = PAGE_ALIGN(start + size);
+	if (end < start)
+		end = 0;
+
+	mmap_read_lock(mm);
+	do {
+		if (fixup_user_fault(mm, start, FAULT_FLAG_WRITE, &unlocked))
+			break;
+		start = (start + PAGE_SIZE) & PAGE_MASK;
+	} while (start != end);
+	mmap_read_unlock(mm);
+
+	if (size > (unsigned long)uaddr - start)
+		return size - ((unsigned long)uaddr - start);
+	return 0;
+}
+EXPORT_SYMBOL(fault_in_safe_writeable);
+
+/**
+ * fault_in_readable - fault in userspace address range for reading
+ * @uaddr: start of user address range
+ * @size: size of user address range
+ *
+ * Returns the number of bytes not faulted in (like copy_to_user() and
+ * copy_from_user()).
+ */
+size_t fault_in_readable(const char __user *uaddr, size_t size)
+{
+	const char __user *start = uaddr, *end;
+	volatile char c;
+
+	if (unlikely(size == 0))
+		return 0;
+	if (!PAGE_ALIGNED(uaddr)) {
+		if (unlikely(__get_user(c, uaddr) != 0))
+			return size;
+		uaddr = (const char __user *)PAGE_ALIGN((unsigned long)uaddr);
+	}
+	end = (const char __user *)PAGE_ALIGN((unsigned long)start + size);
+	if (unlikely(end < start))
+		end = NULL;
+	while (uaddr != end) {
+		if (unlikely(__get_user(c, uaddr) != 0))
+			goto out;
+		uaddr += PAGE_SIZE;
+	}
+
+out:
+	(void)c;
+	if (size > uaddr - start)
+		return size - (uaddr - start);
+	return 0;
+}
+EXPORT_SYMBOL(fault_in_readable);
+
 /**
  * get_dump_page() - pin user page in memory while writing it to core dump
  * @addr: user address
@@ -2733,7 +2851,7 @@ static int internal_get_user_pages_fast(unsigned long start,
 
 	if (WARN_ON_ONCE(gup_flags & ~(FOLL_WRITE | FOLL_LONGTERM |
 				       FOLL_FORCE | FOLL_PIN | FOLL_GET |
-				       FOLL_FAST_ONLY)))
+				       FOLL_FAST_ONLY | FOLL_NOFAULT)))
 		return -EINVAL;
 
 	if (gup_flags & FOLL_PIN)
diff --git a/net/core/bpf_sk_storage.c b/net/core/bpf_sk_storage.c
index 68d2cbf8331a..ea61dfe19c86 100644
--- a/net/core/bpf_sk_storage.c
+++ b/net/core/bpf_sk_storage.c
@@ -929,7 +929,7 @@ static struct bpf_iter_reg bpf_sk_storage_map_reg_info = {
 		{ offsetof(struct bpf_iter__bpf_sk_storage_map, sk),
 		  PTR_TO_BTF_ID_OR_NULL },
 		{ offsetof(struct bpf_iter__bpf_sk_storage_map, value),
-		  PTR_TO_RDWR_BUF_OR_NULL },
+		  PTR_TO_BUF | PTR_MAYBE_NULL },
 	},
 	.seq_info		= &iter_seq_info,
 };
diff --git a/net/core/sock_map.c b/net/core/sock_map.c
index 8288b5382f08..6351b6af7aca 100644
--- a/net/core/sock_map.c
+++ b/net/core/sock_map.c
@@ -1575,7 +1575,7 @@ static struct bpf_iter_reg sock_map_iter_reg = {
 	.ctx_arg_info_size	= 2,
 	.ctx_arg_info		= {
 		{ offsetof(struct bpf_iter__sockmap, key),
-		  PTR_TO_RDONLY_BUF_OR_NULL },
+		  PTR_TO_BUF | PTR_MAYBE_NULL | MEM_RDONLY },
 		{ offsetof(struct bpf_iter__sockmap, sk),
 		  PTR_TO_BTF_ID_OR_NULL },
 	},
-- 
2.17.1

